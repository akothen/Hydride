; ModuleID = 'depthwise_conv'
source_filename = "/home/muchenx2/Hydride/frontends/halide/src/runtime/posix_allocator.cpp"
target datalayout = "e-m:o-i64:64-i128:128-n32:64-S128"
target triple = "arm64-apple-macosx"

%struct.mach_timebase_info = type { i32, i32 }
%"struct.Halide::Runtime::Internal::Synchronization::hash_table" = type { [1024 x %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"] }
%"struct.Halide::Runtime::Internal::Synchronization::hash_bucket" = type { %"class.Halide::Runtime::Internal::Synchronization::word_lock", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* }
%"class.Halide::Runtime::Internal::Synchronization::word_lock" = type { i64 }
%"struct.Halide::Runtime::Internal::Synchronization::queue_data" = type { %"struct.Halide::Runtime::Internal::Synchronization::thread_parker", i64, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, i64 }
%"struct.Halide::Runtime::Internal::Synchronization::thread_parker" = type <{ %struct.pthread_mutex_t, %struct.pthread_mutex_t, i8, [7 x i8] }>
%struct.pthread_mutex_t = type { [8 x i64] }
%"struct.Halide::Runtime::Internal::work_queue_t" = type { %struct.halide_mutex, i32, i32, %"struct.Halide::Runtime::Internal::work"*, i32, i32, i32, [4 x i8], %struct.halide_mutex, %struct.halide_mutex, %struct.halide_mutex, i32, i32, [256 x %struct.halide_thread*], i8, i8, i32 }
%"struct.Halide::Runtime::Internal::work" = type { %struct.halide_parallel_task_t, i32 (i8*, i32, i8*)*, %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"*, i32, %"struct.Halide::Runtime::Internal::work"*, i32, i8*, i32, i32, i32, i8 }
%struct.halide_parallel_task_t = type { i32 (i8*, i32, i32, i8*, i8*)*, i8*, i8*, %struct.halide_semaphore_acquire_t*, i32, i32, i32, i32, i8, [7 x i8] }
%struct.halide_semaphore_acquire_t = type { %struct.halide_semaphore_t*, i32, [4 x i8] }
%struct.halide_semaphore_t = type { [2 x i64] }
%struct.halide_mutex = type { [1 x i64] }
%struct.halide_thread = type opaque
%"class.Halide::Runtime::Internal::TraceBuffer" = type { %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock", i32, i32, [1048576 x i8] }
%"class.Halide::Runtime::Internal::SharedExclusiveSpinLock" = type { i32 }
%struct.halide_trace_event_t = type <{ i8*, i8*, i32*, i8*, %struct.halide_type_t, i32, i32, i32, i32, [4 x i8] }>
%struct.halide_type_t = type { i8, i8, i16 }
%"struct.Halide::Runtime::Internal::CacheEntry" = type { %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"*, i8*, i64, i8*, i32, i32, i32, i32, %struct.halide_dimension_t*, %struct.halide_buffer_t*, i64, i8, [7 x i8] }
%struct.halide_dimension_t = type { i32, i32, i32, i32 }
%struct.halide_buffer_t = type { i64, %struct.halide_device_interface_t*, i8*, i64, %struct.halide_type_t, i32, %struct.halide_dimension_t*, i8* }
%struct.halide_device_interface_t = type { i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, void (i8*, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i64, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, i32*, i32*)*, %struct.halide_device_interface_impl_t* }
%struct.halide_device_interface_impl_t = type { void ()*, void ()*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i64)*, i32 (i8*, %struct.halide_buffer_t*)* }
%struct.halide_device_allocation_pool = type { i32 (i8*)*, %struct.halide_device_allocation_pool* }
%struct.halide_profiler_state = type { %struct.halide_mutex, i32, i32, i32, i32, %struct.halide_profiler_pipeline_stats*, void (i32*, i32*)*, %struct.halide_thread* }
%struct.halide_profiler_pipeline_stats = type { i64, i64, i64, i64, i64, i64, i8*, %struct.halide_profiler_func_stats*, i8*, i32, i32, i32, i32, i32, [4 x i8] }
%struct.halide_profiler_func_stats = type { i64, i64, i64, i64, i64, i64, i64, i8*, i32, [4 x i8] }
%struct.halide_filter_argument_t = type { i8*, i32, i32, %struct.halide_type_t, %struct.halide_scalar_value_t*, %struct.halide_scalar_value_t*, %struct.halide_scalar_value_t*, %struct.halide_scalar_value_t*, i64** }
%struct.halide_scalar_value_t = type { %union.anon.28 }
%union.anon.28 = type { double }
%struct.halide_filter_metadata_t = type { i32, i32, %struct.halide_filter_argument_t*, i8*, i8* }
%"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control" = type { %"struct.Halide::Runtime::Internal::Synchronization::parking_control", i64* }
%"struct.Halide::Runtime::Internal::Synchronization::parking_control" = type { i32 (...)** }
%"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data" = type { %"struct.Halide::Runtime::Internal::Synchronization::thread_parker", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* }
%"struct.Halide::Runtime::Internal::Synchronization::validate_action" = type { i8, [7 x i8], i64 }
%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control" = type { %"struct.Halide::Runtime::Internal::Synchronization::parking_control", i64*, %"class.Halide::Runtime::Internal::Synchronization::word_lock"* }
%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair" = type { %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"*, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* }
%"struct.Halide::Runtime::Internal::spawned_thread" = type { void (i8*)*, i8*, i64 }
%struct.halide_mutex_array = type { %struct.halide_mutex* }
%"struct.Halide::Runtime::Internal::halide_tiff_header" = type <{ i16, i16, i32, i16, [15 x %"struct.Halide::Runtime::Internal::tiff_tag"], i32, [2 x i32], [2 x i32] }>
%"struct.Halide::Runtime::Internal::tiff_tag" = type { i16, i16, i32, %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock" }
%"struct.Halide::Runtime::Internal::CacheBlockHeader" = type { %"struct.Halide::Runtime::Internal::CacheEntry"*, i32, [4 x i8] }
%"struct.Halide::Runtime::Internal::device_copy" = type { i64, i64, i64, [16 x i64], [16 x i64], [16 x i64], i64 }
%"struct.Halide::Runtime::Internal::CpuFeatures" = type { [2 x i64], [2 x i64] }
%struct.halide_pseudostack_slot_t = type { i8*, i64, i64 }

@_ZN6Halide7Runtime8Internal13custom_mallocE = linkonce local_unnamed_addr global i8* (i8*, i64)* @halide_default_malloc, align 8
@_ZN6Halide7Runtime8Internal11custom_freeE = linkonce local_unnamed_addr global void (i8*, i8*)* @halide_default_free, align 8
@_ZN6Halide7Runtime8Internal13error_handlerE = linkonce local_unnamed_addr global void (i8*, i8*)* @halide_default_error, align 8
@.str = private unnamed_addr constant [8 x i8] c"Error: \00", align 1
@_ZN6Halide7Runtime8Internal12custom_printE = linkonce local_unnamed_addr global void (i8*, i8*)* @halide_default_print, align 8
@_ZN6Halide7Runtime8Internal29halide_reference_clock_initedE = linkonce local_unnamed_addr global i8 0, align 1
@_ZN6Halide7Runtime8Internal22halide_reference_clockE = linkonce local_unnamed_addr global i64 0, align 8
@_ZN6Halide7Runtime8Internal20halide_timebase_infoE = linkonce global %struct.mach_timebase_info zeroinitializer, align 4
@_ZN6Halide7Runtime8Internal15Synchronization5tableE = linkonce global %"struct.Halide::Runtime::Internal::Synchronization::hash_table" zeroinitializer, align 8
@_ZN6Halide7Runtime8Internal10work_queueE = linkonce global %"struct.Halide::Runtime::Internal::work_queue_t" { %struct.halide_mutex zeroinitializer, i32 0, i32 0, %"struct.Halide::Runtime::Internal::work"* null, i32 0, i32 0, i32 0, [4 x i8] undef, %struct.halide_mutex zeroinitializer, %struct.halide_mutex zeroinitializer, %struct.halide_mutex zeroinitializer, i32 0, i32 0, [256 x %struct.halide_thread*] zeroinitializer, i8 0, i8 0, i32 0 }, align 8
@_ZN6Halide7Runtime8Internal14custom_do_taskE = linkonce local_unnamed_addr global i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* @halide_default_do_task, align 8
@_ZN6Halide7Runtime8Internal19custom_do_loop_taskE = linkonce local_unnamed_addr global i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)* @halide_default_do_loop_task, align 8
@_ZN6Halide7Runtime8Internal17custom_do_par_forE = linkonce local_unnamed_addr global i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* @halide_default_do_par_for, align 8
@_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE = linkonce_odr unnamed_addr constant { [6 x i8*] } { [6 x i8*] [i8* null, i8* null, i8* bitcast (i1 (%"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)* @_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE to i8*), i8* bitcast (void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)* @_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv to i8*), i8* bitcast (i64 (%"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"*, i32, i1)* @_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib to i8*), i8* bitcast (void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)* @_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb to i8*)] }, align 8
@.str.5 = private unnamed_addr constant [130 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/synchronization_common.h:386 halide_abort_if_false() failed: next != nullptr\0A\00", align 1
@_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE = linkonce_odr unnamed_addr constant { [6 x i8*] } { [6 x i8*] [i8* null, i8* null, i8* bitcast (i1 (%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)* @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE to i8*), i8* bitcast (void (%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"*)* @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv to i8*), i8* bitcast (i64 (%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"*, i32, i1)* @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib to i8*), i8* bitcast (void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)* @_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb to i8*)] }, align 8
@.str.5.6 = private unnamed_addr constant [124 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/synchronization_common.h:994 halide_abort_if_false() failed: val & 0x1\0A\00", align 1
@_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE = linkonce_odr unnamed_addr constant { [6 x i8*] } { [6 x i8*] [i8* null, i8* null, i8* bitcast (i1 (%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)* @_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE to i8*), i8* bitcast (void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)* @_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv to i8*), i8* bitcast (i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)* @_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib to i8*), i8* bitcast (void (%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)* @_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb to i8*)] }, align 8
@.str.6 = private unnamed_addr constant [186 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/thread_pool_common.h:155 halide_abort_if_false() failed: bytes == limit && \22Logic error in thread pool work queue initialization.\\n\22\0A\00", align 1
@.str.3 = private unnamed_addr constant [263 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/thread_pool_common.h:527 halide_abort_if_false() failed: (min_threads <= ((task_parent->task.min_threads * task_parent->active_workers) - task_parent->threads_reserved)) && \22Logic error: thread over commit.\\n\22\0A\00", align 1
@.str.1 = private unnamed_addr constant [15 x i8] c"HL_NUM_THREADS\00", align 1
@.str.2 = private unnamed_addr constant [14 x i8] c"HL_NUMTHREADS\00", align 1
@_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE = linkonce local_unnamed_addr global i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)* @halide_default_do_parallel_tasks, align 8
@_ZN6Halide7Runtime8Internal21custom_semaphore_initE = linkonce local_unnamed_addr global i32 (%struct.halide_semaphore_t*, i32)* @halide_default_semaphore_init, align 8
@_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE = linkonce local_unnamed_addr global i1 (%struct.halide_semaphore_t*, i32)* @halide_default_semaphore_try_acquire, align 8
@_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE = linkonce local_unnamed_addr global i32 (%struct.halide_semaphore_t*, i32)* @halide_default_semaphore_release, align 8
@llvm.global_dtors = appending global [4 x { i32, void ()*, i8* }] [{ i32, void ()*, i8* } { i32 65535, void ()* @halide_thread_pool_cleanup, i8* null }, { i32, void ()*, i8* } { i32 65535, void ()* @halide_trace_cleanup, i8* null }, { i32, void ()*, i8* } { i32 65535, void ()* @halide_cache_cleanup, i8* null }, { i32, void ()*, i8* } { i32 65535, void ()* @halide_profiler_shutdown, i8* null }]
@_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE = linkonce_odr unnamed_addr constant { [6 x i8*] } { [6 x i8*] [i8* null, i8* null, i8* bitcast (i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)* @_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE to i8*), i8* bitcast (void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)* @_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv to i8*), i8* bitcast (i64 (%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"*, i32, i1)* @_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib to i8*), i8* bitcast (void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)* @_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb to i8*)] }, align 8
@.str.4 = private unnamed_addr constant [38 x i8] c"halide_set_num_threads: must be >= 0.\00", align 1
@_ZN6Halide7Runtime8Internal17custom_get_symbolE = linkonce local_unnamed_addr global i8* (i8*)* @halide_default_get_symbol, align 8
@_ZN6Halide7Runtime8Internal19custom_load_libraryE = linkonce local_unnamed_addr global i8* (i8*)* @halide_default_load_library, align 8
@_ZN6Halide7Runtime8Internal25custom_get_library_symbolE = linkonce local_unnamed_addr global i8* (i8*, i8*)* @halide_default_get_library_symbol, align 8
@_ZN6Halide7Runtime8Internal17halide_gpu_deviceE = linkonce local_unnamed_addr global i32 0, align 4
@_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE = linkonce global i8 0, align 1
@_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE = linkonce local_unnamed_addr global i8 0, align 1
@.str.8 = private unnamed_addr constant [14 x i8] c"HL_GPU_DEVICE\00", align 1
@_ZN6Halide7Runtime8Internal19halide_trace_bufferE = linkonce local_unnamed_addr global %"class.Halide::Runtime::Internal::TraceBuffer"* null, align 8
@_ZN6Halide7Runtime8Internal17halide_trace_fileE = linkonce local_unnamed_addr global i32 -1, align 4
@_ZN6Halide7Runtime8Internal22halide_trace_file_lockE = linkonce global i8 0, align 1
@_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE = linkonce local_unnamed_addr global i8 0, align 1
@_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE = linkonce local_unnamed_addr global i8* null, align 8
@_ZN6Halide7Runtime8Internal19halide_custom_traceE = linkonce local_unnamed_addr global i32 (i8*, %struct.halide_trace_event_t*)* @halide_default_trace, align 8
@_ZZ20halide_default_traceE3ids = internal global i32 1, align 4
@.str.32 = private unnamed_addr constant [144 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/tracing.cpp:115 halide_abort_if_false() failed: success && \22Could not write to trace file\22\0A\00", align 1
@.str.31 = private unnamed_addr constant [120 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/tracing.cpp:87 halide_abort_if_false() failed: size <= buffer_size\0A\00", align 1
@.str.1.10 = private unnamed_addr constant [1 x i8] zeroinitializer, align 1
@.str.2.11 = private unnamed_addr constant [140 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/tracing.cpp:218 halide_abort_if_false() failed: print_bits <= 64 && \22Tracing bad type\22\0A\00", align 1
@__const.halide_default_trace.event_types = private unnamed_addr constant [11 x i8*] [i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.3.12, i32 0, i32 0), i8* getelementptr inbounds ([6 x i8], [6 x i8]* @.str.4.13, i32 0, i32 0), i8* getelementptr inbounds ([18 x i8], [18 x i8]* @.str.5.14, i32 0, i32 0), i8* getelementptr inbounds ([16 x i8], [16 x i8]* @.str.6.15, i32 0, i32 0), i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str.7, i32 0, i32 0), i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str.8.16, i32 0, i32 0), i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str.9.17, i32 0, i32 0), i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str.10, i32 0, i32 0), i8* getelementptr inbounds ([15 x i8], [15 x i8]* @.str.11, i32 0, i32 0), i8* getelementptr inbounds ([13 x i8], [13 x i8]* @.str.12, i32 0, i32 0), i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.13, i32 0, i32 0)], align 8
@.str.17 = private unnamed_addr constant [2 x i8] c"<\00", align 1
@.str.20 = private unnamed_addr constant [3 x i8] c">)\00", align 1
@.str.18 = private unnamed_addr constant [5 x i8] c">, <\00", align 1
@.str.22 = private unnamed_addr constant [5 x i8] c" = <\00", align 1
@.str.23 = private unnamed_addr constant [4 x i8] c" = \00", align 1
@.str.24 = private unnamed_addr constant [142 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/tracing.cpp:287 halide_abort_if_false() failed: print_bits >= 16 && \22Tracing a bad type\22\0A\00", align 1
@.str.25 = private unnamed_addr constant [2 x i8] c">\00", align 1
@.str.26 = private unnamed_addr constant [9 x i8] c" tag = \22\00", align 1
@.str.27 = private unnamed_addr constant [2 x i8] c"\22\00", align 1
@.str.3.12 = private unnamed_addr constant [5 x i8] c"Load\00", align 1
@.str.4.13 = private unnamed_addr constant [6 x i8] c"Store\00", align 1
@.str.5.14 = private unnamed_addr constant [18 x i8] c"Begin realization\00", align 1
@.str.6.15 = private unnamed_addr constant [16 x i8] c"End realization\00", align 1
@.str.7 = private unnamed_addr constant [8 x i8] c"Produce\00", align 1
@.str.8.16 = private unnamed_addr constant [12 x i8] c"End produce\00", align 1
@.str.9.17 = private unnamed_addr constant [8 x i8] c"Consume\00", align 1
@.str.10 = private unnamed_addr constant [12 x i8] c"End consume\00", align 1
@.str.11 = private unnamed_addr constant [15 x i8] c"Begin pipeline\00", align 1
@.str.12 = private unnamed_addr constant [13 x i8] c"End pipeline\00", align 1
@.str.13 = private unnamed_addr constant [4 x i8] c"Tag\00", align 1
@.str.28 = private unnamed_addr constant [14 x i8] c"HL_TRACE_FILE\00", align 1
@.str.29 = private unnamed_addr constant [3 x i8] c"ab\00", align 1
@.str.30 = private unnamed_addr constant [139 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/tracing.cpp:351 halide_abort_if_false() failed: file && \22Failed to open trace file\\n\22\0A\00", align 1
@_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE = linkonce local_unnamed_addr global [10 x i16] [i16 3, i16 3, i16 1, i16 2, i16 1, i16 2, i16 1, i16 2, i16 1, i16 2], align 2
@_ZN6Halide7Runtime8Internal31pixel_type_to_matlab_class_codeE = linkonce local_unnamed_addr global [10 x i8] c"\07\06\09\08\0B\0A\0D\0C\0F\0E", align 1
@_ZN6Halide7Runtime8Internal30pixel_type_to_matlab_type_codeE = linkonce local_unnamed_addr global [10 x i8] c"\07\09\02\01\04\03\06\05\0D\0C", align 1
@.str.34 = private unnamed_addr constant [51 x i8] c"Bounds query buffer passed to halide_debug_to_file\00", align 1
@.str.1.35 = private unnamed_addr constant [59 x i8] c"Can't debug_to_file a Func with more than four dimensions\0A\00", align 1
@.str.2.36 = private unnamed_addr constant [3 x i8] c"wb\00", align 1
@.str.3.37 = private unnamed_addr constant [6 x i8] c".tiff\00", align 1
@.str.4.38 = private unnamed_addr constant [5 x i8] c".tif\00", align 1
@.str.5.39 = private unnamed_addr constant [5 x i8] c".mat\00", align 1
@__const.halide_debug_to_file.header = private unnamed_addr constant [129 x i8] c"MATLAB 5.0 MAT-file, produced by Halide                                                                                     \00\01IM\00", align 1
@.str.6.40 = private unnamed_addr constant [53 x i8] c"Can't debug_to_file to a .mat file greater than 4GB\0A\00", align 1
@_ZN6Halide7Runtime8Internal16memoization_lockE = linkonce global %struct.halide_mutex zeroinitializer, align 8
@_ZN6Halide7Runtime8Internal13cache_entriesE = linkonce global [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*] zeroinitializer, align 8
@_ZN6Halide7Runtime8Internal18most_recently_usedE = linkonce local_unnamed_addr global %"struct.Halide::Runtime::Internal::CacheEntry"* null, align 8
@_ZN6Halide7Runtime8Internal19least_recently_usedE = linkonce local_unnamed_addr global %"struct.Halide::Runtime::Internal::CacheEntry"* null, align 8
@_ZN6Halide7Runtime8Internal14max_cache_sizeE = linkonce local_unnamed_addr global i64 1048576, align 8
@_ZN6Halide7Runtime8Internal18current_cache_sizeE = linkonce local_unnamed_addr global i64 0, align 8
@.str.2.42 = private unnamed_addr constant [126 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/cache.cpp:284 halide_abort_if_false() failed: prev_hash_entry != nullptr\0A\00", align 1
@.str.3.43 = private unnamed_addr constant [129 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/cache.cpp:373 halide_abort_if_false() failed: entry->more_recent != nullptr\0A\00", align 1
@.str.4.44 = private unnamed_addr constant [128 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/cache.cpp:377 halide_abort_if_false() failed: least_recently_used == entry\0A\00", align 1
@.str.5.45 = private unnamed_addr constant [129 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/cache.cpp:380 halide_abort_if_false() failed: entry->more_recent != nullptr\0A\00", align 1
@.str.9.46 = private unnamed_addr constant [122 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/cache.cpp:472 halide_abort_if_false() failed: no_host_pointers_equal\0A\00", align 1
@.str.12.47 = private unnamed_addr constant [123 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/cache.cpp:550 halide_abort_if_false() failed: entry->in_use_count > 0\0A\00", align 1
@.str.50 = private unnamed_addr constant [10 x i8] c"<nullptr>\00", align 1
@.str.1.57 = private unnamed_addr constant [5 x i8] c"-nan\00", align 1
@.str.2.58 = private unnamed_addr constant [4 x i8] c"nan\00", align 1
@.str.3.59 = private unnamed_addr constant [5 x i8] c"-inf\00", align 1
@.str.4.60 = private unnamed_addr constant [4 x i8] c"inf\00", align 1
@.str.5.61 = private unnamed_addr constant [14 x i8] c"-0.000000e+00\00", align 1
@.str.6.62 = private unnamed_addr constant [13 x i8] c"0.000000e+00\00", align 1
@.str.7.63 = private unnamed_addr constant [10 x i8] c"-0.000000\00", align 1
@.str.8.64 = private unnamed_addr constant [9 x i8] c"0.000000\00", align 1
@.str.9.65 = private unnamed_addr constant [2 x i8] c"-\00", align 1
@.str.11.67 = private unnamed_addr constant [3 x i8] c"e+\00", align 1
@.str.12.68 = private unnamed_addr constant [3 x i8] c"e-\00", align 1
@.str.13.71 = private unnamed_addr constant [17 x i8] c"0123456789abcdef\00", align 1
@.str.18.72 = private unnamed_addr constant [14 x i8] c"bad_type_code\00", align 1
@.str.17.73 = private unnamed_addr constant [7 x i8] c"handle\00", align 1
@.str.16.74 = private unnamed_addr constant [6 x i8] c"float\00", align 1
@.str.15.75 = private unnamed_addr constant [5 x i8] c"uint\00", align 1
@.str.14.76 = private unnamed_addr constant [4 x i8] c"int\00", align 1
@.str.19.77 = private unnamed_addr constant [2 x i8] c"x\00", align 1
@.str.20.78 = private unnamed_addr constant [8 x i8] c"nullptr\00", align 1
@.str.21.79 = private unnamed_addr constant [8 x i8] c"buffer(\00", align 1
@.str.23.82 = private unnamed_addr constant [4 x i8] c", {\00", align 1
@.str.24.83 = private unnamed_addr constant [2 x i8] c"}\00", align 1
@_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE = linkonce local_unnamed_addr global i8 1, align 1
@_ZN6Halide7Runtime8Internal21allocation_pools_lockE = linkonce global %struct.halide_mutex zeroinitializer, align 8
@_ZN6Halide7Runtime8Internal23device_allocation_poolsE = linkonce local_unnamed_addr global %struct.halide_device_allocation_pool* null, align 8
@_ZN6Halide7Runtime8Internal17device_copy_mutexE = linkonce global %struct.halide_mutex zeroinitializer, align 8
@.str.6.88 = private unnamed_addr constant [20 x i8] c"halide_copy_to_host\00", align 1
@.str.7.89 = private unnamed_addr constant [22 x i8] c"halide_copy_to_device\00", align 1
@.str.9.90 = private unnamed_addr constant [61 x i8] c"halide_copy_to_device does not support switching interfaces\0A\00", align 1
@.str.17.91 = private unnamed_addr constant [21 x i8] c"halide_device_malloc\00", align 1
@.str.20.92 = private unnamed_addr constant [59 x i8] c"halide_device_malloc doesn't support switching interfaces\0A\00", align 1
@.str.16.93 = private unnamed_addr constant [19 x i8] c"halide_device_sync\00", align 1
@.str.21.96 = private unnamed_addr constant [19 x i8] c"halide_device_free\00", align 1
@.str.22.97 = private unnamed_addr constant [127 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/device_interface.cpp:252 halide_abort_if_false() failed: buf->device == 0\0A\00", align 1
@.str.23.98 = private unnamed_addr constant [30 x i8] c"halide_device_and_host_malloc\00", align 1
@.str.25.99 = private unnamed_addr constant [68 x i8] c"halide_device_and_host_malloc doesn't support switching interfaces\0A\00", align 1
@.str.26.100 = private unnamed_addr constant [42 x i8] c"allocating host and device memory failed\0A\00", align 1
@.str.27.101 = private unnamed_addr constant [28 x i8] c"halide_device_and_host_free\00", align 1
@.str.28.102 = private unnamed_addr constant [127 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/device_interface.cpp:317 halide_abort_if_false() failed: buf->device == 0\0A\00", align 1
@.str.29.103 = private unnamed_addr constant [38 x i8] c"halide_default_device_and_host_malloc\00", align 1
@.str.30.104 = private unnamed_addr constant [36 x i8] c"halide_default_device_and_host_free\00", align 1
@.str.31.105 = private unnamed_addr constant [26 x i8] c"halide_device_wrap_native\00", align 1
@.str.32.106 = private unnamed_addr constant [64 x i8] c"halide_device_wrap_native doesn't support switching interfaces\0A\00", align 1
@.str.33.107 = private unnamed_addr constant [28 x i8] c"halide_device_detach_native\00", align 1
@.str.34.108 = private unnamed_addr constant [127 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/device_interface.cpp:403 halide_abort_if_false() failed: buf->device == 0\0A\00", align 1
@.str.35 = private unnamed_addr constant [36 x i8] c"halide_default_device_detach_native\00", align 1
@.str.41 = private unnamed_addr constant [64 x i8] c"halide_buffer_copy does not support switching device interfaces\00", align 1
@.str.58 = private unnamed_addr constant [44 x i8] c"device_interface does not support cropping\0A\00", align 1
@.str.59 = private unnamed_addr constant [43 x i8] c"device_interface does not support slicing\0A\00", align 1
@.str.60 = private unnamed_addr constant [52 x i8] c"destination buffer already has a device allocation\0A\00", align 1
@.str.61 = private unnamed_addr constant [48 x i8] c"src and dst must have identical dimensionality\0A\00", align 1
@.str.64 = private unnamed_addr constant [52 x i8] c"dst must have exactly one fewer dimension than src\0A\00", align 1
@.str.111 = private unnamed_addr constant [41 x i8] c"Bounds inference call to external stage \00", align 1
@.str.1.112 = private unnamed_addr constant [27 x i8] c" returned non-zero value: \00", align 1
@.str.2.113 = private unnamed_addr constant [24 x i8] c"Call to external stage \00", align 1
@.str.3.114 = private unnamed_addr constant [18 x i8] c"Bounds given for \00", align 1
@.str.4.115 = private unnamed_addr constant [5 x i8] c" in \00", align 1
@.str.5.116 = private unnamed_addr constant [8 x i8] c" (from \00", align 1
@.str.6.117 = private unnamed_addr constant [5 x i8] c" to \00", align 1
@.str.7.118 = private unnamed_addr constant [38 x i8] c") do not cover required region (from \00", align 1
@.str.8.119 = private unnamed_addr constant [2 x i8] c")\00", align 1
@.str.9.120 = private unnamed_addr constant [11 x i8] c" has type \00", align 1
@.str.10.121 = private unnamed_addr constant [38 x i8] c" but type of the buffer passed in is \00", align 1
@.str.11.122 = private unnamed_addr constant [31 x i8] c" requires a buffer of exactly \00", align 1
@.str.12.123 = private unnamed_addr constant [43 x i8] c" dimensions, but the buffer passed in has \00", align 1
@.str.13.124 = private unnamed_addr constant [12 x i8] c" dimensions\00", align 1
@.str.14.125 = private unnamed_addr constant [17 x i8] c" is accessed at \00", align 1
@.str.15.126 = private unnamed_addr constant [28 x i8] c", which is before the min (\00", align 1
@.str.16.127 = private unnamed_addr constant [16 x i8] c") in dimension \00", align 1
@.str.17.128 = private unnamed_addr constant [28 x i8] c", which is beyond the max (\00", align 1
@.str.18.129 = private unnamed_addr constant [29 x i8] c"Total allocation for buffer \00", align 1
@.str.19.130 = private unnamed_addr constant [5 x i8] c" is \00", align 1
@.str.20.131 = private unnamed_addr constant [37 x i8] c", which exceeds the maximum size of \00", align 1
@.str.21.132 = private unnamed_addr constant [24 x i8] c"The extents for buffer \00", align 1
@.str.22.133 = private unnamed_addr constant [12 x i8] c" dimension \00", align 1
@.str.23.134 = private unnamed_addr constant [15 x i8] c" is negative (\00", align 1
@.str.24.135 = private unnamed_addr constant [31 x i8] c"Product of extents for buffer \00", align 1
@.str.25.136 = private unnamed_addr constant [29 x i8] c"Applying the constraints on \00", align 1
@.str.26.137 = private unnamed_addr constant [54 x i8] c" to the required region made it smaller in dimension \00", align 1
@.str.27.138 = private unnamed_addr constant [3 x i8] c". \00", align 1
@.str.28.139 = private unnamed_addr constant [16 x i8] c"Required size: \00", align 1
@.str.29.140 = private unnamed_addr constant [19 x i8] c"Constrained size: \00", align 1
@.str.30.141 = private unnamed_addr constant [2 x i8] c".\00", align 1
@.str.31.142 = private unnamed_addr constant [22 x i8] c"Constraint violated: \00", align 1
@.str.32.143 = private unnamed_addr constant [3 x i8] c" (\00", align 1
@.str.33.144 = private unnamed_addr constant [6 x i8] c") == \00", align 1
@.str.34.145 = private unnamed_addr constant [11 x i8] c"Parameter \00", align 1
@.str.35.146 = private unnamed_addr constant [23 x i8] c" but must be at least \00", align 1
@.str.36 = private unnamed_addr constant [22 x i8] c" but must be at most \00", align 1
@.str.37 = private unnamed_addr constant [47 x i8] c"Out of memory (halide_malloc returned nullptr)\00", align 1
@.str.38 = private unnamed_addr constant [17 x i8] c"Buffer argument \00", align 1
@.str.39 = private unnamed_addr constant [12 x i8] c" is nullptr\00", align 1
@.str.40 = private unnamed_addr constant [25 x i8] c"Failed to dump function \00", align 1
@.str.41.147 = private unnamed_addr constant [10 x i8] c" to file \00", align 1
@.str.42 = private unnamed_addr constant [13 x i8] c" with error \00", align 1
@.str.43 = private unnamed_addr constant [21 x i8] c"The host pointer of \00", align 1
@.str.44 = private unnamed_addr constant [22 x i8] c" is not aligned to a \00", align 1
@.str.45 = private unnamed_addr constant [17 x i8] c" bytes boundary.\00", align 1
@.str.46 = private unnamed_addr constant [12 x i8] c"The buffer \00", align 1
@.str.47 = private unnamed_addr constant [53 x i8] c" is dirty on device, but this pipeline was compiled \00", align 1
@.str.48 = private unnamed_addr constant [43 x i8] c"with no support for device to host copies.\00", align 1
@.str.49 = private unnamed_addr constant [55 x i8] c" is null, but the pipeline will access it on the host.\00", align 1
@.str.50.148 = private unnamed_addr constant [30 x i8] c"The folded storage dimension \00", align 1
@.str.51 = private unnamed_addr constant [5 x i8] c" of \00", align 1
@.str.52 = private unnamed_addr constant [36 x i8] c" was accessed out of order by loop \00", align 1
@.str.53 = private unnamed_addr constant [23 x i8] c"Cannot fold dimension \00", align 1
@.str.54 = private unnamed_addr constant [36 x i8] c" because an extern stage accesses [\00", align 1
@.str.55 = private unnamed_addr constant [3 x i8] c", \00", align 1
@.str.56 = private unnamed_addr constant [3 x i8] c"],\00", align 1
@.str.57 = private unnamed_addr constant [47 x i8] c" which is outside the range currently valid: [\00", align 1
@.str.58.149 = private unnamed_addr constant [3 x i8] c"].\00", align 1
@.str.59.150 = private unnamed_addr constant [47 x i8] c" which wraps around the boundary of the fold, \00", align 1
@.str.60.151 = private unnamed_addr constant [30 x i8] c"which occurs at multiples of \00", align 1
@.str.61.152 = private unnamed_addr constant [18 x i8] c"The fold factor (\00", align 1
@.str.62 = private unnamed_addr constant [16 x i8] c") of dimension \00", align 1
@.str.63 = private unnamed_addr constant [61 x i8] c" is too small to store the required region accessed by loop \00", align 1
@.str.64.153 = private unnamed_addr constant [3 x i8] c").\00", align 1
@.str.65 = private unnamed_addr constant [22 x i8] c"Requirement Failed: (\00", align 1
@.str.66 = private unnamed_addr constant [3 x i8] c") \00", align 1
@.str.67 = private unnamed_addr constant [59 x i8] c"A schedule specialized with specialize_fail() was chosen: \00", align 1
@.str.68 = private unnamed_addr constant [55 x i8] c"Buffer has a non-zero device but no device interface.\0A\00", align 1
@.str.69 = private unnamed_addr constant [57 x i8] c"Buffer has a non-null device_interface but device is 0.\0A\00", align 1
@.str.70 = private unnamed_addr constant [49 x i8] c"Buffer has both host and device dirty bits set.\0A\00", align 1
@.str.71 = private unnamed_addr constant [26 x i8] c"Buffer pointer passed to \00", align 1
@.str.72 = private unnamed_addr constant [11 x i8] c" is null.\0A\00", align 1
@.str.73 = private unnamed_addr constant [32 x i8] c"The explicit allocation bound (\00", align 1
@.str.74 = private unnamed_addr constant [45 x i8] c" is too small to store the required region (\00", align 1
@.str.75 = private unnamed_addr constant [77 x i8] c"Buffer could not be cropped (runtime error or unimplemented device option).\0A\00", align 1
@.str.29.163 = private unnamed_addr constant [35 x i8] c"Printer buffer allocation failed.\0A\00", align 1
@.str.7.164 = private unnamed_addr constant [2 x i8] c"\0A\00", align 1
@.str.8.165 = private unnamed_addr constant [14 x i8] c" total time: \00", align 1
@.str.9.166 = private unnamed_addr constant [4 x i8] c" ms\00", align 1
@.str.10.167 = private unnamed_addr constant [12 x i8] c"  samples: \00", align 1
@.str.11.168 = private unnamed_addr constant [9 x i8] c"  runs: \00", align 1
@.str.12.169 = private unnamed_addr constant [13 x i8] c"  time/run: \00", align 1
@.str.13.170 = private unnamed_addr constant [5 x i8] c" ms\0A\00", align 1
@.str.14.171 = private unnamed_addr constant [24 x i8] c" average threads used: \00", align 1
@.str.15.172 = private unnamed_addr constant [20 x i8] c" heap allocations: \00", align 1
@.str.16.173 = private unnamed_addr constant [20 x i8] c"  peak heap usage: \00", align 1
@.str.17.174 = private unnamed_addr constant [8 x i8] c" bytes\0A\00", align 1
@.str.18.175 = private unnamed_addr constant [3 x i8] c"  \00", align 1
@.str.19.176 = private unnamed_addr constant [3 x i8] c": \00", align 1
@.str.20.177 = private unnamed_addr constant [2 x i8] c" \00", align 1
@.str.21.178 = private unnamed_addr constant [3 x i8] c"ms\00", align 1
@.str.22.179 = private unnamed_addr constant [2 x i8] c"(\00", align 1
@.str.23.180 = private unnamed_addr constant [3 x i8] c"%)\00", align 1
@.str.24.181 = private unnamed_addr constant [10 x i8] c"threads: \00", align 1
@.str.25.182 = private unnamed_addr constant [8 x i8] c" peak: \00", align 1
@.str.26.183 = private unnamed_addr constant [7 x i8] c" num: \00", align 1
@.str.27.184 = private unnamed_addr constant [7 x i8] c" avg: \00", align 1
@.str.28.185 = private unnamed_addr constant [9 x i8] c" stack: \00", align 1
@_ZZ25halide_profiler_get_stateE1s = internal global %struct.halide_profiler_state { %struct.halide_mutex zeroinitializer, i32 1, i32 0, i32 0, i32 0, %struct.halide_profiler_pipeline_stats* null, void (i32*, i32*)* null, %struct.halide_thread* null }, align 8
@.str.186 = private unnamed_addr constant [128 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/profiler_common.cpp:246 halide_abort_if_false() failed: p_stats != nullptr\0A\00", align 1
@.str.1.187 = private unnamed_addr constant [128 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/profiler_common.cpp:273 halide_abort_if_false() failed: p_stats != nullptr\0A\00", align 1
@.str.2.188 = private unnamed_addr constant [122 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/profiler_common.cpp:274 halide_abort_if_false() failed: func_id >= 0\0A\00", align 1
@.str.3.189 = private unnamed_addr constant [138 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/profiler_common.cpp:275 halide_abort_if_false() failed: func_id < p_stats->num_funcs\0A\00", align 1
@.str.4.190 = private unnamed_addr constant [128 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/profiler_common.cpp:309 halide_abort_if_false() failed: p_stats != nullptr\0A\00", align 1
@.str.5.191 = private unnamed_addr constant [122 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/profiler_common.cpp:310 halide_abort_if_false() failed: func_id >= 0\0A\00", align 1
@.str.6.192 = private unnamed_addr constant [138 x i8] c"/home/muchenx2/Hydride/frontends/halide/src/runtime/profiler_common.cpp:311 halide_abort_if_false() failed: func_id < p_stats->num_funcs\0A\00", align 1
@_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE = linkonce local_unnamed_addr global i32 (i32, i64*)* @halide_default_can_use_target_features, align 8
@_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE = linkonce global %struct.halide_mutex zeroinitializer, align 8
@_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE = linkonce local_unnamed_addr global i8 0, align 1
@_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE = linkonce global [4 x i64] zeroinitializer, align 8
@.str.197 = private unnamed_addr constant [81 x i8] c"Internal error: wrong structure size passed to halide_can_use_target_features()\0A\00", align 1
@0 = private constant i64 0
@1 = private constant [8 x i64*] [i64* @0, i64* null, i64* null, i64* null, i64* null, i64* null, i64* null, i64* null]
@str = private constant [6 x i8] c"input\00", align 32
@str.200 = private constant [11 x i8] c"input_zero\00", align 32
@2 = private constant i64 0
@3 = private constant i64 0
@4 = private constant i64 0
@5 = private constant [6 x i64*] [i64* @2, i64* null, i64* @3, i64* null, i64* @4, i64* null]
@str.201 = private constant [7 x i8] c"filter\00", align 32
@str.202 = private constant [12 x i8] c"filter_zero\00", align 32
@6 = private constant i64 0
@7 = private constant [2 x i64*] [i64* @6, i64* null]
@str.203 = private constant [5 x i8] c"bias\00", align 32
@str.204 = private constant [17 x i8] c"depth_multiplier\00", align 32
@str.205 = private constant [9 x i8] c"stride_x\00", align 32
@str.206 = private constant [9 x i8] c"stride_y\00", align 32
@str.207 = private constant [11 x i8] c"dilation_x\00", align 32
@str.208 = private constant [11 x i8] c"dilation_y\00", align 32
@str.209 = private constant [18 x i8] c"output_multiplier\00", align 32
@str.210 = private constant [13 x i8] c"output_shift\00", align 32
@str.211 = private constant [12 x i8] c"output_zero\00", align 32
@str.212 = private constant [11 x i8] c"output_min\00", align 32
@str.213 = private constant [11 x i8] c"output_max\00", align 32
@8 = private constant i64 0
@9 = private constant [8 x i64*] [i64* @8, i64* null, i64* null, i64* null, i64* null, i64* null, i64* null, i64* null]
@str.214 = private constant [7 x i8] c"output\00", align 32
@10 = private constant [16 x %struct.halide_filter_argument_t] [%struct.halide_filter_argument_t { i8* getelementptr inbounds ([6 x i8], [6 x i8]* @str, i32 0, i32 0), i32 1, i32 4, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** getelementptr inbounds ([8 x i64*], [8 x i64*]* @1, i32 0, i32 0) }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([11 x i8], [11 x i8]* @str.200, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.201, i32 0, i32 0), i32 1, i32 3, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** getelementptr inbounds ([6 x i64*], [6 x i64*]* @5, i32 0, i32 0) }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([12 x i8], [12 x i8]* @str.202, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([5 x i8], [5 x i8]* @str.203, i32 0, i32 0), i32 1, i32 1, %struct.halide_type_t { i8 0, i8 32, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** getelementptr inbounds ([2 x i64*], [2 x i64*]* @7, i32 0, i32 0) }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([17 x i8], [17 x i8]* @str.204, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 0, i8 32, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([9 x i8], [9 x i8]* @str.205, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 0, i8 32, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([9 x i8], [9 x i8]* @str.206, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 0, i8 32, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([11 x i8], [11 x i8]* @str.207, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 0, i8 32, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([11 x i8], [11 x i8]* @str.208, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 0, i8 32, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([18 x i8], [18 x i8]* @str.209, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 0, i8 32, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([13 x i8], [13 x i8]* @str.210, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 1, i8 32, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([12 x i8], [12 x i8]* @str.211, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([11 x i8], [11 x i8]* @str.212, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([11 x i8], [11 x i8]* @str.213, i32 0, i32 0), i32 0, i32 0, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** null }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.214, i32 0, i32 0), i32 2, i32 4, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** getelementptr inbounds ([8 x i64*], [8 x i64*]* @9, i32 0, i32 0) }]
@str.215 = private constant [38 x i8] c"arm-64-osx-no_asserts-no_bounds_query\00", align 32
@str.216 = private constant [15 x i8] c"depthwise_conv\00", align 32
@depthwise_conv_metadata_storage = private constant %struct.halide_filter_metadata_t { i32 1, i32 16, %struct.halide_filter_argument_t* getelementptr inbounds ([16 x %struct.halide_filter_argument_t], [16 x %struct.halide_filter_argument_t]* @10, i32 0, i32 0), i8* getelementptr inbounds ([38 x i8], [38 x i8]* @str.215, i32 0, i32 0), i8* getelementptr inbounds ([15 x i8], [15 x i8]* @str.216, i32 0, i32 0) }
@switch.table.halide_type_to_string = private unnamed_addr constant [4 x i8*] [i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.14.76, i64 0, i64 0), i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.15.75, i64 0, i64 0), i8* getelementptr inbounds ([6 x i8], [6 x i8]* @.str.16.74, i64 0, i64 0), i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.17.73, i64 0, i64 0)], align 8

; Function Attrs: nounwind mustprogress
define weak i8* @halide_default_malloc(i8* %user_context, i64 %x) #0 {
entry:
  %add = add i64 %x, 32
  %call1 = tail call i8* @malloc(i64 %add) #15
  %cmp = icmp eq i8* %call1, null
  br i1 %cmp, label %cleanup, label %if.end

if.end:                                           ; preds = %entry
  %0 = ptrtoint i8* %call1 to i64
  %sub = add i64 %0, 39
  %and = and i64 %sub, -32
  %1 = inttoptr i64 %and to i8*
  %2 = inttoptr i64 %and to i8**
  %arrayidx = getelementptr inbounds i8*, i8** %2, i64 -1
  store i8* %call1, i8** %arrayidx, align 8, !tbaa !14
  br label %cleanup

cleanup:                                          ; preds = %if.end, %entry
  %retval.0 = phi i8* [ %1, %if.end ], [ null, %entry ]
  ret i8* %retval.0
}

declare i8* @malloc(i64) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak void @halide_default_free(i8* %user_context, i8* %ptr) #0 {
entry:
  %arrayidx = getelementptr inbounds i8, i8* %ptr, i64 -8
  %0 = bitcast i8* %arrayidx to i8**
  %1 = load i8*, i8** %0, align 8, !tbaa !14
  tail call void @free(i8* %1) #15
  ret void
}

declare void @free(i8*) local_unnamed_addr #1

; Function Attrs: nounwind willreturn mustprogress
define weak i8* (i8*, i64)* @halide_set_custom_malloc(i8* (i8*, i64)* %user_malloc) local_unnamed_addr #2 {
entry:
  %0 = load i8* (i8*, i64)*, i8* (i8*, i64)** @_ZN6Halide7Runtime8Internal13custom_mallocE, align 8, !tbaa !14
  store i8* (i8*, i64)* %user_malloc, i8* (i8*, i64)** @_ZN6Halide7Runtime8Internal13custom_mallocE, align 8, !tbaa !14
  ret i8* (i8*, i64)* %0
}

; Function Attrs: nounwind willreturn mustprogress
define weak void (i8*, i8*)* @halide_set_custom_free(void (i8*, i8*)* %user_free) local_unnamed_addr #2 {
entry:
  %0 = load void (i8*, i8*)*, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal11custom_freeE, align 8, !tbaa !14
  store void (i8*, i8*)* %user_free, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal11custom_freeE, align 8, !tbaa !14
  ret void (i8*, i8*)* %0
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_malloc(i8* %user_context, i64 %x) local_unnamed_addr #0 {
entry:
  %0 = load i8* (i8*, i64)*, i8* (i8*, i64)** @_ZN6Halide7Runtime8Internal13custom_mallocE, align 8, !tbaa !14
  %call = tail call i8* %0(i8* %user_context, i64 %x) #15
  ret i8* %call
}

; Function Attrs: nounwind mustprogress
define weak void @halide_free(i8* %user_context, i8* %ptr) local_unnamed_addr #0 {
entry:
  %0 = load void (i8*, i8*)*, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal11custom_freeE, align 8, !tbaa !14
  tail call void %0(i8* %user_context, i8* %ptr) #15
  ret void
}

; Function Attrs: nounwind mustprogress
define weak void @halide_default_error(i8* %user_context, i8* %msg) #0 {
entry:
  %buf = alloca [4096 x i8], align 1
  %0 = getelementptr inbounds [4096 x i8], [4096 x i8]* %buf, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 4096, i8* nonnull %0) #11
  %add.ptr = getelementptr inbounds [4096 x i8], [4096 x i8]* %buf, i64 0, i64 4094
  %call = call i8* @halide_string_to_string(i8* nonnull %0, i8* nonnull %add.ptr, i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str, i64 0, i64 0)) #15
  %add.ptr2 = getelementptr inbounds i8, i8* %call, i64 4094
  %call3 = call i8* @halide_string_to_string(i8* %call, i8* nonnull %add.ptr2, i8* %msg) #15
  %arrayidx = getelementptr inbounds i8, i8* %call3, i64 -1
  %1 = load i8, i8* %arrayidx, align 1, !tbaa !18
  %cmp.not = icmp eq i8 %1, 10
  br i1 %cmp.not, label %if.end, label %if.then

if.then:                                          ; preds = %entry
  store i8 10, i8* %call3, align 1, !tbaa !18
  %arrayidx5 = getelementptr inbounds i8, i8* %call3, i64 1
  store i8 0, i8* %arrayidx5, align 1, !tbaa !18
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  %dst.0 = phi i8* [ %arrayidx5, %if.then ], [ %call3, %entry ]
  %sub.ptr.lhs.cast = ptrtoint i8* %dst.0 to i64
  %sub.ptr.rhs.cast = ptrtoint [4096 x i8]* %buf to i64
  %sub.ptr.sub = sub i64 1, %sub.ptr.rhs.cast
  %add = add i64 %sub.ptr.sub, %sub.ptr.lhs.cast
  %call9 = call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %0, i64 %add) #15
  call void @halide_print(i8* %user_context, i8* nonnull %0) #15
  call void @abort() #15
  call void @llvm.lifetime.end.p0i8(i64 4096, i8* nonnull %0) #11
  ret void
}

; Function Attrs: argmemonly nofree nosync nounwind willreturn
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #3

declare void @abort() local_unnamed_addr #1

; Function Attrs: argmemonly nofree nosync nounwind willreturn
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #3

; Function Attrs: nounwind mustprogress
define weak void @halide_error(i8* %user_context, i8* %msg) local_unnamed_addr #0 {
entry:
  %0 = load void (i8*, i8*)*, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal13error_handlerE, align 8, !tbaa !14
  tail call void %0(i8* %user_context, i8* %msg) #15
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak void (i8*, i8*)* @halide_set_error_handler(void (i8*, i8*)* %handler) local_unnamed_addr #2 {
entry:
  %0 = load void (i8*, i8*)*, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal13error_handlerE, align 8, !tbaa !14
  store void (i8*, i8*)* %handler, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal13error_handlerE, align 8, !tbaa !14
  ret void (i8*, i8*)* %0
}

; Function Attrs: nounwind mustprogress
define weak void @halide_print(i8* %user_context, i8* %msg) local_unnamed_addr #0 {
entry:
  %0 = load void (i8*, i8*)*, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal12custom_printE, align 8, !tbaa !14
  tail call void %0(i8* %user_context, i8* %msg) #15
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak void (i8*, i8*)* @halide_set_custom_print(void (i8*, i8*)* %print) local_unnamed_addr #2 {
entry:
  %0 = load void (i8*, i8*)*, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal12custom_printE, align 8, !tbaa !14
  store void (i8*, i8*)* %print, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal12custom_printE, align 8, !tbaa !14
  ret void (i8*, i8*)* %0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_start_clock(i8* %user_context) local_unnamed_addr #0 {
entry:
  %0 = load i8, i8* @_ZN6Halide7Runtime8Internal29halide_reference_clock_initedE, align 1, !tbaa !19, !range !21
  %tobool.not = icmp eq i8 %0, 0
  br i1 %tobool.not, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  %call = tail call i32 @mach_timebase_info(%struct.mach_timebase_info* nonnull @_ZN6Halide7Runtime8Internal20halide_timebase_infoE) #15
  %call1 = tail call i64 @mach_absolute_time() #15
  store i64 %call1, i64* @_ZN6Halide7Runtime8Internal22halide_reference_clockE, align 8, !tbaa !22
  store i8 1, i8* @_ZN6Halide7Runtime8Internal29halide_reference_clock_initedE, align 1, !tbaa !19
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  ret i32 0
}

declare i32 @mach_timebase_info(%struct.mach_timebase_info*) local_unnamed_addr #1

declare i64 @mach_absolute_time() local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak i64 @halide_current_time_ns(i8* %user_context) local_unnamed_addr #0 {
entry:
  %call = tail call i64 @mach_absolute_time() #15
  %0 = load i64, i64* @_ZN6Halide7Runtime8Internal22halide_reference_clockE, align 8, !tbaa !22
  %sub = sub i64 %call, %0
  %1 = load i32, i32* getelementptr inbounds (%struct.mach_timebase_info, %struct.mach_timebase_info* @_ZN6Halide7Runtime8Internal20halide_timebase_infoE, i64 0, i32 0), align 4, !tbaa !24
  %conv = zext i32 %1 to i64
  %mul = mul i64 %sub, %conv
  %2 = load i32, i32* getelementptr inbounds (%struct.mach_timebase_info, %struct.mach_timebase_info* @_ZN6Halide7Runtime8Internal20halide_timebase_infoE, i64 0, i32 1), align 4, !tbaa !27
  %conv1 = zext i32 %2 to i64
  %div = udiv i64 %mul, %conv1
  ret i64 %div
}

; Function Attrs: nounwind mustprogress
define weak void @halide_sleep_ms(i8* %user_context, i32 %ms) local_unnamed_addr #0 {
entry:
  %mul = mul nsw i32 %ms, 1000
  %call = tail call i32 @usleep(i32 %mul) #15
  ret void
}

declare i32 @usleep(i32) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak void @halide_default_print(i8* %user_context, i8* %str) #0 {
entry:
  %call = tail call i64 @strlen(i8* %str) #15
  %call1 = tail call i64 @write(i32 1, i8* %str, i64 %call) #15
  ret void
}

declare i64 @strlen(i8*) local_unnamed_addr #1

declare i64 @write(i32, i8*, i64) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak i32 @halide_host_cpu_count() local_unnamed_addr #0 {
entry:
  %call = tail call i64 @sysconf(i32 58) #15
  %conv = trunc i64 %call to i32
  ret i32 %conv
}

declare i64 @sysconf(i32) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak void @halide_thread_yield() local_unnamed_addr #0 {
entry:
  %call = tail call i32 @swtch_pri(i32 0) #15
  ret void
}

declare i32 @swtch_pri(i32) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_do_task(i8* %user_context, i32 (i8*, i32, i8*)* %f, i32 %idx, i8* %closure) #0 {
entry:
  %call = tail call i32 %f(i8* %user_context, i32 %idx, i8* %closure) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_do_loop_task(i8* %user_context, i32 (i8*, i32, i32, i8*, i8*)* %f, i32 %min, i32 %extent, i8* %closure, i8* %task_parent) #0 {
entry:
  %call = tail call i32 %f(i8* %user_context, i32 %min, i32 %extent, i8* %closure, i8* %task_parent) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_do_par_for(i8* %user_context, i32 (i8*, i32, i8*)* %f, i32 %min, i32 %size, i8* %closure) #0 {
entry:
  %job = alloca %"struct.Halide::Runtime::Internal::work", align 8
  %cmp = icmp slt i32 %size, 1
  br i1 %cmp, label %return, label %if.end

if.end:                                           ; preds = %entry
  %0 = bitcast %"struct.Halide::Runtime::Internal::work"* %job to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %0) #11
  %fn = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 0
  store i32 (i8*, i32, i32, i8*, i8*)* null, i32 (i8*, i32, i32, i8*, i8*)** %fn, align 8, !tbaa !28
  %min2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 5
  store i32 %min, i32* %min2, align 4, !tbaa !31
  %extent = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 6
  store i32 %size, i32* %extent, align 8, !tbaa !32
  %serial = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 8
  store i8 0, i8* %serial, align 8, !tbaa !33
  %semaphores = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 3
  store %struct.halide_semaphore_acquire_t* null, %struct.halide_semaphore_acquire_t** %semaphores, align 8, !tbaa !34
  %num_semaphores = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 4
  store i32 0, i32* %num_semaphores, align 8, !tbaa !35
  %closure8 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 1
  store i8* %closure, i8** %closure8, align 8, !tbaa !36
  %min_threads = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 7
  store i32 0, i32* %min_threads, align 4, !tbaa !37
  %name = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 0, i32 2
  store i8* null, i8** %name, align 8, !tbaa !38
  %task_fn = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 1
  store i32 (i8*, i32, i8*)* %f, i32 (i8*, i32, i8*)** %task_fn, align 8, !tbaa !39
  %user_context11 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 7
  store i8* %user_context, i8** %user_context11, align 8, !tbaa !40
  %exit_status = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 9
  %active_workers = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 8
  %1 = bitcast i32* %active_workers to <2 x i32>*
  store <2 x i32> zeroinitializer, <2 x i32>* %1, align 8, !tbaa !41
  %next_semaphore = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 10
  store i32 0, i32* %next_semaphore, align 8, !tbaa !42
  %owner_is_sleeping = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 11
  store i8 0, i8* %owner_is_sleeping, align 4, !tbaa !43
  %siblings = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 3
  store %"struct.Halide::Runtime::Internal::work"* %job, %"struct.Halide::Runtime::Internal::work"** %siblings, align 8, !tbaa !44
  %sibling_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 4
  store i32 0, i32* %sibling_count, align 8, !tbaa !45
  %parent_job = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job, i64 0, i32 5
  store %"struct.Halide::Runtime::Internal::work"* null, %"struct.Halide::Runtime::Internal::work"** %parent_job, align 8, !tbaa !46
  call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  call void @_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_(i32 1, %"struct.Halide::Runtime::Internal::work"* nonnull %job, %"struct.Halide::Runtime::Internal::work"* null) #16
  call void @_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE(%"struct.Halide::Runtime::Internal::work"* nonnull %job) #16
  call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %2 = load i32, i32* %exit_status, align 4, !tbaa !47
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %0) #11
  br label %return

return:                                           ; preds = %if.end, %entry
  %retval.0 = phi i32 [ %2, %if.end ], [ 0, %entry ]
  ret i32 %retval.0
}

; Function Attrs: nounwind
define weak void @halide_mutex_lock(%struct.halide_mutex* %mutex) local_unnamed_addr #4 {
entry:
  %control.i.i = alloca %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", align 8
  %state.i = getelementptr %struct.halide_mutex, %struct.halide_mutex* %mutex, i64 0, i32 0, i64 0
  %0 = cmpxchg weak i64* %state.i, i64 0, i64 1 acquire monotonic
  %1 = extractvalue { i64, i1 } %0, 1
  br i1 %1, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex4lockEv.exit, label %if.then.i

if.then.i:                                        ; preds = %entry
  %2 = load atomic i64, i64* %state.i monotonic, align 8
  %3 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i to i8*
  %4 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 0, i32 0
  %lock_state2.i.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 1
  %5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 0
  %6 = ptrtoint %struct.halide_mutex* %mutex to i64
  br label %while.cond.outer.i.i

while.cond.outer.i.i:                             ; preds = %while.cond.outer.i.i.backedge, %if.then.i
  %expected.0.ph.i.i = phi i64 [ %2, %if.then.i ], [ %expected.0.ph.i.i.be, %while.cond.outer.i.i.backedge ]
  %spinner.sroa.0.0.ph.i.i = phi i32 [ 40, %if.then.i ], [ %spinner.sroa.0.0.ph.i.i.be, %while.cond.outer.i.i.backedge ]
  %and55.i.i = and i64 %expected.0.ph.i.i, 1
  %tobool.not56.i.i = icmp eq i64 %and55.i.i, 0
  br i1 %tobool.not56.i.i, label %if.then.i.i, label %if.end4.i.i

if.then.i.i:                                      ; preds = %while.cond.outer.i.i, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i
  %expected.057.i.i = phi i64 [ %9, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i ], [ %expected.0.ph.i.i, %while.cond.outer.i.i ]
  %or.i.i = or i64 %expected.057.i.i, 1
  %7 = cmpxchg weak i64* %state.i, i64 %expected.057.i.i, i64 %or.i.i acquire monotonic
  %8 = extractvalue { i64, i1 } %7, 1
  br i1 %8, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex4lockEv.exit, label %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i

_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i: ; preds = %if.then.i.i
  %9 = extractvalue { i64, i1 } %7, 0
  %and.i.i = and i64 %9, 1
  %tobool.not.i.i = icmp eq i64 %and.i.i, 0
  br i1 %tobool.not.i.i, label %if.then.i.i, label %if.end4.i.i.loopexit

if.end4.i.i.loopexit:                             ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i
  %10 = extractvalue { i64, i1 } %7, 0
  br label %if.end4.i.i

if.end4.i.i:                                      ; preds = %if.end4.i.i.loopexit, %while.cond.outer.i.i
  %expected.0.lcssa.i.i = phi i64 [ %expected.0.ph.i.i, %while.cond.outer.i.i ], [ %10, %if.end4.i.i.loopexit ]
  %cmp.i.i.i = icmp sgt i32 %spinner.sroa.0.0.ph.i.i, 0
  br i1 %cmp.i.i.i, label %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i, label %if.end8.i.i

_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i: ; preds = %if.end4.i.i
  %cmp4.i.not.i.i = icmp eq i32 %spinner.sroa.0.0.ph.i.i, 1
  br i1 %cmp4.i.not.i.i, label %if.end8.i.i, label %if.then6.i.i

if.then6.i.i:                                     ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i
  %dec.i.i.i = add nsw i32 %spinner.sroa.0.0.ph.i.i, -1
  call void @halide_thread_yield() #15
  %11 = load atomic i64, i64* %state.i monotonic, align 8
  br label %while.cond.outer.i.i.backedge

if.end8.i.i:                                      ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i, %if.end4.i.i
  %spinner.sroa.0.152.i.i = phi i32 [ 0, %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i ], [ %spinner.sroa.0.0.ph.i.i, %if.end4.i.i ]
  %and9.i.i = and i64 %expected.0.lcssa.i.i, 2
  %cmp.i.i = icmp eq i64 %and9.i.i, 0
  br i1 %cmp.i.i, label %if.then10.i.i, label %if.end19.i.i

if.then10.i.i:                                    ; preds = %if.end8.i.i
  %or12.i.i = or i64 %expected.0.lcssa.i.i, 2
  %12 = cmpxchg weak i64* %state.i, i64 %expected.0.lcssa.i.i, i64 %or12.i.i monotonic monotonic
  %13 = extractvalue { i64, i1 } %12, 1
  br i1 %13, label %if.end19.i.i, label %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i

_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i: ; preds = %if.then10.i.i
  %14 = extractvalue { i64, i1 } %12, 0
  br label %while.cond.outer.i.i.backedge

if.end19.i.i:                                     ; preds = %if.then10.i.i, %if.end8.i.i
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %3) #11
  store i32 (...)** bitcast (i8** getelementptr inbounds ({ [6 x i8*] }, { [6 x i8*] }* @_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE, i64 0, inrange i32 0, i64 2) to i32 (...)**), i32 (...)*** %4, align 8, !tbaa !48
  store i64* %state.i, i64** %lock_state2.i.i.i, align 8, !tbaa !50
  %call21.i.i = call i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %5, i64 %6) #15
  %cmp22.i.i = icmp eq i64 %call21.i.i, %6
  br i1 %cmp22.i.i, label %cleanup30.critedge.i.i, label %if.end24.i.i

if.end24.i.i:                                     ; preds = %if.end19.i.i
  %15 = load atomic i64, i64* %state.i monotonic, align 8
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %3) #11
  br label %while.cond.outer.i.i.backedge

while.cond.outer.i.i.backedge:                    ; preds = %if.end24.i.i, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i, %if.then6.i.i
  %expected.0.ph.i.i.be = phi i64 [ %14, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i ], [ %15, %if.end24.i.i ], [ %11, %if.then6.i.i ]
  %spinner.sroa.0.0.ph.i.i.be = phi i32 [ %spinner.sroa.0.152.i.i, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i ], [ 40, %if.end24.i.i ], [ %dec.i.i.i, %if.then6.i.i ]
  br label %while.cond.outer.i.i

cleanup30.critedge.i.i:                           ; preds = %if.end19.i.i
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %3) #11
  br label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex4lockEv.exit

_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex4lockEv.exit: ; preds = %if.then.i.i, %cleanup30.critedge.i.i, %entry
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_(i32 %num_jobs, %"struct.Halide::Runtime::Internal::work"* %jobs, %"struct.Halide::Runtime::Internal::work"* %task_parent) local_unnamed_addr #0 {
entry:
  %0 = load i8, i8* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 15), align 1, !tbaa !52, !range !21
  %tobool.not = icmp eq i8 %0, 0
  br i1 %tobool.not, label %land.rhs.i, label %if.end4

land.rhs.i:                                       ; preds = %entry, %while.body.i
  %bytes.011.i = phi i8* [ %incdec.ptr.i, %while.body.i ], [ bitcast (i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 2) to i8*), %entry ]
  %1 = load i8, i8* %bytes.011.i, align 1, !tbaa !18
  %cmp2.i = icmp eq i8 %1, 0
  br i1 %cmp2.i, label %while.body.i, label %do.body.i

while.body.i:                                     ; preds = %land.rhs.i
  %incdec.ptr.i = getelementptr inbounds i8, i8* %bytes.011.i, i64 1
  %exitcond.not.i = icmp eq i8* %incdec.ptr.i, bitcast (%"struct.Halide::Runtime::Internal::work_queue_t"* select (i1 icmp ugt (i8* bitcast (%"struct.Halide::Runtime::Internal::work_queue_t"* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 1) to i8*), i8* getelementptr (i8, i8* bitcast (i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 2) to i8*), i64 1)), %"struct.Halide::Runtime::Internal::work_queue_t"* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 1), %"struct.Halide::Runtime::Internal::work_queue_t"* bitcast (i8* getelementptr (i8, i8* bitcast (i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 2) to i8*), i64 1) to %"struct.Halide::Runtime::Internal::work_queue_t"*)) to i8*)
  br i1 %exitcond.not.i, label %do.body.i, label %land.rhs.i, !llvm.loop !56

do.body.i:                                        ; preds = %while.body.i, %land.rhs.i
  %bytes.0.lcssa.i = phi i8* [ %bytes.011.i, %land.rhs.i ], [ bitcast (%"struct.Halide::Runtime::Internal::work_queue_t"* select (i1 icmp ugt (i8* bitcast (%"struct.Halide::Runtime::Internal::work_queue_t"* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 1) to i8*), i8* getelementptr (i8, i8* bitcast (i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 2) to i8*), i64 1)), %"struct.Halide::Runtime::Internal::work_queue_t"* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 1), %"struct.Halide::Runtime::Internal::work_queue_t"* bitcast (i8* getelementptr (i8, i8* bitcast (i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 2) to i8*), i64 1) to %"struct.Halide::Runtime::Internal::work_queue_t"*)) to i8*), %while.body.i ]
  %cmp3.i = icmp eq i8* %bytes.0.lcssa.i, bitcast (%"struct.Halide::Runtime::Internal::work_queue_t"* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 1) to i8*)
  br i1 %cmp3.i, label %_ZNK6Halide7Runtime8Internal12work_queue_t13assert_zeroedEv.exit, label %if.then.i

if.then.i:                                        ; preds = %do.body.i
  tail call void @halide_print(i8* null, i8* getelementptr inbounds ([186 x i8], [186 x i8]* @.str.6, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %_ZNK6Halide7Runtime8Internal12work_queue_t13assert_zeroedEv.exit

_ZNK6Halide7Runtime8Internal12work_queue_t13assert_zeroedEv.exit: ; preds = %if.then.i, %do.body.i
  %2 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 1), align 8, !tbaa !58
  %tobool1.not = icmp eq i32 %2, 0
  br i1 %tobool1.not, label %if.then2, label %if.end

if.then2:                                         ; preds = %_ZNK6Halide7Runtime8Internal12work_queue_t13assert_zeroedEv.exit
  %call = tail call i32 @_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv() #16
  br label %if.end

if.end:                                           ; preds = %if.then2, %_ZNK6Halide7Runtime8Internal12work_queue_t13assert_zeroedEv.exit
  %3 = phi i32 [ %call, %if.then2 ], [ %2, %_ZNK6Halide7Runtime8Internal12work_queue_t13assert_zeroedEv.exit ]
  %4 = icmp sgt i32 %3, 1
  %spec.select.i = select i1 %4, i32 %3, i32 1
  %5 = icmp slt i32 %spec.select.i, 256
  %call3176 = select i1 %5, i32 %spec.select.i, i32 256
  store i32 %call3176, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 1), align 8, !tbaa !58
  store i8 1, i8* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 15), align 1, !tbaa !52
  br label %if.end4

if.end4:                                          ; preds = %if.end, %entry
  %cmp181 = icmp sgt i32 %num_jobs, 0
  br i1 %cmp181, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %if.end4
  %wide.trip.count = zext i32 %num_jobs to i64
  br label %for.body

for.cond.cleanup.loopexit:                        ; preds = %for.inc
  %phi.bo = and i8 %stealable_jobs.1, 1
  br label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %for.cond.cleanup.loopexit, %if.end4
  %workers_to_wake.0.lcssa = phi i32 [ -1, %if.end4 ], [ %workers_to_wake.1, %for.cond.cleanup.loopexit ]
  %stealable_jobs.0.lcssa = phi i8 [ 0, %if.end4 ], [ %phi.bo, %for.cond.cleanup.loopexit ]
  %job_has_acquires.0.lcssa = phi i8 [ 0, %if.end4 ], [ %spec.select, %for.cond.cleanup.loopexit ]
  %job_may_block.0.lcssa = phi i8 [ 0, %if.end4 ], [ %job_may_block.1, %for.cond.cleanup.loopexit ]
  %min_threads.0.lcssa = phi i32 [ 0, %if.end4 ], [ %add, %for.cond.cleanup.loopexit ]
  %cmp31 = icmp eq %"struct.Halide::Runtime::Internal::work"* %task_parent, null
  br i1 %cmp31, label %if.then32, label %do.body61

for.body:                                         ; preds = %for.inc, %for.body.preheader
  %indvars.iv193 = phi i64 [ 0, %for.body.preheader ], [ %indvars.iv.next194, %for.inc ]
  %min_threads.0187 = phi i32 [ 0, %for.body.preheader ], [ %add, %for.inc ]
  %job_may_block.0185 = phi i8 [ 0, %for.body.preheader ], [ %job_may_block.1, %for.inc ]
  %job_has_acquires.0184 = phi i8 [ 0, %for.body.preheader ], [ %spec.select, %for.inc ]
  %stealable_jobs.0183 = phi i8 [ 0, %for.body.preheader ], [ %stealable_jobs.1, %for.inc ]
  %workers_to_wake.0182 = phi i32 [ -1, %for.body.preheader ], [ %workers_to_wake.1, %for.inc ]
  %min_threads5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv193, i32 0, i32 7
  %6 = load i32, i32* %min_threads5, align 4, !tbaa !37
  %cmp6 = icmp eq i32 %6, 0
  %add = add i32 %6, %min_threads.0187
  %stealable_jobs.1 = select i1 %cmp6, i8 1, i8 %stealable_jobs.0183
  %job_may_block.1 = select i1 %cmp6, i8 %job_may_block.0185, i8 1
  %num_semaphores = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv193, i32 0, i32 4
  %7 = load i32, i32* %num_semaphores, align 8, !tbaa !35
  %cmp16.not = icmp eq i32 %7, 0
  %spec.select = select i1 %cmp16.not, i8 %job_has_acquires.0184, i8 1
  %serial = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv193, i32 0, i32 8
  %8 = load i8, i8* %serial, align 8, !tbaa !33, !range !21
  %tobool22.not = icmp eq i8 %8, 0
  br i1 %tobool22.not, label %if.else24, label %if.then23

if.then23:                                        ; preds = %for.body
  %inc = add nsw i32 %workers_to_wake.0182, 1
  br label %for.inc

if.else24:                                        ; preds = %for.body
  %extent = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv193, i32 0, i32 6
  %9 = load i32, i32* %extent, align 8, !tbaa !32
  %add28 = add nsw i32 %9, %workers_to_wake.0182
  br label %for.inc

for.inc:                                          ; preds = %if.else24, %if.then23
  %workers_to_wake.1 = phi i32 [ %inc, %if.then23 ], [ %add28, %if.else24 ]
  %indvars.iv.next194 = add nuw nsw i64 %indvars.iv193, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next194, %wide.trip.count
  br i1 %exitcond.not, label %for.cond.cleanup.loopexit, label %for.body, !llvm.loop !59

if.then32:                                        ; preds = %for.cond.cleanup
  %10 = or i8 %job_may_block.0.lcssa, %job_has_acquires.0.lcssa
  %11 = and i8 %10, 1
  %12 = icmp eq i8 %11, 0
  %not. = xor i1 %12, true
  %add36 = zext i1 %not. to i32
  %min_threads.2 = add nsw i32 %min_threads.0.lcssa, %add36
  %13 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 4), align 8, !tbaa !60
  %cmp38180 = icmp slt i32 %13, 256
  br i1 %cmp38180, label %land.rhs, label %do.end50

land.rhs:                                         ; preds = %if.then32, %while.body
  %14 = phi i32 [ %inc45, %while.body ], [ %13, %if.then32 ]
  %15 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 1), align 8, !tbaa !58
  %sub = add nsw i32 %15, -1
  %cmp39 = icmp slt i32 %14, %sub
  br i1 %cmp39, label %while.body, label %lor.rhs

lor.rhs:                                          ; preds = %land.rhs
  %add40 = add nsw i32 %14, 1
  %16 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  %sub41 = sub i32 %add40, %16
  %cmp42 = icmp slt i32 %sub41, %min_threads.2
  br i1 %cmp42, label %while.body, label %do.end50

while.body:                                       ; preds = %lor.rhs, %land.rhs
  %17 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 5), align 4, !tbaa !62
  %inc43 = add nsw i32 %17, 1
  store i32 %inc43, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 5), align 4, !tbaa !62
  %call44 = tail call %struct.halide_thread* @halide_spawn_thread(void (i8*)* nonnull @_ZN6Halide7Runtime8Internal13worker_threadEPv, i8* null) #16
  %18 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 4), align 8, !tbaa !60
  %inc45 = add nsw i32 %18, 1
  store i32 %inc45, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 4), align 8, !tbaa !60
  %idxprom46 = sext i32 %18 to i64
  %arrayidx47 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 13, i64 %idxprom46
  store %struct.halide_thread* %call44, %struct.halide_thread** %arrayidx47, align 8, !tbaa !14
  %cmp38 = icmp slt i32 %18, 255
  br i1 %cmp38, label %land.rhs, label %do.end50, !llvm.loop !63

do.end50:                                         ; preds = %while.body, %lor.rhs, %if.then32
  br i1 %12, label %if.end77, label %if.then54

if.then54:                                        ; preds = %do.end50
  %19 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  %inc55 = add nsw i32 %19, 1
  store i32 %inc55, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  br label %if.end77

do.body61:                                        ; preds = %for.cond.cleanup
  %min_threads63 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %task_parent, i64 0, i32 0, i32 7
  %20 = load i32, i32* %min_threads63, align 4, !tbaa !37
  %active_workers = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %task_parent, i64 0, i32 8
  %21 = load i32, i32* %active_workers, align 8, !tbaa !64
  %mul = mul nsw i32 %21, %20
  %threads_reserved = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %task_parent, i64 0, i32 6
  %22 = load i32, i32* %threads_reserved, align 8, !tbaa !65
  %sub64 = sub nsw i32 %mul, %22
  %cmp65.not = icmp sgt i32 %min_threads.0.lcssa, %sub64
  br i1 %cmp65.not, label %if.then66, label %do.end69

if.then66:                                        ; preds = %do.body61
  tail call void @halide_print(i8* null, i8* getelementptr inbounds ([263 x i8], [263 x i8]* @.str.3, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end69

do.end69:                                         ; preds = %if.then66, %do.body61
  %23 = or i8 %job_may_block.0.lcssa, %job_has_acquires.0.lcssa
  %24 = and i8 %23, 1
  %25 = icmp eq i8 %24, 0
  br i1 %25, label %if.end77, label %if.then73

if.then73:                                        ; preds = %do.end69
  %26 = load i32, i32* %threads_reserved, align 8, !tbaa !65
  %inc75 = add nsw i32 %26, 1
  store i32 %inc75, i32* %threads_reserved, align 8, !tbaa !65
  br label %if.end77

if.end77:                                         ; preds = %if.then73, %do.end69, %if.then54, %do.end50
  br i1 %cmp181, label %for.body83.lr.ph, label %for.cond.cleanup82

for.body83.lr.ph:                                 ; preds = %if.end77
  %.promoted = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), align 8, !tbaa !66
  %27 = zext i32 %num_jobs to i64
  %min.iters.check = icmp eq i32 %num_jobs, 1
  br i1 %min.iters.check, label %for.body83.preheader, label %vector.ph

vector.ph:                                        ; preds = %for.body83.lr.ph
  %n.vec = and i64 %27, 4294967294
  %ind.end = and i64 %27, 1
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %vector.recur = phi %"struct.Halide::Runtime::Internal::work"* [ %.promoted, %vector.ph ], [ %31, %vector.body ]
  %offset.idx = sub i64 %27, %index
  %28 = add nsw i64 %offset.idx, -1
  %29 = add i64 %offset.idx, -2
  %30 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %28
  %31 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %29
  %32 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %28, i32 2
  %33 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %29, i32 2
  store %"struct.Halide::Runtime::Internal::work"* %vector.recur, %"struct.Halide::Runtime::Internal::work"** %32, align 8, !tbaa !67
  store %"struct.Halide::Runtime::Internal::work"* %30, %"struct.Halide::Runtime::Internal::work"** %33, align 8, !tbaa !67
  %34 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %28, i32 3
  %35 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %29, i32 3
  store %"struct.Halide::Runtime::Internal::work"* %jobs, %"struct.Halide::Runtime::Internal::work"** %34, align 8, !tbaa !44
  store %"struct.Halide::Runtime::Internal::work"* %jobs, %"struct.Halide::Runtime::Internal::work"** %35, align 8, !tbaa !44
  %36 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %28, i32 4
  %37 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %29, i32 4
  store i32 %num_jobs, i32* %36, align 8, !tbaa !45
  store i32 %num_jobs, i32* %37, align 8, !tbaa !45
  %38 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %28, i32 6
  %39 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %29, i32 6
  store i32 0, i32* %38, align 8, !tbaa !65
  store i32 0, i32* %39, align 8, !tbaa !65
  %index.next = add i64 %index, 2
  %40 = icmp eq i64 %index.next, %n.vec
  br i1 %40, label %middle.block, label %vector.body, !llvm.loop !68

middle.block:                                     ; preds = %vector.body
  %cmp.n = icmp eq i64 %n.vec, %27
  br i1 %cmp.n, label %for.cond80.for.cond.cleanup82_crit_edge, label %for.body83.preheader

for.body83.preheader:                             ; preds = %for.body83.lr.ph, %middle.block
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %27, %for.body83.lr.ph ]
  %scalar.recur.ph = phi %"struct.Halide::Runtime::Internal::work"* [ %31, %middle.block ], [ %.promoted, %for.body83.lr.ph ]
  br label %for.body83

for.cond80.for.cond.cleanup82_crit_edge:          ; preds = %for.body83, %middle.block
  store %"struct.Halide::Runtime::Internal::work"* %jobs, %"struct.Halide::Runtime::Internal::work"** getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), align 8, !tbaa !66
  br label %for.cond.cleanup82

for.cond.cleanup82:                               ; preds = %for.cond80.for.cond.cleanup82_crit_edge, %if.end77
  %41 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 12), align 4, !tbaa !70
  %tobool96 = icmp ne i32 %41, 0
  %42 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 11), align 8
  %43 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 4), align 8
  %cmp98 = icmp slt i32 %42, %43
  %44 = or i1 %tobool96, %cmp98
  %cmp102 = icmp sgt i32 %workers_to_wake.0.lcssa, %42
  %or.cond174 = or i1 %cmp102, %44
  %storemerge = select i1 %or.cond174, i32 %43, i32 %workers_to_wake.0.lcssa
  store i32 %storemerge, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 6), align 8, !tbaa !71
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 8)) #16
  %45 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 6), align 8, !tbaa !71
  %46 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 5), align 4, !tbaa !62
  %cmp106 = icmp sgt i32 %45, %46
  br i1 %cmp106, label %if.then107, label %if.end111

for.body83:                                       ; preds = %for.body83.preheader, %for.body83
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body83 ], [ %indvars.iv.ph, %for.body83.preheader ]
  %scalar.recur = phi %"struct.Halide::Runtime::Internal::work"* [ %arrayidx85, %for.body83 ], [ %scalar.recur.ph, %for.body83.preheader ]
  %indvars.iv.next = add nsw i64 %indvars.iv, -1
  %arrayidx85 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv.next
  %next_job = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv.next, i32 2
  store %"struct.Halide::Runtime::Internal::work"* %scalar.recur, %"struct.Halide::Runtime::Internal::work"** %next_job, align 8, !tbaa !67
  %siblings = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv.next, i32 3
  store %"struct.Halide::Runtime::Internal::work"* %jobs, %"struct.Halide::Runtime::Internal::work"** %siblings, align 8, !tbaa !44
  %sibling_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv.next, i32 4
  store i32 %num_jobs, i32* %sibling_count, align 8, !tbaa !45
  %threads_reserved93 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %jobs, i64 %indvars.iv.next, i32 6
  store i32 0, i32* %threads_reserved93, align 8, !tbaa !65
  %cmp81 = icmp sgt i64 %indvars.iv, 1
  br i1 %cmp81, label %for.body83, label %for.cond80.for.cond.cleanup82_crit_edge, !llvm.loop !72

if.then107:                                       ; preds = %for.cond.cleanup82
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 9)) #16
  %tobool108.not = icmp eq i8 %stealable_jobs.0.lcssa, 0
  br i1 %tobool108.not, label %if.end111, label %if.then109

if.then109:                                       ; preds = %if.then107
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 10)) #16
  br label %if.end111

if.end111:                                        ; preds = %if.then109, %if.then107, %for.cond.cleanup82
  %47 = or i8 %job_may_block.0.lcssa, %job_has_acquires.0.lcssa
  %48 = and i8 %47, 1
  %49 = icmp eq i8 %48, 0
  br i1 %49, label %if.end123, label %if.then115

if.then115:                                       ; preds = %if.end111
  br i1 %cmp31, label %if.else120, label %if.then117

if.then117:                                       ; preds = %if.then115
  %threads_reserved118 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %task_parent, i64 0, i32 6
  %50 = load i32, i32* %threads_reserved118, align 8, !tbaa !65
  %dec119 = add nsw i32 %50, -1
  store i32 %dec119, i32* %threads_reserved118, align 8, !tbaa !65
  br label %if.end123

if.else120:                                       ; preds = %if.then115
  %51 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  %dec121 = add nsw i32 %51, -1
  store i32 %dec121, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  br label %if.end123

if.end123:                                        ; preds = %if.else120, %if.then117, %if.end111
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE(%"struct.Halide::Runtime::Internal::work"* %owned_job) local_unnamed_addr #0 {
entry:
  %active_workers.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %owned_job, i64 0, i32 8
  %tobool.not = icmp eq %"struct.Halide::Runtime::Internal::work"* %owned_job, null
  %extent.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %owned_job, i64 0, i32 0, i32 6
  %exit_status = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %owned_job, i64 0, i32 9
  %parent_job = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %owned_job, i64 0, i32 5
  %siblings56 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %owned_job, i64 0, i32 3
  %owner_is_sleeping = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %owned_job, i64 0, i32 11
  %next_job10 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %owned_job, i64 0, i32 2
  br label %while.cond

while.cond:                                       ; preds = %while.cond.backedge, %entry
  %spin_count.0 = phi i32 [ 0, %entry ], [ %spin_count.0.be, %while.cond.backedge ]
  br i1 %tobool.not, label %cond.false, label %cond.true

cond.true:                                        ; preds = %while.cond
  %0 = load i32, i32* %extent.i, align 8, !tbaa !32
  %tobool.not.i = icmp eq i32 %0, 0
  br i1 %tobool.not.i, label %cond.end, label %if.then

cond.false:                                       ; preds = %while.cond
  %1 = load i8, i8* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 14), align 8, !tbaa !73, !range !21
  %tobool1.not = icmp eq i8 %1, 0
  br i1 %tobool1.not, label %do.end.thread, label %while.end316

cond.end:                                         ; preds = %cond.true
  %2 = load i32, i32* %active_workers.i, align 8, !tbaa !64
  %tobool2.i.not = icmp eq i32 %2, 0
  br i1 %tobool2.i.not, label %while.end316, label %if.then

if.then:                                          ; preds = %cond.end, %cond.true
  %3 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), align 8, !tbaa !66
  %4 = load i32, i32* %exit_status, align 4, !tbaa !47
  %cmp.not = icmp eq i32 %4, 0
  br i1 %cmp.not, label %if.else, label %if.then3

if.then3:                                         ; preds = %if.then
  %5 = load i32, i32* %active_workers.i, align 8, !tbaa !64
  %cmp4 = icmp eq i32 %5, 0
  br i1 %cmp4, label %while.cond6.preheader, label %do.end

while.cond6.preheader:                            ; preds = %if.then3
  %cmp7.not524 = icmp eq %"struct.Halide::Runtime::Internal::work"* %3, %owned_job
  br i1 %cmp7.not524, label %while.end, label %while.body8

while.body8:                                      ; preds = %while.cond6.preheader, %while.body8
  %job.0525 = phi %"struct.Halide::Runtime::Internal::work"* [ %6, %while.body8 ], [ %3, %while.cond6.preheader ]
  %next_job = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.0525, i64 0, i32 2
  %6 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %next_job, align 8, !tbaa !67
  %cmp7.not = icmp eq %"struct.Halide::Runtime::Internal::work"* %6, %owned_job
  br i1 %cmp7.not, label %while.end.loopexit, label %while.body8, !llvm.loop !74

while.end.loopexit:                               ; preds = %while.body8
  %next_job.le = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.0525, i64 0, i32 2
  br label %while.end

while.end:                                        ; preds = %while.end.loopexit, %while.cond6.preheader
  %prev_ptr.0.lcssa = phi %"struct.Halide::Runtime::Internal::work"** [ getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), %while.cond6.preheader ], [ %next_job.le, %while.end.loopexit ]
  %7 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %next_job10, align 8, !tbaa !67
  store %"struct.Halide::Runtime::Internal::work"* %7, %"struct.Halide::Runtime::Internal::work"** %prev_ptr.0.lcssa, align 8, !tbaa !14
  store i32 0, i32* %extent.i, align 8, !tbaa !32
  br label %while.cond.backedge

if.else:                                          ; preds = %if.then
  %8 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %parent_job, align 8, !tbaa !46
  %tobool11.not = icmp eq %"struct.Halide::Runtime::Internal::work"* %8, null
  br i1 %tobool11.not, label %do.end, label %land.lhs.true

land.lhs.true:                                    ; preds = %if.else
  %exit_status13 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %8, i64 0, i32 9
  %9 = load i32, i32* %exit_status13, align 4, !tbaa !47
  %cmp14.not = icmp eq i32 %9, 0
  br i1 %cmp14.not, label %do.end, label %if.then15

if.then15:                                        ; preds = %land.lhs.true
  store i32 %9, i32* %exit_status, align 4, !tbaa !47
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 10)) #16
  br label %while.cond.backedge

do.end:                                           ; preds = %land.lhs.true, %if.else, %if.then3
  %tobool23.not527 = icmp eq %"struct.Halide::Runtime::Internal::work"* %3, null
  br i1 %tobool23.not527, label %if.then105, label %do.end27

do.end.thread:                                    ; preds = %cond.false
  %10 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), align 8, !tbaa !66
  %tobool23.not527563 = icmp eq %"struct.Halide::Runtime::Internal::work"* %10, null
  br i1 %tobool23.not527563, label %if.else112, label %do.end27.us

do.end27.us:                                      ; preds = %do.end.thread, %cleanup.us
  %job.1529.us = phi %"struct.Halide::Runtime::Internal::work"* [ %29, %cleanup.us ], [ %10, %do.end.thread ]
  %prev_ptr.1528.us = phi %"struct.Halide::Runtime::Internal::work"** [ %next_job95.us, %cleanup.us ], [ getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), %do.end.thread ]
  %parent_job29.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529.us, i64 0, i32 5
  %11 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %parent_job29.us, align 8, !tbaa !46
  %cmp30.us = icmp eq %"struct.Halide::Runtime::Internal::work"* %11, null
  br i1 %cmp30.us, label %if.then31.us, label %if.else32.us

if.else32.us:                                     ; preds = %do.end27.us
  %active_workers33.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %11, i64 0, i32 8
  %12 = load i32, i32* %active_workers33.us, align 8, !tbaa !64
  %cmp34.us = icmp eq i32 %12, 0
  %min_threads.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %11, i64 0, i32 0, i32 7
  %13 = load i32, i32* %min_threads.us, align 4, !tbaa !37
  br i1 %cmp34.us, label %if.then35.us, label %if.else38.us

if.else38.us:                                     ; preds = %if.else32.us
  %mul.us = mul nsw i32 %13, %12
  %threads_reserved42.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %11, i64 0, i32 6
  %14 = load i32, i32* %threads_reserved42.us, align 8, !tbaa !65
  %sub43.us = sub nsw i32 %mul.us, %14
  br label %if.end45.us

if.then35.us:                                     ; preds = %if.else32.us
  %threads_reserved.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %11, i64 0, i32 6
  %15 = load i32, i32* %threads_reserved.us, align 8, !tbaa !65
  %sub37.us = sub nsw i32 %13, %15
  br label %if.end45.us

if.then31.us:                                     ; preds = %do.end27.us
  %16 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 4), align 8, !tbaa !60
  %add.us = add nsw i32 %16, 1
  %17 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  %sub.us = sub i32 %add.us, %17
  br label %if.end45.us

if.end45.us:                                      ; preds = %if.then31.us, %if.then35.us, %if.else38.us
  %threads_available.0.us = phi i32 [ %sub.us, %if.then31.us ], [ %sub37.us, %if.then35.us ], [ %sub43.us, %if.else38.us ]
  %min_threads47.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529.us, i64 0, i32 0, i32 7
  %18 = load i32, i32* %min_threads47.us, align 4, !tbaa !37
  %cmp48.not.us = icmp sge i32 %threads_available.0.us, %18
  %serial.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529.us, i64 0, i32 0, i32 8
  %19 = load i8, i8* %serial.us, align 8, !tbaa !33, !range !21
  %tobool69.not.us = icmp eq i8 %19, 0
  br i1 %tobool69.not.us, label %if.end45.us.lor.end73.us_crit_edge, label %lor.rhs70.us

if.end45.us.lor.end73.us_crit_edge:               ; preds = %if.end45.us
  %.0 = and i1 %cmp48.not.us, true
  br label %lor.end73.us

lor.rhs70.us:                                     ; preds = %if.end45.us
  %active_workers71.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529.us, i64 0, i32 8
  %20 = load i32, i32* %active_workers71.us, align 8, !tbaa !64
  %cmp72.us = icmp eq i32 %20, 0
  %.1 = and i1 %cmp48.not.us, %cmp72.us
  br label %lor.end73.us

lor.end73.us:                                     ; preds = %if.end45.us.lor.end73.us_crit_edge, %lor.rhs70.us
  %.phi = phi i1 [ %.0, %if.end45.us.lor.end73.us_crit_edge ], [ %.1, %lor.rhs70.us ]
  br i1 %.phi, label %if.then86.us, label %cleanup.us

if.then86.us:                                     ; preds = %lor.end73.us
  %next_semaphore.i.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529.us, i64 0, i32 10
  %21 = load i32, i32* %next_semaphore.i.us, align 8, !tbaa !42
  %num_semaphores.i.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529.us, i64 0, i32 0, i32 4
  %22 = load i32, i32* %num_semaphores.i.us, align 8, !tbaa !35
  %cmp14.i.us = icmp slt i32 %21, %22
  br i1 %cmp14.i.us, label %for.body.lr.ph.i.us, label %if.else127

for.body.lr.ph.i.us:                              ; preds = %if.then86.us
  %semaphores.i.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529.us, i64 0, i32 0, i32 3
  br label %for.body.i.us

for.body.i.us:                                    ; preds = %for.inc.i.us, %for.body.lr.ph.i.us
  %23 = phi i32 [ %21, %for.body.lr.ph.i.us ], [ %inc.i.us, %for.inc.i.us ]
  %24 = load %struct.halide_semaphore_acquire_t*, %struct.halide_semaphore_acquire_t** %semaphores.i.us, align 8, !tbaa !34
  %idxprom.i.us = sext i32 %23 to i64
  %semaphore.i.us = getelementptr inbounds %struct.halide_semaphore_acquire_t, %struct.halide_semaphore_acquire_t* %24, i64 %idxprom.i.us, i32 0
  %25 = load %struct.halide_semaphore_t*, %struct.halide_semaphore_t** %semaphore.i.us, align 8, !tbaa !75
  %count.i.us = getelementptr inbounds %struct.halide_semaphore_acquire_t, %struct.halide_semaphore_acquire_t* %24, i64 %idxprom.i.us, i32 1
  %26 = load i32, i32* %count.i.us, align 8, !tbaa !77
  %call.i.us = tail call zeroext i1 @halide_default_semaphore_try_acquire(%struct.halide_semaphore_t* %25, i32 %26) #15
  br i1 %call.i.us, label %for.inc.i.us, label %cleanup.us

for.inc.i.us:                                     ; preds = %for.body.i.us
  %27 = load i32, i32* %next_semaphore.i.us, align 8, !tbaa !42
  %inc.i.us = add nsw i32 %27, 1
  store i32 %inc.i.us, i32* %next_semaphore.i.us, align 8, !tbaa !42
  %28 = load i32, i32* %num_semaphores.i.us, align 8, !tbaa !35
  %cmp.i.us = icmp slt i32 %inc.i.us, %28
  br i1 %cmp.i.us, label %for.body.i.us, label %if.else127, !llvm.loop !78

cleanup.us:                                       ; preds = %for.body.i.us, %lor.end73.us
  %next_job95.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529.us, i64 0, i32 2
  %29 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %next_job95.us, align 8, !tbaa !67
  %tobool23.not.us = icmp eq %"struct.Halide::Runtime::Internal::work"* %29, null
  br i1 %tobool23.not.us, label %if.then103, label %do.end27.us

do.end27:                                         ; preds = %do.end, %cleanup
  %job.1529 = phi %"struct.Halide::Runtime::Internal::work"* [ %50, %cleanup ], [ %3, %do.end ]
  %prev_ptr.1528 = phi %"struct.Halide::Runtime::Internal::work"** [ %next_job95, %cleanup ], [ getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), %do.end ]
  %parent_job29 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 5
  %30 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %parent_job29, align 8, !tbaa !46
  %cmp30 = icmp eq %"struct.Halide::Runtime::Internal::work"* %30, null
  br i1 %cmp30, label %if.then31, label %if.else32

if.then31:                                        ; preds = %do.end27
  %31 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 4), align 8, !tbaa !60
  %add = add nsw i32 %31, 1
  %32 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  %sub = sub i32 %add, %32
  br label %if.end45

if.else32:                                        ; preds = %do.end27
  %active_workers33 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %30, i64 0, i32 8
  %33 = load i32, i32* %active_workers33, align 8, !tbaa !64
  %cmp34 = icmp eq i32 %33, 0
  %min_threads = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %30, i64 0, i32 0, i32 7
  %34 = load i32, i32* %min_threads, align 4, !tbaa !37
  br i1 %cmp34, label %if.then35, label %if.else38

if.then35:                                        ; preds = %if.else32
  %threads_reserved = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %30, i64 0, i32 6
  %35 = load i32, i32* %threads_reserved, align 8, !tbaa !65
  %sub37 = sub nsw i32 %34, %35
  br label %if.end45

if.else38:                                        ; preds = %if.else32
  %mul = mul nsw i32 %34, %33
  %threads_reserved42 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %30, i64 0, i32 6
  %36 = load i32, i32* %threads_reserved42, align 8, !tbaa !65
  %sub43 = sub nsw i32 %mul, %36
  br label %if.end45

if.end45:                                         ; preds = %if.else38, %if.then35, %if.then31
  %threads_available.0 = phi i32 [ %sub, %if.then31 ], [ %sub37, %if.then35 ], [ %sub43, %if.else38 ]
  %min_threads47 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 0, i32 7
  %37 = load i32, i32* %min_threads47, align 4, !tbaa !37
  %cmp48.not = icmp slt i32 %threads_available.0, %37
  %siblings = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 3
  %38 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %siblings, align 8, !tbaa !44
  %39 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %siblings56, align 8, !tbaa !44
  %cmp57 = icmp ne %"struct.Halide::Runtime::Internal::work"* %38, %39
  %cmp60 = icmp ne i32 %37, 0
  %serial = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 0, i32 8
  %40 = load i8, i8* %serial, align 8, !tbaa !33, !range !21
  %tobool69.not = icmp eq i8 %40, 0
  br i1 %tobool69.not, label %if.end45.lor.end73_crit_edge, label %lor.rhs70

if.end45.lor.end73_crit_edge:                     ; preds = %if.end45
  %.not472.0 = xor i1 true, true
  br label %lor.end73

lor.rhs70:                                        ; preds = %if.end45
  %active_workers71 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 8
  %41 = load i32, i32* %active_workers71, align 8, !tbaa !64
  %cmp72 = icmp eq i32 %41, 0
  %.not472.1 = xor i1 %cmp72, true
  br label %lor.end73

lor.end73:                                        ; preds = %if.end45.lor.end73_crit_edge, %lor.rhs70
  %.not472.phi = phi i1 [ %.not472.0, %if.end45.lor.end73_crit_edge ], [ %.not472.1, %lor.rhs70 ]
  %.not = and i1 %cmp60, %cmp57
  %brmerge = or i1 %cmp48.not, %.not
  %brmerge473 = or i1 %brmerge, %.not472.phi
  br i1 %brmerge473, label %cleanup, label %if.then86

if.then86:                                        ; preds = %lor.end73
  %next_semaphore.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 10
  %42 = load i32, i32* %next_semaphore.i, align 8, !tbaa !42
  %num_semaphores.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 0, i32 4
  %43 = load i32, i32* %num_semaphores.i, align 8, !tbaa !35
  %cmp14.i = icmp slt i32 %42, %43
  br i1 %cmp14.i, label %for.body.lr.ph.i, label %if.else127

for.body.lr.ph.i:                                 ; preds = %if.then86
  %semaphores.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 0, i32 3
  br label %for.body.i

for.body.i:                                       ; preds = %for.inc.i, %for.body.lr.ph.i
  %44 = phi i32 [ %42, %for.body.lr.ph.i ], [ %inc.i, %for.inc.i ]
  %45 = load %struct.halide_semaphore_acquire_t*, %struct.halide_semaphore_acquire_t** %semaphores.i, align 8, !tbaa !34
  %idxprom.i = sext i32 %44 to i64
  %semaphore.i = getelementptr inbounds %struct.halide_semaphore_acquire_t, %struct.halide_semaphore_acquire_t* %45, i64 %idxprom.i, i32 0
  %46 = load %struct.halide_semaphore_t*, %struct.halide_semaphore_t** %semaphore.i, align 8, !tbaa !75
  %count.i = getelementptr inbounds %struct.halide_semaphore_acquire_t, %struct.halide_semaphore_acquire_t* %45, i64 %idxprom.i, i32 1
  %47 = load i32, i32* %count.i, align 8, !tbaa !77
  %call.i = tail call zeroext i1 @halide_default_semaphore_try_acquire(%struct.halide_semaphore_t* %46, i32 %47) #15
  br i1 %call.i, label %for.inc.i, label %cleanup

for.inc.i:                                        ; preds = %for.body.i
  %48 = load i32, i32* %next_semaphore.i, align 8, !tbaa !42
  %inc.i = add nsw i32 %48, 1
  store i32 %inc.i, i32* %next_semaphore.i, align 8, !tbaa !42
  %49 = load i32, i32* %num_semaphores.i, align 8, !tbaa !35
  %cmp.i = icmp slt i32 %inc.i, %49
  br i1 %cmp.i, label %for.body.i, label %if.else127, !llvm.loop !78

cleanup:                                          ; preds = %for.body.i, %lor.end73
  %next_job95 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1529, i64 0, i32 2
  %50 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %next_job95, align 8, !tbaa !67
  %tobool23.not = icmp eq %"struct.Halide::Runtime::Internal::work"* %50, null
  br i1 %tobool23.not, label %if.then103, label %do.end27

if.then103:                                       ; preds = %cleanup, %cleanup.us
  br i1 %tobool.not, label %if.else112, label %if.then105

if.then105:                                       ; preds = %do.end, %if.then103
  %inc = add nsw i32 %spin_count.0, 1
  %cmp106 = icmp slt i32 %spin_count.0, 40
  br i1 %cmp106, label %if.then107, label %if.else108

if.then107:                                       ; preds = %if.then105
  tail call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  tail call void @halide_thread_yield() #15
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  br label %while.cond.backedge

if.else108:                                       ; preds = %if.then105
  %51 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 12), align 4, !tbaa !70
  %inc109 = add nsw i32 %51, 1
  store i32 %inc109, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 12), align 4, !tbaa !70
  store i8 1, i8* %owner_is_sleeping, align 4, !tbaa !43
  tail call void @halide_cond_wait(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 10), %struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  store i8 0, i8* %owner_is_sleeping, align 4, !tbaa !43
  %52 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 12), align 4, !tbaa !70
  %dec = add nsw i32 %52, -1
  store i32 %dec, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 12), align 4, !tbaa !70
  br label %while.cond.backedge

if.else112:                                       ; preds = %if.then103, %do.end.thread
  %53 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 11), align 8, !tbaa !79
  %inc113 = add nsw i32 %53, 1
  store i32 %inc113, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 11), align 8, !tbaa !79
  %54 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 5), align 4, !tbaa !62
  %55 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 6), align 8, !tbaa !71
  %cmp114 = icmp sgt i32 %54, %55
  br i1 %cmp114, label %if.then115, label %if.else118

if.then115:                                       ; preds = %if.else112
  %dec116 = add nsw i32 %54, -1
  store i32 %dec116, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 5), align 4, !tbaa !62
  tail call void @halide_cond_wait(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 9), %struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %56 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 5), align 4, !tbaa !62
  %inc117 = add nsw i32 %56, 1
  store i32 %inc117, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 5), align 4, !tbaa !62
  br label %if.end124

if.else118:                                       ; preds = %if.else112
  %inc119 = add nsw i32 %spin_count.0, 1
  %cmp120 = icmp slt i32 %spin_count.0, 40
  br i1 %cmp120, label %if.then121, label %if.else122

if.then121:                                       ; preds = %if.else118
  tail call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  tail call void @halide_thread_yield() #15
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  br label %if.end124

if.else122:                                       ; preds = %if.else118
  tail call void @halide_cond_wait(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 8), %struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  br label %if.end124

if.end124:                                        ; preds = %if.else122, %if.then121, %if.then115
  %spin_count.1 = phi i32 [ %spin_count.0, %if.then115 ], [ %inc119, %if.then121 ], [ %inc119, %if.else122 ]
  %57 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 11), align 8, !tbaa !79
  %dec125 = add nsw i32 %57, -1
  store i32 %dec125, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 11), align 8, !tbaa !79
  br label %while.cond.backedge

if.else127:                                       ; preds = %if.then86, %if.then86.us, %for.inc.i, %for.inc.i.us
  %next_semaphore.i521 = phi i32* [ %next_semaphore.i.us, %for.inc.i.us ], [ %next_semaphore.i, %for.inc.i ], [ %next_semaphore.i.us, %if.then86.us ], [ %next_semaphore.i, %if.then86 ]
  %prev_ptr.1519 = phi %"struct.Halide::Runtime::Internal::work"** [ %prev_ptr.1528.us, %for.inc.i.us ], [ %prev_ptr.1528, %for.inc.i ], [ %prev_ptr.1528.us, %if.then86.us ], [ %prev_ptr.1528, %if.then86 ]
  %job.1515 = phi %"struct.Halide::Runtime::Internal::work"* [ %job.1529.us, %for.inc.i.us ], [ %job.1529, %for.inc.i ], [ %job.1529.us, %if.then86.us ], [ %job.1529, %if.then86 ]
  store i32 0, i32* %next_semaphore.i521, align 8, !tbaa !42
  %active_workers132 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 8
  %58 = load i32, i32* %active_workers132, align 8, !tbaa !64
  %inc133 = add nsw i32 %58, 1
  store i32 %inc133, i32* %active_workers132, align 8, !tbaa !64
  %parent_job134 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 5
  %59 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %parent_job134, align 8, !tbaa !46
  %cmp135 = icmp eq %"struct.Halide::Runtime::Internal::work"* %59, null
  %min_threads138 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 7
  %60 = load i32, i32* %min_threads138, align 4, !tbaa !37
  br i1 %cmp135, label %if.then136, label %if.else143

if.then136:                                       ; preds = %if.else127
  %61 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  %add139 = add nsw i32 %61, %60
  store i32 %add139, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  br label %if.end152

if.else143:                                       ; preds = %if.else127
  %threads_reserved147 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %59, i64 0, i32 6
  %62 = load i32, i32* %threads_reserved147, align 8, !tbaa !65
  %add148 = add nsw i32 %62, %60
  store i32 %add148, i32* %threads_reserved147, align 8, !tbaa !65
  br label %if.end152

if.end152:                                        ; preds = %if.else143, %if.then136
  %serial154 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 8
  %63 = load i8, i8* %serial154, align 8, !tbaa !33, !range !21
  %tobool155.not = icmp eq i8 %63, 0
  br i1 %tobool155.not, label %if.else198, label %if.then156

if.then156:                                       ; preds = %if.end152
  %next_job157 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 2
  %64 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %next_job157, align 8, !tbaa !67
  store %"struct.Halide::Runtime::Internal::work"* %64, %"struct.Halide::Runtime::Internal::work"** %prev_ptr.1519, align 8, !tbaa !14
  tail call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %extent163 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 6
  %next_semaphore.i474 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 10
  %num_semaphores.i475 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 4
  %semaphores.i477 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 3
  %user_context = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 7
  %fn = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 0
  %min = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 5
  %closure = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 1
  %65 = bitcast %"struct.Halide::Runtime::Internal::work"* %job.1515 to i8*
  br label %while.cond161.preheader

while.cond161.preheader:                          ; preds = %if.end172, %if.then156
  %iters.0538 = phi i32 [ 1, %if.then156 ], [ 0, %if.end172 ]
  %total_iters.0537 = phi i32 [ 0, %if.then156 ], [ %add178, %if.end172 ]
  %66 = load i32, i32* %extent163, align 8, !tbaa !32
  %sub164531 = sub nsw i32 %66, %total_iters.0537
  %cmp165532 = icmp sgt i32 %sub164531, %iters.0538
  br i1 %cmp165532, label %land.rhs.preheader, label %while.end169

land.rhs.preheader:                               ; preds = %while.cond161.preheader
  %.pre = load i32, i32* %next_semaphore.i474, align 8, !tbaa !42
  %.pre560 = load i32, i32* %num_semaphores.i475, align 8, !tbaa !35
  br label %land.rhs

land.rhs:                                         ; preds = %while.body167, %land.rhs.preheader
  %67 = phi i32 [ %76, %while.body167 ], [ %66, %land.rhs.preheader ]
  %68 = phi i32 [ %77, %while.body167 ], [ %.pre560, %land.rhs.preheader ]
  %69 = phi i32 [ 0, %while.body167 ], [ %.pre, %land.rhs.preheader ]
  %iters.1533 = phi i32 [ %inc168, %while.body167 ], [ %iters.0538, %land.rhs.preheader ]
  %cmp14.i476 = icmp slt i32 %69, %68
  br i1 %cmp14.i476, label %for.body.i483, label %while.body167

for.body.i483:                                    ; preds = %land.rhs, %for.inc.i486
  %70 = phi i32 [ %inc.i484, %for.inc.i486 ], [ %69, %land.rhs ]
  %71 = load %struct.halide_semaphore_acquire_t*, %struct.halide_semaphore_acquire_t** %semaphores.i477, align 8, !tbaa !34
  %idxprom.i479 = sext i32 %70 to i64
  %semaphore.i480 = getelementptr inbounds %struct.halide_semaphore_acquire_t, %struct.halide_semaphore_acquire_t* %71, i64 %idxprom.i479, i32 0
  %72 = load %struct.halide_semaphore_t*, %struct.halide_semaphore_t** %semaphore.i480, align 8, !tbaa !75
  %count.i481 = getelementptr inbounds %struct.halide_semaphore_acquire_t, %struct.halide_semaphore_acquire_t* %71, i64 %idxprom.i479, i32 1
  %73 = load i32, i32* %count.i481, align 8, !tbaa !77
  %call.i482 = tail call zeroext i1 @halide_default_semaphore_try_acquire(%struct.halide_semaphore_t* %72, i32 %73) #15
  br i1 %call.i482, label %for.inc.i486, label %while.end169

for.inc.i486:                                     ; preds = %for.body.i483
  %74 = load i32, i32* %next_semaphore.i474, align 8, !tbaa !42
  %inc.i484 = add nsw i32 %74, 1
  store i32 %inc.i484, i32* %next_semaphore.i474, align 8, !tbaa !42
  %75 = load i32, i32* %num_semaphores.i475, align 8, !tbaa !35
  %cmp.i485 = icmp slt i32 %inc.i484, %75
  br i1 %cmp.i485, label %for.body.i483, label %while.body167.loopexit, !llvm.loop !78

while.body167.loopexit:                           ; preds = %for.inc.i486
  %.pre561 = load i32, i32* %extent163, align 8, !tbaa !32
  br label %while.body167

while.body167:                                    ; preds = %while.body167.loopexit, %land.rhs
  %76 = phi i32 [ %.pre561, %while.body167.loopexit ], [ %67, %land.rhs ]
  %77 = phi i32 [ %75, %while.body167.loopexit ], [ %68, %land.rhs ]
  store i32 0, i32* %next_semaphore.i474, align 8, !tbaa !42
  %inc168 = add nuw nsw i32 %iters.1533, 1
  %sub164 = sub nsw i32 %76, %total_iters.0537
  %cmp165 = icmp sgt i32 %sub164, %inc168
  br i1 %cmp165, label %land.rhs, label %if.end172, !llvm.loop !80

while.end169:                                     ; preds = %for.body.i483, %while.cond161.preheader
  %iters.1523 = phi i32 [ %iters.0538, %while.cond161.preheader ], [ %iters.1533, %for.body.i483 ]
  %cmp170 = icmp eq i32 %iters.1523, 0
  br i1 %cmp170, label %while.end179, label %if.end172

if.end172:                                        ; preds = %while.body167, %while.end169
  %iters.1523566 = phi i32 [ %iters.1523, %while.end169 ], [ %inc168, %while.body167 ]
  %78 = load i8*, i8** %user_context, align 8, !tbaa !40
  %79 = load i32 (i8*, i32, i32, i8*, i8*)*, i32 (i8*, i32, i32, i8*, i8*)** %fn, align 8, !tbaa !28
  %80 = load i32, i32* %min, align 4, !tbaa !31
  %add175 = add nsw i32 %80, %total_iters.0537
  %81 = load i8*, i8** %closure, align 8, !tbaa !36
  %call177 = tail call i32 @halide_do_loop_task(i8* %78, i32 (i8*, i32, i32, i8*, i8*)* %79, i32 %add175, i32 %iters.1523566, i8* %81, i8* nonnull %65) #16
  %add178 = add nuw nsw i32 %iters.1523566, %total_iters.0537
  %cmp159 = icmp eq i32 %call177, 0
  br i1 %cmp159, label %while.cond161.preheader, label %while.end179, !llvm.loop !81

while.end179:                                     ; preds = %if.end172, %while.end169
  %cmp170568 = phi i1 [ true, %while.end169 ], [ false, %if.end172 ]
  %result.0.lcssa = phi i32 [ 0, %while.end169 ], [ %call177, %if.end172 ]
  %total_iters.0.lcssa = phi i32 [ %total_iters.0537, %while.end169 ], [ %add178, %if.end172 ]
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %82 = load i32, i32* %min, align 4, !tbaa !31
  %add182 = add nsw i32 %82, %total_iters.0.lcssa
  store i32 %add182, i32* %min, align 4, !tbaa !31
  %83 = load i32, i32* %extent163, align 8, !tbaa !32
  %sub185 = sub nsw i32 %83, %total_iters.0.lcssa
  store i32 %sub185, i32* %extent163, align 8, !tbaa !32
  br i1 %cmp170568, label %if.else190, label %if.end230.thread505

if.end230.thread505:                              ; preds = %while.end179
  store i32 0, i32* %extent163, align 8, !tbaa !32
  br label %if.then238

if.else190:                                       ; preds = %while.end179
  %cmp193 = icmp sgt i32 %sub185, 0
  br i1 %cmp193, label %if.then194, label %if.end271

if.then194:                                       ; preds = %if.else190
  %84 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), align 8, !tbaa !66
  store %"struct.Halide::Runtime::Internal::work"* %84, %"struct.Halide::Runtime::Internal::work"** %next_job157, align 8, !tbaa !67
  store %"struct.Halide::Runtime::Internal::work"* %job.1515, %"struct.Halide::Runtime::Internal::work"** getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 3), align 8, !tbaa !66
  br label %if.end271

if.else198:                                       ; preds = %if.end152
  %myjob.sroa.0.0..sroa_idx = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 0
  %myjob.sroa.0.0.copyload = load i32 (i8*, i32, i32, i8*, i8*)*, i32 (i8*, i32, i32, i8*, i8*)** %myjob.sroa.0.0..sroa_idx, align 8, !tbaa.struct !82
  %myjob.sroa.4.0..sroa_idx327 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 1
  %myjob.sroa.4.0.copyload = load i8*, i8** %myjob.sroa.4.0..sroa_idx327, align 8, !tbaa.struct !83
  %myjob.sroa.6333.0..sroa_idx334 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 5
  %myjob.sroa.6333.0.copyload = load i32, i32* %myjob.sroa.6333.0..sroa_idx334, align 4
  %myjob.sroa.8340.0..sroa_idx341 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 1
  %myjob.sroa.8340.0.copyload = load i32 (i8*, i32, i8*)*, i32 (i8*, i32, i8*)** %myjob.sroa.8340.0..sroa_idx341, align 8, !tbaa.struct !84
  %myjob.sroa.10347.0..sroa_idx348 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 7
  %myjob.sroa.10347.0.copyload = load i8*, i8** %myjob.sroa.10347.0..sroa_idx348, align 8, !tbaa.struct !85
  %inc201 = add nsw i32 %myjob.sroa.6333.0.copyload, 1
  store i32 %inc201, i32* %myjob.sroa.6333.0..sroa_idx334, align 4, !tbaa !31
  %extent203 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 6
  %85 = load i32, i32* %extent203, align 8, !tbaa !32
  %dec204 = add nsw i32 %85, -1
  store i32 %dec204, i32* %extent203, align 8, !tbaa !32
  %cmp207 = icmp eq i32 %dec204, 0
  br i1 %cmp207, label %if.then208, label %if.end210

if.then208:                                       ; preds = %if.else198
  %next_job209 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 2
  %86 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %next_job209, align 8, !tbaa !67
  store %"struct.Halide::Runtime::Internal::work"* %86, %"struct.Halide::Runtime::Internal::work"** %prev_ptr.1519, align 8, !tbaa !14
  br label %if.end210

if.end210:                                        ; preds = %if.then208, %if.else198
  tail call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %tobool211.not = icmp eq i32 (i8*, i32, i8*)* %myjob.sroa.8340.0.copyload, null
  br i1 %tobool211.not, label %if.else220, label %if.then212

if.then212:                                       ; preds = %if.end210
  %call219 = tail call i32 @halide_do_task(i8* %myjob.sroa.10347.0.copyload, i32 (i8*, i32, i8*)* nonnull %myjob.sroa.8340.0.copyload, i32 %myjob.sroa.6333.0.copyload, i8* %myjob.sroa.4.0.copyload) #16
  br label %if.end230

if.else220:                                       ; preds = %if.end210
  %87 = bitcast %"struct.Halide::Runtime::Internal::work"* %job.1515 to i8*
  %call228 = tail call i32 @halide_do_loop_task(i8* %myjob.sroa.10347.0.copyload, i32 (i8*, i32, i32, i8*, i8*)* %myjob.sroa.0.0.copyload, i32 %myjob.sroa.6333.0.copyload, i32 1, i8* %myjob.sroa.4.0.copyload, i8* %87) #16
  br label %if.end230

if.end230:                                        ; preds = %if.else220, %if.then212
  %result.1 = phi i32 [ %call219, %if.then212 ], [ %call228, %if.else220 ]
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %cmp237.not = icmp eq i32 %result.1, 0
  br i1 %cmp237.not, label %if.end271, label %if.then238

if.then238:                                       ; preds = %if.end230, %if.end230.thread505
  %result.2510 = phi i32 [ %result.0.lcssa, %if.end230.thread505 ], [ %result.1, %if.end230 ]
  %exit_status239 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 9
  store i32 %result.2510, i32* %exit_status239, align 4, !tbaa !47
  %sibling_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 4
  %88 = load i32, i32* %sibling_count, align 8, !tbaa !45
  %cmp240540 = icmp sgt i32 %88, 0
  br i1 %cmp240540, label %do.end243.lr.ph, label %if.end271

do.end243.lr.ph:                                  ; preds = %if.then238
  %siblings244 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 3
  %89 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %siblings244, align 8, !tbaa !44
  %wide.trip.count = zext i32 %88 to i64
  br label %do.end243

do.end243:                                        ; preds = %for.inc, %do.end243.lr.ph
  %indvars.iv = phi i64 [ 0, %do.end243.lr.ph ], [ %indvars.iv.next, %for.inc ]
  %wake_owners.0541 = phi i8 [ 0, %do.end243.lr.ph ], [ %wake_owners.1, %for.inc ]
  %exit_status245 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %89, i64 %indvars.iv, i32 9
  %90 = load i32, i32* %exit_status245, align 4, !tbaa !47
  %cmp246 = icmp eq i32 %90, 0
  br i1 %cmp246, label %if.then247, label %for.inc

if.then247:                                       ; preds = %do.end243
  store i32 %result.2510, i32* %exit_status245, align 4, !tbaa !47
  %91 = load i32, i32* %active_workers132, align 8, !tbaa !64
  %cmp253 = icmp eq i32 %91, 0
  br i1 %cmp253, label %land.rhs254, label %land.end260

land.rhs254:                                      ; preds = %if.then247
  %owner_is_sleeping258 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %89, i64 %indvars.iv, i32 11
  %92 = load i8, i8* %owner_is_sleeping258, align 4, !tbaa !43, !range !21
  br label %land.end260

land.end260:                                      ; preds = %land.rhs254, %if.then247
  %93 = phi i8 [ 0, %if.then247 ], [ %92, %land.rhs254 ]
  %94 = and i8 %wake_owners.0541, 1
  %or = or i8 %93, %94
  br label %for.inc

for.inc:                                          ; preds = %land.end260, %do.end243
  %wake_owners.1 = phi i8 [ %or, %land.end260 ], [ %wake_owners.0541, %do.end243 ]
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %if.end271, label %do.end243, !llvm.loop !86

if.end271:                                        ; preds = %for.inc, %if.then238, %if.end230, %if.then194, %if.else190
  %wake_owners.2 = phi i8 [ 0, %if.end230 ], [ 0, %if.else190 ], [ 0, %if.then194 ], [ 0, %if.then238 ], [ %wake_owners.1, %for.inc ]
  %95 = load %"struct.Halide::Runtime::Internal::work"*, %"struct.Halide::Runtime::Internal::work"** %parent_job134, align 8, !tbaa !46
  %cmp273 = icmp eq %"struct.Halide::Runtime::Internal::work"* %95, null
  %96 = load i32, i32* %min_threads138, align 4, !tbaa !37
  br i1 %cmp273, label %if.then274, label %if.else281

if.then274:                                       ; preds = %if.end271
  %97 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  %sub277 = sub nsw i32 %97, %96
  store i32 %sub277, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 16), align 4, !tbaa !61
  br label %if.end290

if.else281:                                       ; preds = %if.end271
  %threads_reserved285 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %95, i64 0, i32 6
  %98 = load i32, i32* %threads_reserved285, align 8, !tbaa !65
  %sub286 = sub nsw i32 %98, %96
  store i32 %sub286, i32* %threads_reserved285, align 8, !tbaa !65
  br label %if.end290

if.end290:                                        ; preds = %if.else281, %if.then274
  %99 = load i32, i32* %active_workers132, align 8, !tbaa !64
  %dec292 = add nsw i32 %99, -1
  store i32 %dec292, i32* %active_workers132, align 8, !tbaa !64
  %100 = and i8 %wake_owners.2, 1
  %tobool296.not = icmp eq i8 %100, 0
  br i1 %tobool296.not, label %lor.lhs.false297, label %if.then310

lor.lhs.false297:                                 ; preds = %if.end290
  %cmp299 = icmp eq i32 %dec292, 0
  br i1 %cmp299, label %land.lhs.true300, label %while.cond.backedge

land.lhs.true300:                                 ; preds = %lor.lhs.false297
  %extent302 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 0, i32 6
  %101 = load i32, i32* %extent302, align 8, !tbaa !32
  %cmp303 = icmp eq i32 %101, 0
  br i1 %cmp303, label %land.lhs.true307, label %lor.lhs.false304

lor.lhs.false304:                                 ; preds = %land.lhs.true300
  %exit_status305 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 9
  %102 = load i32, i32* %exit_status305, align 4, !tbaa !47
  %cmp306.not = icmp eq i32 %102, 0
  br i1 %cmp306.not, label %while.cond.backedge, label %land.lhs.true307

land.lhs.true307:                                 ; preds = %lor.lhs.false304, %land.lhs.true300
  %owner_is_sleeping308 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %job.1515, i64 0, i32 11
  %103 = load i8, i8* %owner_is_sleeping308, align 4, !tbaa !43, !range !21
  %tobool309.not = icmp eq i8 %103, 0
  br i1 %tobool309.not, label %while.cond.backedge, label %if.then310

if.then310:                                       ; preds = %land.lhs.true307, %if.end290
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 10)) #16
  br label %while.cond.backedge

while.cond.backedge:                              ; preds = %if.then310, %land.lhs.true307, %lor.lhs.false304, %lor.lhs.false297, %if.end124, %if.else108, %if.then107, %if.then15, %while.end
  %spin_count.0.be = phi i32 [ %spin_count.0, %while.end ], [ %spin_count.0, %if.then15 ], [ %inc, %if.then107 ], [ %inc, %if.else108 ], [ %spin_count.1, %if.end124 ], [ 0, %if.then310 ], [ 0, %land.lhs.true307 ], [ 0, %lor.lhs.false304 ], [ 0, %lor.lhs.false297 ]
  br label %while.cond, !llvm.loop !87

while.end316:                                     ; preds = %cond.end, %cond.false
  ret void
}

; Function Attrs: nounwind
define weak void @halide_mutex_unlock(%struct.halide_mutex* %mutex) local_unnamed_addr #4 {
entry:
  %control.i.i = alloca %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", align 8
  %state.i = getelementptr %struct.halide_mutex, %struct.halide_mutex* %mutex, i64 0, i32 0, i64 0
  %0 = cmpxchg weak i64* %state.i, i64 1, i64 0 release monotonic
  %1 = extractvalue { i64, i1 } %0, 1
  br i1 %1, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit, label %if.then.i

if.then.i:                                        ; preds = %entry
  %2 = cmpxchg i64* %state.i, i64 1, i64 0 release monotonic
  %3 = extractvalue { i64, i1 } %2, 1
  br i1 %3, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit, label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i
  %4 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %4) #11
  %5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 0, i32 0
  store i32 (...)** bitcast (i8** getelementptr inbounds ({ [6 x i8*] }, { [6 x i8*] }* @_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE, i64 0, inrange i32 0, i64 2) to i32 (...)**), i32 (...)*** %5, align 8, !tbaa !48
  %lock_state2.i.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 1
  store i64* %state.i, i64** %lock_state2.i.i.i, align 8, !tbaa !50
  %6 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 0
  %7 = ptrtoint %struct.halide_mutex* %mutex to i64
  %call3.i.i = call i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %6, i64 %7) #15
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %4) #11
  br label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit

_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit: ; preds = %if.end.i.i, %if.then.i, %entry
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, i64 %addr) local_unnamed_addr #0 align 2 {
entry:
  %call = tail call nonnull align 8 dereferenceable(24) %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* @_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy(i64 %addr) #16
  %head = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 1
  %0 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %head, align 8, !tbaa !14
  %tail = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 2
  %1 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::parking_control"* %this to i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)***
  %state.i60 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 0, i32 0
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 0
  br label %while.cond

while.cond:                                       ; preds = %cleanup, %entry
  %data_location.0 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** [ %head, %entry ], [ %data_location.1, %cleanup ]
  %prev.0 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ null, %entry ], [ %prev.1, %cleanup ]
  %data.0 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %0, %entry ], [ %data.1, %cleanup ]
  %retval.0 = phi i64 [ undef, %entry ], [ %retval.1, %cleanup ]
  %cmp.not = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.0, null
  br i1 %cmp.not, label %while.end22, label %while.body

while.body:                                       ; preds = %while.cond
  %sleep_address = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.0, i64 0, i32 1
  %2 = load atomic i64, i64* %sleep_address monotonic, align 8
  %cmp2 = icmp eq i64 %2, %addr
  %next3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.0, i64 0, i32 2
  %3 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next3, align 8, !tbaa !88
  br i1 %cmp2, label %if.then, label %cleanup

if.then:                                          ; preds = %while.body
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %3, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %data_location.0, align 8, !tbaa !14
  %4 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %tail, align 8, !tbaa !93
  %cmp4 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %4, %data.0
  br i1 %cmp4, label %if.then5, label %while.cond7.preheader

while.cond7.preheader:                            ; preds = %if.then
  %cmp872.not = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %3, null
  br i1 %cmp872.not, label %if.end, label %while.body9

if.then5:                                         ; preds = %if.then
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %prev.0, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %tail, align 8, !tbaa !93
  br label %if.end

while.body9:                                      ; preds = %while.cond7.preheader, %while.body9
  %data2.073 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %6, %while.body9 ], [ %3, %while.cond7.preheader ]
  %sleep_address10 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data2.073, i64 0, i32 1
  %5 = load atomic i64, i64* %sleep_address10 monotonic, align 8
  %cmp11 = icmp eq i64 %5, %addr
  %next12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data2.073, i64 0, i32 2
  %6 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next12, align 8, !tbaa !88
  %cmp8 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %6, null
  %7 = or i1 %cmp11, %cmp8
  br i1 %7, label %if.end.loopexit, label %while.body9, !llvm.loop !96

if.end.loopexit:                                  ; preds = %while.body9
  %frombool = zext i1 %cmp11 to i8
  br label %if.end

if.end:                                           ; preds = %if.end.loopexit, %if.then5, %while.cond7.preheader
  %more_waiters.1 = phi i8 [ 0, %if.then5 ], [ 0, %while.cond7.preheader ], [ %frombool, %if.end.loopexit ]
  %tobool13 = icmp ne i8 %more_waiters.1, 0
  %vtable = load i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)**, i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)*** %1, align 8, !tbaa !48
  %vfn = getelementptr inbounds i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)*, i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)** %vtable, i64 2
  %8 = load i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)*, i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)** %vfn, align 8
  %call14 = tail call i64 %8(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, i32 1, i1 zeroext %tobool13) #15
  %unpark_info = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.0, i64 0, i32 3
  store i64 %call14, i64* %unpark_info, align 8, !tbaa !97
  %mutex.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.0, i64 0, i32 0, i32 0
  %call.i = tail call i32 @pthread_mutex_lock(%struct.pthread_mutex_t* nonnull %mutex.i) #15
  %9 = atomicrmw and i64* %state.i60, i64 -2 release
  %and.i61 = and i64 %9, 2
  %cmp.i62 = icmp ne i64 %and.i61, 0
  %cmp3.not.i63 = icmp ult i64 %9, 4
  %or.cond.i64 = or i1 %cmp3.not.i63, %cmp.i62
  br i1 %or.cond.i64, label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66, label %if.then.i65

if.then.i65:                                      ; preds = %if.end
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex) #15
  br label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66

_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66: ; preds = %if.then.i65, %if.end
  %should_park.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.0, i64 0, i32 0, i32 2
  store i8 0, i8* %should_park.i, align 8, !tbaa !98
  %condvar.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.0, i64 0, i32 0, i32 1
  %call.i67 = tail call i32 @pthread_cond_signal(%struct.pthread_mutex_t* nonnull %condvar.i) #15
  %call.i69 = tail call i32 @pthread_mutex_unlock(%struct.pthread_mutex_t* nonnull %mutex.i) #15
  %10 = zext i8 %more_waiters.1 to i64
  br label %cleanup

cleanup:                                          ; preds = %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66, %while.body
  %data_location.1 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** [ %data_location.0, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66 ], [ %next3, %while.body ]
  %prev.1 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %prev.0, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66 ], [ %data.0, %while.body ]
  %data.1 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %data.0, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66 ], [ %3, %while.body ]
  %retval.1 = phi i64 [ %10, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit66 ], [ %retval.0, %while.body ]
  br i1 %cmp2, label %cleanup27, label %while.cond, !llvm.loop !99

while.end22:                                      ; preds = %while.cond
  %vtable23 = load i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)**, i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)*** %1, align 8, !tbaa !48
  %vfn24 = getelementptr inbounds i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)*, i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)** %vtable23, i64 2
  %11 = load i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)*, i64 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, i32, i1)** %vfn24, align 8
  %call25 = tail call i64 %11(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, i32 0, i1 zeroext false) #15
  %12 = atomicrmw and i64* %state.i60, i64 -2 release
  %and.i = and i64 %12, 2
  %cmp.i = icmp ne i64 %and.i, 0
  %cmp3.not.i = icmp ult i64 %12, 4
  %or.cond.i = or i1 %cmp3.not.i, %cmp.i
  br i1 %or.cond.i, label %cleanup27, label %if.then.i

if.then.i:                                        ; preds = %while.end22
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex) #15
  br label %cleanup27

cleanup27:                                        ; preds = %cleanup, %if.then.i, %while.end22
  %retval.2 = phi i64 [ 0, %while.end22 ], [ 0, %if.then.i ], [ %retval.1, %cleanup ]
  ret i64 %retval.2
}

; Function Attrs: nounwind mustprogress
define linkonce nonnull align 8 dereferenceable(24) %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* @_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy(i64 %addr) local_unnamed_addr #0 {
entry:
  %mul.i = mul i64 %addr, -7046029254386353131
  %shr.i = lshr i64 %mul.i, 54
  %arrayidx = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_table", %"struct.Halide::Runtime::Internal::Synchronization::hash_table"* @_ZN6Halide7Runtime8Internal15Synchronization5tableE, i64 0, i32 0, i64 %shr.i
  %state.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx, i64 0, i32 0, i32 0
  %0 = cmpxchg weak i64* %state.i, i64 0, i64 1 acquire monotonic
  %1 = extractvalue { i64, i1 } %0, 1
  br i1 %1, label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit, label %if.then.i

if.then.i:                                        ; preds = %entry
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex) #15
  br label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit

_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit: ; preds = %if.then.i, %entry
  ret %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx
}

declare i32 @pthread_mutex_lock(%struct.pthread_mutex_t*) local_unnamed_addr #1

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %this) local_unnamed_addr #4 align 2 {
entry:
  %state = getelementptr inbounds %"class.Halide::Runtime::Internal::Synchronization::word_lock", %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %this, i64 0, i32 0
  %0 = load atomic i64, i64* %state monotonic, align 8
  br label %while.cond

while.cond:                                       ; preds = %if.end, %entry
  %expected.0 = phi i64 [ %0, %entry ], [ %3, %if.end ]
  %and = and i64 %expected.0, 2
  %tobool.not = icmp ne i64 %and, 0
  %cmp = icmp ult i64 %expected.0, 4
  %or.cond = or i1 %cmp, %tobool.not
  br i1 %or.cond, label %cleanup75, label %if.end

if.end:                                           ; preds = %while.cond
  %or = or i64 %expected.0, 2
  %1 = cmpxchg weak i64* %state, i64 %expected.0, i64 %or acquire monotonic
  %2 = extractvalue { i64, i1 } %1, 1
  %3 = extractvalue { i64, i1 } %1, 0
  br i1 %2, label %while.cond11, label %while.cond

while.cond11:                                     ; preds = %if.end, %cleanup70
  %.pn.pn = phi { i64, i1 } [ %.pn, %cleanup70 ], [ %1, %if.end ]
  %expected.3 = extractvalue { i64, i1 } %.pn.pn, 0
  %and13 = and i64 %expected.3, -4
  %4 = inttoptr i64 %and13 to %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*
  %tail14 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %4, i64 0, i32 3
  %tail.0143 = load %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %tail14, align 8, !tbaa !100
  %cmp16144 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %tail.0143, null
  br i1 %cmp16144, label %while.body17, label %while.end23

while.body17:                                     ; preds = %while.cond11, %do.end
  %current.0145 = phi %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* [ %5, %do.end ], [ %4, %while.cond11 ]
  %next18 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %current.0145, i64 0, i32 1
  %5 = load %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %next18, align 8, !tbaa !102
  %cmp19.not = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %5, null
  br i1 %cmp19.not, label %if.then20, label %do.end

if.then20:                                        ; preds = %while.body17
  tail call void @halide_print(i8* null, i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.5, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %if.then20, %while.body17
  %prev = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %5, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %current.0145, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %prev, align 8, !tbaa !103
  %tail22 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %5, i64 0, i32 3
  %tail.0 = load %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %tail22, align 8, !tbaa !100
  %cmp16 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %tail.0, null
  br i1 %cmp16, label %while.body17, label %while.end23, !llvm.loop !104

while.end23:                                      ; preds = %do.end, %while.cond11
  %tail.0.lcssa = phi %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* [ %tail.0143, %while.cond11 ], [ %tail.0, %do.end ]
  store %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %tail.0.lcssa, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %tail14, align 8, !tbaa !100
  %and25 = and i64 %expected.3, 1
  %tobool26.not = icmp eq i64 %and25, 0
  br i1 %tobool26.not, label %if.end35, label %if.then27

if.then27:                                        ; preds = %while.end23
  %and29 = and i64 %expected.3, -3
  %6 = cmpxchg weak i64* %state, i64 %expected.3, i64 %and29 acq_rel monotonic
  %7 = extractvalue { i64, i1 } %6, 1
  br i1 %7, label %cleanup75, label %cleanup70

if.end35:                                         ; preds = %while.end23
  %prev36 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %tail.0.lcssa, i64 0, i32 2
  %8 = load %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %prev36, align 8, !tbaa !103
  %cmp37 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %8, null
  br i1 %cmp37, label %while.body41, label %if.else62

while.body41:                                     ; preds = %if.end35, %if.end47
  %expected.5146 = phi i64 [ %11, %if.end47 ], [ %expected.3, %if.end35 ]
  %and43 = and i64 %expected.5146, 1
  %9 = cmpxchg weak i64* %state, i64 %expected.5146, i64 %and43 acq_rel monotonic
  %10 = extractvalue { i64, i1 } %9, 1
  br i1 %10, label %if.end66, label %if.end47

if.end47:                                         ; preds = %while.body41
  %11 = extractvalue { i64, i1 } %9, 0
  %cmp49 = icmp ult i64 %11, 4
  br i1 %cmp49, label %while.body41, label %cleanup70, !llvm.loop !105

if.else62:                                        ; preds = %if.end35
  store %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %8, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %tail14, align 8, !tbaa !100
  %12 = atomicrmw and i64* %state, i64 -3 release
  br label %if.end66

if.end66:                                         ; preds = %while.body41, %if.else62
  %mutex.i103 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %tail.0.lcssa, i64 0, i32 0, i32 0
  %call.i104 = tail call i32 @pthread_mutex_lock(%struct.pthread_mutex_t* nonnull %mutex.i103) #15
  %should_park.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %tail.0.lcssa, i64 0, i32 0, i32 2
  store i8 0, i8* %should_park.i, align 8, !tbaa !98
  %condvar.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %tail.0.lcssa, i64 0, i32 0, i32 1
  %call.i101 = tail call i32 @pthread_cond_signal(%struct.pthread_mutex_t* nonnull %condvar.i) #15
  %call.i = tail call i32 @pthread_mutex_unlock(%struct.pthread_mutex_t* nonnull %mutex.i103) #15
  br label %cleanup75

cleanup70:                                        ; preds = %if.end47, %if.then27
  %.pn = phi { i64, i1 } [ %6, %if.then27 ], [ %9, %if.end47 ]
  fence acquire
  br label %while.cond11

cleanup75:                                        ; preds = %while.cond, %if.then27, %if.end66
  ret void
}

declare i32 @pthread_cond_signal(%struct.pthread_mutex_t*) local_unnamed_addr #1

declare i32 @pthread_mutex_unlock(%struct.pthread_mutex_t*) local_unnamed_addr #1

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %this) local_unnamed_addr #4 align 2 {
entry:
  %node = alloca %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", align 8
  %state = getelementptr inbounds %"class.Halide::Runtime::Internal::Synchronization::word_lock", %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %this, i64 0, i32 0
  %0 = load atomic i64, i64* %state monotonic, align 8
  %1 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %node to i8*
  %should_park.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %node, i64 0, i32 0, i32 2
  %mutex2.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %node, i64 0, i32 0, i32 0
  %condvar3.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %node, i64 0, i32 0, i32 1
  %next.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %node, i64 0, i32 1
  %tail.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data", %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %node, i64 0, i32 3
  %2 = ptrtoint %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %node to i64
  %3 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %next.i to <2 x %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*>*
  br label %while.cond.outer

while.cond.outer:                                 ; preds = %while.cond.outer.backedge, %entry
  %expected.0.ph = phi i64 [ %0, %entry ], [ %expected.0.ph.be, %while.cond.outer.backedge ]
  %spinner.sroa.0.0.ph = phi i32 [ 40, %entry ], [ %spinner.sroa.0.0.ph.be, %while.cond.outer.backedge ]
  %and46 = and i64 %expected.0.ph, 1
  %tobool.not47 = icmp eq i64 %and46, 0
  br i1 %tobool.not47, label %if.then, label %if.end4

if.then:                                          ; preds = %while.cond.outer, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit
  %expected.048 = phi i64 [ %6, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit ], [ %expected.0.ph, %while.cond.outer ]
  %or = or i64 %expected.048, 1
  %4 = cmpxchg weak i64* %state, i64 %expected.048, i64 %or acquire monotonic
  %5 = extractvalue { i64, i1 } %4, 1
  br i1 %5, label %cleanup23, label %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit

_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit: ; preds = %if.then
  %6 = extractvalue { i64, i1 } %4, 0
  %and = and i64 %6, 1
  %tobool.not = icmp eq i64 %and, 0
  br i1 %tobool.not, label %if.then, label %if.end4.loopexit

if.end4.loopexit:                                 ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit
  %7 = extractvalue { i64, i1 } %4, 0
  br label %if.end4

if.end4:                                          ; preds = %if.end4.loopexit, %while.cond.outer
  %expected.0.lcssa = phi i64 [ %expected.0.ph, %while.cond.outer ], [ %7, %if.end4.loopexit ]
  %cmp.not = icmp ugt i64 %expected.0.lcssa, 3
  %cmp.i = icmp sgt i32 %spinner.sroa.0.0.ph, 0
  %or.cond = and i1 %cmp.i, %cmp.not
  br i1 %or.cond, label %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit, label %if.end9

_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit: ; preds = %if.end4
  %cmp4.i = icmp sgt i32 %spinner.sroa.0.0.ph, 1
  br i1 %cmp4.i, label %if.then7, label %if.end9

if.then7:                                         ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit
  %dec.i = add nsw i32 %spinner.sroa.0.0.ph, -1
  call void @halide_thread_yield() #15
  %8 = load atomic i64, i64* %state monotonic, align 8
  br label %while.cond.outer.backedge

if.end9:                                          ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit, %if.end4
  %spinner.sroa.0.2 = phi i32 [ %spinner.sroa.0.0.ph, %if.end4 ], [ 0, %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit ]
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %1) #11
  store i8 0, i8* %should_park.i.i, align 8, !tbaa !98
  %call.i.i29 = call i32 @pthread_mutex_init(%struct.pthread_mutex_t* nonnull %mutex2.i.i, i8* null) #15
  %call4.i.i = call i32 @pthread_cond_init(%struct.pthread_mutex_t* nonnull %condvar3.i.i, i8* null) #15
  store <2 x %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*> zeroinitializer, <2 x %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*>* %3, align 8, !tbaa !14
  store %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* null, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %tail.i, align 8, !tbaa !100
  store i8 1, i8* %should_park.i.i, align 8, !tbaa !98
  %and10 = and i64 %expected.0.lcssa, -4
  %cmp11 = icmp eq i64 %and10, 0
  br i1 %cmp11, label %if.then12, label %if.else

if.then12:                                        ; preds = %if.end9
  store %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %node, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %tail.i, align 8, !tbaa !100
  br label %if.end13

if.else:                                          ; preds = %if.end9
  %9 = inttoptr i64 %and10 to %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"*
  store %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"* %9, %"struct.Halide::Runtime::Internal::Synchronization::word_lock_queue_data"** %next.i, align 8, !tbaa !102
  br label %if.end13

if.end13:                                         ; preds = %if.else, %if.then12
  %and15 = and i64 %expected.0.lcssa, 3
  %or16 = or i64 %and15, %2
  %10 = cmpxchg weak i64* %state, i64 %expected.0.lcssa, i64 %or16 release monotonic
  %11 = extractvalue { i64, i1 } %10, 1
  br i1 %11, label %if.then19, label %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_release_relaxedEPyS4_S4_.exit

_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_release_relaxedEPyS4_S4_.exit: ; preds = %if.end13
  %12 = extractvalue { i64, i1 } %10, 0
  br label %if.end22

if.then19:                                        ; preds = %if.end13
  %call.i = call i32 @pthread_mutex_lock(%struct.pthread_mutex_t* nonnull %mutex2.i.i) #15
  %13 = load i8, i8* %should_park.i.i, align 8, !tbaa !98, !range !21
  %tobool.not6.i = icmp eq i8 %13, 0
  br i1 %tobool.not6.i, label %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit, label %while.body.i

while.body.i:                                     ; preds = %if.then19, %while.body.i
  %call3.i = call i32 @pthread_cond_wait(%struct.pthread_mutex_t* nonnull %condvar3.i.i, %struct.pthread_mutex_t* nonnull %mutex2.i.i) #15
  %14 = load i8, i8* %should_park.i.i, align 8, !tbaa !98, !range !21
  %tobool.not.i = icmp eq i8 %14, 0
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit, label %while.body.i, !llvm.loop !106

_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit: ; preds = %while.body.i, %if.then19
  %call5.i = call i32 @pthread_mutex_unlock(%struct.pthread_mutex_t* nonnull %mutex2.i.i) #15
  %15 = load atomic i64, i64* %state monotonic, align 8
  br label %if.end22

if.end22:                                         ; preds = %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_release_relaxedEPyS4_S4_.exit
  %expected.3 = phi i64 [ %15, %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit ], [ %12, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_release_relaxedEPyS4_S4_.exit ]
  %spinner.sroa.0.3 = phi i32 [ 40, %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit ], [ %spinner.sroa.0.2, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_release_relaxedEPyS4_S4_.exit ]
  %call.i.i = call i32 @pthread_cond_destroy(%struct.pthread_mutex_t* nonnull %condvar3.i.i) #15
  %call2.i.i = call i32 @pthread_mutex_destroy(%struct.pthread_mutex_t* nonnull %mutex2.i.i) #15
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %1) #11
  br label %while.cond.outer.backedge

while.cond.outer.backedge:                        ; preds = %if.end22, %if.then7
  %expected.0.ph.be = phi i64 [ %8, %if.then7 ], [ %expected.3, %if.end22 ]
  %spinner.sroa.0.0.ph.be = phi i32 [ %dec.i, %if.then7 ], [ %spinner.sroa.0.3, %if.end22 ]
  br label %while.cond.outer

cleanup23:                                        ; preds = %if.then
  ret void
}

declare i32 @pthread_mutex_init(%struct.pthread_mutex_t*, i8*) local_unnamed_addr #1

declare i32 @pthread_cond_init(%struct.pthread_mutex_t*, i8*) local_unnamed_addr #1

declare i32 @pthread_cond_wait(%struct.pthread_mutex_t*, %struct.pthread_mutex_t*) local_unnamed_addr #1

declare i32 @pthread_cond_destroy(%struct.pthread_mutex_t*) local_unnamed_addr #1

declare i32 @pthread_mutex_destroy(%struct.pthread_mutex_t*) local_unnamed_addr #1

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr zeroext i1 @_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control8validateERNS2_15validate_actionE(%"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* nonnull dereferenceable(16) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action) unnamed_addr #2 align 2 {
entry:
  %lock_state = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %this, i64 0, i32 1
  %0 = load i64*, i64** %lock_state, align 8, !tbaa !50
  %1 = load atomic i64, i64* %0 monotonic, align 8
  %cmp = icmp eq i64 %1, 3
  ret i1 %cmp
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr void @_ZN6Halide7Runtime8Internal15Synchronization15parking_control12before_sleepEv(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this) unnamed_addr #2 align 2 {
entry:
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr i64 @_ZN6Halide7Runtime8Internal15Synchronization21mutex_parking_control6unparkEib(%"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* nonnull dereferenceable(16) %this, i32 %unparked, i1 zeroext %more_waiters) unnamed_addr #2 align 2 {
entry:
  %0 = select i1 %more_waiters, i64 2, i64 0
  %lock_state = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %this, i64 0, i32 1
  %1 = load i64*, i64** %lock_state, align 8, !tbaa !50
  store atomic i64 %0, i64* %1 release, align 8
  ret i64 0
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr void @_ZN6Halide7Runtime8Internal15Synchronization15parking_control16requeue_callbackERKNS2_15validate_actionEbb(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action, i1 zeroext %one_to_wake, i1 zeroext %some_requeued) unnamed_addr #2 align 2 {
entry:
  ret void
}

; Function Attrs: nounwind
define weak void @halide_cond_broadcast(%struct.halide_mutex* %cond) local_unnamed_addr #4 {
entry:
  %control.i = alloca %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", align 8
  %state.i = getelementptr %struct.halide_mutex, %struct.halide_mutex* %cond, i64 0, i32 0, i64 0
  %0 = load atomic i64, i64* %state.i monotonic, align 8
  %cmp.i = icmp eq i64 %0, 0
  br i1 %cmp.i, label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond9broadcastEv.exit, label %if.end.i

if.end.i:                                         ; preds = %entry
  %1 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 24, i8* nonnull %1) #11
  %2 = inttoptr i64 %0 to %"class.Halide::Runtime::Internal::Synchronization::word_lock"*
  %3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 0, i32 0
  store i32 (...)** bitcast (i8** getelementptr inbounds ({ [6 x i8*] }, { [6 x i8*] }* @_ZTVN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE, i64 0, inrange i32 0, i64 2) to i32 (...)**), i32 (...)*** %3, align 8, !tbaa !48
  %cond_state2.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 1
  store i64* %state.i, i64** %cond_state2.i.i, align 8, !tbaa !107
  %mutex3.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 2
  store %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %2, %"class.Halide::Runtime::Internal::Synchronization::word_lock"** %mutex3.i.i, align 8, !tbaa !109
  %4 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 0
  %5 = ptrtoint %struct.halide_mutex* %cond to i64
  %call.i = call i32 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %4, i64 %5, i64 %0, i64 0) #15
  call void @llvm.lifetime.end.p0i8(i64 24, i8* nonnull %1) #11
  br label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond9broadcastEv.exit

_ZN6Halide7Runtime8Internal15Synchronization9fast_cond9broadcastEv.exit: ; preds = %if.end.i, %entry
  ret void
}

; Function Attrs: nounwind mustprogress
define weak zeroext i1 @halide_default_semaphore_try_acquire(%struct.halide_semaphore_t* %s, i32 %n) #0 {
entry:
  %cmp = icmp eq i32 %n, 0
  br i1 %cmp, label %return, label %if.end

if.end:                                           ; preds = %entry
  %value = bitcast %struct.halide_semaphore_t* %s to i32*
  %0 = load atomic i32, i32* %value acquire, align 4
  %sub12 = sub nsw i32 %0, %n
  %cmp113 = icmp sgt i32 %sub12, -1
  br i1 %cmp113, label %land.rhs, label %return

land.rhs:                                         ; preds = %if.end, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_130atomic_cas_weak_relacq_relaxedIiEEbPT_S6_S6_.exit
  %sub15 = phi i32 [ %sub, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_130atomic_cas_weak_relacq_relaxedIiEEbPT_S6_S6_.exit ], [ %sub12, %if.end ]
  %expected.014 = phi i32 [ %3, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_130atomic_cas_weak_relacq_relaxedIiEEbPT_S6_S6_.exit ], [ %0, %if.end ]
  %1 = cmpxchg weak i32* %value, i32 %expected.014, i32 %sub15 acq_rel monotonic
  %2 = extractvalue { i32, i1 } %1, 1
  br i1 %2, label %return.loopexit, label %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_130atomic_cas_weak_relacq_relaxedIiEEbPT_S6_S6_.exit

_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_130atomic_cas_weak_relacq_relaxedIiEEbPT_S6_S6_.exit: ; preds = %land.rhs
  %3 = extractvalue { i32, i1 } %1, 0
  %sub = sub nsw i32 %3, %n
  %cmp1 = icmp sgt i32 %sub, -1
  br i1 %cmp1, label %land.rhs, label %return.loopexit

return.loopexit:                                  ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_130atomic_cas_weak_relacq_relaxedIiEEbPT_S6_S6_.exit, %land.rhs
  %4 = extractvalue { i32, i1 } %1, 1
  br label %return

return:                                           ; preds = %return.loopexit, %if.end, %entry
  %retval.0 = phi i1 [ true, %entry ], [ false, %if.end ], [ %4, %return.loopexit ]
  ret i1 %retval.0
}

; Function Attrs: nounwind
define weak void @halide_cond_wait(%struct.halide_mutex* %cond, %struct.halide_mutex* %mutex) local_unnamed_addr #4 {
entry:
  %control.i.i.i = alloca %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", align 8
  %control.i = alloca %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", align 8
  %0 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 24, i8* nonnull %0) #11
  %state.i = getelementptr %struct.halide_mutex, %struct.halide_mutex* %cond, i64 0, i32 0, i64 0
  %1 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 0, i32 0
  store i32 (...)** bitcast (i8** getelementptr inbounds ({ [6 x i8*] }, { [6 x i8*] }* @_ZTVN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE, i64 0, inrange i32 0, i64 2) to i32 (...)**), i32 (...)*** %1, align 8, !tbaa !48
  %cond_state2.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 1
  store i64* %state.i, i64** %cond_state2.i.i, align 8, !tbaa !110
  %mutex3.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 2
  %2 = bitcast %"class.Halide::Runtime::Internal::Synchronization::word_lock"** %mutex3.i.i to %struct.halide_mutex**
  store %struct.halide_mutex* %mutex, %struct.halide_mutex** %2, align 8, !tbaa !112
  %3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 0
  %4 = ptrtoint %struct.halide_mutex* %cond to i64
  %call.i = call i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %3, i64 %4) #15
  %5 = ptrtoint %struct.halide_mutex* %mutex to i64
  %cmp.not.i = icmp eq i64 %call.i, %5
  %6 = getelementptr %struct.halide_mutex, %struct.halide_mutex* %mutex, i64 0, i32 0, i64 0
  br i1 %cmp.not.i, label %if.else.i, label %if.then.i

if.then.i:                                        ; preds = %entry
  %7 = cmpxchg weak i64* %6, i64 0, i64 1 acquire monotonic
  %8 = extractvalue { i64, i1 } %7, 1
  br i1 %8, label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond4waitEPNS2_10fast_mutexE.exit, label %if.then.i.i

if.then.i.i:                                      ; preds = %if.then.i
  %9 = load atomic i64, i64* %6 monotonic, align 8
  %10 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i.i to i8*
  %11 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i.i, i64 0, i32 0, i32 0
  %lock_state2.i.i.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i.i, i64 0, i32 1
  %12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i.i, i64 0, i32 0
  br label %while.cond.outer.i.i.i

while.cond.outer.i.i.i:                           ; preds = %while.cond.outer.i.i.i.backedge, %if.then.i.i
  %expected.0.ph.i.i.i = phi i64 [ %9, %if.then.i.i ], [ %expected.0.ph.i.i.i.be, %while.cond.outer.i.i.i.backedge ]
  %spinner.sroa.0.0.ph.i.i.i = phi i32 [ 40, %if.then.i.i ], [ %spinner.sroa.0.0.ph.i.i.i.be, %while.cond.outer.i.i.i.backedge ]
  %and55.i.i.i = and i64 %expected.0.ph.i.i.i, 1
  %tobool.not56.i.i.i = icmp eq i64 %and55.i.i.i, 0
  br i1 %tobool.not56.i.i.i, label %if.then.i.i.i, label %if.end4.i.i.i

if.then.i.i.i:                                    ; preds = %while.cond.outer.i.i.i, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i.i
  %expected.057.i.i.i = phi i64 [ %15, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i.i ], [ %expected.0.ph.i.i.i, %while.cond.outer.i.i.i ]
  %or.i.i.i = or i64 %expected.057.i.i.i, 1
  %13 = cmpxchg weak i64* %6, i64 %expected.057.i.i.i, i64 %or.i.i.i acquire monotonic
  %14 = extractvalue { i64, i1 } %13, 1
  br i1 %14, label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond4waitEPNS2_10fast_mutexE.exit, label %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i.i

_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i.i: ; preds = %if.then.i.i.i
  %15 = extractvalue { i64, i1 } %13, 0
  %and.i.i.i = and i64 %15, 1
  %tobool.not.i.i.i = icmp eq i64 %and.i.i.i, 0
  br i1 %tobool.not.i.i.i, label %if.then.i.i.i, label %if.end4.i.i.i.loopexit

if.end4.i.i.i.loopexit:                           ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_acquire_relaxedEPyS4_S4_.exit.i.i.i
  %16 = extractvalue { i64, i1 } %13, 0
  br label %if.end4.i.i.i

if.end4.i.i.i:                                    ; preds = %if.end4.i.i.i.loopexit, %while.cond.outer.i.i.i
  %expected.0.lcssa.i.i.i = phi i64 [ %expected.0.ph.i.i.i, %while.cond.outer.i.i.i ], [ %16, %if.end4.i.i.i.loopexit ]
  %cmp.i.i.i.i = icmp sgt i32 %spinner.sroa.0.0.ph.i.i.i, 0
  br i1 %cmp.i.i.i.i, label %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i.i, label %if.end8.i.i.i

_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i.i: ; preds = %if.end4.i.i.i
  %cmp4.i.not.i.i.i = icmp eq i32 %spinner.sroa.0.0.ph.i.i.i, 1
  br i1 %cmp4.i.not.i.i.i, label %if.end8.i.i.i, label %if.then6.i.i.i

if.then6.i.i.i:                                   ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i.i
  %dec.i.i.i.i = add nsw i32 %spinner.sroa.0.0.ph.i.i.i, -1
  call void @halide_thread_yield() #15
  %17 = load atomic i64, i64* %6 monotonic, align 8
  br label %while.cond.outer.i.i.i.backedge

if.end8.i.i.i:                                    ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i.i, %if.end4.i.i.i
  %spinner.sroa.0.152.i.i.i = phi i32 [ 0, %_ZN6Halide7Runtime8Internal15Synchronization12spin_control11should_spinEv.exit.i.i.i ], [ %spinner.sroa.0.0.ph.i.i.i, %if.end4.i.i.i ]
  %and9.i.i.i = and i64 %expected.0.lcssa.i.i.i, 2
  %cmp.i.i.i = icmp eq i64 %and9.i.i.i, 0
  br i1 %cmp.i.i.i, label %if.then10.i.i.i, label %if.end19.i.i.i

if.then10.i.i.i:                                  ; preds = %if.end8.i.i.i
  %or12.i.i.i = or i64 %expected.0.lcssa.i.i.i, 2
  %18 = cmpxchg weak i64* %6, i64 %expected.0.lcssa.i.i.i, i64 %or12.i.i.i monotonic monotonic
  %19 = extractvalue { i64, i1 } %18, 1
  br i1 %19, label %if.end19.i.i.i, label %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i.i

_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i.i: ; preds = %if.then10.i.i.i
  %20 = extractvalue { i64, i1 } %18, 0
  br label %while.cond.outer.i.i.i.backedge

if.end19.i.i.i:                                   ; preds = %if.then10.i.i.i, %if.end8.i.i.i
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %10) #11
  store i32 (...)** bitcast (i8** getelementptr inbounds ({ [6 x i8*] }, { [6 x i8*] }* @_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE, i64 0, inrange i32 0, i64 2) to i32 (...)**), i32 (...)*** %11, align 8, !tbaa !48
  store i64* %6, i64** %lock_state2.i.i.i.i, align 8, !tbaa !50
  %call21.i.i.i = call i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %12, i64 %5) #15
  %cmp22.i.i.i = icmp eq i64 %call21.i.i.i, %5
  br i1 %cmp22.i.i.i, label %cleanup30.critedge.i.i.i, label %if.end24.i.i.i

if.end24.i.i.i:                                   ; preds = %if.end19.i.i.i
  %21 = load atomic i64, i64* %6 monotonic, align 8
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %10) #11
  br label %while.cond.outer.i.i.i.backedge

while.cond.outer.i.i.i.backedge:                  ; preds = %if.end24.i.i.i, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i.i, %if.then6.i.i.i
  %expected.0.ph.i.i.i.be = phi i64 [ %20, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i.i ], [ %21, %if.end24.i.i.i ], [ %17, %if.then6.i.i.i ]
  %spinner.sroa.0.0.ph.i.i.i.be = phi i32 [ %spinner.sroa.0.152.i.i.i, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i.i.i ], [ 40, %if.end24.i.i.i ], [ %dec.i.i.i.i, %if.then6.i.i.i ]
  br label %while.cond.outer.i.i.i

cleanup30.critedge.i.i.i:                         ; preds = %if.end19.i.i.i
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %10) #11
  br label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond4waitEPNS2_10fast_mutexE.exit

if.else.i:                                        ; preds = %entry
  %22 = load atomic i64, i64* %6 monotonic, align 8
  %and.i = and i64 %22, 1
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %if.then2.i, label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond4waitEPNS2_10fast_mutexE.exit

if.then2.i:                                       ; preds = %if.else.i
  call void @halide_print(i8* null, i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.5.6, i64 0, i64 0)) #15
  call void @abort() #15
  br label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond4waitEPNS2_10fast_mutexE.exit

_ZN6Halide7Runtime8Internal15Synchronization9fast_cond4waitEPNS2_10fast_mutexE.exit: ; preds = %if.then.i.i.i, %if.then2.i, %if.else.i, %cleanup30.critedge.i.i.i, %if.then.i
  call void @llvm.lifetime.end.p0i8(i64 24, i8* nonnull %0) #11
  ret void
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_do_loop_task(i8* %user_context, i32 (i8*, i32, i32, i8*, i8*)* %f, i32 %min, i32 %size, i8* %closure, i8* %task_parent) local_unnamed_addr #0 {
entry:
  %0 = load i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)*, i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)** @_ZN6Halide7Runtime8Internal19custom_do_loop_taskE, align 8, !tbaa !14
  %call = tail call i32 %0(i8* %user_context, i32 (i8*, i32, i32, i8*, i8*)* %f, i32 %min, i32 %size, i8* %closure, i8* %task_parent) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_do_task(i8* %user_context, i32 (i8*, i32, i8*)* %f, i32 %idx, i8* %closure) local_unnamed_addr #0 {
entry:
  %0 = load i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)*, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)** @_ZN6Halide7Runtime8Internal14custom_do_taskE, align 8, !tbaa !14
  %call = tail call i32 %0(i8* %user_context, i32 (i8*, i32, i8*)* %f, i32 %idx, i8* %closure) #15
  ret i32 %call
}

; Function Attrs: nounwind
define linkonce i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control4parkEy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, i64 %addr) local_unnamed_addr #4 align 2 {
entry:
  %queue_data = alloca %"struct.Halide::Runtime::Internal::Synchronization::queue_data", align 8
  %action = alloca %"struct.Halide::Runtime::Internal::Synchronization::validate_action", align 8
  %0 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data to i8*
  call void @llvm.lifetime.start.p0i8(i64 160, i8* nonnull %0) #11
  %should_park.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data, i64 0, i32 0, i32 2
  store i8 0, i8* %should_park.i.i, align 8, !tbaa !98
  %mutex2.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data, i64 0, i32 0, i32 0
  %call.i.i = call i32 @pthread_mutex_init(%struct.pthread_mutex_t* nonnull %mutex2.i.i, i8* null) #15
  %condvar3.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data, i64 0, i32 0, i32 1
  %call4.i.i = call i32 @pthread_cond_init(%struct.pthread_mutex_t* nonnull %condvar3.i.i, i8* null) #15
  %sleep_address.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data, i64 0, i32 1
  store i64 0, i64* %sleep_address.i, align 8, !tbaa !113
  %next.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* null, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next.i, align 8, !tbaa !88
  %unpark_info.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data, i64 0, i32 3
  store i64 0, i64* %unpark_info.i, align 8, !tbaa !97
  %call = call nonnull align 8 dereferenceable(24) %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* @_ZN6Halide7Runtime8Internal15Synchronization11lock_bucketEy(i64 %addr) #16
  %1 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::validate_action", %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* %action, i64 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %1) #11
  store i8 0, i8* %1, align 8, !tbaa !114
  %invalid_unpark_info.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::validate_action", %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* %action, i64 0, i32 2
  store i64 0, i64* %invalid_unpark_info.i, align 8, !tbaa !116
  %2 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::parking_control"* %this to i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)***
  %vtable = load i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)**, i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)*** %2, align 8, !tbaa !48
  %3 = load i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)*, i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)** %vtable, align 8
  %call2 = call zeroext i1 %3(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action) #15
  br i1 %call2, label %if.end, label %if.then

if.then:                                          ; preds = %entry
  %state.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 0, i32 0
  %4 = atomicrmw and i64* %state.i, i64 -2 release
  %and.i = and i64 %4, 2
  %cmp.i = icmp ne i64 %and.i, 0
  %cmp3.not.i = icmp ult i64 %4, 4
  %or.cond.i = or i1 %cmp3.not.i, %cmp.i
  br i1 %or.cond.i, label %cleanup, label %if.then.i

if.then.i:                                        ; preds = %if.then
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 0
  call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex) #15
  br label %cleanup

if.end:                                           ; preds = %entry
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* null, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next.i, align 8, !tbaa !88
  store i64 %addr, i64* %sleep_address.i, align 8, !tbaa !113
  store i8 1, i8* %should_park.i.i, align 8, !tbaa !98
  %head = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 1
  %5 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %head, align 8, !tbaa !117
  %cmp.not = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %5, null
  %tail = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 2
  %6 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %tail, align 8
  %next4 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %6, i64 0, i32 2
  %head.sink = select i1 %cmp.not, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %head, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next4
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %head.sink, align 8, !tbaa !14
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %queue_data, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %tail, align 8, !tbaa !93
  %state.i23 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 0, i32 0
  %7 = atomicrmw and i64* %state.i23, i64 -2 release
  %and.i24 = and i64 %7, 2
  %cmp.i25 = icmp ne i64 %and.i24, 0
  %cmp3.not.i26 = icmp ult i64 %7, 4
  %or.cond.i27 = or i1 %cmp3.not.i26, %cmp.i25
  br i1 %or.cond.i27, label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit29, label %if.then.i28

if.then.i28:                                      ; preds = %if.end
  %mutex8 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %call, i64 0, i32 0
  call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex8) #15
  br label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit29

_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit29: ; preds = %if.then.i28, %if.end
  %8 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::parking_control"* %this to void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)***
  %vtable9 = load void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)**, void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)*** %8, align 8, !tbaa !48
  %vfn10 = getelementptr inbounds void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)*, void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)** %vtable9, i64 1
  %9 = load void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)*, void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*)** %vfn10, align 8
  call void %9(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this) #15
  %call.i = call i32 @pthread_mutex_lock(%struct.pthread_mutex_t* nonnull %mutex2.i.i) #15
  %10 = load i8, i8* %should_park.i.i, align 8, !tbaa !98, !range !21
  %tobool.not6.i = icmp eq i8 %10, 0
  br i1 %tobool.not6.i, label %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit, label %while.body.i

while.body.i:                                     ; preds = %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit29, %while.body.i
  %call3.i = call i32 @pthread_cond_wait(%struct.pthread_mutex_t* nonnull %condvar3.i.i, %struct.pthread_mutex_t* nonnull %mutex2.i.i) #15
  %11 = load i8, i8* %should_park.i.i, align 8, !tbaa !98, !range !21
  %tobool.not.i = icmp eq i8 %11, 0
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit, label %while.body.i, !llvm.loop !106

_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit: ; preds = %while.body.i, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit29
  %call5.i = call i32 @pthread_mutex_unlock(%struct.pthread_mutex_t* nonnull %mutex2.i.i) #15
  br label %cleanup

cleanup:                                          ; preds = %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit, %if.then.i, %if.then
  %unpark_info.i.sink = phi i64* [ %unpark_info.i, %_ZN6Halide7Runtime8Internal15Synchronization13thread_parker4parkEv.exit ], [ %invalid_unpark_info.i, %if.then ], [ %invalid_unpark_info.i, %if.then.i ]
  %12 = load i64, i64* %unpark_info.i.sink, align 8, !tbaa !22
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %1) #11
  %call.i.i22 = call i32 @pthread_cond_destroy(%struct.pthread_mutex_t* nonnull %condvar3.i.i) #15
  %call2.i.i = call i32 @pthread_mutex_destroy(%struct.pthread_mutex_t* nonnull %mutex2.i.i) #15
  call void @llvm.lifetime.end.p0i8(i64 160, i8* nonnull %0) #11
  ret i64 %12
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr zeroext i1 @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control8validateERNS2_15validate_actionE(%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* nonnull dereferenceable(24) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action) unnamed_addr #2 align 2 {
entry:
  %cond_state = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %this, i64 0, i32 1
  %0 = load i64*, i64** %cond_state, align 8, !tbaa !110
  %1 = load atomic i64, i64* %0 monotonic, align 8
  %cmp = icmp eq i64 %1, 0
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %this, i64 0, i32 2
  %2 = load %"class.Halide::Runtime::Internal::Synchronization::word_lock"*, %"class.Halide::Runtime::Internal::Synchronization::word_lock"** %mutex, align 8, !tbaa !112
  %3 = ptrtoint %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %2 to i64
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  store atomic i64 %3, i64* %0 monotonic, align 8
  br label %cleanup

if.else:                                          ; preds = %entry
  %cmp4.not = icmp eq i64 %1, %3
  br i1 %cmp4.not, label %cleanup, label %if.then5

if.then5:                                         ; preds = %if.else
  %invalid_unpark_info = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::validate_action", %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* %action, i64 0, i32 2
  store i64 %3, i64* %invalid_unpark_info, align 8, !tbaa !116
  br label %cleanup

cleanup:                                          ; preds = %if.then5, %if.else, %if.then
  %retval.0 = phi i1 [ false, %if.then5 ], [ true, %if.else ], [ true, %if.then ]
  ret i1 %retval.0
}

; Function Attrs: nounwind
define linkonce_odr void @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control12before_sleepEv(%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* nonnull dereferenceable(24) %this) unnamed_addr #4 align 2 {
entry:
  %control.i.i = alloca %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", align 8
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %this, i64 0, i32 2
  %0 = load %"class.Halide::Runtime::Internal::Synchronization::word_lock"*, %"class.Halide::Runtime::Internal::Synchronization::word_lock"** %mutex, align 8, !tbaa !112
  %state.i = getelementptr inbounds %"class.Halide::Runtime::Internal::Synchronization::word_lock", %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %0, i64 0, i32 0
  %1 = cmpxchg weak i64* %state.i, i64 1, i64 0 release monotonic
  %2 = extractvalue { i64, i1 } %1, 1
  br i1 %2, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit, label %if.then.i

if.then.i:                                        ; preds = %entry
  %3 = cmpxchg i64* %state.i, i64 1, i64 0 release monotonic
  %4 = extractvalue { i64, i1 } %3, 1
  br i1 %4, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit, label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i
  %5 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %5) #11
  %6 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 0, i32 0
  store i32 (...)** bitcast (i8** getelementptr inbounds ({ [6 x i8*] }, { [6 x i8*] }* @_ZTVN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE, i64 0, inrange i32 0, i64 2) to i32 (...)**), i32 (...)*** %6, align 8, !tbaa !48
  %lock_state2.i.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 1
  store i64* %state.i, i64** %lock_state2.i.i.i, align 8, !tbaa !50
  %7 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::mutex_parking_control"* %control.i.i, i64 0, i32 0
  %8 = ptrtoint %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %0 to i64
  %call3.i.i = call i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %7, i64 %8) #15
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %5) #11
  br label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit

_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex6unlockEv.exit: ; preds = %if.end.i.i, %if.then.i, %entry
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr i64 @_ZN6Halide7Runtime8Internal15Synchronization20wait_parking_control6unparkEib(%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* nonnull dereferenceable(24) %this, i32 %unparked, i1 zeroext %more_waiters) unnamed_addr #2 align 2 {
entry:
  br i1 %more_waiters, label %if.end, label %if.then

if.then:                                          ; preds = %entry
  %cond_state = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %this, i64 0, i32 1
  %0 = load i64*, i64** %cond_state, align 8, !tbaa !110
  store atomic i64 0, i64* %0 monotonic, align 8
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  ret i64 0
}

; Function Attrs: nounwind
define linkonce i32 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control14unpark_requeueEyyy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, i64 %addr_from, i64 %addr_to, i64 %unpark_info) local_unnamed_addr #4 align 2 {
entry:
  %buckets = alloca %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair", align 8
  %action = alloca %"struct.Halide::Runtime::Internal::Synchronization::validate_action", align 8
  %0 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* %buckets to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %0) #11
  call void @_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy(%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* nonnull sret(%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair") align 8 %buckets, i64 %addr_from, i64 %addr_to) #16
  %1 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::validate_action", %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* %action, i64 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %1) #11
  store i8 0, i8* %1, align 8, !tbaa !114
  %invalid_unpark_info.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::validate_action", %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* %action, i64 0, i32 2
  store i64 0, i64* %invalid_unpark_info.i, align 8, !tbaa !116
  %2 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::parking_control"* %this to i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)***
  %vtable = load i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)**, i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)*** %2, align 8, !tbaa !48
  %3 = load i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)*, i1 (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*)** %vtable, align 8
  %call = call zeroext i1 %3(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action) #15
  br i1 %call, label %if.end, label %if.then

if.then:                                          ; preds = %entry
  call void @_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE(%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* nonnull align 8 dereferenceable(16) %buckets) #16
  br label %cleanup

if.end:                                           ; preds = %entry
  %from = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair", %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* %buckets, i64 0, i32 0
  %4 = load %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"*, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %from, align 8, !tbaa !118
  %head = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %4, i64 0, i32 1
  %5 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %head, align 8, !tbaa !14
  %cmp.not92 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %5, null
  br i1 %cmp.not92, label %if.end38, label %while.body

while.body:                                       ; preds = %if.end, %if.end22
  %wakeup.098 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %wakeup.2, %if.end22 ], [ null, %if.end ]
  %requeue_tail.097 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %requeue_tail.2, %if.end22 ], [ null, %if.end ]
  %requeue.096 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %requeue.3, %if.end22 ], [ null, %if.end ]
  %data.095 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %7, %if.end22 ], [ %5, %if.end ]
  %prev.094 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %prev.1, %if.end22 ], [ null, %if.end ]
  %data_location.093 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** [ %data_location.1, %if.end22 ], [ %head, %if.end ]
  %sleep_address = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.095, i64 0, i32 1
  %6 = load atomic i64, i64* %sleep_address monotonic, align 8
  %next2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.095, i64 0, i32 2
  %7 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next2, align 8, !tbaa !88
  %cmp3 = icmp eq i64 %6, %addr_from
  br i1 %cmp3, label %if.then4, label %if.end22

if.then4:                                         ; preds = %while.body
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %7, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %data_location.093, align 8, !tbaa !14
  %8 = load %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"*, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %from, align 8, !tbaa !118
  %tail = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %8, i64 0, i32 2
  %9 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %tail, align 8, !tbaa !93
  %cmp6 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %9, %data.095
  br i1 %cmp6, label %if.then7, label %if.end10

if.then7:                                         ; preds = %if.then4
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %prev.094, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %tail, align 8, !tbaa !93
  br label %if.end10

if.end10:                                         ; preds = %if.then7, %if.then4
  %10 = load i8, i8* %1, align 8, !tbaa !114, !range !21
  %tobool = icmp ne i8 %10, 0
  %cmp11 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %wakeup.098, null
  %or.cond = and i1 %cmp11, %tobool
  br i1 %or.cond, label %if.end22, label %if.else

if.else:                                          ; preds = %if.end10
  %cmp13 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %requeue.096, null
  br i1 %cmp13, label %if.end17, label %if.else15

if.else15:                                        ; preds = %if.else
  %next16 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %requeue_tail.097, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %data.095, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next16, align 8, !tbaa !88
  br label %if.end17

if.end17:                                         ; preds = %if.else15, %if.else
  %requeue.1 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %requeue.096, %if.else15 ], [ %data.095, %if.else ]
  store atomic i64 %addr_to, i64* %sleep_address monotonic, align 8
  br label %if.end22

if.end22:                                         ; preds = %if.end17, %if.end10, %while.body
  %data_location.1 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** [ %data_location.093, %if.end10 ], [ %data_location.093, %if.end17 ], [ %next2, %while.body ]
  %prev.1 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %prev.094, %if.end10 ], [ %prev.094, %if.end17 ], [ %data.095, %while.body ]
  %requeue.3 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %requeue.096, %if.end10 ], [ %requeue.1, %if.end17 ], [ %requeue.096, %while.body ]
  %requeue_tail.2 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %requeue_tail.097, %if.end10 ], [ %data.095, %if.end17 ], [ %requeue_tail.097, %while.body ]
  %wakeup.2 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %data.095, %if.end10 ], [ %wakeup.098, %if.end17 ], [ %wakeup.098, %while.body ]
  %cmp.not = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %7, null
  br i1 %cmp.not, label %while.end, label %while.body, !llvm.loop !120

while.end:                                        ; preds = %if.end22
  %cmp23.not = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %requeue.3, null
  br i1 %cmp23.not, label %if.end38, label %if.then24

if.then24:                                        ; preds = %while.end
  %next25 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %requeue_tail.2, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* null, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next25, align 8, !tbaa !88
  %to = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair", %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* %buckets, i64 0, i32 1
  %11 = load %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"*, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %to, align 8, !tbaa !121
  %head26 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %11, i64 0, i32 1
  %12 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %head26, align 8, !tbaa !117
  %cmp27 = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %12, null
  br i1 %cmp27, label %if.end35, label %if.else31

if.else31:                                        ; preds = %if.then24
  %tail33 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %11, i64 0, i32 2
  %13 = load %"struct.Halide::Runtime::Internal::Synchronization::queue_data"*, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %tail33, align 8, !tbaa !93
  %next34 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %13, i64 0, i32 2
  br label %if.end35

if.end35:                                         ; preds = %if.else31, %if.then24
  %next34.sink = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** [ %next34, %if.else31 ], [ %head26, %if.then24 ]
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %requeue.3, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %next34.sink, align 8, !tbaa !14
  %tail37 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %11, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %requeue_tail.2, %"struct.Halide::Runtime::Internal::Synchronization::queue_data"** %tail37, align 8, !tbaa !93
  br label %if.end38

if.end38:                                         ; preds = %if.end35, %while.end, %if.end
  %cmp23106 = phi i1 [ true, %if.end35 ], [ false, %while.end ], [ false, %if.end ]
  %wakeup.0.lcssa105 = phi %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* [ %wakeup.2, %if.end35 ], [ %wakeup.2, %while.end ], [ null, %if.end ]
  %cmp39 = icmp ne %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %wakeup.0.lcssa105, null
  %14 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::parking_control"* %this to void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)***
  %vtable41 = load void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)**, void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)*** %14, align 8, !tbaa !48
  %vfn42 = getelementptr inbounds void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)*, void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)** %vtable41, i64 3
  %15 = load void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)*, void (%"struct.Halide::Runtime::Internal::Synchronization::parking_control"*, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"*, i1, i1)** %vfn42, align 8
  call void %15(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action, i1 zeroext %cmp39, i1 zeroext %cmp23106) #15
  br i1 %cmp39, label %if.then44, label %if.else48

if.then44:                                        ; preds = %if.end38
  %unpark_info45 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %wakeup.0.lcssa105, i64 0, i32 3
  store i64 %unpark_info, i64* %unpark_info45, align 8, !tbaa !97
  %mutex.i89 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %wakeup.0.lcssa105, i64 0, i32 0, i32 0
  %call.i90 = call i32 @pthread_mutex_lock(%struct.pthread_mutex_t* nonnull %mutex.i89) #15
  call void @_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE(%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* nonnull align 8 dereferenceable(16) %buckets) #16
  %should_park.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %wakeup.0.lcssa105, i64 0, i32 0, i32 2
  store i8 0, i8* %should_park.i, align 8, !tbaa !98
  %condvar.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::queue_data", %"struct.Halide::Runtime::Internal::Synchronization::queue_data"* %wakeup.0.lcssa105, i64 0, i32 0, i32 1
  %call.i88 = call i32 @pthread_cond_signal(%struct.pthread_mutex_t* nonnull %condvar.i) #15
  %call.i = call i32 @pthread_mutex_unlock(%struct.pthread_mutex_t* nonnull %mutex.i89) #15
  br label %if.end49

if.else48:                                        ; preds = %if.end38
  call void @_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE(%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* nonnull align 8 dereferenceable(16) %buckets) #16
  br label %if.end49

if.end49:                                         ; preds = %if.else48, %if.then44
  %16 = load i8, i8* %1, align 8
  %17 = and i8 %16, 1
  %tobool52 = icmp ne i8 %17, 0
  %18 = and i1 %cmp39, %tobool52
  %conv = zext i1 %18 to i32
  br label %cleanup

cleanup:                                          ; preds = %if.end49, %if.then
  %retval.0 = phi i32 [ %conv, %if.end49 ], [ 0, %if.then ]
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %1) #11
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %0) #11
  ret i32 %retval.0
}

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal15Synchronization16lock_bucket_pairEyy(%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* noalias sret(%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair") align 8 %agg.result, i64 %addr_from, i64 %addr_to) local_unnamed_addr #4 {
entry:
  %mul.i = mul i64 %addr_from, -7046029254386353131
  %shr.i = lshr i64 %mul.i, 54
  %mul.i37 = mul i64 %addr_to, -7046029254386353131
  %shr.i38 = lshr i64 %mul.i37, 54
  %cmp = icmp eq i64 %shr.i, %shr.i38
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %arrayidx = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_table", %"struct.Halide::Runtime::Internal::Synchronization::hash_table"* @_ZN6Halide7Runtime8Internal15Synchronization5tableE, i64 0, i32 0, i64 %shr.i
  %state.i42 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx, i64 0, i32 0, i32 0
  %0 = cmpxchg weak i64* %state.i42, i64 0, i64 1 acquire monotonic
  %1 = extractvalue { i64, i1 } %0, 1
  br i1 %1, label %cleanup, label %if.then.i43

if.then.i43:                                      ; preds = %if.then
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex) #15
  br label %cleanup

if.else:                                          ; preds = %entry
  %cmp2 = icmp ult i64 %shr.i, %shr.i38
  br i1 %cmp2, label %if.then3, label %if.else9

if.then3:                                         ; preds = %if.else
  %arrayidx5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_table", %"struct.Halide::Runtime::Internal::Synchronization::hash_table"* @_ZN6Halide7Runtime8Internal15Synchronization5tableE, i64 0, i32 0, i64 %shr.i
  %arrayidx6 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_table", %"struct.Halide::Runtime::Internal::Synchronization::hash_table"* @_ZN6Halide7Runtime8Internal15Synchronization5tableE, i64 0, i32 0, i64 %shr.i38
  %state.i52 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx5, i64 0, i32 0, i32 0
  %2 = cmpxchg weak i64* %state.i52, i64 0, i64 1 acquire monotonic
  %3 = extractvalue { i64, i1 } %2, 1
  br i1 %3, label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit54, label %if.then.i53

if.then.i53:                                      ; preds = %if.then3
  %mutex7 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx5, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex7) #15
  br label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit54

_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit54: ; preds = %if.then.i53, %if.then3
  %state.i49 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx6, i64 0, i32 0, i32 0
  %4 = cmpxchg weak i64* %state.i49, i64 0, i64 1 acquire monotonic
  %5 = extractvalue { i64, i1 } %4, 1
  br i1 %5, label %cleanup, label %if.then.i50

if.then.i50:                                      ; preds = %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit54
  %mutex8 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx6, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex8) #15
  br label %cleanup

if.else9:                                         ; preds = %if.else
  %arrayidx11 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_table", %"struct.Halide::Runtime::Internal::Synchronization::hash_table"* @_ZN6Halide7Runtime8Internal15Synchronization5tableE, i64 0, i32 0, i64 %shr.i38
  %arrayidx13 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_table", %"struct.Halide::Runtime::Internal::Synchronization::hash_table"* @_ZN6Halide7Runtime8Internal15Synchronization5tableE, i64 0, i32 0, i64 %shr.i
  %state.i39 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx11, i64 0, i32 0, i32 0
  %6 = cmpxchg weak i64* %state.i39, i64 0, i64 1 acquire monotonic
  %7 = extractvalue { i64, i1 } %6, 1
  br i1 %7, label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit41, label %if.then.i40

if.then.i40:                                      ; preds = %if.else9
  %mutex14 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx11, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex14) #15
  br label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit41

_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit41: ; preds = %if.then.i40, %if.else9
  %state.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx13, i64 0, i32 0, i32 0
  %8 = cmpxchg weak i64* %state.i, i64 0, i64 1 acquire monotonic
  %9 = extractvalue { i64, i1 } %8, 1
  br i1 %9, label %cleanup, label %if.then.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit41
  %mutex15 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx13, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock9lock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex15) #15
  br label %cleanup

cleanup:                                          ; preds = %if.then.i, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit41, %if.then.i50, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit54, %if.then.i43, %if.then
  %arrayidx13.sink = phi %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* [ %arrayidx, %if.then ], [ %arrayidx, %if.then.i43 ], [ %arrayidx5, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit54 ], [ %arrayidx5, %if.then.i50 ], [ %arrayidx13, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit41 ], [ %arrayidx13, %if.then.i ]
  %arrayidx11.sink = phi %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* [ %arrayidx, %if.then ], [ %arrayidx, %if.then.i43 ], [ %arrayidx6, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit54 ], [ %arrayidx6, %if.then.i50 ], [ %arrayidx11, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock4lockEv.exit41 ], [ %arrayidx11, %if.then.i ]
  %from2.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair", %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* %agg.result, i64 0, i32 0
  store %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx13.sink, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %from2.i, align 8, !tbaa !14
  %to3.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair", %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* %agg.result, i64 0, i32 1
  store %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %arrayidx11.sink, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %to3.i, align 8, !tbaa !14
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal15Synchronization18unlock_bucket_pairERNS2_11bucket_pairE(%"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* nonnull align 8 dereferenceable(16) %buckets) local_unnamed_addr #0 {
entry:
  %from = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair", %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* %buckets, i64 0, i32 0
  %0 = load %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"*, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %from, align 8, !tbaa !118
  %to = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair", %"struct.Halide::Runtime::Internal::Synchronization::bucket_pair"* %buckets, i64 0, i32 1
  %1 = load %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"*, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %to, align 8, !tbaa !121
  %cmp = icmp eq %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %0, %1
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %state.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %0, i64 0, i32 0, i32 0
  %2 = atomicrmw and i64* %state.i, i64 -2 release
  %and.i = and i64 %2, 2
  %cmp.i = icmp ne i64 %and.i, 0
  %cmp3.not.i = icmp ult i64 %2, 4
  %or.cond.i = or i1 %cmp3.not.i, %cmp.i
  br i1 %or.cond.i, label %if.end15, label %if.then.i

if.then.i:                                        ; preds = %if.then
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %0, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex) #15
  br label %if.end15

if.else:                                          ; preds = %entry
  %cmp4 = icmp ugt %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %0, %1
  br i1 %cmp4, label %if.then5, label %if.else10

if.then5:                                         ; preds = %if.else
  %state.i25 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %0, i64 0, i32 0, i32 0
  %3 = atomicrmw and i64* %state.i25, i64 -2 release
  %and.i26 = and i64 %3, 2
  %cmp.i27 = icmp ne i64 %and.i26, 0
  %cmp3.not.i28 = icmp ult i64 %3, 4
  %or.cond.i29 = or i1 %cmp3.not.i28, %cmp.i27
  br i1 %or.cond.i29, label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit31, label %if.then.i30

if.then.i30:                                      ; preds = %if.then5
  %mutex7 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %0, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex7) #15
  br label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit31

_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit31: ; preds = %if.then.i30, %if.then5
  %4 = load %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"*, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %to, align 8, !tbaa !121
  %state.i32 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %4, i64 0, i32 0, i32 0
  %5 = atomicrmw and i64* %state.i32, i64 -2 release
  %and.i33 = and i64 %5, 2
  %cmp.i34 = icmp ne i64 %and.i33, 0
  %cmp3.not.i35 = icmp ult i64 %5, 4
  %or.cond.i36 = or i1 %cmp3.not.i35, %cmp.i34
  br i1 %or.cond.i36, label %if.end15, label %if.then.i37

if.then.i37:                                      ; preds = %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit31
  %mutex9 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %4, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex9) #15
  br label %if.end15

if.else10:                                        ; preds = %if.else
  %state.i39 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %1, i64 0, i32 0, i32 0
  %6 = atomicrmw and i64* %state.i39, i64 -2 release
  %and.i40 = and i64 %6, 2
  %cmp.i41 = icmp ne i64 %and.i40, 0
  %cmp3.not.i42 = icmp ult i64 %6, 4
  %or.cond.i43 = or i1 %cmp3.not.i42, %cmp.i41
  br i1 %or.cond.i43, label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit45, label %if.then.i44

if.then.i44:                                      ; preds = %if.else10
  %mutex12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %1, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex12) #15
  br label %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit45

_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit45: ; preds = %if.then.i44, %if.else10
  %7 = load %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"*, %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"** %from, align 8, !tbaa !118
  %state.i46 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %7, i64 0, i32 0, i32 0
  %8 = atomicrmw and i64* %state.i46, i64 -2 release
  %and.i47 = and i64 %8, 2
  %cmp.i48 = icmp ne i64 %and.i47, 0
  %cmp3.not.i49 = icmp ult i64 %8, 4
  %or.cond.i50 = or i1 %cmp3.not.i49, %cmp.i48
  br i1 %or.cond.i50, label %if.end15, label %if.then.i51

if.then.i51:                                      ; preds = %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit45
  %mutex14 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket", %"struct.Halide::Runtime::Internal::Synchronization::hash_bucket"* %7, i64 0, i32 0
  tail call void @_ZN6Halide7Runtime8Internal15Synchronization9word_lock11unlock_fullEv(%"class.Halide::Runtime::Internal::Synchronization::word_lock"* nonnull dereferenceable(8) %mutex14) #15
  br label %if.end15

if.end15:                                         ; preds = %if.then.i51, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit45, %if.then.i37, %_ZN6Halide7Runtime8Internal15Synchronization9word_lock6unlockEv.exit31, %if.then.i, %if.then
  ret void
}

; Function Attrs: nounwind
define linkonce_odr zeroext i1 @_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control8validateERNS2_15validate_actionE(%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* nonnull dereferenceable(24) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action) unnamed_addr #4 align 2 {
entry:
  %cond_state = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %this, i64 0, i32 1
  %0 = load i64*, i64** %cond_state, align 8, !tbaa !107
  %1 = load atomic i64, i64* %0 monotonic, align 8
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %this, i64 0, i32 2
  %2 = load %"class.Halide::Runtime::Internal::Synchronization::word_lock"*, %"class.Halide::Runtime::Internal::Synchronization::word_lock"** %mutex, align 8, !tbaa !109
  %3 = ptrtoint %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %2 to i64
  %cmp.not = icmp eq i64 %1, %3
  br i1 %cmp.not, label %if.end, label %cleanup

if.end:                                           ; preds = %entry
  store atomic i64 0, i64* %0 monotonic, align 8
  %4 = load %"class.Halide::Runtime::Internal::Synchronization::word_lock"*, %"class.Halide::Runtime::Internal::Synchronization::word_lock"** %mutex, align 8, !tbaa !109
  %state.i = getelementptr inbounds %"class.Halide::Runtime::Internal::Synchronization::word_lock", %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %4, i64 0, i32 0
  %5 = load atomic i64, i64* %state.i monotonic, align 8
  %and11.i = and i64 %5, 1
  %tobool.not12.i = icmp eq i64 %and11.i, 0
  br i1 %tobool.not12.i, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex21make_parked_if_lockedEv.exit, label %if.end.i

if.end.i:                                         ; preds = %if.end, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i
  %val.013.i = phi i64 [ %8, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i ], [ %5, %if.end ]
  %or.i = or i64 %val.013.i, 2
  %6 = cmpxchg weak i64* %state.i, i64 %val.013.i, i64 %or.i monotonic monotonic
  %7 = extractvalue { i64, i1 } %6, 1
  br i1 %7, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex21make_parked_if_lockedEv.exit, label %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i

_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i: ; preds = %if.end.i
  %8 = extractvalue { i64, i1 } %6, 0
  %and.i = and i64 %8, 1
  %tobool.not.i = icmp eq i64 %and.i, 0
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex21make_parked_if_lockedEv.exit, label %if.end.i

_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex21make_parked_if_lockedEv.exit: ; preds = %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i, %if.end.i, %if.end
  %tobool.not.lcssa.i = phi i8 [ 1, %if.end ], [ 1, %_ZN6Halide7Runtime8Internal15Synchronization12_GLOBAL__N_131atomic_cas_weak_relaxed_relaxedEPyS4_S4_.exit.i ], [ 0, %if.end.i ]
  %unpark_one = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::validate_action", %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* %action, i64 0, i32 0
  store i8 %tobool.not.lcssa.i, i8* %unpark_one, align 8, !tbaa !114
  br label %cleanup

cleanup:                                          ; preds = %_ZN6Halide7Runtime8Internal15Synchronization10fast_mutex21make_parked_if_lockedEv.exit, %entry
  ret i1 %cmp.not
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control6unparkEib(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, i32 %unparked, i1 zeroext %more_waiters) unnamed_addr #2 align 2 {
entry:
  ret i64 0
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr void @_ZN6Halide7Runtime8Internal15Synchronization25broadcast_parking_control16requeue_callbackERKNS2_15validate_actionEbb(%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* nonnull dereferenceable(24) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action, i1 zeroext %one_to_wake, i1 zeroext %some_requeued) unnamed_addr #2 align 2 {
entry:
  %unpark_one = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::validate_action", %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* %action, i64 0, i32 0
  %0 = load i8, i8* %unpark_one, align 8, !tbaa !114, !range !21
  %tobool.not = icmp ne i8 %0, 0
  %1 = and i1 %tobool.not, %some_requeued
  br i1 %1, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  %mutex = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %this, i64 0, i32 2
  %2 = load %"class.Halide::Runtime::Internal::Synchronization::word_lock"*, %"class.Halide::Runtime::Internal::Synchronization::word_lock"** %mutex, align 8, !tbaa !109
  %state.i = getelementptr inbounds %"class.Halide::Runtime::Internal::Synchronization::word_lock", %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %2, i64 0, i32 0
  %3 = atomicrmw or i64* %state.i, i64 2 monotonic
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce i32 @_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv() local_unnamed_addr #0 {
entry:
  %call = tail call i8* @getenv(i8* getelementptr inbounds ([15 x i8], [15 x i8]* @.str.1, i64 0, i64 0)) #15
  %tobool.not = icmp eq i8* %call, null
  br i1 %tobool.not, label %if.end, label %cond.true

if.end:                                           ; preds = %entry
  %call1 = tail call i8* @getenv(i8* getelementptr inbounds ([14 x i8], [14 x i8]* @.str.2, i64 0, i64 0)) #15
  %tobool2.not = icmp eq i8* %call1, null
  br i1 %tobool2.not, label %cond.false, label %cond.true

cond.true:                                        ; preds = %if.end, %entry
  %threads_str.010 = phi i8* [ %call1, %if.end ], [ %call, %entry ]
  %call3 = tail call i32 @atoi(i8* nonnull %threads_str.010) #15
  br label %cond.end

cond.false:                                       ; preds = %if.end
  %call4 = tail call i32 @halide_host_cpu_count() #15
  br label %cond.end

cond.end:                                         ; preds = %cond.false, %cond.true
  %cond = phi i32 [ %call3, %cond.true ], [ %call4, %cond.false ]
  ret i32 %cond
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal13worker_threadEPv(i8* %arg) #0 {
entry:
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %0 = bitcast i8* %arg to %"struct.Halide::Runtime::Internal::work"*
  tail call void @_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE(%"struct.Halide::Runtime::Internal::work"* %0) #16
  tail call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  ret void
}

; Function Attrs: nounwind mustprogress
define weak %struct.halide_thread* @halide_spawn_thread(void (i8*)* %f, i8* %closure) local_unnamed_addr #0 {
entry:
  %call = tail call i8* @malloc(i64 24) #15
  %f1 = bitcast i8* %call to void (i8*)**
  store void (i8*)* %f, void (i8*)** %f1, align 8, !tbaa !122
  %closure2 = getelementptr inbounds i8, i8* %call, i64 8
  %0 = bitcast i8* %closure2 to i8**
  store i8* %closure, i8** %0, align 8, !tbaa !125
  %handle = getelementptr inbounds i8, i8* %call, i64 16
  %1 = bitcast i8* %handle to i64*
  store i64 0, i64* %1, align 8, !tbaa !126
  %call4 = tail call i32 @pthread_create(i64* nonnull %1, i8* null, i8* (i8*)* nonnull @_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv, i8* %call) #15
  %2 = bitcast i8* %call to %struct.halide_thread*
  ret %struct.halide_thread* %2
}

; Function Attrs: nounwind mustprogress
define linkonce i8* @_ZN6Halide7Runtime8Internal19spawn_thread_helperEPv(i8* %arg) #0 {
entry:
  %f = bitcast i8* %arg to void (i8*)**
  %0 = load void (i8*)*, void (i8*)** %f, align 8, !tbaa !122
  %closure = getelementptr inbounds i8, i8* %arg, i64 8
  %1 = bitcast i8* %closure to i8**
  %2 = load i8*, i8** %1, align 8, !tbaa !125
  tail call void %0(i8* %2) #15
  ret i8* null
}

declare i32 @pthread_create(i64*, i8*, i8* (i8*)*, i8*) local_unnamed_addr #1

declare i8* @getenv(i8*) local_unnamed_addr #1

declare i32 @atoi(i8*) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_do_parallel_tasks(i8* %user_context, i32 %num_tasks, %struct.halide_parallel_task_t* %tasks, i8* %task_parent) #0 {
entry:
  %conv = sext i32 %num_tasks to i64
  %0 = alloca %"struct.Halide::Runtime::Internal::work", i64 %conv, align 8
  %cmp76 = icmp sgt i32 %num_tasks, 0
  br i1 %cmp76, label %for.body, label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %for.inc, %entry
  %num_tasks.addr.0.lcssa = phi i32 [ %num_tasks, %entry ], [ %num_tasks.addr.1, %for.inc ]
  %cmp17 = icmp eq i32 %num_tasks.addr.0.lcssa, 0
  br i1 %cmp17, label %cleanup, label %if.end19

for.body:                                         ; preds = %entry, %for.inc
  %indvars.iv82 = phi i64 [ %indvars.iv.next83, %for.inc ], [ 0, %entry ]
  %num_tasks.addr.078 = phi i32 [ %num_tasks.addr.1, %for.inc ], [ %num_tasks, %entry ]
  %tasks.addr.077 = phi %struct.halide_parallel_task_t* [ %tasks.addr.1, %for.inc ], [ %tasks, %entry ]
  %extent = getelementptr inbounds %struct.halide_parallel_task_t, %struct.halide_parallel_task_t* %tasks.addr.077, i64 0, i32 6
  %1 = load i32, i32* %extent, align 8, !tbaa !127
  %cmp1 = icmp slt i32 %1, 1
  br i1 %cmp1, label %if.then, label %if.end

if.then:                                          ; preds = %for.body
  %dec = add nsw i32 %num_tasks.addr.078, -1
  br label %for.inc

if.end:                                           ; preds = %for.body
  %incdec.ptr = getelementptr inbounds %struct.halide_parallel_task_t, %struct.halide_parallel_task_t* %tasks.addr.077, i64 1
  %arrayidx = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv82
  %2 = bitcast %"struct.Halide::Runtime::Internal::work"* %arrayidx to i8*
  %3 = bitcast %struct.halide_parallel_task_t* %tasks.addr.077 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 dereferenceable(56) %2, i8* nonnull align 8 dereferenceable(56) %3, i64 56, i1 false), !tbaa.struct !128
  %task_fn = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv82, i32 1
  store i32 (i8*, i32, i8*)* null, i32 (i8*, i32, i8*)** %task_fn, align 8, !tbaa !39
  %user_context6 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv82, i32 7
  store i8* %user_context, i8** %user_context6, align 8, !tbaa !40
  %active_workers = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv82, i32 8
  %4 = bitcast i32* %active_workers to <2 x i32>*
  store <2 x i32> zeroinitializer, <2 x i32>* %4, align 8, !tbaa !41
  %next_semaphore = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv82, i32 10
  store i32 0, i32* %next_semaphore, align 8, !tbaa !42
  %owner_is_sleeping = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv82, i32 11
  store i8 0, i8* %owner_is_sleeping, align 4, !tbaa !43
  %parent_job = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv82, i32 5
  %5 = bitcast %"struct.Halide::Runtime::Internal::work"** %parent_job to i8**
  store i8* %task_parent, i8** %5, align 8, !tbaa !46
  br label %for.inc

for.inc:                                          ; preds = %if.end, %if.then
  %tasks.addr.1 = phi %struct.halide_parallel_task_t* [ %tasks.addr.077, %if.then ], [ %incdec.ptr, %if.end ]
  %num_tasks.addr.1 = phi i32 [ %dec, %if.then ], [ %num_tasks.addr.078, %if.end ]
  %indvars.iv.next83 = add nuw nsw i64 %indvars.iv82, 1
  %6 = sext i32 %num_tasks.addr.1 to i64
  %cmp = icmp slt i64 %indvars.iv.next83, %6
  br i1 %cmp, label %for.body, label %for.cond.cleanup, !llvm.loop !129

if.end19:                                         ; preds = %for.cond.cleanup
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %7 = bitcast i8* %task_parent to %"struct.Halide::Runtime::Internal::work"*
  call void @_ZN6Halide7Runtime8Internal27enqueue_work_already_lockedEiPNS1_4workES3_(i32 %num_tasks.addr.0.lcssa, %"struct.Halide::Runtime::Internal::work"* nonnull %0, %"struct.Halide::Runtime::Internal::work"* %7) #16
  %cmp2373 = icmp sgt i32 %num_tasks.addr.0.lcssa, 0
  br i1 %cmp2373, label %for.body25.preheader, label %for.cond.cleanup24

for.body25.preheader:                             ; preds = %if.end19
  %wide.trip.count = zext i32 %num_tasks.addr.0.lcssa to i64
  br label %for.body25

for.cond.cleanup24:                               ; preds = %for.body25, %if.end19
  %exit_status20.0.lcssa = phi i32 [ 0, %if.end19 ], [ %spec.select, %for.body25 ]
  call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  br label %cleanup

for.body25:                                       ; preds = %for.body25, %for.body25.preheader
  %indvars.iv = phi i64 [ 0, %for.body25.preheader ], [ %indvars.iv.next, %for.body25 ]
  %exit_status20.074 = phi i32 [ 0, %for.body25.preheader ], [ %spec.select, %for.body25 ]
  %add.ptr = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv
  call void @_ZN6Halide7Runtime8Internal28worker_thread_already_lockedEPNS1_4workE(%"struct.Halide::Runtime::Internal::work"* nonnull %add.ptr) #16
  %exit_status28 = getelementptr inbounds %"struct.Halide::Runtime::Internal::work", %"struct.Halide::Runtime::Internal::work"* %0, i64 %indvars.iv, i32 9
  %8 = load i32, i32* %exit_status28, align 4, !tbaa !47
  %cmp29.not = icmp eq i32 %8, 0
  %spec.select = select i1 %cmp29.not, i32 %exit_status20.074, i32 %8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %for.cond.cleanup24, label %for.body25, !llvm.loop !130

cleanup:                                          ; preds = %for.cond.cleanup24, %for.cond.cleanup
  %retval.0 = phi i32 [ %exit_status20.0.lcssa, %for.cond.cleanup24 ], [ 0, %for.cond.cleanup ]
  ret i32 %retval.0
}

; Function Attrs: argmemonly nofree nosync nounwind willreturn
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* noalias nocapture writeonly, i8* noalias nocapture readonly, i64, i1 immarg) #3

; Function Attrs: nounwind willreturn mustprogress
define weak i32 @halide_default_semaphore_init(%struct.halide_semaphore_t* %s, i32 %n) #2 {
entry:
  %value = bitcast %struct.halide_semaphore_t* %s to i32*
  store atomic i32 %n, i32* %value release, align 4
  ret i32 %n
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_semaphore_release(%struct.halide_semaphore_t* %s, i32 %n) #0 {
entry:
  %value = bitcast %struct.halide_semaphore_t* %s to i32*
  %0 = atomicrmw add i32* %value, i32 %n acq_rel
  %cmp = icmp eq i32 %0, 0
  %cmp1 = icmp ne i32 %n, 0
  %or.cond = and i1 %cmp1, %cmp
  br i1 %or.cond, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 8)) #16
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 10)) #16
  tail call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  %add = add nsw i32 %0, %n
  ret i32 %add
}

; Function Attrs: nounwind mustprogress
define weak void @halide_thread_pool_cleanup() #0 {
entry:
  tail call void @halide_shutdown_thread_pool() #16
  ret void
}

; Function Attrs: nounwind mustprogress
define weak void @halide_shutdown_thread_pool() local_unnamed_addr #0 {
entry:
  %0 = load i8, i8* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 15), align 1, !tbaa !52, !range !21
  %tobool.not = icmp eq i8 %0, 0
  br i1 %tobool.not, label %if.end, label %if.then

if.then:                                          ; preds = %entry
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  store i8 1, i8* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 14), align 8, !tbaa !73
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 10)) #16
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 8)) #16
  tail call void @halide_cond_broadcast(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 9)) #16
  tail call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %1 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 4), align 8, !tbaa !60
  %cmp4 = icmp sgt i32 %1, 0
  br i1 %cmp4, label %for.body, label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %for.body, %if.then
  %call.i = tail call i8* @memset(i8* nonnull bitcast (i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 2) to i8*), i32 0, i64 2116) #15
  br label %if.end

for.body:                                         ; preds = %if.then, %for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body ], [ 0, %if.then ]
  %arrayidx = getelementptr inbounds %"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 13, i64 %indvars.iv
  %2 = load %struct.halide_thread*, %struct.halide_thread** %arrayidx, align 8, !tbaa !14
  tail call void @halide_join_thread(%struct.halide_thread* %2) #16
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %3 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 4), align 8, !tbaa !60
  %4 = sext i32 %3 to i64
  %cmp = icmp slt i64 %indvars.iv.next, %4
  br i1 %cmp, label %for.body, label %for.cond.cleanup, !llvm.loop !131

if.end:                                           ; preds = %for.cond.cleanup, %entry
  ret void
}

declare i8* @memset(i8*, i32, i64) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak void @halide_join_thread(%struct.halide_thread* %thread_arg) local_unnamed_addr #0 {
entry:
  %ret = alloca i8*, align 8
  %0 = bitcast %struct.halide_thread* %thread_arg to %"struct.Halide::Runtime::Internal::spawned_thread"*
  %1 = bitcast i8** %ret to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %1) #11
  store i8* null, i8** %ret, align 8, !tbaa !14
  %handle = getelementptr inbounds %"struct.Halide::Runtime::Internal::spawned_thread", %"struct.Halide::Runtime::Internal::spawned_thread"* %0, i64 0, i32 2
  %2 = load i64, i64* %handle, align 8, !tbaa !126
  %call = call i32 @pthread_join(i64 %2, i8** nonnull %ret) #15
  %3 = bitcast %struct.halide_thread* %thread_arg to i8*
  call void @free(i8* %3) #15
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %1) #11
  ret void
}

declare i32 @pthread_join(i64, i8**) local_unnamed_addr #1

; Function Attrs: nounwind
define weak void @halide_cond_signal(%struct.halide_mutex* %cond) local_unnamed_addr #4 {
entry:
  %control.i = alloca %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", align 8
  %state.i = getelementptr %struct.halide_mutex, %struct.halide_mutex* %cond, i64 0, i32 0, i64 0
  %0 = load atomic i64, i64* %state.i monotonic, align 8
  %cmp.i = icmp eq i64 %0, 0
  br i1 %cmp.i, label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond6signalEv.exit, label %if.end.i

if.end.i:                                         ; preds = %entry
  %1 = bitcast %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 24, i8* nonnull %1) #11
  %2 = inttoptr i64 %0 to %"class.Halide::Runtime::Internal::Synchronization::word_lock"*
  %3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 0, i32 0
  store i32 (...)** bitcast (i8** getelementptr inbounds ({ [6 x i8*] }, { [6 x i8*] }* @_ZTVN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE, i64 0, inrange i32 0, i64 2) to i32 (...)**), i32 (...)*** %3, align 8, !tbaa !48
  %cond_state2.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 1
  store i64* %state.i, i64** %cond_state2.i.i, align 8, !tbaa !132
  %mutex3.i.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 2
  store %"class.Halide::Runtime::Internal::Synchronization::word_lock"* %2, %"class.Halide::Runtime::Internal::Synchronization::word_lock"** %mutex3.i.i, align 8, !tbaa !134
  %4 = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %control.i, i64 0, i32 0
  %5 = ptrtoint %struct.halide_mutex* %cond to i64
  %call.i = call i64 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control10unpark_oneEy(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %4, i64 %5) #15
  call void @llvm.lifetime.end.p0i8(i64 24, i8* nonnull %1) #11
  br label %_ZN6Halide7Runtime8Internal15Synchronization9fast_cond6signalEv.exit

_ZN6Halide7Runtime8Internal15Synchronization9fast_cond6signalEv.exit: ; preds = %if.end.i, %entry
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr zeroext i1 @_ZN6Halide7Runtime8Internal15Synchronization15parking_control8validateERNS2_15validate_actionE(%"struct.Halide::Runtime::Internal::Synchronization::parking_control"* nonnull dereferenceable(8) %this, %"struct.Halide::Runtime::Internal::Synchronization::validate_action"* nonnull align 8 dereferenceable(16) %action) unnamed_addr #2 align 2 {
entry:
  ret i1 true
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce_odr i64 @_ZN6Halide7Runtime8Internal15Synchronization22signal_parking_control6unparkEib(%"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* nonnull dereferenceable(24) %this, i32 %unparked, i1 zeroext %more_waiters) unnamed_addr #2 align 2 {
entry:
  br i1 %more_waiters, label %if.end, label %if.then

if.then:                                          ; preds = %entry
  %cond_state = getelementptr inbounds %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control", %"struct.Halide::Runtime::Internal::Synchronization::wait_parking_control"* %this, i64 0, i32 1
  %0 = load i64*, i64** %cond_state, align 8, !tbaa !132
  store atomic i64 0, i64* %0 monotonic, align 8
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  ret i64 0
}

; Function Attrs: nounwind mustprogress
define weak %struct.halide_mutex_array* @halide_mutex_array_create(i32 %sz) local_unnamed_addr #0 {
entry:
  %call = tail call i8* @halide_malloc(i8* null, i64 8) #15
  %0 = bitcast i8* %call to %struct.halide_mutex_array*
  %cmp = icmp eq i8* %call, null
  br i1 %cmp, label %cleanup, label %if.end

if.end:                                           ; preds = %entry
  %conv = sext i32 %sz to i64
  %mul = shl nsw i64 %conv, 3
  %call1 = tail call i8* @halide_malloc(i8* null, i64 %mul) #15
  %1 = bitcast i8* %call to i8**
  store i8* %call1, i8** %1, align 8, !tbaa !135
  %cmp4 = icmp eq i8* %call1, null
  br i1 %cmp4, label %if.then5, label %if.end6

if.then5:                                         ; preds = %if.end
  tail call void @halide_free(i8* null, i8* nonnull %call) #15
  br label %cleanup

if.end6:                                          ; preds = %if.end
  %call10 = tail call i8* @memset(i8* nonnull %call1, i32 0, i64 %mul) #15
  br label %cleanup

cleanup:                                          ; preds = %if.end6, %if.then5, %entry
  %retval.0 = phi %struct.halide_mutex_array* [ null, %if.then5 ], [ %0, %if.end6 ], [ null, %entry ]
  ret %struct.halide_mutex_array* %retval.0
}

; Function Attrs: nounwind mustprogress
define weak void @halide_mutex_array_destroy(i8* %user_context, i8* %array) local_unnamed_addr #0 {
entry:
  %0 = bitcast i8* %array to i8**
  %1 = load i8*, i8** %0, align 8, !tbaa !135
  tail call void @halide_free(i8* %user_context, i8* %1) #15
  tail call void @halide_free(i8* %user_context, i8* %array) #15
  ret void
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_mutex_array_lock(%struct.halide_mutex_array* %array, i32 %entry1) local_unnamed_addr #0 {
entry:
  %array2 = getelementptr inbounds %struct.halide_mutex_array, %struct.halide_mutex_array* %array, i64 0, i32 0
  %0 = load %struct.halide_mutex*, %struct.halide_mutex** %array2, align 8, !tbaa !135
  %idxprom = sext i32 %entry1 to i64
  %arrayidx = getelementptr inbounds %struct.halide_mutex, %struct.halide_mutex* %0, i64 %idxprom
  tail call void @halide_mutex_lock(%struct.halide_mutex* %arrayidx) #16
  ret i32 0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_mutex_array_unlock(%struct.halide_mutex_array* %array, i32 %entry1) local_unnamed_addr #0 {
entry:
  %array2 = getelementptr inbounds %struct.halide_mutex_array, %struct.halide_mutex_array* %array, i64 0, i32 0
  %0 = load %struct.halide_mutex*, %struct.halide_mutex** %array2, align 8, !tbaa !135
  %idxprom = sext i32 %entry1 to i64
  %arrayidx = getelementptr inbounds %struct.halide_mutex, %struct.halide_mutex* %0, i64 %idxprom
  tail call void @halide_mutex_unlock(%struct.halide_mutex* %arrayidx) #16
  ret i32 0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_set_num_threads(i32 %n) local_unnamed_addr #0 {
entry:
  %cmp = icmp slt i32 %n, 0
  br i1 %cmp, label %if.end3.thread, label %if.end

if.end3.thread:                                   ; preds = %entry
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([38 x i8], [38 x i8]* @.str.4, i64 0, i64 0)) #15
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %0 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 1), align 8, !tbaa !58
  br label %if.else.i

if.end:                                           ; preds = %entry
  tail call void @halide_mutex_lock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  %cmp1 = icmp eq i32 %n, 0
  br i1 %cmp1, label %if.then2, label %if.end3

if.then2:                                         ; preds = %if.end
  %call = tail call i32 @_ZN6Halide7Runtime8Internal27default_desired_num_threadsEv() #16
  br label %if.end3

if.end3:                                          ; preds = %if.then2, %if.end
  %n.addr.0 = phi i32 [ %call, %if.then2 ], [ %n, %if.end ]
  %1 = load i32, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 1), align 8, !tbaa !58
  %cmp.i = icmp sgt i32 %n.addr.0, 256
  br i1 %cmp.i, label %_ZN6Halide7Runtime8Internal17clamp_num_threadsEi.exit, label %if.else.i

if.else.i:                                        ; preds = %if.end3, %if.end3.thread
  %2 = phi i32 [ %0, %if.end3.thread ], [ %1, %if.end3 ]
  %n.addr.012 = phi i32 [ %n, %if.end3.thread ], [ %n.addr.0, %if.end3 ]
  %3 = icmp sgt i32 %n.addr.012, 1
  %spec.select.i = select i1 %3, i32 %n.addr.012, i32 1
  br label %_ZN6Halide7Runtime8Internal17clamp_num_threadsEi.exit

_ZN6Halide7Runtime8Internal17clamp_num_threadsEi.exit: ; preds = %if.else.i, %if.end3
  %4 = phi i32 [ %2, %if.else.i ], [ %1, %if.end3 ]
  %call48 = phi i32 [ %spec.select.i, %if.else.i ], [ 256, %if.end3 ]
  store i32 %call48, i32* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 1), align 8, !tbaa !58
  tail call void @halide_mutex_unlock(%struct.halide_mutex* getelementptr inbounds (%"struct.Halide::Runtime::Internal::work_queue_t", %"struct.Halide::Runtime::Internal::work_queue_t"* @_ZN6Halide7Runtime8Internal10work_queueE, i64 0, i32 0)) #16
  ret i32 %4
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* @halide_set_custom_do_task(i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* %f) local_unnamed_addr #2 {
entry:
  %0 = load i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)*, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)** @_ZN6Halide7Runtime8Internal14custom_do_taskE, align 8, !tbaa !14
  store i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* %f, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)** @_ZN6Halide7Runtime8Internal14custom_do_taskE, align 8, !tbaa !14
  ret i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* %0
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)* @halide_set_custom_do_loop_task(i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)* %f) local_unnamed_addr #2 {
entry:
  %0 = load i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)*, i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)** @_ZN6Halide7Runtime8Internal19custom_do_loop_taskE, align 8, !tbaa !14
  store i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)* %f, i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)** @_ZN6Halide7Runtime8Internal19custom_do_loop_taskE, align 8, !tbaa !14
  ret i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)* %0
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* @halide_set_custom_do_par_for(i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* %f) local_unnamed_addr #2 {
entry:
  %0 = load i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)*, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)** @_ZN6Halide7Runtime8Internal17custom_do_par_forE, align 8, !tbaa !14
  store i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* %f, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)** @_ZN6Halide7Runtime8Internal17custom_do_par_forE, align 8, !tbaa !14
  ret i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* %0
}

; Function Attrs: nounwind willreturn mustprogress
define weak void @halide_set_custom_parallel_runtime(i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* %do_par_for, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* %do_task, i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)* %do_loop_task, i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)* %do_parallel_tasks, i32 (%struct.halide_semaphore_t*, i32)* %semaphore_init, i1 (%struct.halide_semaphore_t*, i32)* %semaphore_try_acquire, i32 (%struct.halide_semaphore_t*, i32)* %semaphore_release) local_unnamed_addr #2 {
entry:
  store i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* %do_par_for, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)** @_ZN6Halide7Runtime8Internal17custom_do_par_forE, align 8, !tbaa !14
  store i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* %do_task, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)** @_ZN6Halide7Runtime8Internal14custom_do_taskE, align 8, !tbaa !14
  store i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)* %do_loop_task, i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)** @_ZN6Halide7Runtime8Internal19custom_do_loop_taskE, align 8, !tbaa !14
  store i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)* %do_parallel_tasks, i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)** @_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE, align 8, !tbaa !14
  store i32 (%struct.halide_semaphore_t*, i32)* %semaphore_init, i32 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal21custom_semaphore_initE, align 8, !tbaa !14
  store i1 (%struct.halide_semaphore_t*, i32)* %semaphore_try_acquire, i1 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE, align 8, !tbaa !14
  store i32 (%struct.halide_semaphore_t*, i32)* %semaphore_release, i32 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE, align 8, !tbaa !14
  ret void
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_do_par_for(i8* %user_context, i32 (i8*, i32, i8*)* %f, i32 %min, i32 %size, i8* %closure) local_unnamed_addr #0 {
entry:
  %0 = load i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)*, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)** @_ZN6Halide7Runtime8Internal17custom_do_par_forE, align 8, !tbaa !14
  %call = tail call i32 %0(i8* %user_context, i32 (i8*, i32, i8*)* %f, i32 %min, i32 %size, i8* %closure) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_do_parallel_tasks(i8* %user_context, i32 %num_tasks, %struct.halide_parallel_task_t* %tasks, i8* %task_parent) local_unnamed_addr #0 {
entry:
  %0 = load i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)*, i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)** @_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE, align 8, !tbaa !14
  %call = tail call i32 %0(i8* %user_context, i32 %num_tasks, %struct.halide_parallel_task_t* %tasks, i8* %task_parent) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_semaphore_init(%struct.halide_semaphore_t* %sema, i32 %count) local_unnamed_addr #0 {
entry:
  %0 = load i32 (%struct.halide_semaphore_t*, i32)*, i32 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal21custom_semaphore_initE, align 8, !tbaa !14
  %call = tail call i32 %0(%struct.halide_semaphore_t* %sema, i32 %count) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_semaphore_release(%struct.halide_semaphore_t* %sema, i32 %count) local_unnamed_addr #0 {
entry:
  %0 = load i32 (%struct.halide_semaphore_t*, i32)*, i32 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE, align 8, !tbaa !14
  %call = tail call i32 %0(%struct.halide_semaphore_t* %sema, i32 %count) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak zeroext i1 @halide_semaphore_try_acquire(%struct.halide_semaphore_t* %sema, i32 %count) local_unnamed_addr #0 {
entry:
  %0 = load i1 (%struct.halide_semaphore_t*, i32)*, i1 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE, align 8, !tbaa !14
  %call = tail call zeroext i1 %0(%struct.halide_semaphore_t* %sema, i32 %count) #15
  ret i1 %call
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_default_get_symbol(i8* %name) #0 {
entry:
  %call = tail call i8* @dlsym(i8* nonnull inttoptr (i64 -2 to i8*), i8* %name) #15
  ret i8* %call
}

declare i8* @dlsym(i8*, i8*) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak i8* @halide_default_load_library(i8* %name) #0 {
entry:
  %call = tail call i8* @dlopen(i8* %name, i32 5) #15
  ret i8* %call
}

declare i8* @dlopen(i8*, i32) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak i8* @halide_default_get_library_symbol(i8* %lib, i8* %name) #0 {
entry:
  %cmp = icmp eq i8* %lib, null
  %spec.select = select i1 %cmp, i8* inttoptr (i64 -2 to i8*), i8* %lib
  %call = tail call i8* @dlsym(i8* %spec.select, i8* %name) #15
  ret i8* %call
}

; Function Attrs: nounwind willreturn mustprogress
define weak i8* (i8*)* @halide_set_custom_get_symbol(i8* (i8*)* %f) local_unnamed_addr #2 {
entry:
  %0 = load i8* (i8*)*, i8* (i8*)** @_ZN6Halide7Runtime8Internal17custom_get_symbolE, align 8, !tbaa !14
  store i8* (i8*)* %f, i8* (i8*)** @_ZN6Halide7Runtime8Internal17custom_get_symbolE, align 8, !tbaa !14
  ret i8* (i8*)* %0
}

; Function Attrs: nounwind willreturn mustprogress
define weak i8* (i8*)* @halide_set_custom_load_library(i8* (i8*)* %f) local_unnamed_addr #2 {
entry:
  %0 = load i8* (i8*)*, i8* (i8*)** @_ZN6Halide7Runtime8Internal19custom_load_libraryE, align 8, !tbaa !14
  store i8* (i8*)* %f, i8* (i8*)** @_ZN6Halide7Runtime8Internal19custom_load_libraryE, align 8, !tbaa !14
  ret i8* (i8*)* %0
}

; Function Attrs: nounwind willreturn mustprogress
define weak i8* (i8*, i8*)* @halide_set_custom_get_library_symbol(i8* (i8*, i8*)* %f) local_unnamed_addr #2 {
entry:
  %0 = load i8* (i8*, i8*)*, i8* (i8*, i8*)** @_ZN6Halide7Runtime8Internal25custom_get_library_symbolE, align 8, !tbaa !14
  store i8* (i8*, i8*)* %f, i8* (i8*, i8*)** @_ZN6Halide7Runtime8Internal25custom_get_library_symbolE, align 8, !tbaa !14
  ret i8* (i8*, i8*)* %0
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_get_symbol(i8* %name) local_unnamed_addr #0 {
entry:
  %0 = load i8* (i8*)*, i8* (i8*)** @_ZN6Halide7Runtime8Internal17custom_get_symbolE, align 8, !tbaa !14
  %call = tail call i8* %0(i8* %name) #15
  ret i8* %call
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_load_library(i8* %name) local_unnamed_addr #0 {
entry:
  %0 = load i8* (i8*)*, i8* (i8*)** @_ZN6Halide7Runtime8Internal19custom_load_libraryE, align 8, !tbaa !14
  %call = tail call i8* %0(i8* %name) #15
  ret i8* %call
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_get_library_symbol(i8* %lib, i8* %name) local_unnamed_addr #0 {
entry:
  %0 = load i8* (i8*, i8*)*, i8* (i8*, i8*)** @_ZN6Halide7Runtime8Internal25custom_get_library_symbolE, align 8, !tbaa !14
  %call = tail call i8* %0(i8* %lib, i8* %name) #15
  ret i8* %call
}

; Function Attrs: nounwind willreturn mustprogress
define weak void @halide_set_gpu_device(i32 %d) local_unnamed_addr #2 {
entry:
  store i32 %d, i32* @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, align 4, !tbaa !41
  store i8 1, i8* @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, align 1, !tbaa !19
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_get_gpu_device(i8* %user_context) local_unnamed_addr #4 {
entry:
  br label %while.cond.i

while.cond.i:                                     ; preds = %while.cond.i, %entry
  %0 = atomicrmw volatile xchg i8* @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE, i8 1 acquire
  %tobool.not.i = icmp eq i8 %0, 0
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit, label %while.cond.i, !llvm.loop !137

_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit: ; preds = %while.cond.i
  %1 = load i8, i8* @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, align 1, !tbaa !19, !range !21
  %tobool.not = icmp eq i8 %1, 0
  br i1 %tobool.not, label %if.then, label %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit.if.end4_crit_edge

_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit.if.end4_crit_edge: ; preds = %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
  %.pre = load i32, i32* @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, align 4, !tbaa !41
  br label %if.end4

if.then:                                          ; preds = %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
  %call = tail call i8* @getenv(i8* getelementptr inbounds ([14 x i8], [14 x i8]* @.str.8, i64 0, i64 0)) #15
  %tobool1.not = icmp eq i8* %call, null
  br i1 %tobool1.not, label %if.end, label %if.then2

if.then2:                                         ; preds = %if.then
  %call3 = tail call i32 @atoi(i8* nonnull %call) #15
  br label %if.end

if.end:                                           ; preds = %if.then2, %if.then
  %storemerge = phi i32 [ %call3, %if.then2 ], [ -1, %if.then ]
  store i32 %storemerge, i32* @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, align 4, !tbaa !41
  store i8 1, i8* @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, align 1, !tbaa !19
  br label %if.end4

if.end4:                                          ; preds = %if.end, %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit.if.end4_crit_edge
  %2 = phi i32 [ %.pre, %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit.if.end4_crit_edge ], [ %storemerge, %if.end ]
  store atomic volatile i8 0, i8* @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE release, align 1
  ret i32 %2
}

; Function Attrs: nounwind
define weak i32 @halide_default_trace(i8* %user_context, %struct.halide_trace_event_t* %e) #4 {
entry:
  %0 = atomicrmw add i32* @_ZZ20halide_default_traceE3ids, i32 1 seq_cst
  %call = tail call i32 @halide_get_trace_file(i8* %user_context) #16
  %cmp = icmp sgt i32 %call, 0
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %type = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 4
  %lanes = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 4, i32 2
  %1 = load i16, i16* %lanes, align 2, !tbaa !138
  %conv = zext i16 %1 to i32
  %bits.i = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 4, i32 1
  %2 = load i8, i8* %bits.i, align 1, !tbaa !144
  %conv.i = zext i8 %2 to i32
  %add.i = add nuw nsw i32 %conv.i, 7
  %div.i = lshr i32 %add.i, 3
  %mul = mul nuw nsw i32 %div.i, %conv
  %dimensions = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 8
  %3 = load i32, i32* %dimensions, align 8, !tbaa !145
  %mul3 = shl i32 %3, 2
  %func = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 0
  %4 = load i8*, i8** %func, align 8, !tbaa !146
  %call4 = tail call i64 @strlen(i8* %4) #15
  %5 = trunc i64 %call4 to i32
  %conv5 = add i32 %5, 1
  %trace_tag = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 3
  %6 = load i8*, i8** %trace_tag, align 8, !tbaa !147
  %tobool.not = icmp eq i8* %6, null
  br i1 %tobool.not, label %cond.end, label %cond.true

cond.true:                                        ; preds = %if.then
  %call7 = tail call i64 @strlen(i8* nonnull %6) #15
  %7 = trunc i64 %call7 to i32
  %phi.cast = add i32 %7, 1
  br label %cond.end

cond.end:                                         ; preds = %cond.true, %if.then
  %cond = phi i32 [ %phi.cast, %cond.true ], [ 1, %if.then ]
  %add11 = add i32 %mul3, 31
  %add12 = add i32 %add11, %mul
  %add13 = add i32 %add12, %conv5
  %add14 = add i32 %add13, %cond
  %and = and i32 %add14, -4
  %8 = load %"class.Halide::Runtime::Internal::TraceBuffer"*, %"class.Halide::Runtime::Internal::TraceBuffer"** @_ZN6Halide7Runtime8Internal19halide_trace_bufferE, align 8, !tbaa !14
  %lock.i.i.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %8, i64 0, i32 0, i32 0
  %cmp.i.i = icmp ult i32 %and, 1048577
  %cursor.i.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %8, i64 0, i32 1
  %overage.i.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %8, i64 0, i32 2
  %arraydecay.i.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %8, i64 0, i32 3, i64 0
  br i1 %cmp.i.i, label %while.body.i.i.us.i.preheader, label %while.body.i.i.i.preheader

while.body.i.i.i.preheader:                       ; preds = %cond.end
  %9 = bitcast i32* %cursor.i.i to <2 x i32>*
  br label %while.body.i.i.i

while.body.i.i.us.i.preheader:                    ; preds = %cond.end
  %10 = bitcast i32* %cursor.i.i to <2 x i32>*
  br label %while.body.i.i.us.i

while.body.i.i.us.i:                              ; preds = %while.body.i.i.us.i.backedge, %while.body.i.i.us.i.preheader
  %11 = load volatile i32, i32* %lock.i.i.i, align 4, !tbaa !148
  %and.i.i.us.i = and i32 %11, 1073741823
  %add.i.i.us.i = add nuw nsw i32 %and.i.i.us.i, 1
  %12 = cmpxchg i32* %lock.i.i.i, i32 %and.i.i.us.i, i32 %add.i.i.us.i seq_cst seq_cst
  %13 = extractvalue { i32, i1 } %12, 1
  br i1 %13, label %do.end.i.us.i, label %while.body.i.i.us.i.backedge

do.end.i.us.i:                                    ; preds = %while.body.i.i.us.i
  %14 = atomicrmw add i32* %cursor.i.i, i32 %and seq_cst
  %add.i.us.i = add i32 %14, %and
  %cmp2.i.us.i = icmp ugt i32 %add.i.us.i, 1048576
  br i1 %cmp2.i.us.i, label %while.body.us.i, label %_ZN6Halide7Runtime8Internal11TraceBuffer14acquire_packetEPvij.exit

while.body.us.i:                                  ; preds = %do.end.i.us.i
  %15 = atomicrmw add i32* %overage.i.i, i32 %and seq_cst
  %16 = atomicrmw sub i32* %lock.i.i.i, i32 1 seq_cst
  br label %while.body.i.i5.us.i

while.body.i.i5.us.i:                             ; preds = %while.body.i.i5.us.i, %while.body.us.i
  %17 = atomicrmw or i32* %lock.i.i.i, i32 1073741824 seq_cst
  %18 = cmpxchg i32* %lock.i.i.i, i32 1073741824, i32 -2147483648 seq_cst seq_cst
  %19 = extractvalue { i32, i1 } %18, 1
  br i1 %19, label %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.us.i, label %while.body.i.i5.us.i

_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.us.i: ; preds = %while.body.i.i5.us.i
  %20 = load i32, i32* %cursor.i.i, align 4, !tbaa !150
  %tobool.not.i.us.i = icmp eq i32 %20, 0
  br i1 %tobool.not.i.us.i, label %do.end.critedge.i.us.i, label %if.then.i9.us.i

if.then.i9.us.i:                                  ; preds = %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.us.i
  %21 = load i32, i32* %overage.i.i, align 4, !tbaa !152
  %sub.i.us.i = sub i32 %20, %21
  store i32 %sub.i.us.i, i32* %cursor.i.i, align 4, !tbaa !150
  %conv.i.us.i = zext i32 %sub.i.us.i to i64
  %call.i.us.i = tail call i64 @write(i32 %call, i8* nonnull %arraydecay.i.i, i64 %conv.i.us.i) #15
  %conv5.i.us.i = trunc i64 %call.i.us.i to i32
  %cmp.i8.us.i = icmp eq i32 %sub.i.us.i, %conv5.i.us.i
  store <2 x i32> zeroinitializer, <2 x i32>* %10, align 4, !tbaa !41
  %22 = atomicrmw and i32* %lock.i.i.i, i32 2147483647 seq_cst
  br i1 %cmp.i8.us.i, label %while.body.i.i.us.i.backedge, label %if.then10.i.us.i

if.then10.i.us.i:                                 ; preds = %if.then.i9.us.i
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([144 x i8], [144 x i8]* @.str.32, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %while.body.i.i.us.i.backedge

do.end.critedge.i.us.i:                           ; preds = %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.us.i
  %23 = atomicrmw and i32* %lock.i.i.i, i32 2147483647 seq_cst
  br label %while.body.i.i.us.i.backedge

while.body.i.i.us.i.backedge:                     ; preds = %do.end.critedge.i.us.i, %if.then10.i.us.i, %if.then.i9.us.i, %while.body.i.i.us.i
  br label %while.body.i.i.us.i, !llvm.loop !153

while.body.i.i.i:                                 ; preds = %while.body.i.i.i.backedge, %while.body.i.i.i.preheader
  %24 = load volatile i32, i32* %lock.i.i.i, align 4, !tbaa !148
  %and.i.i.i = and i32 %24, 1073741823
  %add.i.i.i = add nuw nsw i32 %and.i.i.i, 1
  %25 = cmpxchg i32* %lock.i.i.i, i32 %and.i.i.i, i32 %add.i.i.i seq_cst seq_cst
  %26 = extractvalue { i32, i1 } %25, 1
  br i1 %26, label %if.then.i.i, label %while.body.i.i.i.backedge

if.then.i.i:                                      ; preds = %while.body.i.i.i
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([120 x i8], [120 x i8]* @.str.31, i64 0, i64 0)) #15
  tail call void @abort() #15
  %27 = atomicrmw add i32* %cursor.i.i, i32 %and seq_cst
  %add.i.i400 = add i32 %27, %and
  %cmp2.i.i = icmp ugt i32 %add.i.i400, 1048576
  br i1 %cmp2.i.i, label %while.body.i, label %_ZN6Halide7Runtime8Internal11TraceBuffer14acquire_packetEPvij.exit

while.body.i:                                     ; preds = %if.then.i.i
  %28 = atomicrmw add i32* %overage.i.i, i32 %and seq_cst
  %29 = atomicrmw sub i32* %lock.i.i.i, i32 1 seq_cst
  br label %while.body.i.i5.i

while.body.i.i5.i:                                ; preds = %while.body.i.i5.i, %while.body.i
  %30 = atomicrmw or i32* %lock.i.i.i, i32 1073741824 seq_cst
  %31 = cmpxchg i32* %lock.i.i.i, i32 1073741824, i32 -2147483648 seq_cst seq_cst
  %32 = extractvalue { i32, i1 } %31, 1
  br i1 %32, label %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.i, label %while.body.i.i5.i

_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.i: ; preds = %while.body.i.i5.i
  %33 = load i32, i32* %cursor.i.i, align 4, !tbaa !150
  %tobool.not.i.i = icmp eq i32 %33, 0
  br i1 %tobool.not.i.i, label %do.end.critedge.i.i, label %if.then.i9.i

if.then.i9.i:                                     ; preds = %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.i
  %34 = load i32, i32* %overage.i.i, align 4, !tbaa !152
  %sub.i.i = sub i32 %33, %34
  store i32 %sub.i.i, i32* %cursor.i.i, align 4, !tbaa !150
  %conv.i.i = zext i32 %sub.i.i to i64
  %call.i.i401 = tail call i64 @write(i32 %call, i8* nonnull %arraydecay.i.i, i64 %conv.i.i) #15
  %conv5.i.i = trunc i64 %call.i.i401 to i32
  %cmp.i8.i = icmp eq i32 %sub.i.i, %conv5.i.i
  store <2 x i32> zeroinitializer, <2 x i32>* %9, align 4, !tbaa !41
  %35 = atomicrmw and i32* %lock.i.i.i, i32 2147483647 seq_cst
  br i1 %cmp.i8.i, label %while.body.i.i.i.backedge, label %if.then10.i.i

if.then10.i.i:                                    ; preds = %if.then.i9.i
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([144 x i8], [144 x i8]* @.str.32, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %while.body.i.i.i.backedge

do.end.critedge.i.i:                              ; preds = %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i.i
  %36 = atomicrmw and i32* %lock.i.i.i, i32 2147483647 seq_cst
  br label %while.body.i.i.i.backedge

while.body.i.i.i.backedge:                        ; preds = %do.end.critedge.i.i, %if.then10.i.i, %if.then.i9.i, %while.body.i.i.i
  br label %while.body.i.i.i, !llvm.loop !153

_ZN6Halide7Runtime8Internal11TraceBuffer14acquire_packetEPvij.exit: ; preds = %if.then.i.i, %do.end.i.us.i
  %.lcssa.i = phi i32 [ %14, %do.end.i.us.i ], [ %27, %if.then.i.i ]
  %idx.ext.i.i = zext i32 %.lcssa.i to i64
  %add.ptr.i.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %8, i64 0, i32 3, i64 %idx.ext.i.i
  %cmp16 = icmp ugt i32 %and, 4096
  br i1 %cmp16, label %if.then17, label %if.end

if.then17:                                        ; preds = %_ZN6Halide7Runtime8Internal11TraceBuffer14acquire_packetEPvij.exit
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i404 = icmp eq i8* %call.i, null
  br i1 %tobool.not.i404, label %if.then.i415, label %if.else.i421

if.then.i415:                                     ; preds = %if.then17
  %conv.i4071 = zext i32 %and to i64
  %call.i4082 = tail call i8* @halide_uint64_to_string(i8* null, i8* null, i64 %conv.i4071, i32 1) #15
  %call.i41117 = tail call i8* @halide_string_to_string(i8* %call.i4082, i8* null, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.7.164, i64 0, i64 0)) #15
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE0ELy1024EED2Ev.exit

if.else.i421:                                     ; preds = %if.then17
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %conv.i4073 = zext i32 %and to i64
  %call.i4084 = tail call i8* @halide_uint64_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i64 %conv.i4073, i32 1) #15
  %call.i411 = tail call i8* @halide_string_to_string(i8* %call.i4084, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.7.164, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i.i416 = ptrtoint i8* %call.i411 to i64
  %sub.ptr.rhs.cast.i.i417 = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i418 = sub i64 1, %sub.ptr.rhs.cast.i.i417
  %add.i.i419 = add i64 %sub.ptr.sub.i.i418, %sub.ptr.lhs.cast.i.i416
  %call.i.i420 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* null, i8* nonnull %call.i, i64 %add.i.i419) #15
  tail call void @halide_print(i8* null, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE0ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE0ELy1024EED2Ev.exit: ; preds = %if.else.i421, %if.then.i415
  tail call void @free(i8* %call.i) #15
  br label %if.end

if.end:                                           ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE0ELy1024EED2Ev.exit, %_ZN6Halide7Runtime8Internal11TraceBuffer14acquire_packetEPvij.exit
  %size = bitcast i8* %add.ptr.i.i to i32*
  store i32 %and, i32* %size, align 4, !tbaa !154
  %id = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 4
  %37 = bitcast i8* %id to i32*
  store i32 %0, i32* %37, align 4, !tbaa !156
  %38 = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 8
  %39 = bitcast %struct.halide_type_t* %type to i32*
  %40 = bitcast i8* %38 to i32*
  %41 = load i32, i32* %39, align 8
  store i32 %41, i32* %40, align 4
  %event = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 5
  %event22 = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 12
  %42 = bitcast i32* %event to <4 x i32>*
  %43 = load <4 x i32>, <4 x i32>* %42, align 4, !tbaa !18
  %dimensions26 = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 24
  %44 = bitcast i8* %dimensions26 to i32*
  %45 = bitcast i8* %event22 to <4 x i32>*
  store <4 x i32> %43, <4 x i32>* %45, align 4, !tbaa !18
  %coordinates = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 2
  %46 = load i32*, i32** %coordinates, align 8, !tbaa !157
  %tobool27.not = icmp eq i32* %46, null
  br i1 %tobool27.not, label %if.end33, label %if.then28

if.then28:                                        ; preds = %if.end
  %47 = bitcast i32* %46 to i8*
  %48 = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 28
  %conv31 = zext i32 %mul3 to i64
  %call32 = tail call i8* @memcpy(i8* nonnull %48, i8* nonnull %47, i64 %conv31) #15
  br label %if.end33

if.end33:                                         ; preds = %if.then28, %if.end
  %value = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 1
  %49 = load i8*, i8** %value, align 8, !tbaa !158
  %tobool34.not = icmp eq i8* %49, null
  br i1 %tobool34.not, label %if.end40, label %if.then35

if.then35:                                        ; preds = %if.end33
  %50 = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 28
  %51 = bitcast i8* %50 to i32*
  %52 = load i32, i32* %44, align 4, !tbaa !159
  %idx.ext.i = sext i32 %52 to i64
  %add.ptr.i424 = getelementptr inbounds i32, i32* %51, i64 %idx.ext.i
  %53 = bitcast i32* %add.ptr.i424 to i8*
  %conv38 = zext i32 %mul to i64
  %call39 = tail call i8* @memcpy(i8* nonnull %53, i8* nonnull %49, i64 %conv38) #15
  br label %if.end40

if.end40:                                         ; preds = %if.end33, %if.then35
  %54 = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 28
  %55 = bitcast i8* %54 to i32*
  %56 = load i32, i32* %44, align 4, !tbaa !159
  %idx.ext.i.i425 = sext i32 %56 to i64
  %add.ptr.i.i426 = getelementptr inbounds i32, i32* %55, i64 %idx.ext.i.i425
  %57 = bitcast i32* %add.ptr.i.i426 to i8*
  %lanes.i = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 10
  %58 = bitcast i8* %lanes.i to i16*
  %59 = load i16, i16* %58, align 2, !tbaa !160
  %conv.i427 = zext i16 %59 to i32
  %60 = getelementptr inbounds i8, i8* %add.ptr.i.i, i64 9
  %61 = load i8, i8* %60, align 1, !tbaa !144
  %conv.i.i428 = zext i8 %61 to i32
  %add.i.i429 = add nuw nsw i32 %conv.i.i428, 7
  %div.i.i = lshr i32 %add.i.i429, 3
  %mul.i = mul nuw nsw i32 %div.i.i, %conv.i427
  %idx.ext.i430 = zext i32 %mul.i to i64
  %add.ptr.i431 = getelementptr inbounds i8, i8* %57, i64 %idx.ext.i430
  %62 = load i8*, i8** %func, align 8, !tbaa !146
  %conv43 = zext i32 %conv5 to i64
  %call44 = tail call i8* @memcpy(i8* nonnull %add.ptr.i431, i8* %62, i64 %conv43) #15
  %63 = load i32, i32* %44, align 4, !tbaa !159
  %idx.ext.i.i.i = sext i32 %63 to i64
  %add.ptr.i.i.i = getelementptr inbounds i32, i32* %55, i64 %idx.ext.i.i.i
  %64 = bitcast i32* %add.ptr.i.i.i to i8*
  %65 = load i16, i16* %58, align 2, !tbaa !160
  %conv.i.i432 = zext i16 %65 to i32
  %66 = load i8, i8* %60, align 1, !tbaa !144
  %conv.i.i.i = zext i8 %66 to i32
  %add.i.i.i433 = add nuw nsw i32 %conv.i.i.i, 7
  %div.i.i.i = lshr i32 %add.i.i.i433, 3
  %mul.i.i = mul nuw nsw i32 %div.i.i.i, %conv.i.i432
  %idx.ext.i.i434 = zext i32 %mul.i.i to i64
  %add.ptr.i.i435 = getelementptr inbounds i8, i8* %64, i64 %idx.ext.i.i434
  br label %while.cond.i437

while.cond.i437:                                  ; preds = %while.cond.i437, %if.end40
  %f.0.i = phi i8* [ %add.ptr.i.i435, %if.end40 ], [ %incdec.ptr.i, %while.cond.i437 ]
  %incdec.ptr.i = getelementptr inbounds i8, i8* %f.0.i, i64 1
  %67 = load i8, i8* %f.0.i, align 1, !tbaa !18
  %tobool.not.i436 = icmp eq i8 %67, 0
  br i1 %tobool.not.i436, label %_ZN21halide_trace_packet_t9trace_tagEv.exit, label %while.cond.i437, !llvm.loop !161

_ZN21halide_trace_packet_t9trace_tagEv.exit:      ; preds = %while.cond.i437
  %68 = load i8*, i8** %trace_tag, align 8, !tbaa !147
  %tobool47.not = icmp eq i8* %68, null
  %spec.select = select i1 %tobool47.not, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.1.10, i64 0, i64 0), i8* %68
  %conv53 = zext i32 %cond to i64
  %call54 = tail call i8* @memcpy(i8* nonnull %incdec.ptr.i, i8* %spec.select, i64 %conv53) #15
  %69 = load %"class.Halide::Runtime::Internal::TraceBuffer"*, %"class.Halide::Runtime::Internal::TraceBuffer"** @_ZN6Halide7Runtime8Internal19halide_trace_bufferE, align 8, !tbaa !14
  fence seq_cst
  %lock.i.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %69, i64 0, i32 0, i32 0
  %70 = atomicrmw sub i32* %lock.i.i, i32 1 seq_cst
  %71 = load i32, i32* %event, align 4, !tbaa !162
  %cmp56 = icmp eq i32 %71, 9
  br i1 %cmp56, label %if.then57, label %if.end277

if.then57:                                        ; preds = %_ZN21halide_trace_packet_t9trace_tagEv.exit
  %72 = load %"class.Halide::Runtime::Internal::TraceBuffer"*, %"class.Halide::Runtime::Internal::TraceBuffer"** @_ZN6Halide7Runtime8Internal19halide_trace_bufferE, align 8, !tbaa !14
  %lock.i.i438 = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %72, i64 0, i32 0, i32 0
  br label %while.body.i.i

while.body.i.i:                                   ; preds = %while.body.i.i, %if.then57
  %73 = atomicrmw or i32* %lock.i.i438, i32 1073741824 seq_cst
  %74 = cmpxchg i32* %lock.i.i438, i32 1073741824, i32 -2147483648 seq_cst seq_cst
  %75 = extractvalue { i32, i1 } %74, 1
  br i1 %75, label %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i, label %while.body.i.i

_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i: ; preds = %while.body.i.i
  %cursor.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %72, i64 0, i32 1
  %76 = load i32, i32* %cursor.i, align 4, !tbaa !150
  %tobool.not.i439 = icmp eq i32 %76, 0
  br i1 %tobool.not.i439, label %do.end.critedge.i, label %if.then.i442

if.then.i442:                                     ; preds = %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i
  %overage.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %72, i64 0, i32 2
  %77 = load i32, i32* %overage.i, align 4, !tbaa !152
  %sub.i = sub i32 %76, %77
  store i32 %sub.i, i32* %cursor.i, align 4, !tbaa !150
  %arraydecay.i = getelementptr inbounds %"class.Halide::Runtime::Internal::TraceBuffer", %"class.Halide::Runtime::Internal::TraceBuffer"* %72, i64 0, i32 3, i64 0
  %conv.i440 = zext i32 %sub.i to i64
  %call.i441 = tail call i64 @write(i32 %call, i8* nonnull %arraydecay.i, i64 %conv.i440) #15
  %conv5.i = trunc i64 %call.i441 to i32
  %cmp.i = icmp eq i32 %sub.i, %conv5.i
  %78 = bitcast i32* %cursor.i to <2 x i32>*
  store <2 x i32> zeroinitializer, <2 x i32>* %78, align 4, !tbaa !41
  %79 = atomicrmw and i32* %lock.i.i438, i32 2147483647 seq_cst
  br i1 %cmp.i, label %if.end277, label %if.then10.i

if.then10.i:                                      ; preds = %if.then.i442
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([144 x i8], [144 x i8]* @.str.32, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %if.end277

do.end.critedge.i:                                ; preds = %_ZN6Halide7Runtime8Internal23SharedExclusiveSpinLock17acquire_exclusiveEv.exit.i
  %80 = atomicrmw and i32* %lock.i.i438, i32 2147483647 seq_cst
  br label %if.end277

if.else:                                          ; preds = %entry
  %call.i445 = tail call i8* @malloc(i64 4096) #15
  %tobool.not.i448 = icmp eq i8* %call.i445, null
  br i1 %tobool.not.i448, label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EEC2EPvPc.exit, label %if.then6.i451

if.then6.i451:                                    ; preds = %if.else
  %add.ptr.i449 = getelementptr inbounds i8, i8* %call.i445, i64 4095
  store i8 0, i8* %add.ptr.i449, align 1, !tbaa !18
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EEC2EPvPc.exit: ; preds = %if.then6.i451, %if.else
  %ss.sroa.74.0 = phi i8* [ %add.ptr.i449, %if.then6.i451 ], [ null, %if.else ]
  %bits = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 4, i32 1
  %81 = load i8, i8* %bits, align 1, !tbaa !163
  %conv60 = zext i8 %81 to i32
  br label %while.cond

while.cond:                                       ; preds = %while.cond, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EEC2EPvPc.exit
  %print_bits.0 = phi i32 [ 8, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EEC2EPvPc.exit ], [ %shl, %while.cond ]
  %cmp61 = icmp slt i32 %print_bits.0, %conv60
  %shl = shl i32 %print_bits.0, 1
  br i1 %cmp61, label %while.cond, label %do.body, !llvm.loop !164

do.body:                                          ; preds = %while.cond
  %cmp62 = icmp slt i32 %print_bits.0, 65
  br i1 %cmp62, label %do.end, label %if.then63

if.then63:                                        ; preds = %do.body
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([140 x i8], [140 x i8]* @.str.2.11, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %if.then63, %do.body
  %event65 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 5
  %82 = load i32, i32* %event65, align 4, !tbaa !162
  %cmp66 = icmp slt i32 %82, 2
  %idxprom = zext i32 %82 to i64
  %arrayidx = getelementptr inbounds [11 x i8*], [11 x i8*]* @__const.halide_default_trace.event_types, i64 0, i64 %idxprom
  %83 = load i8*, i8** %arrayidx, align 8, !tbaa !14
  %call.i456 = tail call i8* @halide_string_to_string(i8* %call.i445, i8* %ss.sroa.74.0, i8* %83) #15
  %call.i459 = tail call i8* @halide_string_to_string(i8* %call.i456, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.20.177, i64 0, i64 0)) #15
  %func70 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 0
  %84 = load i8*, i8** %func70, align 8, !tbaa !146
  %call.i462 = tail call i8* @halide_string_to_string(i8* %call.i459, i8* %ss.sroa.74.0, i8* %84) #15
  %call.i465 = tail call i8* @halide_string_to_string(i8* %call.i462, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.141, i64 0, i64 0)) #15
  %value_index73 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 7
  %85 = load i32, i32* %value_index73, align 4, !tbaa !165
  %conv.i468 = sext i32 %85 to i64
  %call.i469 = tail call i8* @halide_int64_to_string(i8* %call.i465, i8* %ss.sroa.74.0, i64 %conv.i468, i32 1) #15
  %call.i472 = tail call i8* @halide_string_to_string(i8* %call.i469, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.22.179, i64 0, i64 0)) #15
  %lanes77 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 4, i32 2
  %86 = load i16, i16* %lanes77, align 2, !tbaa !138
  %cmp79 = icmp ugt i16 %86, 1
  br i1 %cmp79, label %if.then80, label %if.end82

if.then80:                                        ; preds = %do.end
  %call.i475 = tail call i8* @halide_string_to_string(i8* %call.i472, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.17, i64 0, i64 0)) #15
  br label %if.end82

if.end82:                                         ; preds = %if.then80, %do.end
  %ss.sroa.7.0 = phi i8* [ %call.i475, %if.then80 ], [ %call.i472, %do.end ]
  %dimensions83 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 8
  %87 = load i32, i32* %dimensions83, align 8, !tbaa !145
  %cmp84660 = icmp sgt i32 %87, 0
  br i1 %cmp84660, label %if.end100.peel, label %for.cond.cleanup

if.end100.peel:                                   ; preds = %if.end82
  %coordinates101 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 2
  %.pre = load i32*, i32** %coordinates101, align 8, !tbaa !157
  %.pre673 = load i32, i32* %.pre, align 4, !tbaa !41
  %conv.i484.peel = sext i32 %.pre673 to i64
  %call.i485.peel = tail call i8* @halide_int64_to_string(i8* %ss.sroa.7.0, i8* %ss.sroa.74.0, i64 %conv.i484.peel, i32 1) #15
  %88 = load i32, i32* %dimensions83, align 8, !tbaa !145
  %cmp84.peel = icmp sgt i32 %88, 1
  br i1 %cmp84.peel, label %if.then86, label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %if.end100, %if.end100.peel, %if.end82
  %ss.sroa.7.1.lcssa = phi i8* [ %ss.sroa.7.0, %if.end82 ], [ %call.i485.peel, %if.end100.peel ], [ %call.i485, %if.end100 ]
  %89 = load i16, i16* %lanes77, align 2, !tbaa !138
  %cmp108 = icmp ugt i16 %89, 1
  %.sink684 = select i1 %cmp108, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.20, i64 0, i64 0), i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.8.119, i64 0, i64 0)
  %call.i491 = tail call i8* @halide_string_to_string(i8* %ss.sroa.7.1.lcssa, i8* %ss.sroa.74.0, i8* %.sink684) #15
  br i1 %cmp66, label %if.then115, label %if.end263

if.then86:                                        ; preds = %if.end100.peel, %if.end100
  %indvars.iv670 = phi i64 [ %indvars.iv.next671, %if.end100 ], [ 1, %if.end100.peel ]
  %ss.sroa.7.1661 = phi i8* [ %call.i485, %if.end100 ], [ %call.i485.peel, %if.end100.peel ]
  %90 = load i16, i16* %lanes77, align 2, !tbaa !138
  %cmp90 = icmp ugt i16 %90, 1
  br i1 %cmp90, label %land.lhs.true, label %if.else97.split

land.lhs.true:                                    ; preds = %if.then86
  %conv89 = zext i16 %90 to i32
  %91 = trunc i64 %indvars.iv670 to i32
  %rem = urem i32 %91, %conv89
  %cmp94 = icmp eq i32 %rem, 0
  br i1 %cmp94, label %if.end100, label %if.else97.split

if.else97.split:                                  ; preds = %if.then86, %land.lhs.true
  br label %if.end100

if.end100:                                        ; preds = %land.lhs.true, %if.else97.split
  %.sink = phi i8* [ getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0), %if.else97.split ], [ getelementptr inbounds ([5 x i8], [5 x i8]* @.str.18, i64 0, i64 0), %land.lhs.true ]
  %call.i4786 = tail call i8* @halide_string_to_string(i8* %ss.sroa.7.1661, i8* %ss.sroa.74.0, i8* %.sink) #15
  %92 = load i32*, i32** %coordinates101, align 8, !tbaa !157
  %arrayidx103 = getelementptr inbounds i32, i32* %92, i64 %indvars.iv670
  %93 = load i32, i32* %arrayidx103, align 4, !tbaa !41
  %conv.i484 = sext i32 %93 to i64
  %call.i485 = tail call i8* @halide_int64_to_string(i8* %call.i4786, i8* %ss.sroa.74.0, i64 %conv.i484, i32 1) #15
  %indvars.iv.next671 = add nuw nsw i64 %indvars.iv670, 1
  %94 = load i32, i32* %dimensions83, align 8, !tbaa !145
  %95 = sext i32 %94 to i64
  %cmp84 = icmp slt i64 %indvars.iv.next671, %95
  br i1 %cmp84, label %if.then86, label %for.cond.cleanup, !llvm.loop !166

if.then115:                                       ; preds = %for.cond.cleanup
  %96 = load i16, i16* %lanes77, align 2, !tbaa !138
  %cmp119 = icmp ugt i16 %96, 1
  %.sink685 = select i1 %cmp119, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.22, i64 0, i64 0), i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.23, i64 0, i64 0)
  %call.i497 = tail call i8* @halide_string_to_string(i8* %call.i491, i8* %ss.sroa.74.0, i8* %.sink685) #15
  %97 = load i16, i16* %lanes77, align 2, !tbaa !138
  %cmp130655.not = icmp eq i16 %97, 0
  br i1 %cmp130655.not, label %if.end263, label %if.end136.peel

if.end136.peel:                                   ; preds = %if.then115
  %code = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 4, i32 0
  %value245 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 1
  %98 = bitcast i8** %value245 to i8***
  %cmp214 = icmp sgt i32 %print_bits.0, 15
  %99 = bitcast i8** %value245 to i16**
  %100 = bitcast i8** %value245 to float**
  %101 = bitcast i8** %value245 to double**
  %102 = bitcast i8** %value245 to i32**
  %103 = bitcast i8** %value245 to i64**
  %.pre674 = load i8, i8* %code, align 8, !tbaa !168
  switch i8 %.pre674, label %for.inc253.peel [
    i8 0, label %if.then140.peel
    i8 1, label %if.then176.peel
    i8 2, label %do.body213.peel
    i8 3, label %if.then244.peel
  ]

if.then244.peel:                                  ; preds = %if.end136.peel
  %104 = load i8**, i8*** %98, align 8, !tbaa !158
  %105 = load i8*, i8** %104, align 8, !tbaa !14
  %call.i543.peel = tail call i8* @halide_pointer_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i8* %105) #15
  br label %for.inc253.peel

do.body213.peel:                                  ; preds = %if.end136.peel
  br i1 %cmp214, label %do.end218.peel, label %if.then215.peel

if.then215.peel:                                  ; preds = %do.body213.peel
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([142 x i8], [142 x i8]* @.str.24, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end218.peel

do.end218.peel:                                   ; preds = %if.then215.peel, %do.body213.peel
  switch i32 %print_bits.0, label %if.else232.peel [
    i32 32, label %if.then220.peel
    i32 16, label %if.then227.peel
  ]

if.then227.peel:                                  ; preds = %do.end218.peel
  %106 = load i16*, i16** %99, align 8, !tbaa !158
  %107 = load i16, i16* %106, align 2, !tbaa !169
  %call.i535.peel = tail call double @halide_float16_bits_to_double(i16 zeroext %107) #15
  %call2.i.peel = tail call i8* @halide_double_to_string(i8* %call.i497, i8* %ss.sroa.74.0, double %call.i535.peel, i32 1) #15
  br label %for.inc253.peel

if.then220.peel:                                  ; preds = %do.end218.peel
  %108 = load float*, float** %100, align 8, !tbaa !158
  %109 = load float, float* %108, align 4, !tbaa !170
  %conv.i533.peel = fpext float %109 to double
  %call.i534.peel = tail call i8* @halide_double_to_string(i8* %call.i497, i8* %ss.sroa.74.0, double %conv.i533.peel, i32 0) #15
  br label %for.inc253.peel

if.else232.peel:                                  ; preds = %do.end218.peel
  %110 = load double*, double** %101, align 8, !tbaa !158
  %111 = load double, double* %110, align 8, !tbaa !172
  %call.i540.peel = tail call i8* @halide_double_to_string(i8* %call.i497, i8* %ss.sroa.74.0, double %111, i32 1) #15
  br label %for.inc253.peel

if.then176.peel:                                  ; preds = %if.end136.peel
  switch i32 %print_bits.0, label %if.else199.peel [
    i32 8, label %if.then178.peel
    i32 16, label %if.then186.peel
    i32 32, label %if.then194.peel
  ]

if.then194.peel:                                  ; preds = %if.then176.peel
  %112 = load i32*, i32** %102, align 8, !tbaa !158
  %113 = load i32, i32* %112, align 4, !tbaa !41
  %conv.i526.peel = zext i32 %113 to i64
  %call.i527.peel = tail call i8* @halide_uint64_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i64 %conv.i526.peel, i32 1) #15
  br label %for.inc253.peel

if.then186.peel:                                  ; preds = %if.then176.peel
  %114 = load i16*, i16** %99, align 8, !tbaa !158
  %115 = load i16, i16* %114, align 2, !tbaa !169
  %conv.i522.peel = zext i16 %115 to i64
  %call.i523.peel = tail call i8* @halide_int64_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i64 %conv.i522.peel, i32 1) #15
  br label %for.inc253.peel

if.then178.peel:                                  ; preds = %if.then176.peel
  %116 = load i8*, i8** %value245, align 8, !tbaa !158
  %117 = load i8, i8* %116, align 1, !tbaa !18
  %conv.i518.peel = zext i8 %117 to i64
  %call.i519.peel = tail call i8* @halide_int64_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i64 %conv.i518.peel, i32 1) #15
  br label %for.inc253.peel

if.else199.peel:                                  ; preds = %if.then176.peel
  %118 = load i64*, i64** %103, align 8, !tbaa !158
  %119 = load i64, i64* %118, align 8, !tbaa !22
  %call.i530.peel = tail call i8* @halide_uint64_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i64 %119, i32 1) #15
  br label %for.inc253.peel

if.then140.peel:                                  ; preds = %if.end136.peel
  switch i32 %print_bits.0, label %if.else163.peel [
    i32 8, label %if.then142.peel
    i32 16, label %if.then150.peel
    i32 32, label %if.then158.peel
  ]

if.then158.peel:                                  ; preds = %if.then140.peel
  %120 = load i32*, i32** %102, align 8, !tbaa !158
  %121 = load i32, i32* %120, align 4, !tbaa !41
  %conv.i511.peel = sext i32 %121 to i64
  %call.i512.peel = tail call i8* @halide_int64_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i64 %conv.i511.peel, i32 1) #15
  br label %for.inc253.peel

if.then150.peel:                                  ; preds = %if.then140.peel
  %122 = load i16*, i16** %99, align 8, !tbaa !158
  %123 = load i16, i16* %122, align 2, !tbaa !169
  %conv.i507.peel = sext i16 %123 to i64
  %call.i508.peel = tail call i8* @halide_int64_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i64 %conv.i507.peel, i32 1) #15
  br label %for.inc253.peel

if.then142.peel:                                  ; preds = %if.then140.peel
  %124 = load i8*, i8** %value245, align 8, !tbaa !158
  %125 = load i8, i8* %124, align 1, !tbaa !18
  %conv.i503.peel = sext i8 %125 to i64
  %call.i504.peel = tail call i8* @halide_int64_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i64 %conv.i503.peel, i32 1) #15
  br label %for.inc253.peel

if.else163.peel:                                  ; preds = %if.then140.peel
  %126 = load i64*, i64** %103, align 8, !tbaa !158
  %127 = load i64, i64* %126, align 8, !tbaa !22
  %call.i515.peel = tail call i8* @halide_int64_to_string(i8* %call.i497, i8* %ss.sroa.74.0, i64 %127, i32 1) #15
  br label %for.inc253.peel

for.inc253.peel:                                  ; preds = %if.else163.peel, %if.then142.peel, %if.then150.peel, %if.then158.peel, %if.else199.peel, %if.then178.peel, %if.then186.peel, %if.then194.peel, %if.else232.peel, %if.then220.peel, %if.then227.peel, %if.then244.peel, %if.end136.peel
  %ss.sroa.7.7.peel = phi i8* [ %call.i497, %if.end136.peel ], [ %call.i543.peel, %if.then244.peel ], [ %call.i540.peel, %if.else232.peel ], [ %call2.i.peel, %if.then227.peel ], [ %call.i534.peel, %if.then220.peel ], [ %call.i530.peel, %if.else199.peel ], [ %call.i527.peel, %if.then194.peel ], [ %call.i523.peel, %if.then186.peel ], [ %call.i519.peel, %if.then178.peel ], [ %call.i515.peel, %if.else163.peel ], [ %call.i512.peel, %if.then158.peel ], [ %call.i508.peel, %if.then150.peel ], [ %call.i504.peel, %if.then142.peel ]
  %128 = load i16, i16* %lanes77, align 2, !tbaa !138
  %cmp130.peel = icmp ugt i16 %128, 1
  br i1 %cmp130.peel, label %if.end136, label %if.end263

for.cond.cleanup131:                              ; preds = %for.inc253
  %cmp259 = icmp ugt i16 %154, 1
  br i1 %cmp259, label %if.then260, label %if.end263

if.end136:                                        ; preds = %for.inc253.peel, %for.inc253
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.inc253 ], [ 1, %for.inc253.peel ]
  %ss.sroa.7.5656 = phi i8* [ %ss.sroa.7.7, %for.inc253 ], [ %ss.sroa.7.7.peel, %for.inc253.peel ]
  %call.i500 = tail call i8* @halide_string_to_string(i8* %ss.sroa.7.5656, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #15
  %129 = load i8, i8* %code, align 8, !tbaa !168
  switch i8 %129, label %for.inc253 [
    i8 0, label %if.then140
    i8 1, label %if.then176
    i8 2, label %do.body213
    i8 3, label %if.then244
  ]

if.then140:                                       ; preds = %if.end136
  switch i32 %print_bits.0, label %if.else163 [
    i32 8, label %if.then142
    i32 16, label %if.then150
    i32 32, label %if.then158
  ]

if.then142:                                       ; preds = %if.then140
  %130 = load i8*, i8** %value245, align 8, !tbaa !158
  %arrayidx145 = getelementptr inbounds i8, i8* %130, i64 %indvars.iv
  %131 = load i8, i8* %arrayidx145, align 1, !tbaa !18
  %conv.i503 = sext i8 %131 to i64
  %call.i504 = tail call i8* @halide_int64_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i64 %conv.i503, i32 1) #15
  br label %for.inc253

if.then150:                                       ; preds = %if.then140
  %132 = load i16*, i16** %99, align 8, !tbaa !158
  %arrayidx153 = getelementptr inbounds i16, i16* %132, i64 %indvars.iv
  %133 = load i16, i16* %arrayidx153, align 2, !tbaa !169
  %conv.i507 = sext i16 %133 to i64
  %call.i508 = tail call i8* @halide_int64_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i64 %conv.i507, i32 1) #15
  br label %for.inc253

if.then158:                                       ; preds = %if.then140
  %134 = load i32*, i32** %102, align 8, !tbaa !158
  %arrayidx161 = getelementptr inbounds i32, i32* %134, i64 %indvars.iv
  %135 = load i32, i32* %arrayidx161, align 4, !tbaa !41
  %conv.i511 = sext i32 %135 to i64
  %call.i512 = tail call i8* @halide_int64_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i64 %conv.i511, i32 1) #15
  br label %for.inc253

if.else163:                                       ; preds = %if.then140
  %136 = load i64*, i64** %103, align 8, !tbaa !158
  %arrayidx166 = getelementptr inbounds i64, i64* %136, i64 %indvars.iv
  %137 = load i64, i64* %arrayidx166, align 8, !tbaa !22
  %call.i515 = tail call i8* @halide_int64_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i64 %137, i32 1) #15
  br label %for.inc253

if.then176:                                       ; preds = %if.end136
  switch i32 %print_bits.0, label %if.else199 [
    i32 8, label %if.then178
    i32 16, label %if.then186
    i32 32, label %if.then194
  ]

if.then178:                                       ; preds = %if.then176
  %138 = load i8*, i8** %value245, align 8, !tbaa !158
  %arrayidx181 = getelementptr inbounds i8, i8* %138, i64 %indvars.iv
  %139 = load i8, i8* %arrayidx181, align 1, !tbaa !18
  %conv.i518 = zext i8 %139 to i64
  %call.i519 = tail call i8* @halide_int64_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i64 %conv.i518, i32 1) #15
  br label %for.inc253

if.then186:                                       ; preds = %if.then176
  %140 = load i16*, i16** %99, align 8, !tbaa !158
  %arrayidx189 = getelementptr inbounds i16, i16* %140, i64 %indvars.iv
  %141 = load i16, i16* %arrayidx189, align 2, !tbaa !169
  %conv.i522 = zext i16 %141 to i64
  %call.i523 = tail call i8* @halide_int64_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i64 %conv.i522, i32 1) #15
  br label %for.inc253

if.then194:                                       ; preds = %if.then176
  %142 = load i32*, i32** %102, align 8, !tbaa !158
  %arrayidx197 = getelementptr inbounds i32, i32* %142, i64 %indvars.iv
  %143 = load i32, i32* %arrayidx197, align 4, !tbaa !41
  %conv.i526 = zext i32 %143 to i64
  %call.i527 = tail call i8* @halide_uint64_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i64 %conv.i526, i32 1) #15
  br label %for.inc253

if.else199:                                       ; preds = %if.then176
  %144 = load i64*, i64** %103, align 8, !tbaa !158
  %arrayidx202 = getelementptr inbounds i64, i64* %144, i64 %indvars.iv
  %145 = load i64, i64* %arrayidx202, align 8, !tbaa !22
  %call.i530 = tail call i8* @halide_uint64_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i64 %145, i32 1) #15
  br label %for.inc253

do.body213:                                       ; preds = %if.end136
  br i1 %cmp214, label %do.end218, label %if.then215

if.then215:                                       ; preds = %do.body213
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([142 x i8], [142 x i8]* @.str.24, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end218

do.end218:                                        ; preds = %if.then215, %do.body213
  switch i32 %print_bits.0, label %if.else232 [
    i32 32, label %if.then220
    i32 16, label %if.then227
  ]

if.then220:                                       ; preds = %do.end218
  %146 = load float*, float** %100, align 8, !tbaa !158
  %arrayidx223 = getelementptr inbounds float, float* %146, i64 %indvars.iv
  %147 = load float, float* %arrayidx223, align 4, !tbaa !170
  %conv.i533 = fpext float %147 to double
  %call.i534 = tail call i8* @halide_double_to_string(i8* %call.i500, i8* %ss.sroa.74.0, double %conv.i533, i32 0) #15
  br label %for.inc253

if.then227:                                       ; preds = %do.end218
  %148 = load i16*, i16** %99, align 8, !tbaa !158
  %arrayidx230 = getelementptr inbounds i16, i16* %148, i64 %indvars.iv
  %149 = load i16, i16* %arrayidx230, align 2, !tbaa !169
  %call.i535 = tail call double @halide_float16_bits_to_double(i16 zeroext %149) #15
  %call2.i = tail call i8* @halide_double_to_string(i8* %call.i500, i8* %ss.sroa.74.0, double %call.i535, i32 1) #15
  br label %for.inc253

if.else232:                                       ; preds = %do.end218
  %150 = load double*, double** %101, align 8, !tbaa !158
  %arrayidx235 = getelementptr inbounds double, double* %150, i64 %indvars.iv
  %151 = load double, double* %arrayidx235, align 8, !tbaa !172
  %call.i540 = tail call i8* @halide_double_to_string(i8* %call.i500, i8* %ss.sroa.74.0, double %151, i32 1) #15
  br label %for.inc253

if.then244:                                       ; preds = %if.end136
  %152 = load i8**, i8*** %98, align 8, !tbaa !158
  %arrayidx247 = getelementptr inbounds i8*, i8** %152, i64 %indvars.iv
  %153 = load i8*, i8** %arrayidx247, align 8, !tbaa !14
  %call.i543 = tail call i8* @halide_pointer_to_string(i8* %call.i500, i8* %ss.sroa.74.0, i8* %153) #15
  br label %for.inc253

for.inc253:                                       ; preds = %if.then244, %if.else232, %if.then227, %if.then220, %if.else199, %if.then194, %if.then186, %if.then178, %if.else163, %if.then158, %if.then150, %if.then142, %if.end136
  %ss.sroa.7.7 = phi i8* [ %call.i500, %if.end136 ], [ %call.i543, %if.then244 ], [ %call.i540, %if.else232 ], [ %call2.i, %if.then227 ], [ %call.i534, %if.then220 ], [ %call.i530, %if.else199 ], [ %call.i527, %if.then194 ], [ %call.i523, %if.then186 ], [ %call.i519, %if.then178 ], [ %call.i515, %if.else163 ], [ %call.i512, %if.then158 ], [ %call.i508, %if.then150 ], [ %call.i504, %if.then142 ]
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %154 = load i16, i16* %lanes77, align 2, !tbaa !138
  %155 = zext i16 %154 to i64
  %cmp130 = icmp ult i64 %indvars.iv.next, %155
  br i1 %cmp130, label %if.end136, label %for.cond.cleanup131, !llvm.loop !174

if.then260:                                       ; preds = %for.cond.cleanup131
  %call.i546 = tail call i8* @halide_string_to_string(i8* %ss.sroa.7.7, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.25, i64 0, i64 0)) #15
  br label %if.end263

if.end263:                                        ; preds = %if.then260, %for.cond.cleanup131, %for.inc253.peel, %if.then115, %for.cond.cleanup
  %ss.sroa.7.8 = phi i8* [ %call.i546, %if.then260 ], [ %ss.sroa.7.7, %for.cond.cleanup131 ], [ %call.i491, %for.cond.cleanup ], [ %ss.sroa.7.7.peel, %for.inc253.peel ], [ %call.i497, %if.then115 ]
  %trace_tag264 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %e, i64 0, i32 3
  %156 = load i8*, i8** %trace_tag264, align 8, !tbaa !147
  %tobool265.not = icmp eq i8* %156, null
  br i1 %tobool265.not, label %if.end274, label %land.lhs.true266

land.lhs.true266:                                 ; preds = %if.end263
  %157 = load i8, i8* %156, align 1, !tbaa !18
  %tobool268.not = icmp eq i8 %157, 0
  br i1 %tobool268.not, label %if.end274, label %if.then269

if.then269:                                       ; preds = %land.lhs.true266
  %call.i549 = tail call i8* @halide_string_to_string(i8* %ss.sroa.7.8, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([9 x i8], [9 x i8]* @.str.26, i64 0, i64 0)) #15
  %158 = load i8*, i8** %trace_tag264, align 8, !tbaa !147
  %call.i552 = tail call i8* @halide_string_to_string(i8* %call.i549, i8* %ss.sroa.74.0, i8* %158) #15
  %call.i555 = tail call i8* @halide_string_to_string(i8* %call.i552, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.27, i64 0, i64 0)) #15
  br label %if.end274

if.end274:                                        ; preds = %if.then269, %land.lhs.true266, %if.end263
  %ss.sroa.7.9 = phi i8* [ %ss.sroa.7.8, %if.end263 ], [ %ss.sroa.7.8, %land.lhs.true266 ], [ %call.i555, %if.then269 ]
  %call.i558 = tail call i8* @halide_string_to_string(i8* %ss.sroa.7.9, i8* %ss.sroa.74.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.7.164, i64 0, i64 0)) #15
  br label %while.cond.i560

while.cond.i560:                                  ; preds = %while.cond.i560, %if.end274
  %159 = atomicrmw volatile xchg i8* @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE, i8 1 acquire
  %tobool.not.i559 = icmp eq i8 %159, 0
  br i1 %tobool.not.i559, label %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit, label %while.cond.i560, !llvm.loop !175

_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit: ; preds = %while.cond.i560
  br i1 %tobool.not.i448, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  store atomic volatile i8 0, i8* @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE release, align 1
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
  %sub.ptr.lhs.cast.i.i563 = ptrtoint i8* %call.i558 to i64
  %sub.ptr.rhs.cast.i.i564 = ptrtoint i8* %call.i445 to i64
  %sub.ptr.sub.i.i565 = sub i64 1, %sub.ptr.rhs.cast.i.i564
  %add.i.i566 = add i64 %sub.ptr.sub.i.i565, %sub.ptr.lhs.cast.i.i563
  %call.i.i567 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i445, i64 %add.i.i566) #15
  tail call void @halide_print(i8* %user_context, i8* nonnull %call.i445) #15
  store atomic volatile i8 0, i8* @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE release, align 1
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i445, i64 %add.i.i566) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i445) #15
  br label %if.end277

if.end277:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy4096EED2Ev.exit, %do.end.critedge.i, %if.then10.i, %if.then.i442, %_ZN21halide_trace_packet_t9trace_tagEv.exit
  ret i32 %0
}

; Function Attrs: nounwind
define weak i32 @halide_get_trace_file(i8* %user_context) local_unnamed_addr #4 {
entry:
  br label %while.cond.i

while.cond.i:                                     ; preds = %while.cond.i, %entry
  %0 = atomicrmw volatile xchg i8* @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE, i8 1 acquire
  %tobool.not.i = icmp eq i8 %0, 0
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit, label %while.cond.i, !llvm.loop !175

_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit: ; preds = %while.cond.i
  %1 = load i32, i32* @_ZN6Halide7Runtime8Internal17halide_trace_fileE, align 4, !tbaa !41
  %cmp = icmp slt i32 %1, 0
  br i1 %cmp, label %if.then, label %if.end11

if.then:                                          ; preds = %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
  %call = tail call i8* @getenv(i8* getelementptr inbounds ([14 x i8], [14 x i8]* @.str.28, i64 0, i64 0)) #15
  %tobool.not = icmp eq i8* %call, null
  br i1 %tobool.not, label %if.else, label %if.then1

if.then1:                                         ; preds = %if.then
  %call2 = tail call i8* @fopen(i8* nonnull %call, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.29, i64 0, i64 0)) #15
  %tobool3.not = icmp eq i8* %call2, null
  br i1 %tobool3.not, label %if.then4, label %do.end

if.then4:                                         ; preds = %if.then1
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([139 x i8], [139 x i8]* @.str.30, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %if.then4, %if.then1
  %call5 = tail call i32 @fileno(i8* %call2) #15
  tail call void @halide_set_trace_file(i32 %call5) #16
  store i8* %call2, i8** @_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE, align 8, !tbaa !14
  %2 = load %"class.Halide::Runtime::Internal::TraceBuffer"*, %"class.Halide::Runtime::Internal::TraceBuffer"** @_ZN6Halide7Runtime8Internal19halide_trace_bufferE, align 8, !tbaa !14
  %tobool6.not = icmp eq %"class.Halide::Runtime::Internal::TraceBuffer"* %2, null
  br i1 %tobool6.not, label %if.then7, label %if.end11

if.then7:                                         ; preds = %do.end
  %call8 = tail call i8* @malloc(i64 1048588) #15
  store i8* %call8, i8** bitcast (%"class.Halide::Runtime::Internal::TraceBuffer"** @_ZN6Halide7Runtime8Internal19halide_trace_bufferE to i8**), align 8, !tbaa !14
  %cursor.i = getelementptr inbounds i8, i8* %call8, i64 4
  %3 = bitcast i8* %cursor.i to <2 x i32>*
  store <2 x i32> zeroinitializer, <2 x i32>* %3, align 4, !tbaa !41
  %lock.i.i = bitcast i8* %call8 to i32*
  store volatile i32 0, i32* %lock.i.i, align 4, !tbaa !148
  br label %if.end11

if.else:                                          ; preds = %if.then
  tail call void @halide_set_trace_file(i32 0) #16
  br label %if.end11

if.end11:                                         ; preds = %if.else, %if.then7, %do.end, %_ZN6Halide7Runtime8Internal14ScopedSpinLockC2EPVc.exit
  %4 = load i32, i32* @_ZN6Halide7Runtime8Internal17halide_trace_fileE, align 4, !tbaa !41
  store atomic volatile i8 0, i8* @_ZN6Halide7Runtime8Internal22halide_trace_file_lockE release, align 1
  ret i32 %4
}

declare i8* @memcpy(i8*, i8*, i64) local_unnamed_addr #1

declare i8* @fopen(i8*, i8*) local_unnamed_addr #1

declare i32 @fileno(i8*) local_unnamed_addr #1

; Function Attrs: nounwind willreturn mustprogress
define weak void @halide_set_trace_file(i32 %fd) local_unnamed_addr #2 {
entry:
  store i32 %fd, i32* @_ZN6Halide7Runtime8Internal17halide_trace_fileE, align 4, !tbaa !41
  ret void
}

; Function Attrs: nounwind mustprogress
define weak void @halide_trace_cleanup() #0 {
entry:
  %call = tail call i32 @halide_shutdown_trace() #16
  ret void
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_shutdown_trace() local_unnamed_addr #0 {
entry:
  %0 = load i8*, i8** @_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE, align 8, !tbaa !14
  %tobool.not = icmp eq i8* %0, null
  br i1 %tobool.not, label %return, label %if.then

if.then:                                          ; preds = %entry
  %call = tail call i32 @fclose(i8* nonnull %0) #15
  store i32 0, i32* @_ZN6Halide7Runtime8Internal17halide_trace_fileE, align 4, !tbaa !41
  store i8 0, i8* @_ZN6Halide7Runtime8Internal29halide_trace_file_initializedE, align 1, !tbaa !19
  store i8* null, i8** @_ZN6Halide7Runtime8Internal35halide_trace_file_internally_openedE, align 8, !tbaa !14
  %1 = load %"class.Halide::Runtime::Internal::TraceBuffer"*, %"class.Halide::Runtime::Internal::TraceBuffer"** @_ZN6Halide7Runtime8Internal19halide_trace_bufferE, align 8, !tbaa !14
  %tobool1.not = icmp eq %"class.Halide::Runtime::Internal::TraceBuffer"* %1, null
  br i1 %tobool1.not, label %return, label %if.then2

if.then2:                                         ; preds = %if.then
  %2 = bitcast %"class.Halide::Runtime::Internal::TraceBuffer"* %1 to i8*
  tail call void @free(i8* nonnull %2) #15
  br label %return

return:                                           ; preds = %if.then2, %if.then, %entry
  %retval.0 = phi i32 [ %call, %if.then2 ], [ %call, %if.then ], [ 0, %entry ]
  ret i32 %retval.0
}

declare i32 @fclose(i8*) local_unnamed_addr #1

; Function Attrs: nounwind willreturn mustprogress
define weak i32 (i8*, %struct.halide_trace_event_t*)* @halide_set_custom_trace(i32 (i8*, %struct.halide_trace_event_t*)* %t) local_unnamed_addr #2 {
entry:
  %0 = load i32 (i8*, %struct.halide_trace_event_t*)*, i32 (i8*, %struct.halide_trace_event_t*)** @_ZN6Halide7Runtime8Internal19halide_custom_traceE, align 8, !tbaa !14
  store i32 (i8*, %struct.halide_trace_event_t*)* %t, i32 (i8*, %struct.halide_trace_event_t*)** @_ZN6Halide7Runtime8Internal19halide_custom_traceE, align 8, !tbaa !14
  ret i32 (i8*, %struct.halide_trace_event_t*)* %0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_trace(i8* %user_context, %struct.halide_trace_event_t* %e) local_unnamed_addr #0 {
entry:
  %0 = load i32 (i8*, %struct.halide_trace_event_t*)*, i32 (i8*, %struct.halide_trace_event_t*)** @_ZN6Halide7Runtime8Internal19halide_custom_traceE, align 8, !tbaa !14
  %call = tail call i32 %0(i8* %user_context, %struct.halide_trace_event_t* %e) #15
  ret i32 %call
}

; Function Attrs: nounwind
define weak i32 @halide_trace_helper(i8* %user_context, i8* %func, i8* %value, i32* %coords, i32 %type_code, i32 %type_bits, i32 %type_lanes, i32 %code, i32 %parent_id, i32 %value_index, i32 %dimensions, i8* %trace_tag) local_unnamed_addr #4 {
entry:
  %event = alloca %struct.halide_trace_event_t, align 8
  %0 = bitcast %struct.halide_trace_event_t* %event to i8*
  call void @llvm.lifetime.start.p0i8(i64 56, i8* nonnull %0) #11
  %code.i.i = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 4, i32 0
  %bits.i.i = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 4, i32 1
  %lanes.i.i = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 4, i32 2
  %func1 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 0
  store i8* %func, i8** %func1, align 8, !tbaa !146
  %value2 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 1
  store i8* %value, i8** %value2, align 8, !tbaa !158
  %coordinates = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 2
  store i32* %coords, i32** %coordinates, align 8, !tbaa !157
  %trace_tag3 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 3
  store i8* %trace_tag, i8** %trace_tag3, align 8, !tbaa !147
  %conv = trunc i32 %type_code to i8
  store i8 %conv, i8* %code.i.i, align 8, !tbaa !168
  %conv5 = trunc i32 %type_bits to i8
  store i8 %conv5, i8* %bits.i.i, align 1, !tbaa !163
  %conv7 = trunc i32 %type_lanes to i16
  store i16 %conv7, i16* %lanes.i.i, align 2, !tbaa !138
  %event9 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 5
  store i32 %code, i32* %event9, align 4, !tbaa !162
  %parent_id10 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 6
  store i32 %parent_id, i32* %parent_id10, align 8, !tbaa !176
  %value_index11 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 7
  store i32 %value_index, i32* %value_index11, align 4, !tbaa !165
  %dimensions12 = getelementptr inbounds %struct.halide_trace_event_t, %struct.halide_trace_event_t* %event, i64 0, i32 8
  store i32 %dimensions, i32* %dimensions12, align 8, !tbaa !145
  %call = call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %0, i64 56) #15
  %add = add nsw i32 %type_bits, 7
  %div = sdiv i32 %add, 8
  %mul = mul nsw i32 %div, %type_lanes
  %conv13 = sext i32 %mul to i64
  %call14 = call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* %value, i64 %conv13) #15
  %1 = bitcast i32* %coords to i8*
  %conv15 = sext i32 %dimensions to i64
  %mul16 = shl nsw i64 %conv15, 2
  %call17 = call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* %1, i64 %mul16) #15
  %call18 = call i32 @halide_trace(i8* %user_context, %struct.halide_trace_event_t* nonnull %event) #15
  call void @llvm.lifetime.end.p0i8(i64 56, i8* nonnull %0) #11
  ret i32 %call18
}

; Function Attrs: nounwind mustprogress
define linkonce zeroext i1 @_ZN6Halide7Runtime8Internal9ends_withEPKcS3_(i8* %filename, i8* %suffix) local_unnamed_addr #0 {
entry:
  br label %while.cond

while.cond:                                       ; preds = %while.cond, %entry
  %f.0 = phi i8* [ %filename, %entry ], [ %incdec.ptr, %while.cond ]
  %0 = load i8, i8* %f.0, align 1, !tbaa !18
  %tobool.not = icmp eq i8 %0, 0
  %incdec.ptr = getelementptr inbounds i8, i8* %f.0, i64 1
  br i1 %tobool.not, label %while.cond1, label %while.cond, !llvm.loop !177

while.cond1:                                      ; preds = %while.cond, %while.cond1
  %s.0 = phi i8* [ %incdec.ptr4, %while.cond1 ], [ %suffix, %while.cond ]
  %1 = load i8, i8* %s.0, align 1, !tbaa !18
  %tobool2.not = icmp eq i8 %1, 0
  %incdec.ptr4 = getelementptr inbounds i8, i8* %s.0, i64 1
  br i1 %tobool2.not, label %while.cond6.preheader, label %while.cond1, !llvm.loop !178

while.cond6.preheader:                            ; preds = %while.cond1
  %cmp34 = icmp ne i8* %s.0, %suffix
  %cmp735 = icmp ne i8* %f.0, %filename
  %2 = and i1 %cmp735, %cmp34
  br i1 %2, label %if.end, label %while.cond6.preheader.while.end13_crit_edge

while.cond6.preheader.while.end13_crit_edge:      ; preds = %while.cond6.preheader
  %cmp16.0 = icmp eq i8 0, 0
  br label %while.end13

if.end:                                           ; preds = %while.cond6.preheader, %if.end.while.body8_crit_edge
  %f.13649 = phi i8* [ %incdec.ptr11, %if.end.while.body8_crit_edge ], [ %f.0, %while.cond6.preheader ]
  %s.13748 = phi i8* [ %incdec.ptr12, %if.end.while.body8_crit_edge ], [ %s.0, %while.cond6.preheader ]
  %incdec.ptr11 = getelementptr inbounds i8, i8* %f.13649, i64 -1
  %incdec.ptr12 = getelementptr inbounds i8, i8* %s.13748, i64 -1
  %cmp = icmp ne i8* %incdec.ptr12, %suffix
  %cmp7 = icmp ne i8* %incdec.ptr11, %filename
  %3 = and i1 %cmp7, %cmp
  %.pre = load i8, i8* %incdec.ptr11, align 1, !tbaa !18
  %.pre45 = load i8, i8* %incdec.ptr12, align 1, !tbaa !18
  br i1 %3, label %if.end.while.body8_crit_edge, label %if.end.while.end13_crit_edge, !llvm.loop !179

if.end.while.end13_crit_edge:                     ; preds = %if.end
  %cmp16.1 = icmp eq i8 %.pre, %.pre45
  br label %while.end13

if.end.while.body8_crit_edge:                     ; preds = %if.end
  %cmp10.not = icmp eq i8 %.pre, %.pre45
  br i1 %cmp10.not, label %if.end, label %cleanup

while.end13:                                      ; preds = %if.end.while.end13_crit_edge, %while.cond6.preheader.while.end13_crit_edge
  %cmp16.phi = phi i1 [ %cmp16.0, %while.cond6.preheader.while.end13_crit_edge ], [ %cmp16.1, %if.end.while.end13_crit_edge ]
  br label %cleanup

cleanup:                                          ; preds = %if.end.while.body8_crit_edge, %while.end13
  %retval.0 = phi i1 [ %cmp16.phi, %while.end13 ], [ false, %if.end.while.body8_crit_edge ]
  ret i1 %retval.0
}

; Function Attrs: nounwind
define weak i32 @halide_debug_to_file(i8* %user_context, i8* %filename, i32 %type_code, %struct.halide_buffer_t* %buf) local_unnamed_addr #4 {
entry:
  %shape = alloca [4 x %struct.halide_dimension_t], align 16
  %header = alloca %"struct.Halide::Runtime::Internal::halide_tiff_header", align 2
  %offset = alloca i32, align 4
  %count = alloca i32, align 4
  %array_name = alloca [256 x i8], align 1
  %array_name43 = getelementptr inbounds [256 x i8], [256 x i8]* %array_name, i64 0, i64 0
  %header198 = alloca [129 x i8], align 1
  %tags = alloca [8 x i32], align 4
  %extents = alloca [4 x i32], align 4
  %name_header = alloca [2 x i32], align 4
  %payload_header = alloca [2 x i32], align 4
  %header289 = alloca [5 x i32], align 4
  %temp = alloca [4096 x i8], align 1
  %idx = alloca [4 x i32], align 4
  %zero = alloca i64, align 8
  %host.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 2
  %0 = load i8*, i8** %host.i, align 8, !tbaa !180
  %cmp.i = icmp eq i8* %0, null
  br i1 %cmp.i, label %_ZNK15halide_buffer_t15is_bounds_queryEv.exit, label %if.end

_ZNK15halide_buffer_t15is_bounds_queryEv.exit:    ; preds = %entry
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp2.i = icmp eq i64 %1, 0
  br i1 %cmp2.i, label %if.then, label %if.end

if.then:                                          ; preds = %_ZNK15halide_buffer_t15is_bounds_queryEv.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.34, i64 0, i64 0)) #15
  br label %return

if.end:                                           ; preds = %_ZNK15halide_buffer_t15is_bounds_queryEv.exit, %entry
  %dimensions = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 5
  %2 = load i32, i32* %dimensions, align 4, !tbaa !183
  %cmp = icmp sgt i32 %2, 4
  br i1 %cmp, label %if.then1, label %if.end2

if.then1:                                         ; preds = %if.end
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([59 x i8], [59 x i8]* @.str.1.35, i64 0, i64 0)) #15
  br label %return

if.end2:                                          ; preds = %if.end
  %call3 = tail call i32 @halide_copy_to_host(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %cmp4.not = icmp eq i32 %call3, 0
  br i1 %cmp4.not, label %if.end6, label %return

if.end6:                                          ; preds = %if.end2
  %call.i598 = tail call i8* @fopen(i8* %filename, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.2.36, i64 0, i64 0)) #15
  %cmp.i601.not = icmp eq i8* %call.i598, null
  br i1 %cmp.i601.not, label %return, label %if.end9

if.end9:                                          ; preds = %if.end6
  %3 = bitcast [4 x %struct.halide_dimension_t]* %shape to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %3) #11
  %min.i = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 0, i32 0
  %extent.i = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 0, i32 1
  %4 = bitcast [4 x %struct.halide_dimension_t]* %shape to <4 x i32>*
  store <4 x i32> zeroinitializer, <4 x i32>* %4, align 16, !tbaa !41
  %min.i.1 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 1, i32 0
  %extent.i.1 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 1, i32 1
  %5 = bitcast i32* %min.i.1 to <4 x i32>*
  store <4 x i32> zeroinitializer, <4 x i32>* %5, align 16, !tbaa !41
  %min.i.2 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 2, i32 0
  %extent.i.2 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 2, i32 1
  %6 = bitcast i32* %min.i.2 to <4 x i32>*
  store <4 x i32> zeroinitializer, <4 x i32>* %6, align 16, !tbaa !41
  %min.i.3 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 3, i32 0
  %extent.i.3 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 3, i32 1
  %7 = bitcast i32* %min.i.3 to <4 x i32>*
  store <4 x i32> zeroinitializer, <4 x i32>* %7, align 16, !tbaa !41
  %8 = load i32, i32* %dimensions, align 4, !tbaa !183
  %cmp11875 = icmp sgt i32 %8, 0
  br i1 %cmp11875, label %for.body.lr.ph, label %for.body22.preheader

for.body.lr.ph:                                   ; preds = %if.end9
  %dim = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 6
  %9 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim, align 8, !tbaa !184
  %10 = zext i32 %8 to i64
  %11 = add nsw i64 %10, -1
  %12 = icmp ult i64 %11, 3
  %13 = select i1 %12, i64 %10, i64 4
  br label %for.body

for.cond19.preheader:                             ; preds = %for.body
  %cmp20873 = icmp slt i32 %8, 4
  br i1 %cmp20873, label %for.body22.preheader, label %for.cond.cleanup21

for.body22.preheader:                             ; preds = %for.cond19.preheader, %if.end9
  %elts.0.lcssa907 = phi i64 [ %mul, %for.cond19.preheader ], [ 1, %if.end9 ]
  %14 = sext i32 %8 to i64
  %15 = sub i32 3, %8
  %16 = zext i32 %15 to i64
  %17 = add nuw nsw i64 %16, 1
  %min.iters.check = icmp eq i32 %15, 0
  br i1 %min.iters.check, label %for.body22.preheader162, label %vector.ph

vector.ph:                                        ; preds = %for.body22.preheader
  %n.vec = and i64 %17, 8589934590
  %ind.end = add nsw i64 %n.vec, %14
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %14
  %induction36 = add i64 %offset.idx, 1
  %18 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 %offset.idx, i32 0
  %19 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 %induction36, i32 0
  %20 = bitcast i32* %18 to <2 x i32>*
  store <2 x i32> <i32 0, i32 1>, <2 x i32>* %20, align 16, !tbaa !41
  %21 = bitcast i32* %19 to <2 x i32>*
  store <2 x i32> <i32 0, i32 1>, <2 x i32>* %21, align 16, !tbaa !41
  %22 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 %offset.idx, i32 2
  %23 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 %induction36, i32 2
  store i32 0, i32* %22, align 8, !tbaa !185
  store i32 0, i32* %23, align 8, !tbaa !185
  %index.next = add i64 %index, 2
  %24 = icmp eq i64 %index.next, %n.vec
  br i1 %24, label %middle.block, label %vector.body, !llvm.loop !187

middle.block:                                     ; preds = %vector.body
  %cmp.n = icmp eq i64 %17, %n.vec
  br i1 %cmp.n, label %for.cond.cleanup21, label %for.body22.preheader162

for.body22.preheader162:                          ; preds = %for.body22.preheader, %middle.block
  %indvars.iv.ph = phi i64 [ %14, %for.body22.preheader ], [ %ind.end, %middle.block ]
  br label %for.body22

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %indvars.iv887 = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next888, %for.body ]
  %elts.0877 = phi i64 [ 1, %for.body.lr.ph ], [ %mul, %for.body ]
  %arrayidx = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %9, i64 %indvars.iv887
  %arrayidx14 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 %indvars.iv887
  %25 = bitcast %struct.halide_dimension_t* %arrayidx14 to i8*
  %26 = bitcast %struct.halide_dimension_t* %arrayidx to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 dereferenceable(16) %25, i8* nonnull align 4 dereferenceable(16) %26, i64 16, i1 false), !tbaa.struct !188
  %extent = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 %indvars.iv887, i32 1
  %27 = load i32, i32* %extent, align 4, !tbaa !189
  %conv903 = zext i32 %27 to i64
  %mul = mul i64 %elts.0877, %conv903
  %indvars.iv.next888 = add nuw nsw i64 %indvars.iv887, 1
  %exitcond.not17 = icmp eq i64 %indvars.iv.next888, %13
  br i1 %exitcond.not17, label %for.cond19.preheader, label %for.body, !llvm.loop !190

for.cond.cleanup21:                               ; preds = %for.body22, %middle.block, %for.cond19.preheader
  %elts.0.lcssa906 = phi i64 [ %mul, %for.cond19.preheader ], [ %elts.0.lcssa907, %middle.block ], [ %elts.0.lcssa907, %for.body22 ]
  %bits.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 4, i32 1
  %28 = load i8, i8* %bits.i, align 1, !tbaa !144
  %conv.i = zext i8 %28 to i32
  %add.i = add nuw nsw i32 %conv.i, 7
  %div.i = lshr i32 %add.i, 3
  %call34 = tail call zeroext i1 @_ZN6Halide7Runtime8Internal9ends_withEPKcS3_(i8* %filename, i8* getelementptr inbounds ([6 x i8], [6 x i8]* @.str.3.37, i64 0, i64 0)) #16
  br i1 %call34, label %if.then36, label %lor.lhs.false

for.body22:                                       ; preds = %for.body22.preheader162, %for.body22
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body22 ], [ %indvars.iv.ph, %for.body22.preheader162 ]
  %min = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 %indvars.iv, i32 0
  %29 = bitcast i32* %min to <2 x i32>*
  store <2 x i32> <i32 0, i32 1>, <2 x i32>* %29, align 16, !tbaa !41
  %stride = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %shape, i64 0, i64 %indvars.iv, i32 2
  store i32 0, i32* %stride, align 8, !tbaa !185
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %lftr.wideiv = trunc i64 %indvars.iv.next to i32
  %exitcond886.not = icmp eq i32 %lftr.wideiv, 4
  br i1 %exitcond886.not, label %for.cond.cleanup21, label %for.body22, !llvm.loop !191

lor.lhs.false:                                    ; preds = %for.cond.cleanup21
  %call35 = tail call zeroext i1 @_ZN6Halide7Runtime8Internal9ends_withEPKcS3_(i8* %filename, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.4.38, i64 0, i64 0)) #16
  br i1 %call35, label %if.then36, label %if.else164

if.then36:                                        ; preds = %lor.lhs.false, %for.cond.cleanup21
  %30 = load i32, i32* %extent.i, align 4, !tbaa !189
  %31 = load i32, i32* %extent.i.1, align 4, !tbaa !189
  %32 = load i32, i32* %extent.i.3, align 4, !tbaa !189
  %switch = icmp ult i32 %32, 2
  %33 = load i32, i32* %extent.i.2, align 4, !tbaa !189
  %cmp50 = icmp slt i32 %33, 5
  %or.cond = and i1 %switch, %cmp50
  %depth.0 = select i1 %or.cond, i32 1, i32 %33
  %channels.0 = select i1 %or.cond, i32 %33, i32 %32
  %34 = bitcast %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header to i8*
  call void @llvm.lifetime.start.p0i8(i64 210, i8* nonnull %34) #11
  %byte_order_marker = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 0
  store i16 18761, i16* %byte_order_marker, align 2, !tbaa !192
  %version = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 1
  store i16 42, i16* %version, align 2, !tbaa !194
  %ifd0_offset = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 2
  store i32 8, i32* %ifd0_offset, align 2, !tbaa !195
  %entry_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 3
  store i16 15, i16* %entry_count, align 2, !tbaa !196
  %tag_code2.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 0, i32 0
  store i16 256, i16* %tag_code2.i, align 2, !tbaa !197
  %type_code.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 0, i32 1
  store i16 4, i16* %type_code.i, align 2, !tbaa !199
  %count3.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 0, i32 2
  store i32 1, i32* %count3.i, align 2, !tbaa !200
  %i32.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 0, i32 3, i32 0
  store i32 %30, i32* %i32.i, align 2, !tbaa !18
  %tag_code2.i632 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 1, i32 0
  store i16 257, i16* %tag_code2.i632, align 2, !tbaa !197
  %type_code.i633 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 1, i32 1
  store i16 4, i16* %type_code.i633, align 2, !tbaa !199
  %count3.i634 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 1, i32 2
  store i32 1, i32* %count3.i634, align 2, !tbaa !200
  %i32.i635 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 1, i32 3, i32 0
  store i32 %31, i32* %i32.i635, align 2, !tbaa !18
  %35 = trunc i32 %add.i to i16
  %conv68 = and i16 %35, 504
  %tag_code2.i643 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 2, i32 0
  store i16 258, i16* %tag_code2.i643, align 2, !tbaa !197
  %type_code.i644 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 2, i32 1
  store i16 3, i16* %type_code.i644, align 2, !tbaa !199
  %count3.i645 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 2, i32 2
  store i32 1, i32* %count3.i645, align 2, !tbaa !200
  %value4.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 2, i32 3
  %i16.i = bitcast %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock"* %value4.i to i16*
  store i16 %conv68, i16* %i16.i, align 2, !tbaa !18
  %tag_code2.i653 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 3, i32 0
  store i16 259, i16* %tag_code2.i653, align 2, !tbaa !197
  %type_code.i654 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 3, i32 1
  store i16 3, i16* %type_code.i654, align 2, !tbaa !199
  %count3.i655 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 3, i32 2
  store i32 1, i32* %count3.i655, align 2, !tbaa !200
  %value4.i656 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 3, i32 3
  %i16.i657 = bitcast %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock"* %value4.i656 to i16*
  store i16 1, i16* %i16.i657, align 2, !tbaa !18
  %cmp71 = icmp sgt i32 %channels.0, 2
  %conv72 = select i1 %cmp71, i16 2, i16 1
  %tag_code2.i665 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 4, i32 0
  store i16 262, i16* %tag_code2.i665, align 2, !tbaa !197
  %type_code.i666 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 4, i32 1
  store i16 3, i16* %type_code.i666, align 2, !tbaa !199
  %count3.i667 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 4, i32 2
  store i32 1, i32* %count3.i667, align 2, !tbaa !200
  %value4.i668 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 4, i32 3
  %i16.i669 = bitcast %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock"* %value4.i668 to i16*
  store i16 %conv72, i16* %i16.i669, align 2, !tbaa !18
  %tag_code2.i677 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 5, i32 0
  store i16 273, i16* %tag_code2.i677, align 2, !tbaa !197
  %type_code.i678 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 5, i32 1
  store i16 4, i16* %type_code.i678, align 2, !tbaa !199
  %count3.i679 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 5, i32 2
  store i32 %channels.0, i32* %count3.i679, align 2, !tbaa !200
  %i32.i680 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 5, i32 3, i32 0
  store i32 210, i32* %i32.i680, align 2, !tbaa !18
  %conv75 = trunc i32 %channels.0 to i16
  %tag_code2.i688 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 6, i32 0
  store i16 277, i16* %tag_code2.i688, align 2, !tbaa !197
  %type_code.i689 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 6, i32 1
  store i16 3, i16* %type_code.i689, align 2, !tbaa !199
  %count3.i690 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 6, i32 2
  store i32 1, i32* %count3.i690, align 2, !tbaa !200
  %value4.i691 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 6, i32 3
  %i16.i692 = bitcast %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock"* %value4.i691 to i16*
  store i16 %conv75, i16* %i16.i692, align 2, !tbaa !18
  %tag_code2.i706 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 7, i32 0
  store i16 278, i16* %tag_code2.i706, align 2, !tbaa !197
  %type_code.i707 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 7, i32 1
  store i16 4, i16* %type_code.i707, align 2, !tbaa !199
  %count3.i708 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 7, i32 2
  store i32 1, i32* %count3.i708, align 2, !tbaa !200
  %i32.i709 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 7, i32 3, i32 0
  store i32 %31, i32* %i32.i709, align 2, !tbaa !18
  %cmp80 = icmp eq i32 %channels.0, 1
  %36 = trunc i64 %elts.0.lcssa906 to i32
  %conv86595 = mul i32 %div.i, %36
  %mul84 = shl i32 %channels.0, 2
  %add = add i32 %mul84, 210
  %add.sink = select i1 %cmp80, i32 %conv86595, i32 %add
  %37 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 8, i32 0
  store i16 279, i16* %37, align 2
  %38 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 8, i32 1
  store i16 4, i16* %38, align 2
  %39 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 8, i32 2
  store i32 %channels.0, i32* %39, align 2
  %40 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 8, i32 3, i32 0
  store i32 %add.sink, i32* %40, align 2
  %tag_code2.i732 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 9, i32 0
  store i16 282, i16* %tag_code2.i732, align 2, !tbaa !197
  %type_code3.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 9, i32 1
  store i16 5, i16* %type_code3.i, align 2, !tbaa !199
  %count4.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 9, i32 2
  %41 = bitcast i32* %count4.i to <2 x i32>*
  store <2 x i32> <i32 1, i32 194>, <2 x i32>* %41, align 2, !tbaa !18
  %tag_code2.i741 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 10, i32 0
  store i16 283, i16* %tag_code2.i741, align 2, !tbaa !197
  %type_code3.i742 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 10, i32 1
  store i16 5, i16* %type_code3.i742, align 2, !tbaa !199
  %count4.i743 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 10, i32 2
  %42 = bitcast i32* %count4.i743 to <2 x i32>*
  store <2 x i32> <i32 1, i32 202>, <2 x i32>* %42, align 2, !tbaa !18
  %tag_code2.i759 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 11, i32 0
  store i16 284, i16* %tag_code2.i759, align 2, !tbaa !197
  %type_code.i760 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 11, i32 1
  store i16 3, i16* %type_code.i760, align 2, !tbaa !199
  %count3.i761 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 11, i32 2
  store i32 1, i32* %count3.i761, align 2, !tbaa !200
  %value4.i762 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 11, i32 3
  %i16.i763 = bitcast %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock"* %value4.i762 to i16*
  store i16 2, i16* %i16.i763, align 2, !tbaa !18
  %tag_code2.i754 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 12, i32 0
  store i16 296, i16* %tag_code2.i754, align 2, !tbaa !197
  %type_code.i755 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 12, i32 1
  store i16 3, i16* %type_code.i755, align 2, !tbaa !199
  %count3.i756 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 12, i32 2
  store i32 1, i32* %count3.i756, align 2, !tbaa !200
  %value4.i757 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 12, i32 3
  %i16.i758 = bitcast %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock"* %value4.i757 to i16*
  store i16 1, i16* %i16.i758, align 2, !tbaa !18
  %idxprom92 = sext i32 %type_code to i64
  %arrayidx93 = getelementptr inbounds [10 x i16], [10 x i16]* @_ZN6Halide7Runtime8Internal30pixel_type_to_tiff_sample_typeE, i64 0, i64 %idxprom92
  %43 = load i16, i16* %arrayidx93, align 2, !tbaa !169
  %tag_code2.i749 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 13, i32 0
  store i16 339, i16* %tag_code2.i749, align 2, !tbaa !197
  %type_code.i750 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 13, i32 1
  store i16 3, i16* %type_code.i750, align 2, !tbaa !199
  %count3.i751 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 13, i32 2
  store i32 1, i32* %count3.i751, align 2, !tbaa !200
  %value4.i752 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 13, i32 3
  %i16.i753 = bitcast %"class.Halide::Runtime::Internal::SharedExclusiveSpinLock"* %value4.i752 to i16*
  store i16 %43, i16* %i16.i753, align 2, !tbaa !18
  %tag_code2.i745 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 14, i32 0
  store i16 -32539, i16* %tag_code2.i745, align 2, !tbaa !197
  %type_code.i746 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 14, i32 1
  store i16 4, i16* %type_code.i746, align 2, !tbaa !199
  %count3.i747 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 14, i32 2
  store i32 1, i32* %count3.i747, align 2, !tbaa !200
  %i32.i748 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 4, i64 14, i32 3, i32 0
  store i32 %depth.0, i32* %i32.i748, align 2, !tbaa !18
  %ifd0_end = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 5
  %44 = bitcast i32* %ifd0_end to <4 x i32>*
  store <4 x i32> <i32 0, i32 1, i32 1, i32 1>, <4 x i32>* %44, align 2, !tbaa !41
  %arrayidx100 = getelementptr inbounds %"struct.Halide::Runtime::Internal::halide_tiff_header", %"struct.Halide::Runtime::Internal::halide_tiff_header"* %header, i64 0, i32 7, i64 1
  store i32 1, i32* %arrayidx100, align 2, !tbaa !41
  %call.i736 = call i64 @fwrite(i8* nonnull %34, i64 210, i64 1, i8* nonnull %call.i598) #15
  %cmp.i737.not = icmp eq i64 %call.i736, 0
  br i1 %cmp.i737.not, label %cleanup154, label %if.end103

if.end103:                                        ; preds = %if.then36
  %cmp104 = icmp sgt i32 %channels.0, 1
  br i1 %cmp104, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731.lr.ph, label %cleanup154.thread

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731.lr.ph: ; preds = %if.end103
  %45 = bitcast i32* %offset to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #11
  %mul108 = shl i32 %channels.0, 3
  %add109 = add i32 %mul108, 210
  store i32 %add109, i32* %offset, align 4, !tbaa !41
  %mul123 = mul i32 %depth.0, %div.i
  %46 = load i32, i32* %extent.i, align 4
  %47 = load i32, i32* %extent.i.1, align 4
  %mul124 = mul i32 %mul123, %46
  %mul125 = mul i32 %mul124, %47
  br label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731: ; preds = %if.end118, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731.lr.ph
  %i111.0865 = phi i32 [ 0, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731.lr.ph ], [ %inc128, %if.end118 ]
  %call.i727 = call i64 @fwrite(i8* nonnull %45, i64 4, i64 1, i8* nonnull %call.i598) #15
  %cmp.i728.not = icmp eq i64 %call.i727, 0
  br i1 %cmp.i728.not, label %cleanup151.thread, label %if.end118

if.end118:                                        ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731
  %48 = load i32, i32* %offset, align 4, !tbaa !41
  %add126 = add nsw i32 %mul125, %48
  store i32 %add126, i32* %offset, align 4, !tbaa !41
  %inc128 = add nuw nsw i32 %i111.0865, 1
  %exitcond881.not = icmp eq i32 %inc128, %channels.0
  br i1 %exitcond881.not, label %for.end129, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731, !llvm.loop !201

for.end129:                                       ; preds = %if.end118
  %49 = bitcast i32* %count to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #11
  store i32 %mul125, i32* %count, align 4, !tbaa !41
  %inc146.1 = add nuw nsw i32 0, 1
  br label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720

for.cond138:                                      ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720
  %exitcond.not = icmp eq i32 %inc146.phi, %channels.0
  br i1 %exitcond.not, label %cleanup151, label %for.cond138._ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720_crit_edge, !llvm.loop !202

for.cond138._ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720_crit_edge: ; preds = %for.cond138
  %inc146.0 = add nuw nsw i32 %inc146.phi, 1
  br label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720: ; preds = %for.cond138._ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720_crit_edge, %for.end129
  %inc146.phi = phi i32 [ %inc146.0, %for.cond138._ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720_crit_edge ], [ %inc146.1, %for.end129 ]
  %call.i716 = call i64 @fwrite(i8* nonnull %49, i64 4, i64 1, i8* nonnull %call.i598) #15
  %cmp.i717.not = icmp eq i64 %call.i716, 0
  br i1 %cmp.i717.not, label %select.unfold, label %for.cond138

select.unfold:                                    ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit720
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #11
  br label %cleanup151.thread

cleanup151.thread:                                ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731, %select.unfold
  %retval.2.ph = phi i32 [ -5, %select.unfold ], [ -4, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit731 ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #11
  br label %cleanup154

cleanup151:                                       ; preds = %for.cond138
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #11
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #11
  br label %cleanup154.thread

cleanup154.thread:                                ; preds = %cleanup151, %if.end103
  call void @llvm.lifetime.end.p0i8(i64 210, i8* nonnull %34) #11
  br label %if.end311

cleanup154:                                       ; preds = %cleanup151.thread, %if.then36
  %retval.4 = phi i32 [ -3, %if.then36 ], [ %retval.2.ph, %cleanup151.thread ]
  call void @llvm.lifetime.end.p0i8(i64 210, i8* nonnull %34) #11
  br label %cleanup433

if.else164:                                       ; preds = %lor.lhs.false
  %call165 = tail call zeroext i1 @_ZN6Halide7Runtime8Internal9ends_withEPKcS3_(i8* %filename, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.5.39, i64 0, i64 0)) #16
  br i1 %call165, label %while.cond, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit631

while.cond:                                       ; preds = %if.else164, %while.cond
  %end.0 = phi i8* [ %incdec.ptr167, %while.cond ], [ %filename, %if.else164 ]
  %50 = load i8, i8* %end.0, align 1, !tbaa !18
  %tobool.not = icmp eq i8 %50, 0
  %incdec.ptr167 = getelementptr inbounds i8, i8* %end.0, i64 1
  br i1 %tobool.not, label %while.body171, label %while.cond, !llvm.loop !203

while.cond174.preheader:                          ; preds = %while.body171
  %end.1872.lcssa883 = ptrtoint i8* %end.1872 to i64
  %51 = ptrtoint i8* %filename to i64
  %52 = sub i64 %51, %end.1872.lcssa883
  br label %while.cond174

while.body171:                                    ; preds = %while.cond, %while.body171
  %end.1872 = phi i8* [ %incdec.ptr172.ptr, %while.body171 ], [ %end.0, %while.cond ]
  %incdec.ptr172.ptr = getelementptr inbounds i8, i8* %end.1872, i64 -1
  %.pr = load i8, i8* %incdec.ptr172.ptr, align 1, !tbaa !18
  %cmp170.not = icmp eq i8 %.pr, 46
  br i1 %cmp170.not, label %while.cond174.preheader, label %while.body171, !llvm.loop !204

while.cond174:                                    ; preds = %land.rhs176, %while.cond174.preheader
  %start.0.idx = phi i64 [ %start.0.add, %land.rhs176 ], [ -1, %while.cond174.preheader ]
  %start.0.ptr.ptr = getelementptr inbounds i8, i8* %end.1872, i64 %start.0.idx
  %cmp175.not = icmp eq i8* %start.0.ptr.ptr, %filename
  br i1 %cmp175.not, label %while.end183, label %land.rhs176

land.rhs176:                                      ; preds = %while.cond174
  %start.0.add = add nsw i64 %start.0.idx, -1
  %arrayidx177.ptr = getelementptr inbounds i8, i8* %end.1872, i64 %start.0.add
  %53 = load i8, i8* %arrayidx177.ptr, align 1, !tbaa !18
  %cmp179.not = icmp eq i8 %53, 47
  br i1 %cmp179.not, label %while.end183, label %while.cond174, !llvm.loop !205

while.end183:                                     ; preds = %land.rhs176, %while.cond174
  %start.0.idx.lcssa = phi i64 [ %52, %while.cond174 ], [ %start.0.idx, %land.rhs176 ]
  %54 = trunc i64 %start.0.idx.lcssa to i32
  %conv184 = xor i32 %54, -1
  %55 = getelementptr inbounds [256 x i8], [256 x i8]* %array_name, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %55) #11
  %cmp186.not868 = icmp eq i64 %start.0.idx.lcssa, -1
  br i1 %cmp186.not868, label %iter.check, label %while.body187.preheader

while.body187.preheader:                          ; preds = %while.end183
  %56 = xor i64 %start.0.idx.lcssa, -1
  %min.iters.check40 = icmp ugt i64 %start.0.idx.lcssa, -33
  br i1 %min.iters.check40, label %while.body187.preheader160, label %vector.memcheck

vector.memcheck:                                  ; preds = %while.body187.preheader
  %57 = xor i64 %start.0.idx.lcssa, -1
  %scevgep44 = getelementptr [256 x i8], [256 x i8]* %array_name, i64 0, i64 %57
  %scevgep45 = getelementptr i8, i8* %end.1872, i64 %start.0.idx.lcssa
  %scevgep46 = getelementptr i8, i8* %end.1872, i64 -1
  %bound0 = icmp ult i8* %array_name43, %scevgep46
  %bound1 = icmp ult i8* %scevgep45, %scevgep44
  %found.conflict = and i1 %bound0, %bound1
  br i1 %found.conflict, label %while.body187.preheader160, label %vector.ph42

vector.ph42:                                      ; preds = %vector.memcheck
  %n.vec48 = and i64 %56, -32
  %ind.end52 = getelementptr [256 x i8], [256 x i8]* %array_name, i64 0, i64 %n.vec48
  %ind.end54 = add i64 %start.0.idx.lcssa, %n.vec48
  br label %vector.body39

vector.body39:                                    ; preds = %vector.body39, %vector.ph42
  %index49 = phi i64 [ 0, %vector.ph42 ], [ %index.next50, %vector.body39 ]
  %next.gep = getelementptr [256 x i8], [256 x i8]* %array_name, i64 0, i64 %index49
  %offset.idx57 = add i64 %start.0.idx.lcssa, %index49
  %58 = getelementptr inbounds i8, i8* %end.1872, i64 %offset.idx57
  %59 = bitcast i8* %58 to <16 x i8>*
  %wide.load = load <16 x i8>, <16 x i8>* %59, align 1, !tbaa !18, !alias.scope !206
  %60 = getelementptr inbounds i8, i8* %58, i64 16
  %61 = bitcast i8* %60 to <16 x i8>*
  %wide.load58 = load <16 x i8>, <16 x i8>* %61, align 1, !tbaa !18, !alias.scope !206
  %62 = bitcast i8* %next.gep to <16 x i8>*
  store <16 x i8> %wide.load, <16 x i8>* %62, align 1, !tbaa !18, !alias.scope !209, !noalias !206
  %63 = getelementptr i8, i8* %next.gep, i64 16
  %64 = bitcast i8* %63 to <16 x i8>*
  store <16 x i8> %wide.load58, <16 x i8>* %64, align 1, !tbaa !18, !alias.scope !209, !noalias !206
  %index.next50 = add i64 %index49, 32
  %65 = icmp eq i64 %index.next50, %n.vec48
  br i1 %65, label %middle.block37, label %vector.body39, !llvm.loop !211

middle.block37:                                   ; preds = %vector.body39
  %cmp.n55 = icmp eq i64 %n.vec48, %56
  br i1 %cmp.n55, label %while.cond191.preheader, label %while.body187.preheader160

while.body187.preheader160:                       ; preds = %vector.memcheck, %while.body187.preheader, %middle.block37
  %dst.0870.ph = phi i8* [ %55, %vector.memcheck ], [ %55, %while.body187.preheader ], [ %ind.end52, %middle.block37 ]
  %start.1869.idx.ph = phi i64 [ %start.0.idx.lcssa, %vector.memcheck ], [ %start.0.idx.lcssa, %while.body187.preheader ], [ %ind.end54, %middle.block37 ]
  br label %while.body187

while.cond191.preheader:                          ; preds = %while.body187, %middle.block37
  %incdec.ptr189.lcssa = phi i8* [ %ind.end52, %middle.block37 ], [ %incdec.ptr189, %while.body187 ]
  %add.ptr = getelementptr inbounds [256 x i8], [256 x i8]* %array_name, i64 0, i64 256
  %cmp193866 = icmp ult i8* %incdec.ptr189.lcssa, %add.ptr
  br i1 %cmp193866, label %iter.check, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705

iter.check:                                       ; preds = %while.cond191.preheader, %while.end183
  %dst.0.lcssa911 = phi i8* [ %incdec.ptr189.lcssa, %while.cond191.preheader ], [ %55, %while.end183 ]
  %dst.0.lcssa91162 = ptrtoint i8* %dst.0.lcssa911 to i64
  %scevgep = getelementptr inbounds [256 x i8], [256 x i8]* %array_name, i64 1, i64 0
  %66 = sub i64 0, %dst.0.lcssa91162
  %scevgep63 = getelementptr [256 x i8], [256 x i8]* %array_name, i64 1, i64 %66
  %exitcount.ptrcnt.to.int = ptrtoint i8* %scevgep63 to i64
  %min.iters.check65 = icmp ult i8* %scevgep63, inttoptr (i64 4 to i8*)
  br i1 %min.iters.check65, label %while.body194.preheader, label %vector.main.loop.iter.check

vector.main.loop.iter.check:                      ; preds = %iter.check
  %min.iters.check67 = icmp ult i8* %scevgep63, inttoptr (i64 32 to i8*)
  br i1 %min.iters.check67, label %vec.epilog.ph, label %vector.ph68

vector.ph68:                                      ; preds = %vector.main.loop.iter.check
  %n.vec70 = and i64 %exitcount.ptrcnt.to.int, -32
  %next.gep74.0 = getelementptr i8, i8* %dst.0.lcssa911, i64 0
  %index.next72.0 = add i64 0, 32
  br label %vector.body61

vector.body61:                                    ; preds = %vector.body61.vector.body61_crit_edge, %vector.ph68
  %index.next72.phi = phi i64 [ %index.next72.0, %vector.ph68 ], [ %index.next72.1, %vector.body61.vector.body61_crit_edge ]
  %next.gep74.phi = phi i8* [ %next.gep74.0, %vector.ph68 ], [ %next.gep74.1, %vector.body61.vector.body61_crit_edge ]
  %67 = bitcast i8* %next.gep74.phi to <16 x i8>*
  store <16 x i8> zeroinitializer, <16 x i8>* %67, align 1, !tbaa !18
  %68 = getelementptr i8, i8* %next.gep74.phi, i64 16
  %69 = bitcast i8* %68 to <16 x i8>*
  store <16 x i8> zeroinitializer, <16 x i8>* %69, align 1, !tbaa !18
  %70 = icmp eq i64 %index.next72.phi, %n.vec70
  br i1 %70, label %middle.block59, label %vector.body61.vector.body61_crit_edge, !llvm.loop !212

vector.body61.vector.body61_crit_edge:            ; preds = %vector.body61
  %next.gep74.1 = getelementptr i8, i8* %dst.0.lcssa911, i64 %index.next72.phi
  %index.next72.1 = add i64 %index.next72.phi, 32
  br label %vector.body61

middle.block59:                                   ; preds = %vector.body61
  %cmp.n73 = icmp eq i64 %n.vec70, %exitcount.ptrcnt.to.int
  br i1 %cmp.n73, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705, label %vec.epilog.iter.check

vec.epilog.iter.check:                            ; preds = %middle.block59
  %ind.end85 = getelementptr i8, i8* %dst.0.lcssa911, i64 %n.vec70
  %n.vec.remaining = and i64 %exitcount.ptrcnt.to.int, 28
  %min.epilog.iters.check = icmp eq i64 %n.vec.remaining, 0
  br i1 %min.epilog.iters.check, label %while.body194.preheader, label %vec.epilog.ph

vec.epilog.ph:                                    ; preds = %vector.main.loop.iter.check, %vec.epilog.iter.check
  %vec.epilog.resume.val = phi i64 [ %n.vec70, %vec.epilog.iter.check ], [ 0, %vector.main.loop.iter.check ]
  %71 = sub i64 0, %dst.0.lcssa91162
  %scevgep76 = getelementptr [256 x i8], [256 x i8]* %array_name, i64 1, i64 %71
  %exitcount.ptrcnt.to.int78 = ptrtoint i8* %scevgep76 to i64
  %n.vec80 = and i64 %exitcount.ptrcnt.to.int78, -4
  %ind.end84 = getelementptr i8, i8* %dst.0.lcssa911, i64 %n.vec80
  br label %vec.epilog.vector.body

vec.epilog.vector.body:                           ; preds = %vec.epilog.vector.body, %vec.epilog.ph
  %index81 = phi i64 [ %vec.epilog.resume.val, %vec.epilog.ph ], [ %index.next82, %vec.epilog.vector.body ]
  %next.gep87 = getelementptr i8, i8* %dst.0.lcssa911, i64 %index81
  %72 = bitcast i8* %next.gep87 to <4 x i8>*
  store <4 x i8> zeroinitializer, <4 x i8>* %72, align 1, !tbaa !18
  %index.next82 = add i64 %index81, 4
  %73 = icmp eq i64 %index.next82, %n.vec80
  br i1 %73, label %vec.epilog.middle.block, label %vec.epilog.vector.body, !llvm.loop !213

vec.epilog.middle.block:                          ; preds = %vec.epilog.vector.body
  %cmp.n86 = icmp eq i64 %n.vec80, %exitcount.ptrcnt.to.int78
  br i1 %cmp.n86, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705, label %while.body194.preheader

while.body194.preheader:                          ; preds = %iter.check, %vec.epilog.iter.check, %vec.epilog.middle.block
  %dst.1867.ph = phi i8* [ %dst.0.lcssa911, %iter.check ], [ %ind.end85, %vec.epilog.iter.check ], [ %ind.end84, %vec.epilog.middle.block ]
  br label %while.body194

while.body187:                                    ; preds = %while.body187.preheader160, %while.body187
  %dst.0870 = phi i8* [ %incdec.ptr189, %while.body187 ], [ %dst.0870.ph, %while.body187.preheader160 ]
  %start.1869.idx = phi i64 [ %start.1869.add, %while.body187 ], [ %start.1869.idx.ph, %while.body187.preheader160 ]
  %start.1869.ptr = getelementptr inbounds i8, i8* %end.1872, i64 %start.1869.idx
  %start.1869.add = add nuw nsw i64 %start.1869.idx, 1
  %74 = load i8, i8* %start.1869.ptr, align 1, !tbaa !18
  %incdec.ptr189 = getelementptr inbounds i8, i8* %dst.0870, i64 1
  store i8 %74, i8* %dst.0870, align 1, !tbaa !18
  %cmp186.not = icmp eq i64 %start.1869.add, -1
  br i1 %cmp186.not, label %while.cond191.preheader, label %while.body187, !llvm.loop !214

while.body194:                                    ; preds = %while.body194.preheader, %while.body194
  %dst.1867 = phi i8* [ %incdec.ptr195, %while.body194 ], [ %dst.1867.ph, %while.body194.preheader ]
  %incdec.ptr195 = getelementptr inbounds i8, i8* %dst.1867, i64 1
  store i8 0, i8* %dst.1867, align 1, !tbaa !18
  %exitcond882.not = icmp eq i8* %incdec.ptr195, %scevgep
  br i1 %exitcond882.not, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705, label %while.body194, !llvm.loop !215

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705: ; preds = %while.body194, %middle.block59, %vec.epilog.middle.block, %while.cond191.preheader
  %add197 = sub i32 6, %54
  %and = and i32 %add197, -8
  %75 = getelementptr inbounds [129 x i8], [129 x i8]* %header198, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 129, i8* nonnull %75) #11
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 1 dereferenceable(129) %75, i8* nonnull align 1 dereferenceable(129) getelementptr inbounds ([129 x i8], [129 x i8]* @__const.halide_debug_to_file.header, i64 0, i64 0), i64 129, i1 false)
  %call.i701 = call i64 @fwrite(i8* nonnull %75, i64 128, i64 1, i8* nonnull %call.i598) #15
  %76 = load i32, i32* %dimensions, align 4, !tbaa !183
  %cmp19.i.i = icmp sgt i32 %76, 0
  br i1 %cmp19.i.i, label %for.body.lr.ph.i.i, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge: ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705
  %sub.i694.0 = sub nsw i64 1, 0
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

for.body.lr.ph.i.i:                               ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705
  %dim.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 6
  %77 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i.i, align 8, !tbaa !184
  %wide.trip.count.i.i = zext i32 %76 to i64
  %min.iters.check91 = icmp ult i32 %76, 3
  br i1 %min.iters.check91, label %for.body.i.i.preheader, label %vector.ph92

vector.ph92:                                      ; preds = %for.body.lr.ph.i.i
  %n.vec94 = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body90

vector.body90:                                    ; preds = %pred.load.continue103, %vector.ph92
  %index95 = phi i64 [ 0, %vector.ph92 ], [ %index.next96, %pred.load.continue103 ]
  %vec.phi = phi i64 [ 0, %vector.ph92 ], [ %predphi, %pred.load.continue103 ]
  %vec.phi101 = phi i64 [ 0, %vector.ph92 ], [ %predphi104, %pred.load.continue103 ]
  %induction100 = or i64 %index95, 1
  %78 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %index95, i32 2
  %79 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %induction100, i32 2
  %80 = load i32, i32* %78, align 4, !tbaa !185
  %81 = load i32, i32* %79, align 4, !tbaa !185
  %82 = icmp sgt i32 %80, 0
  %83 = icmp sgt i32 %81, 0
  %84 = zext i32 %80 to i64
  %85 = zext i32 %81 to i64
  br i1 %82, label %pred.load.if, label %pred.load.continue

pred.load.if:                                     ; preds = %vector.body90
  %86 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %index95, i32 1
  %87 = load i32, i32* %86, align 4, !tbaa !189
  br label %pred.load.continue

pred.load.continue:                               ; preds = %pred.load.if, %vector.body90
  %88 = phi i32 [ poison, %vector.body90 ], [ %87, %pred.load.if ]
  br i1 %83, label %pred.load.if102, label %pred.load.continue103

pred.load.if102:                                  ; preds = %pred.load.continue
  %89 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %induction100, i32 1
  %90 = load i32, i32* %89, align 4, !tbaa !189
  br label %pred.load.continue103

pred.load.continue103:                            ; preds = %pred.load.if102, %pred.load.continue
  %91 = phi i32 [ poison, %pred.load.continue ], [ %90, %pred.load.if102 ]
  %92 = add nsw i32 %88, -1
  %93 = add nsw i32 %91, -1
  %94 = sext i32 %92 to i64
  %95 = sext i32 %93 to i64
  %96 = mul nsw i64 %94, %84
  %97 = mul nsw i64 %95, %85
  %98 = select i1 %82, i64 %96, i64 0
  %predphi = add i64 %vec.phi, %98
  %99 = select i1 %83, i64 %97, i64 0
  %predphi104 = add i64 %vec.phi101, %99
  %index.next96 = add i64 %index95, 2
  %100 = icmp eq i64 %index.next96, %n.vec94
  br i1 %100, label %middle.block88, label %vector.body90, !llvm.loop !217

middle.block88:                                   ; preds = %pred.load.continue103
  %bin.rdx = add i64 %predphi104, %predphi
  %cmp.n98 = icmp eq i64 %n.vec94, %wide.trip.count.i.i
  br i1 %cmp.n98, label %for.body.i13.i.preheader, label %for.body.i.i.preheader

for.body.i.i.preheader:                           ; preds = %for.body.lr.ph.i.i, %middle.block88
  %indvars.iv.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %n.vec94, %middle.block88 ]
  %index.021.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %bin.rdx, %middle.block88 ]
  br label %for.body.i.i

for.body.i.i:                                     ; preds = %for.body.i.i.preheader, %if.end.i.i
  %indvars.iv.i.i = phi i64 [ %indvars.iv.next.i.i, %if.end.i.i ], [ %indvars.iv.i.i.ph, %for.body.i.i.preheader ]
  %index.021.i.i = phi i64 [ %index.1.i.i, %if.end.i.i ], [ %index.021.i.i.ph, %for.body.i.i.preheader ]
  %stride2.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %indvars.iv.i.i, i32 2
  %101 = load i32, i32* %stride2.i.i, align 4, !tbaa !185
  %cmp3.i.i = icmp sgt i32 %101, 0
  br i1 %cmp3.i.i, label %if.then.i.i, label %if.end.i.i

if.then.i.i:                                      ; preds = %for.body.i.i
  %conv18.i.i = zext i32 %101 to i64
  %extent.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %indvars.iv.i.i, i32 1
  %102 = load i32, i32* %extent.i.i, align 4, !tbaa !189
  %sub.i.i = add nsw i32 %102, -1
  %conv7.i.i = sext i32 %sub.i.i to i64
  %mul.i.i = mul nsw i64 %conv7.i.i, %conv18.i.i
  %add.i.i693 = add nsw i64 %mul.i.i, %index.021.i.i
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %for.body.i.i
  %index.1.i.i = phi i64 [ %add.i.i693, %if.then.i.i ], [ %index.021.i.i, %for.body.i.i ]
  %indvars.iv.next.i.i = add nuw nsw i64 %indvars.iv.i.i, 1
  %exitcond.not.i.i = icmp eq i64 %indvars.iv.next.i.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i.i, label %for.body.i13.i.preheader, label %for.body.i.i, !llvm.loop !218

for.body.i13.i.preheader:                         ; preds = %if.end.i.i, %middle.block88
  %index.1.i.i.lcssa = phi i64 [ %bin.rdx, %middle.block88 ], [ %index.1.i.i, %if.end.i.i ]
  %min.iters.check108 = icmp ult i32 %76, 3
  br i1 %min.iters.check108, label %for.body.i13.i.preheader157, label %vector.ph109

vector.ph109:                                     ; preds = %for.body.i13.i.preheader
  %n.vec111 = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body107

vector.body107:                                   ; preds = %pred.load.continue123, %vector.ph109
  %index112 = phi i64 [ 0, %vector.ph109 ], [ %index.next113, %pred.load.continue123 ]
  %vec.phi118 = phi i64 [ 0, %vector.ph109 ], [ %predphi124, %pred.load.continue123 ]
  %vec.phi119 = phi i64 [ 0, %vector.ph109 ], [ %predphi125, %pred.load.continue123 ]
  %induction117 = or i64 %index112, 1
  %103 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %index112, i32 2
  %104 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %induction117, i32 2
  %105 = load i32, i32* %103, align 4, !tbaa !185
  %106 = load i32, i32* %104, align 4, !tbaa !185
  %107 = icmp slt i32 %105, 0
  %108 = icmp slt i32 %106, 0
  %109 = sext i32 %105 to i64
  %110 = sext i32 %106 to i64
  br i1 %107, label %pred.load.if120, label %pred.load.continue121

pred.load.if120:                                  ; preds = %vector.body107
  %111 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %index112, i32 1
  %112 = load i32, i32* %111, align 4, !tbaa !189
  br label %pred.load.continue121

pred.load.continue121:                            ; preds = %pred.load.if120, %vector.body107
  %113 = phi i32 [ poison, %vector.body107 ], [ %112, %pred.load.if120 ]
  br i1 %108, label %pred.load.if122, label %pred.load.continue123

pred.load.if122:                                  ; preds = %pred.load.continue121
  %114 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %induction117, i32 1
  %115 = load i32, i32* %114, align 4, !tbaa !189
  br label %pred.load.continue123

pred.load.continue123:                            ; preds = %pred.load.if122, %pred.load.continue121
  %116 = phi i32 [ poison, %pred.load.continue121 ], [ %115, %pred.load.if122 ]
  %117 = add nsw i32 %113, -1
  %118 = add nsw i32 %116, -1
  %119 = sext i32 %117 to i64
  %120 = sext i32 %118 to i64
  %121 = mul nsw i64 %119, %109
  %122 = mul nsw i64 %120, %110
  %123 = select i1 %107, i64 %121, i64 0
  %predphi124 = add i64 %vec.phi118, %123
  %124 = select i1 %108, i64 %122, i64 0
  %predphi125 = add i64 %vec.phi119, %124
  %index.next113 = add i64 %index112, 2
  %125 = icmp eq i64 %index.next113, %n.vec111
  br i1 %125, label %middle.block105, label %vector.body107, !llvm.loop !219

middle.block105:                                  ; preds = %pred.load.continue123
  %bin.rdx126 = add i64 %predphi125, %predphi124
  %cmp.n115 = icmp eq i64 %n.vec111, %wide.trip.count.i.i
  br i1 %cmp.n115, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i.preheader157

for.body.i13.i.preheader157:                      ; preds = %for.body.i13.i.preheader, %middle.block105
  %indvars.iv.i10.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %n.vec111, %middle.block105 ]
  %index.018.i.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %bin.rdx126, %middle.block105 ]
  br label %for.body.i13.i

for.body.i13.i:                                   ; preds = %for.body.i13.i.preheader157, %if.end.i24.i
  %indvars.iv.i10.i = phi i64 [ %indvars.iv.next.i22.i, %if.end.i24.i ], [ %indvars.iv.i10.i.ph, %for.body.i13.i.preheader157 ]
  %index.018.i.i = phi i64 [ %index.1.i21.i, %if.end.i24.i ], [ %index.018.i.i.ph, %for.body.i13.i.preheader157 ]
  %stride2.i11.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %indvars.iv.i10.i, i32 2
  %126 = load i32, i32* %stride2.i11.i, align 4, !tbaa !185
  %cmp3.i12.i = icmp slt i32 %126, 0
  br i1 %cmp3.i12.i, label %if.then.i20.i, label %if.end.i24.i

if.then.i20.i:                                    ; preds = %for.body.i13.i
  %conv.i14.i = sext i32 %126 to i64
  %extent.i15.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %77, i64 %indvars.iv.i10.i, i32 1
  %127 = load i32, i32* %extent.i15.i, align 4, !tbaa !189
  %sub.i16.i = add nsw i32 %127, -1
  %conv7.i17.i = sext i32 %sub.i16.i to i64
  %mul.i18.i = mul nsw i64 %conv7.i17.i, %conv.i14.i
  %add.i19.i = add nsw i64 %mul.i18.i, %index.018.i.i
  br label %if.end.i24.i

if.end.i24.i:                                     ; preds = %if.then.i20.i, %for.body.i13.i
  %index.1.i21.i = phi i64 [ %add.i19.i, %if.then.i20.i ], [ %index.018.i.i, %for.body.i13.i ]
  %indvars.iv.next.i22.i = add nuw nsw i64 %indvars.iv.i10.i, 1
  %exitcond.not.i23.i = icmp eq i64 %indvars.iv.next.i22.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i23.i, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i, !llvm.loop !220

_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit: ; preds = %if.end.i24.i, %middle.block105
  %index.1.i21.i.lcssa = phi i64 [ %bin.rdx126, %middle.block105 ], [ %index.1.i21.i, %if.end.i24.i ]
  %add8.i.i = add nsw i64 %index.1.i.i.lcssa, 1
  %sub.i694.1 = sub nsw i64 %add8.i.i, %index.1.i21.i.lcssa
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

_ZNK15halide_buffer_t13size_in_bytesEv.exit:      ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
  %add8.i27.i = phi i64 [ 1, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %add8.i.i, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %sub.i694.phi = phi i64 [ %sub.i694.0, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit705._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %sub.i694.1, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %128 = load i8, i8* %bits.i, align 1, !tbaa !144
  %conv.i.i696 = zext i8 %128 to i64
  %add.i4.i = add nuw nsw i64 %conv.i.i696, 7
  %div.i.i697 = lshr i64 %add.i4.i, 3
  %mul.i698 = mul i64 %div.i.i697, %sub.i694.phi
  %129 = trunc i64 %mul.i698 to i32
  %130 = add i32 %129, 7
  %131 = and i32 %130, 7
  %conv204 = xor i32 %131, 7
  %conv205 = zext i32 %conv204 to i64
  %add206 = add i64 %mul.i698, %conv205
  %tobool207.not = icmp ult i64 %add206, 4294967296
  br i1 %tobool207.not, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit687, label %cleanup278.thread

cleanup278.thread:                                ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit
  call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([53 x i8], [53 x i8]* @.str.6.40, i64 0, i64 0)) #15
  call void @llvm.lifetime.end.p0i8(i64 129, i8* nonnull %75) #11
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %55) #11
  br label %cleanup433

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit687: ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit
  %132 = icmp sgt i32 %76, 2
  %spec.store.select = select i1 %132, i32 %76, i32 2
  %133 = bitcast [8 x i32]* %tags to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %133) #11
  %arrayinit.begin = getelementptr inbounds [8 x i32], [8 x i32]* %tags, i64 0, i64 0
  store i32 14, i32* %arrayinit.begin, align 4, !tbaa !41
  %arrayinit.element = getelementptr inbounds [8 x i32], [8 x i32]* %tags, i64 0, i64 1
  %add214 = shl i32 %spec.store.select, 2
  %and215 = add i32 %add214, 4
  %mul216 = and i32 %and215, -8
  %add217 = add i32 %and, 40
  %add218 = add i32 %add217, %mul216
  %add220 = add i32 %add218, %129
  %add221 = add i32 %add220, %conv204
  store i32 %add221, i32* %arrayinit.element, align 4, !tbaa !41
  %arrayinit.element222 = getelementptr inbounds [8 x i32], [8 x i32]* %tags, i64 0, i64 2
  %134 = bitcast i32* %arrayinit.element222 to <2 x i32>*
  store <2 x i32> <i32 6, i32 8>, <2 x i32>* %134, align 4, !tbaa !41
  %arrayinit.element224 = getelementptr inbounds [8 x i32], [8 x i32]* %tags, i64 0, i64 4
  %idxprom225 = sext i32 %type_code to i64
  %arrayidx226 = getelementptr inbounds [10 x i8], [10 x i8]* @_ZN6Halide7Runtime8Internal31pixel_type_to_matlab_class_codeE, i64 0, i64 %idxprom225
  %135 = load i8, i8* %arrayidx226, align 1, !tbaa !18
  %conv227 = zext i8 %135 to i32
  store i32 %conv227, i32* %arrayinit.element224, align 4, !tbaa !41
  %arrayinit.element228 = getelementptr inbounds [8 x i32], [8 x i32]* %tags, i64 0, i64 5
  %136 = bitcast i32* %arrayinit.element228 to <2 x i32>*
  store <2 x i32> <i32 1, i32 5>, <2 x i32>* %136, align 4, !tbaa !41
  %arrayinit.element230 = getelementptr inbounds [8 x i32], [8 x i32]* %tags, i64 0, i64 7
  store i32 %add214, i32* %arrayinit.element230, align 4, !tbaa !41
  %call.i683 = call i64 @fwrite(i8* nonnull %133, i64 32, i64 1, i8* nonnull %call.i598) #15
  %cmp.i684.not = icmp eq i64 %call.i683, 0
  br i1 %cmp.i684.not, label %cleanup278, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit676

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit676: ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit687
  %137 = bitcast [4 x i32]* %extents to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %137) #11
  %arrayinit.begin235 = getelementptr inbounds [4 x i32], [4 x i32]* %extents, i64 0, i64 0
  %138 = load i32, i32* %extent.i, align 4, !tbaa !189
  store i32 %138, i32* %arrayinit.begin235, align 4, !tbaa !41
  %arrayinit.element238 = getelementptr inbounds [4 x i32], [4 x i32]* %extents, i64 0, i64 1
  %139 = load i32, i32* %extent.i.1, align 4, !tbaa !189
  store i32 %139, i32* %arrayinit.element238, align 4, !tbaa !41
  %arrayinit.element241 = getelementptr inbounds [4 x i32], [4 x i32]* %extents, i64 0, i64 2
  %140 = load i32, i32* %extent.i.2, align 4, !tbaa !189
  store i32 %140, i32* %arrayinit.element241, align 4, !tbaa !41
  %arrayinit.element244 = getelementptr inbounds [4 x i32], [4 x i32]* %extents, i64 0, i64 3
  %141 = load i32, i32* %extent.i.3, align 4, !tbaa !189
  store i32 %141, i32* %arrayinit.element244, align 4, !tbaa !41
  %conv248 = sext i32 %mul216 to i64
  %call.i672 = call i64 @fwrite(i8* nonnull %137, i64 %conv248, i64 1, i8* nonnull %call.i598) #15
  %cmp.i673.not = icmp eq i64 %call.i672, 0
  br i1 %cmp.i673.not, label %cleanup274, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit664

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit664: ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit676
  %142 = bitcast [2 x i32]* %name_header to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %142) #11
  %arrayinit.begin252 = getelementptr inbounds [2 x i32], [2 x i32]* %name_header, i64 0, i64 0
  store i32 1, i32* %arrayinit.begin252, align 4, !tbaa !41
  %arrayinit.element253 = getelementptr inbounds [2 x i32], [2 x i32]* %name_header, i64 0, i64 1
  store i32 %conv184, i32* %arrayinit.element253, align 4, !tbaa !41
  %call.i660 = call i64 @fwrite(i8* nonnull %142, i64 8, i64 1, i8* nonnull %call.i598) #15
  %cmp.i661.not = icmp eq i64 %call.i660, 0
  br i1 %cmp.i661.not, label %cleanup273, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit652

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit652: ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit664
  %conv258 = zext i32 %and to i64
  %call.i648 = call i64 @fwrite(i8* nonnull %55, i64 %conv258, i64 1, i8* nonnull %call.i598) #15
  %cmp.i649.not = icmp eq i64 %call.i648, 0
  br i1 %cmp.i649.not, label %cleanup273, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit642

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit642: ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit652
  %143 = bitcast [2 x i32]* %payload_header to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %143) #11
  %arrayinit.begin262 = getelementptr inbounds [2 x i32], [2 x i32]* %payload_header, i64 0, i64 0
  %arrayidx264 = getelementptr inbounds [10 x i8], [10 x i8]* @_ZN6Halide7Runtime8Internal30pixel_type_to_matlab_type_codeE, i64 0, i64 %idxprom225
  %144 = load i8, i8* %arrayidx264, align 1, !tbaa !18
  %conv265 = zext i8 %144 to i32
  store i32 %conv265, i32* %arrayinit.begin262, align 4, !tbaa !41
  %arrayinit.element266 = getelementptr inbounds [2 x i32], [2 x i32]* %payload_header, i64 0, i64 1
  store i32 %129, i32* %arrayinit.element266, align 4, !tbaa !41
  %call.i638 = call i64 @fwrite(i8* nonnull %143, i64 8, i64 1, i8* nonnull %call.i598) #15
  %cmp.i639.not = icmp eq i64 %call.i638, 0
  %cleanup.dest.slot.5 = zext i1 %cmp.i639.not to i32
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %143) #11
  br label %cleanup273

cleanup273:                                       ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit642, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit652, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit664
  %cleanup.dest.slot.6 = phi i32 [ %cleanup.dest.slot.5, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit642 ], [ 1, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit664 ], [ 1, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit652 ]
  %retval.6 = phi i32 [ -11, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit642 ], [ -9, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit664 ], [ -10, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit652 ]
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %142) #11
  br label %cleanup274

cleanup274:                                       ; preds = %cleanup273, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit676
  %cleanup.dest.slot.7 = phi i32 [ %cleanup.dest.slot.6, %cleanup273 ], [ 1, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit676 ]
  %retval.7 = phi i32 [ %retval.6, %cleanup273 ], [ -8, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit676 ]
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %137) #11
  br label %cleanup278

cleanup278:                                       ; preds = %cleanup274, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit687
  %cleanup.dest.slot.8 = phi i32 [ %cleanup.dest.slot.7, %cleanup274 ], [ 1, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit687 ]
  %retval.8 = phi i32 [ %retval.7, %cleanup274 ], [ -7, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit687 ]
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %133) #11
  call void @llvm.lifetime.end.p0i8(i64 129, i8* nonnull %75) #11
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %55) #11
  %cond442 = icmp eq i32 %cleanup.dest.slot.8, 0
  br i1 %cond442, label %if.end311, label %cleanup433

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit631: ; preds = %if.else164
  %145 = bitcast [5 x i32]* %header289 to i8*
  call void @llvm.lifetime.start.p0i8(i64 20, i8* nonnull %145) #11
  %arrayinit.begin290 = getelementptr inbounds [5 x i32], [5 x i32]* %header289, i64 0, i64 0
  %146 = load i32, i32* %extent.i, align 4, !tbaa !189
  store i32 %146, i32* %arrayinit.begin290, align 4, !tbaa !41
  %arrayinit.element293 = getelementptr inbounds [5 x i32], [5 x i32]* %header289, i64 0, i64 1
  %147 = load i32, i32* %extent.i.1, align 4, !tbaa !189
  store i32 %147, i32* %arrayinit.element293, align 4, !tbaa !41
  %arrayinit.element296 = getelementptr inbounds [5 x i32], [5 x i32]* %header289, i64 0, i64 2
  %148 = load i32, i32* %extent.i.2, align 4, !tbaa !189
  store i32 %148, i32* %arrayinit.element296, align 4, !tbaa !41
  %arrayinit.element299 = getelementptr inbounds [5 x i32], [5 x i32]* %header289, i64 0, i64 3
  %149 = load i32, i32* %extent.i.3, align 4, !tbaa !189
  store i32 %149, i32* %arrayinit.element299, align 4, !tbaa !41
  %arrayinit.element302 = getelementptr inbounds [5 x i32], [5 x i32]* %header289, i64 0, i64 4
  store i32 %type_code, i32* %arrayinit.element302, align 4, !tbaa !41
  %call.i627 = call i64 @fwrite(i8* nonnull %145, i64 20, i64 1, i8* nonnull %call.i598) #15
  %cmp.i628.not = icmp eq i64 %call.i627, 0
  call void @llvm.lifetime.end.p0i8(i64 20, i8* nonnull %145) #11
  br i1 %cmp.i628.not, label %cleanup433, label %if.end311

if.end311:                                        ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit631, %cleanup278, %cleanup154.thread
  %final_padding_bytes.0 = phi i32 [ %conv204, %cleanup278 ], [ 0, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit631 ], [ 0, %cleanup154.thread ]
  %150 = getelementptr inbounds [4096 x i8], [4096 x i8]* %temp, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 4096, i8* nonnull %150) #11
  %div = udiv i32 4096, %div.i
  %151 = load i32, i32* %extent.i.3, align 4, !tbaa !189
  %cmp320856 = icmp sgt i32 %151, 0
  br i1 %cmp320856, label %for.body322.lr.ph, label %if.end412

for.body322.lr.ph:                                ; preds = %if.end311
  %152 = load i32, i32* %min.i.3, align 16, !tbaa !221
  %153 = bitcast [4 x i32]* %idx to i8*
  %arrayinit.begin357 = getelementptr inbounds [4 x i32], [4 x i32]* %idx, i64 0, i64 0
  %arrayinit.element358 = getelementptr inbounds [4 x i32], [4 x i32]* %idx, i64 0, i64 1
  %arrayinit.element359 = getelementptr inbounds [4 x i32], [4 x i32]* %idx, i64 0, i64 2
  %arrayinit.element360 = getelementptr inbounds [4 x i32], [4 x i32]* %idx, i64 0, i64 3
  %dim.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 6
  %conv368 = zext i32 %div.i to i64
  %mul373 = mul nuw nsw i32 %div, %div.i
  %conv374 = zext i32 %mul373 to i64
  %.pre891 = load i32, i32* %min.i.2, align 16, !tbaa !221
  %.pre892 = load i32, i32* %extent.i.2, align 4, !tbaa !189
  %.pre893 = load i32, i32* %min.i.1, align 16
  %.pre894 = load i32, i32* %extent.i.1, align 4
  %.pre = load i32, i32* %extent.i, align 4
  %154 = load i32, i32* %min.i, align 16
  %155 = load i32, i32* %extent.i, align 4
  %156 = load i32, i32* %min.i, align 16
  %add352 = add nsw i32 %156, %155
  %.pre15 = load i32, i32* %extent.i.1, align 4
  %.pre16 = load i32, i32* %min.i.1, align 16
  %.pre896 = load i32, i32* %extent.i.2, align 4
  %.pre897 = load i32, i32* %min.i.2, align 16
  %.pre898 = load i32, i32* %extent.i.3, align 4
  %.pre899 = load i32, i32* %min.i.3, align 16
  br label %for.body322

for.body322:                                      ; preds = %for.inc399, %for.body322.lr.ph
  %157 = phi i32 [ %152, %for.body322.lr.ph ], [ %218, %for.inc399 ]
  %158 = phi i32 [ %151, %for.body322.lr.ph ], [ %219, %for.inc399 ]
  %159 = phi i32 [ %.pre891, %for.body322.lr.ph ], [ %220, %for.inc399 ]
  %160 = phi i32 [ %.pre892, %for.body322.lr.ph ], [ %221, %for.inc399 ]
  %161 = phi i32 [ %.pre892, %for.body322.lr.ph ], [ %222, %for.inc399 ]
  %162 = phi i32 [ %.pre891, %for.body322.lr.ph ], [ %223, %for.inc399 ]
  %dim3.0858 = phi i32 [ %152, %for.body322.lr.ph ], [ %inc400, %for.inc399 ]
  %counter.0857 = phi i32 [ 0, %for.body322.lr.ph ], [ %counter.1.lcssa, %for.inc399 ]
  %cmp331848 = icmp sgt i32 %161, 0
  br i1 %cmp331848, label %for.body333, label %for.inc399

for.body333:                                      ; preds = %for.body322, %for.inc394
  %163 = phi i32 [ %212, %for.inc394 ], [ %.pre893, %for.body322 ]
  %164 = phi i32 [ %213, %for.inc394 ], [ %.pre894, %for.body322 ]
  %165 = phi i32 [ %214, %for.inc394 ], [ %159, %for.body322 ]
  %166 = phi i32 [ %215, %for.inc394 ], [ %160, %for.body322 ]
  %167 = phi i32 [ %216, %for.inc394 ], [ %.pre894, %for.body322 ]
  %168 = phi i32 [ %217, %for.inc394 ], [ %.pre893, %for.body322 ]
  %dim2.0850 = phi i32 [ %inc395, %for.inc394 ], [ %162, %for.body322 ]
  %counter.1849 = phi i32 [ %counter.2.lcssa, %for.inc394 ], [ %counter.0857, %for.body322 ]
  %cmp342839 = icmp sgt i32 %167, 0
  br i1 %cmp342839, label %for.body344, label %for.inc394

for.body344:                                      ; preds = %for.body333, %for.inc389
  %169 = phi i32 [ %209, %for.inc389 ], [ %163, %for.body333 ]
  %170 = phi i32 [ %210, %for.inc389 ], [ %164, %for.body333 ]
  %171 = phi i32 [ %211, %for.inc389 ], [ %.pre, %for.body333 ]
  %dim1.0841 = phi i32 [ %inc390, %for.inc389 ], [ %168, %for.body333 ]
  %counter.2840 = phi i32 [ %counter.6797, %for.inc389 ], [ %counter.1849, %for.body333 ]
  %cmp353834 = icmp sgt i32 %171, 0
  br i1 %cmp353834, label %for.body355, label %for.inc389

for.body355:                                      ; preds = %for.body344, %for.inc384
  %dim0.0836 = phi i32 [ %inc385, %for.inc384 ], [ %154, %for.body344 ]
  %counter.3835 = phi i32 [ %counter.4, %for.inc384 ], [ %counter.2840, %for.body344 ]
  %inc356 = add nsw i32 %counter.3835, 1
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %153) #11
  store i32 %dim0.0836, i32* %arrayinit.begin357, align 4, !tbaa !41
  store i32 %dim1.0841, i32* %arrayinit.element358, align 4, !tbaa !41
  store i32 %dim2.0850, i32* %arrayinit.element359, align 4, !tbaa !41
  store i32 %dim3.0858, i32* %arrayinit.element360, align 4, !tbaa !41
  %172 = load i32, i32* %dimensions, align 4, !tbaa !183
  %cmp17.i = icmp sgt i32 %172, 0
  br i1 %cmp17.i, label %for.body.lr.ph.i, label %_ZNK15halide_buffer_t10address_ofEPKi.exit

for.body.lr.ph.i:                                 ; preds = %for.body355
  %173 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i, align 8, !tbaa !184
  %wide.trip.count.i = zext i32 %172 to i64
  %stride.i621920 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %173, i64 0, i32 2
  %174 = load i32, i32* %stride.i621920, align 4, !tbaa !185
  %conv.i622921 = sext i32 %174 to i64
  %min.i623922 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %173, i64 0, i32 0
  %175 = load i32, i32* %min.i623922, align 4, !tbaa !221
  %sub.i923 = sub nsw i32 %dim0.0836, %175
  %conv7.i924 = sext i32 %sub.i923 to i64
  %mul.i925 = mul nsw i64 %conv7.i924, %conv.i622921
  %exitcond.not.i927 = icmp eq i32 %172, 1
  br i1 %exitcond.not.i927, label %_ZNK15halide_buffer_t10address_ofEPKi.exit, label %for.body.i.for.body.i_crit_edge.preheader, !llvm.loop !222

for.body.i.for.body.i_crit_edge.preheader:        ; preds = %for.body.lr.ph.i
  %stride.i62126 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %173, i64 1, i32 2
  %176 = load i32, i32* %stride.i62126, align 4, !tbaa !185
  %conv.i62227 = sext i32 %176 to i64
  %min.i62328 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %173, i64 1, i32 0
  %177 = load i32, i32* %min.i62328, align 4, !tbaa !221
  %sub.i29 = sub nsw i32 %dim1.0841, %177
  %conv7.i30 = sext i32 %sub.i29 to i64
  %mul.i31 = mul nsw i64 %conv7.i30, %conv.i62227
  %add.i62432 = add nsw i64 %mul.i31, %mul.i925
  %exitcond.not.i33 = icmp eq i32 %172, 2
  br i1 %exitcond.not.i33, label %_ZNK15halide_buffer_t10address_ofEPKi.exit, label %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.lr.ph, !llvm.loop !222

for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.lr.ph: ; preds = %for.body.i.for.body.i_crit_edge.preheader
  %178 = add nsw i64 %wide.trip.count.i, -2
  %min.iters.check131 = icmp ult i64 %178, 5
  br i1 %min.iters.check131, label %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.preheader, label %vector.ph132

vector.ph132:                                     ; preds = %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.lr.ph
  %n.mod.vf133 = and i64 %178, 3
  %179 = icmp eq i64 %n.mod.vf133, 0
  %180 = select i1 %179, i64 4, i64 %n.mod.vf133
  %n.vec134 = sub nsw i64 %178, %180
  %ind.end138 = add nsw i64 %n.vec134, 2
  %181 = insertelement <2 x i64> <i64 poison, i64 0>, i64 %add.i62432, i32 0
  br label %vector.body130

vector.body130:                                   ; preds = %vector.body130, %vector.ph132
  %index135 = phi i64 [ 0, %vector.ph132 ], [ %index.next136, %vector.body130 ]
  %vec.phi141 = phi <2 x i64> [ %181, %vector.ph132 ], [ %201, %vector.body130 ]
  %vec.phi142 = phi <2 x i64> [ zeroinitializer, %vector.ph132 ], [ %202, %vector.body130 ]
  %offset.idx140 = or i64 %index135, 2
  %182 = add i64 %offset.idx140, 2
  %183 = getelementptr inbounds [4 x i32], [4 x i32]* %idx, i64 0, i64 %offset.idx140
  %184 = bitcast i32* %183 to <2 x i32>*
  %wide.load143 = load <2 x i32>, <2 x i32>* %184, align 4, !tbaa !41
  %185 = getelementptr inbounds i32, i32* %183, i64 2
  %186 = bitcast i32* %185 to <2 x i32>*
  %wide.load144 = load <2 x i32>, <2 x i32>* %186, align 4, !tbaa !41
  %187 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %173, i64 %offset.idx140, i32 2
  %188 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %173, i64 %182, i32 2
  %189 = getelementptr inbounds i32, i32* %187, i64 -2
  %190 = bitcast i32* %189 to <8 x i32>*
  %191 = getelementptr inbounds i32, i32* %188, i64 -2
  %192 = bitcast i32* %191 to <8 x i32>*
  %wide.vec = load <8 x i32>, <8 x i32>* %190, align 4, !tbaa !41
  %wide.vec145 = load <8 x i32>, <8 x i32>* %192, align 4, !tbaa !41
  %strided.vec = shufflevector <8 x i32> %wide.vec, <8 x i32> poison, <2 x i32> <i32 0, i32 4>
  %strided.vec146 = shufflevector <8 x i32> %wide.vec145, <8 x i32> poison, <2 x i32> <i32 0, i32 4>
  %strided.vec147 = shufflevector <8 x i32> %wide.vec, <8 x i32> poison, <2 x i32> <i32 2, i32 6>
  %strided.vec148 = shufflevector <8 x i32> %wide.vec145, <8 x i32> poison, <2 x i32> <i32 2, i32 6>
  %193 = sext <2 x i32> %strided.vec147 to <2 x i64>
  %194 = sext <2 x i32> %strided.vec148 to <2 x i64>
  %195 = sub nsw <2 x i32> %wide.load143, %strided.vec
  %196 = sub nsw <2 x i32> %wide.load144, %strided.vec146
  %197 = sext <2 x i32> %195 to <2 x i64>
  %198 = sext <2 x i32> %196 to <2 x i64>
  %199 = mul nsw <2 x i64> %197, %193
  %200 = mul nsw <2 x i64> %198, %194
  %201 = add <2 x i64> %199, %vec.phi141
  %202 = add <2 x i64> %200, %vec.phi142
  %index.next136 = add i64 %index135, 4
  %203 = icmp eq i64 %index.next136, %n.vec134
  br i1 %203, label %middle.block128, label %vector.body130, !llvm.loop !223

middle.block128:                                  ; preds = %vector.body130
  %bin.rdx149 = add <2 x i64> %202, %201
  %204 = call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %bin.rdx149)
  br label %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.preheader

for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.preheader: ; preds = %middle.block128, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.lr.ph
  %indvars.iv.next.i35.ph = phi i64 [ 2, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.lr.ph ], [ %ind.end138, %middle.block128 ]
  %add.i62434.ph = phi i64 [ %add.i62432, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.lr.ph ], [ %204, %middle.block128 ]
  br label %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge

for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge: ; preds = %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.preheader, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge
  %indvars.iv.next.i35 = phi i64 [ %indvars.iv.next.i, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge ], [ %indvars.iv.next.i35.ph, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.preheader ]
  %add.i62434 = phi i64 [ %add.i624, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge ], [ %add.i62434.ph, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge.preheader ]
  %arrayidx3.i.phi.trans.insert.phi.trans.insert = getelementptr inbounds [4 x i32], [4 x i32]* %idx, i64 0, i64 %indvars.iv.next.i35
  %.pre895.pre = load i32, i32* %arrayidx3.i.phi.trans.insert.phi.trans.insert, align 4, !tbaa !41
  %stride.i621 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %173, i64 %indvars.iv.next.i35, i32 2
  %205 = load i32, i32* %stride.i621, align 4, !tbaa !185
  %conv.i622 = sext i32 %205 to i64
  %min.i623 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %173, i64 %indvars.iv.next.i35, i32 0
  %206 = load i32, i32* %min.i623, align 4, !tbaa !221
  %sub.i = sub nsw i32 %.pre895.pre, %206
  %conv7.i = sext i32 %sub.i to i64
  %mul.i = mul nsw i64 %conv7.i, %conv.i622
  %add.i624 = add nsw i64 %mul.i, %add.i62434
  %indvars.iv.next.i = add nuw nsw i64 %indvars.iv.next.i35, 1
  %exitcond.not.i = icmp eq i64 %indvars.iv.next.i, %wide.trip.count.i
  br i1 %exitcond.not.i, label %_ZNK15halide_buffer_t10address_ofEPKi.exit, label %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge, !llvm.loop !224

_ZNK15halide_buffer_t10address_ofEPKi.exit:       ; preds = %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge, %for.body.i.for.body.i_crit_edge.preheader, %for.body.lr.ph.i, %for.body355
  %index.0.lcssa.i = phi i64 [ 0, %for.body355 ], [ %mul.i925, %for.body.lr.ph.i ], [ %add.i62432, %for.body.i.for.body.i_crit_edge.preheader ], [ %add.i624, %for.body.i.for.body.i_crit_edge.for.body.i.for.body.i_crit_edge_crit_edge ]
  %207 = load i8*, i8** %host.i, align 8, !tbaa !180
  %208 = load i8, i8* %bits.i, align 1, !tbaa !144
  %conv.i.i = zext i8 %208 to i64
  %add.i.i = add nuw nsw i64 %conv.i.i, 7
  %div.i.i = lshr i64 %add.i.i, 3
  %mul9.i = mul nsw i64 %div.i.i, %index.0.lcssa.i
  %add.ptr.i = getelementptr inbounds i8, i8* %207, i64 %mul9.i
  %mul366 = mul nsw i32 %counter.3835, %div.i
  %idx.ext = sext i32 %mul366 to i64
  %add.ptr367 = getelementptr inbounds [4096 x i8], [4096 x i8]* %temp, i64 0, i64 %idx.ext
  %call369 = call i8* @memcpy(i8* nonnull %add.ptr367, i8* %add.ptr.i, i64 %conv368) #15
  %cmp370 = icmp eq i32 %inc356, %div
  br i1 %cmp370, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit619, label %for.inc384

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit619: ; preds = %_ZNK15halide_buffer_t10address_ofEPKi.exit
  %call.i615 = call i64 @fwrite(i8* nonnull %150, i64 %conv374, i64 1, i8* nonnull %call.i598) #15
  %cmp.i616.not = icmp eq i64 %call.i615, 0
  br i1 %cmp.i616.not, label %cleanup425.loopexit, label %for.inc384

for.inc384:                                       ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit619, %_ZNK15halide_buffer_t10address_ofEPKi.exit
  %counter.4 = phi i32 [ 0, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit619 ], [ %inc356, %_ZNK15halide_buffer_t10address_ofEPKi.exit ]
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %153) #11
  %inc385 = add nsw i32 %dim0.0836, 1
  %cmp353 = icmp slt i32 %inc385, %add352
  br i1 %cmp353, label %for.body355, label %for.inc389, !llvm.loop !225

for.inc389:                                       ; preds = %for.inc384, %for.body344
  %209 = phi i32 [ %169, %for.body344 ], [ %.pre16, %for.inc384 ]
  %210 = phi i32 [ %170, %for.body344 ], [ %.pre15, %for.inc384 ]
  %211 = phi i32 [ %171, %for.body344 ], [ %155, %for.inc384 ]
  %counter.6797 = phi i32 [ %counter.2840, %for.body344 ], [ %counter.4, %for.inc384 ]
  %inc390 = add nsw i32 %dim1.0841, 1
  %add341 = add nsw i32 %209, %210
  %cmp342 = icmp slt i32 %inc390, %add341
  br i1 %cmp342, label %for.body344, label %for.inc394, !llvm.loop !226

for.inc394:                                       ; preds = %for.inc389, %for.body333
  %212 = phi i32 [ %163, %for.body333 ], [ %209, %for.inc389 ]
  %213 = phi i32 [ %164, %for.body333 ], [ %210, %for.inc389 ]
  %214 = phi i32 [ %165, %for.body333 ], [ %.pre897, %for.inc389 ]
  %215 = phi i32 [ %166, %for.body333 ], [ %.pre896, %for.inc389 ]
  %216 = phi i32 [ %167, %for.body333 ], [ %210, %for.inc389 ]
  %217 = phi i32 [ %168, %for.body333 ], [ %209, %for.inc389 ]
  %counter.2.lcssa = phi i32 [ %counter.1849, %for.body333 ], [ %counter.6797, %for.inc389 ]
  %inc395 = add nsw i32 %dim2.0850, 1
  %add330 = add nsw i32 %215, %214
  %cmp331 = icmp slt i32 %inc395, %add330
  br i1 %cmp331, label %for.body333, label %for.inc399, !llvm.loop !227

for.inc399:                                       ; preds = %for.inc394, %for.body322
  %218 = phi i32 [ %157, %for.body322 ], [ %.pre899, %for.inc394 ]
  %219 = phi i32 [ %158, %for.body322 ], [ %.pre898, %for.inc394 ]
  %220 = phi i32 [ %159, %for.body322 ], [ %214, %for.inc394 ]
  %221 = phi i32 [ %160, %for.body322 ], [ %215, %for.inc394 ]
  %222 = phi i32 [ %161, %for.body322 ], [ %215, %for.inc394 ]
  %223 = phi i32 [ %162, %for.body322 ], [ %214, %for.inc394 ]
  %counter.1.lcssa = phi i32 [ %counter.0857, %for.body322 ], [ %counter.2.lcssa, %for.inc394 ]
  %inc400 = add nsw i32 %dim3.0858, 1
  %add319 = add nsw i32 %219, %218
  %cmp320 = icmp slt i32 %inc400, %add319
  br i1 %cmp320, label %for.body322, label %for.end403, !llvm.loop !228

for.end403:                                       ; preds = %for.inc399
  %cmp404 = icmp sgt i32 %counter.1.lcssa, 0
  br i1 %cmp404, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit612, label %if.end412

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit612: ; preds = %for.end403
  %mul407 = mul nsw i32 %counter.1.lcssa, %div.i
  %conv408 = sext i32 %mul407 to i64
  %call.i608 = call i64 @fwrite(i8* nonnull %150, i64 %conv408, i64 1, i8* nonnull %call.i598) #15
  %cmp.i609.not = icmp eq i64 %call.i608, 0
  br i1 %cmp.i609.not, label %cleanup425, label %if.end412

if.end412:                                        ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit612, %for.end403, %if.end311
  %224 = bitcast i64* %zero to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %224) #11
  store i64 0, i64* %zero, align 8, !tbaa !22
  %tobool413.not = icmp eq i32 %final_padding_bytes.0, 0
  br i1 %tobool413.not, label %if.end423, label %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit

_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit: ; preds = %if.end412
  %conv415 = zext i32 %final_padding_bytes.0 to i64
  %call.i604 = call i64 @fwrite(i8* nonnull %224, i64 %conv415, i64 1, i8* nonnull %call.i598) #15
  %cmp.i605.not = icmp eq i64 %call.i604, 0
  br i1 %cmp.i605.not, label %cleanup424, label %if.end423

if.end423:                                        ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit, %if.end412
  br label %cleanup424

cleanup424:                                       ; preds = %if.end423, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit
  %retval.21 = phi i32 [ 0, %if.end423 ], [ -16, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit ]
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %224) #11
  br label %cleanup425

cleanup425.loopexit:                              ; preds = %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit619
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %153) #11
  br label %cleanup425

cleanup425:                                       ; preds = %cleanup425.loopexit, %cleanup424, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit612
  %retval.22 = phi i32 [ %retval.21, %cleanup424 ], [ -14, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit612 ], [ -13, %cleanup425.loopexit ]
  call void @llvm.lifetime.end.p0i8(i64 4096, i8* nonnull %150) #11
  br label %cleanup433

cleanup433:                                       ; preds = %cleanup425, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit631, %cleanup278, %cleanup278.thread, %cleanup154
  %retval.23 = phi i32 [ %retval.22, %cleanup425 ], [ %retval.4, %cleanup154 ], [ %retval.8, %cleanup278 ], [ -12, %_ZN6Halide7Runtime8Internal10ScopedFile5writeEPKvm.exit631 ], [ -6, %cleanup278.thread ]
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %3) #11
  %call.i = call i32 @fclose(i8* nonnull %call.i598) #15
  br label %return

return:                                           ; preds = %cleanup433, %if.end6, %if.end2, %if.then1, %if.then
  %retval.26 = phi i32 [ -1, %if.then ], [ -1, %if.then1 ], [ %call3, %if.end2 ], [ %retval.23, %cleanup433 ], [ -2, %if.end6 ]
  ret i32 %retval.26
}

declare i64 @fwrite(i8*, i64, i64, i8*) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define weak void @halide_cache_cleanup() #0 {
entry:
  tail call void @halide_memoization_cache_cleanup() #16
  ret void
}

; Function Attrs: nounwind mustprogress
define weak void @halide_memoization_cache_cleanup() local_unnamed_addr #0 {
entry:
  br label %for.body

for.cond.cleanup:                                 ; preds = %while.end
  store i64 0, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  store %"struct.Halide::Runtime::Internal::CacheEntry"* null, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE, align 8, !tbaa !14
  store %"struct.Halide::Runtime::Internal::CacheEntry"* null, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE, align 8, !tbaa !14
  ret void

for.body:                                         ; preds = %while.end, %entry
  %__begin1.018 = phi %"struct.Halide::Runtime::Internal::CacheEntry"** [ getelementptr inbounds ([256 x %"struct.Halide::Runtime::Internal::CacheEntry"*], [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*]* @_ZN6Halide7Runtime8Internal13cache_entriesE, i64 0, i64 0), %entry ], [ %incdec.ptr, %while.end ]
  %0 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %__begin1.018, align 8, !tbaa !14
  store %"struct.Halide::Runtime::Internal::CacheEntry"* null, %"struct.Halide::Runtime::Internal::CacheEntry"** %__begin1.018, align 8, !tbaa !14
  %cmp2.not16 = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %0, null
  br i1 %cmp2.not16, label %while.end, label %while.body

while.body:                                       ; preds = %for.body, %while.body
  %entry1.017 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %1, %while.body ], [ %0, %for.body ]
  %next3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry1.017, i64 0, i32 0
  %1 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next3, align 8, !tbaa !229
  tail call void @_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv(%"struct.Halide::Runtime::Internal::CacheEntry"* nonnull dereferenceable(96) %entry1.017) #16
  %2 = bitcast %"struct.Halide::Runtime::Internal::CacheEntry"* %entry1.017 to i8*
  tail call void @halide_free(i8* null, i8* nonnull %2) #15
  %cmp2.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %1, null
  br i1 %cmp2.not, label %while.end, label %while.body, !llvm.loop !231

while.end:                                        ; preds = %while.body, %for.body
  %incdec.ptr = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %__begin1.018, i64 1
  %cmp.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"** %incdec.ptr, getelementptr inbounds ([256 x %"struct.Halide::Runtime::Internal::CacheEntry"*], [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*]* @_ZN6Halide7Runtime8Internal13cache_entriesE, i64 1, i64 0)
  br i1 %cmp.not, label %for.cond.cleanup, label %for.body
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv(%"struct.Halide::Runtime::Internal::CacheEntry"* nonnull dereferenceable(96) %this) local_unnamed_addr #0 align 2 {
entry:
  %tuple_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 8
  %0 = load i32, i32* %tuple_count, align 8, !tbaa !232
  %cmp11.not = icmp eq i32 %0, 0
  br i1 %cmp11.not, label %for.cond.cleanup, label %for.body.lr.ph

for.body.lr.ph:                                   ; preds = %entry
  %buf = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 11
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.body, %entry
  %metadata_storage = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 3
  %1 = load i8*, i8** %metadata_storage, align 8, !tbaa !233
  tail call void @halide_free(i8* null, i8* %1) #15
  ret void

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %indvars.iv = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next, %for.body ]
  %2 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf, align 8, !tbaa !234
  %arrayidx = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i64 %indvars.iv
  %call = tail call i32 @halide_device_free(i8* null, %struct.halide_buffer_t* %arrayidx) #15
  %3 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf, align 8, !tbaa !234
  %host = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i64 %indvars.iv, i32 2
  %4 = load i8*, i8** %host, align 8, !tbaa !180
  %call6 = tail call %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* %4) #16
  %5 = bitcast %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call6 to i8*
  tail call void @halide_free(i8* null, i8* %5) #15
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %6 = load i32, i32* %tuple_count, align 8, !tbaa !232
  %7 = zext i32 %6 to i64
  %cmp = icmp ult i64 %indvars.iv.next, %7
  br i1 %cmp, label %for.body, label %for.cond.cleanup, !llvm.loop !235
}

; Function Attrs: nounwind willreturn mustprogress
define linkonce %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* %host) local_unnamed_addr #2 {
entry:
  %add.ptr = getelementptr inbounds i8, i8* %host, i64 -32
  %0 = bitcast i8* %add.ptr to %"struct.Halide::Runtime::Internal::CacheBlockHeader"*
  ret %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %0
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx(%"struct.Halide::Runtime::Internal::device_copy"* nonnull align 8 dereferenceable(416) %copy, i32 %d, i64 %src_off, i64 %dst_off) local_unnamed_addr #0 {
entry:
  %cmp41 = icmp sgt i32 %d, -1
  br i1 %cmp41, label %land.rhs, label %while.end

land.rhs:                                         ; preds = %entry, %while.body
  %d.addr.042 = phi i32 [ %dec, %while.body ], [ %d, %entry ]
  %idxprom36 = zext i32 %d.addr.042 to i64
  %arrayidx = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 3, i64 %idxprom36
  %0 = load i64, i64* %arrayidx, align 8, !tbaa !22
  %cmp1 = icmp eq i64 %0, 1
  br i1 %cmp1, label %while.body, label %while.end

while.body:                                       ; preds = %land.rhs
  %dec = add nsw i32 %d.addr.042, -1
  %cmp = icmp sgt i32 %d.addr.042, 0
  br i1 %cmp, label %land.rhs, label %if.then, !llvm.loop !236

while.end:                                        ; preds = %land.rhs, %entry
  %d.addr.0.lcssa = phi i32 [ %d, %entry ], [ %d.addr.042, %land.rhs ]
  %cmp2 = icmp eq i32 %d.addr.0.lcssa, -1
  br i1 %cmp2, label %if.then, label %for.cond.preheader

for.cond.preheader:                               ; preds = %while.end
  %idxprom5 = sext i32 %d.addr.0.lcssa to i64
  %arrayidx6 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 3, i64 %idxprom5
  %1 = load i64, i64* %arrayidx6, align 8, !tbaa !22
  %cmp737.not = icmp eq i64 %1, 0
  br i1 %cmp737.not, label %if.end, label %for.body.lr.ph

for.body.lr.ph:                                   ; preds = %for.cond.preheader
  %sub = add nsw i32 %d.addr.0.lcssa, -1
  %arrayidx9 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 4, i64 %idxprom5
  %arrayidx12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 5, i64 %idxprom5
  %inc.0 = add nuw i64 0, 1
  br label %for.body

if.then:                                          ; preds = %while.body, %while.end
  %src = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 0
  %2 = load i64, i64* %src, align 8, !tbaa !237
  %add = add i64 %2, %src_off
  %3 = inttoptr i64 %add to i8*
  %dst = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 1
  %4 = load i64, i64* %dst, align 8, !tbaa !239
  %add3 = add i64 %4, %dst_off
  %5 = inttoptr i64 %add3 to i8*
  %chunk_size = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 6
  %6 = load i64, i64* %chunk_size, align 8, !tbaa !240
  %call = tail call i8* @memcpy(i8* %5, i8* %3, i64 %6) #15
  br label %if.end

for.body:                                         ; preds = %for.body.for.body_crit_edge, %for.body.lr.ph
  %inc.phi = phi i64 [ %inc.0, %for.body.lr.ph ], [ %inc.1, %for.body.for.body_crit_edge ]
  %src_off.addr.039 = phi i64 [ %src_off, %for.body.lr.ph ], [ %add10, %for.body.for.body_crit_edge ]
  %dst_off.addr.038 = phi i64 [ %dst_off, %for.body.lr.ph ], [ %add13, %for.body.for.body_crit_edge ]
  tail call void @_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx(%"struct.Halide::Runtime::Internal::device_copy"* nonnull align 8 dereferenceable(416) %copy, i32 %sub, i64 %src_off.addr.039, i64 %dst_off.addr.038) #16
  %7 = load i64, i64* %arrayidx9, align 8, !tbaa !22
  %add10 = add i64 %7, %src_off.addr.039
  %8 = load i64, i64* %arrayidx12, align 8, !tbaa !22
  %add13 = add i64 %8, %dst_off.addr.038
  %9 = load i64, i64* %arrayidx6, align 8, !tbaa !22
  %cmp7 = icmp ult i64 %inc.phi, %9
  br i1 %cmp7, label %for.body.for.body_crit_edge, label %if.end, !llvm.loop !241

for.body.for.body_crit_edge:                      ; preds = %for.body
  %inc.1 = add nuw i64 %inc.phi, 1
  br label %for.body

if.end:                                           ; preds = %for.body, %if.then, %for.cond.preheader
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv(%"struct.Halide::Runtime::Internal::device_copy"* nonnull align 8 dereferenceable(416) %copy, i8* %user_context) local_unnamed_addr #0 {
entry:
  %src = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 0
  %0 = load i64, i64* %src, align 8, !tbaa !237
  %dst = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 1
  %1 = load i64, i64* %dst, align 8, !tbaa !239
  %cmp.not = icmp eq i64 %0, %1
  br i1 %cmp.not, label %if.end, label %if.then

if.then:                                          ; preds = %entry
  %src_begin = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %copy, i64 0, i32 2
  %2 = load i64, i64* %src_begin, align 8, !tbaa !242
  tail call void @_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx(%"struct.Halide::Runtime::Internal::device_copy"* nonnull align 8 dereferenceable(416) %copy, i32 15, i64 %2, i64 0) #16
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b(%"struct.Halide::Runtime::Internal::device_copy"* noalias sret(%"struct.Halide::Runtime::Internal::device_copy") align 8 %agg.result, %struct.halide_buffer_t* %src, i1 zeroext %src_host, %struct.halide_buffer_t* %dst, i1 zeroext %dst_host) local_unnamed_addr #0 {
entry:
  %c = alloca %"struct.Halide::Runtime::Internal::device_copy", align 8
  %0 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %c to i8*
  call void @llvm.lifetime.start.p0i8(i64 416, i8* nonnull %0) #11
  br i1 %src_host, label %cond.true, label %cond.false

cond.true:                                        ; preds = %entry
  %host = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 2
  %1 = load i8*, i8** %host, align 8, !tbaa !180
  %2 = ptrtoint i8* %1 to i64
  br label %cond.end

cond.false:                                       ; preds = %entry
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 0
  %3 = load i64, i64* %device, align 8, !tbaa !182
  br label %cond.end

cond.end:                                         ; preds = %cond.false, %cond.true
  %cond = phi i64 [ %2, %cond.true ], [ %3, %cond.false ]
  %src2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 0
  store i64 %cond, i64* %src2, align 8, !tbaa !237
  br i1 %dst_host, label %cond.true4, label %cond.false6

cond.true4:                                       ; preds = %cond.end
  %host5 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 2
  %4 = load i8*, i8** %host5, align 8, !tbaa !180
  %5 = ptrtoint i8* %4 to i64
  br label %cond.end8

cond.false6:                                      ; preds = %cond.end
  %device7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 0
  %6 = load i64, i64* %device7, align 8, !tbaa !182
  br label %cond.end8

cond.end8:                                        ; preds = %cond.false6, %cond.true4
  %cond9 = phi i64 [ %5, %cond.true4 ], [ %6, %cond.false6 ]
  %dst10 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 1
  store i64 %cond9, i64* %dst10, align 8, !tbaa !239
  %bits.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 4, i32 1
  %7 = load i8, i8* %bits.i, align 1, !tbaa !144
  %conv.i = zext i8 %7 to i32
  %add.i = add nuw nsw i32 %conv.i, 7
  %div.i = lshr i32 %add.i, 3
  %conv = zext i32 %div.i to i64
  %chunk_size = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 6
  store i64 %conv, i64* %chunk_size, align 8, !tbaa !240
  %arrayidx = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 0
  %arrayidx12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 0
  %arrayidx14 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 0
  %arrayidx.1 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 1
  %8 = bitcast i64* %arrayidx to <2 x i64>*
  store <2 x i64> <i64 1, i64 1>, <2 x i64>* %8, align 8, !tbaa !22
  %arrayidx12.1 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 1
  %9 = bitcast i64* %arrayidx12 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %9, align 8, !tbaa !22
  %arrayidx14.1 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 1
  %10 = bitcast i64* %arrayidx14 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %10, align 8, !tbaa !22
  %arrayidx.2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 2
  %arrayidx12.2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 2
  %arrayidx14.2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 2
  %arrayidx.3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 3
  %11 = bitcast i64* %arrayidx.2 to <2 x i64>*
  store <2 x i64> <i64 1, i64 1>, <2 x i64>* %11, align 8, !tbaa !22
  %arrayidx12.3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 3
  %12 = bitcast i64* %arrayidx12.2 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %12, align 8, !tbaa !22
  %arrayidx14.3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 3
  %13 = bitcast i64* %arrayidx14.2 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %13, align 8, !tbaa !22
  %arrayidx.4 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 4
  %arrayidx12.4 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 4
  %arrayidx14.4 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 4
  %arrayidx.5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 5
  %14 = bitcast i64* %arrayidx.4 to <2 x i64>*
  store <2 x i64> <i64 1, i64 1>, <2 x i64>* %14, align 8, !tbaa !22
  %arrayidx12.5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 5
  %15 = bitcast i64* %arrayidx12.4 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %15, align 8, !tbaa !22
  %arrayidx14.5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 5
  %16 = bitcast i64* %arrayidx14.4 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %16, align 8, !tbaa !22
  %arrayidx.6 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 6
  %arrayidx12.6 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 6
  %arrayidx14.6 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 6
  %arrayidx.7 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 7
  %17 = bitcast i64* %arrayidx.6 to <2 x i64>*
  store <2 x i64> <i64 1, i64 1>, <2 x i64>* %17, align 8, !tbaa !22
  %arrayidx12.7 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 7
  %18 = bitcast i64* %arrayidx12.6 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %18, align 8, !tbaa !22
  %arrayidx14.7 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 7
  %19 = bitcast i64* %arrayidx14.6 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %19, align 8, !tbaa !22
  %arrayidx.8 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 8
  %arrayidx12.8 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 8
  %arrayidx14.8 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 8
  %arrayidx.9 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 9
  %20 = bitcast i64* %arrayidx.8 to <2 x i64>*
  store <2 x i64> <i64 1, i64 1>, <2 x i64>* %20, align 8, !tbaa !22
  %arrayidx12.9 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 9
  %21 = bitcast i64* %arrayidx12.8 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %21, align 8, !tbaa !22
  %arrayidx14.9 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 9
  %22 = bitcast i64* %arrayidx14.8 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %22, align 8, !tbaa !22
  %arrayidx.10 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 10
  %arrayidx12.10 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 10
  %arrayidx14.10 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 10
  %arrayidx.11 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 11
  %23 = bitcast i64* %arrayidx.10 to <2 x i64>*
  store <2 x i64> <i64 1, i64 1>, <2 x i64>* %23, align 8, !tbaa !22
  %arrayidx12.11 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 11
  %24 = bitcast i64* %arrayidx12.10 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %24, align 8, !tbaa !22
  %arrayidx14.11 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 11
  %25 = bitcast i64* %arrayidx14.10 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %25, align 8, !tbaa !22
  %arrayidx.12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 12
  %arrayidx12.12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 12
  %arrayidx14.12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 12
  %arrayidx.13 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 13
  %26 = bitcast i64* %arrayidx.12 to <2 x i64>*
  store <2 x i64> <i64 1, i64 1>, <2 x i64>* %26, align 8, !tbaa !22
  %arrayidx12.13 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 13
  %27 = bitcast i64* %arrayidx12.12 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %27, align 8, !tbaa !22
  %arrayidx14.13 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 13
  %28 = bitcast i64* %arrayidx14.12 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %28, align 8, !tbaa !22
  %arrayidx.14 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 14
  %arrayidx12.14 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 14
  %arrayidx14.14 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 14
  %arrayidx.15 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 15
  %29 = bitcast i64* %arrayidx.14 to <2 x i64>*
  store <2 x i64> <i64 1, i64 1>, <2 x i64>* %29, align 8, !tbaa !22
  %arrayidx12.15 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 15
  %30 = bitcast i64* %arrayidx12.14 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %30, align 8, !tbaa !22
  %arrayidx14.15 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 15
  %31 = bitcast i64* %arrayidx14.14 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %31, align 8, !tbaa !22
  %src_begin = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 2
  %dimensions = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 5
  %32 = load i32, i32* %dimensions, align 4, !tbaa !183
  %cmp17272 = icmp sgt i32 %32, 0
  br i1 %cmp17272, label %for.body19.lr.ph, label %cond.end8.for.cond.cleanup18_crit_edge

cond.end8.for.cond.cleanup18_crit_edge:           ; preds = %cond.end8
  %mul37.0 = mul i64 %conv, 0
  br label %for.cond.cleanup18

for.body19.lr.ph:                                 ; preds = %cond.end8
  %dim = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 6
  %33 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim, align 8, !tbaa !184
  %dim23 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 6
  %34 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim23, align 8, !tbaa !184
  %wide.trip.count = zext i32 %32 to i64
  %min.iters.check = icmp ult i32 %32, 5
  br i1 %min.iters.check, label %for.body19.preheader, label %vector.ph

for.body19.preheader:                             ; preds = %middle.block, %for.body19.lr.ph
  %indvars.iv284.ph = phi i64 [ 0, %for.body19.lr.ph ], [ %n.vec, %middle.block ]
  %.ph = phi i64 [ 0, %for.body19.lr.ph ], [ %61, %middle.block ]
  br label %for.body19

vector.ph:                                        ; preds = %for.body19.lr.ph
  %n.mod.vf = and i64 %wide.trip.count, 3
  %35 = icmp eq i64 %n.mod.vf, 0
  %36 = select i1 %35, i64 4, i64 %n.mod.vf
  %n.vec = sub nsw i64 %wide.trip.count, %36
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %vec.phi = phi <2 x i64> [ zeroinitializer, %vector.ph ], [ %58, %vector.body ]
  %vec.phi2 = phi <2 x i64> [ zeroinitializer, %vector.ph ], [ %59, %vector.body ]
  %37 = or i64 %index, 2
  %38 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %33, i64 %index, i32 2
  %39 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %33, i64 %37, i32 2
  %40 = getelementptr inbounds i32, i32* %38, i64 -2
  %41 = bitcast i32* %40 to <8 x i32>*
  %42 = getelementptr inbounds i32, i32* %39, i64 -2
  %43 = bitcast i32* %42 to <8 x i32>*
  %wide.vec = load <8 x i32>, <8 x i32>* %41, align 4, !tbaa !41
  %wide.vec3 = load <8 x i32>, <8 x i32>* %43, align 4, !tbaa !41
  %strided.vec5 = shufflevector <8 x i32> %wide.vec, <8 x i32> poison, <2 x i32> <i32 2, i32 6>
  %strided.vec6 = shufflevector <8 x i32> %wide.vec3, <8 x i32> poison, <2 x i32> <i32 2, i32 6>
  %44 = sext <2 x i32> %strided.vec5 to <2 x i64>
  %45 = sext <2 x i32> %strided.vec6 to <2 x i64>
  %46 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %34, i64 %index, i32 0
  %47 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %34, i64 %37, i32 0
  %48 = bitcast i32* %46 to <8 x i32>*
  %49 = bitcast i32* %47 to <8 x i32>*
  %wide.vec7 = load <8 x i32>, <8 x i32>* %48, align 4, !tbaa !221
  %wide.vec8 = load <8 x i32>, <8 x i32>* %49, align 4, !tbaa !221
  %50 = sub nsw <8 x i32> %wide.vec7, %wide.vec
  %51 = shufflevector <8 x i32> %50, <8 x i32> undef, <2 x i32> <i32 0, i32 4>
  %52 = sub nsw <8 x i32> %wide.vec8, %wide.vec3
  %53 = shufflevector <8 x i32> %52, <8 x i32> undef, <2 x i32> <i32 0, i32 4>
  %54 = sext <2 x i32> %51 to <2 x i64>
  %55 = sext <2 x i32> %53 to <2 x i64>
  %56 = mul nsw <2 x i64> %54, %44
  %57 = mul nsw <2 x i64> %55, %45
  %58 = add <2 x i64> %56, %vec.phi
  %59 = add <2 x i64> %57, %vec.phi2
  %index.next = add i64 %index, 4
  %60 = icmp eq i64 %index.next, %n.vec
  br i1 %60, label %middle.block, label %vector.body, !llvm.loop !243

middle.block:                                     ; preds = %vector.body
  %bin.rdx = add <2 x i64> %59, %58
  %61 = call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %bin.rdx)
  br label %for.body19.preheader

for.cond.cleanup18:                               ; preds = %for.body19.for.cond.cleanup18_crit_edge, %cond.end8.for.cond.cleanup18_crit_edge
  %mul37.phi = phi i64 [ %mul37.0, %cond.end8.for.cond.cleanup18_crit_edge ], [ %mul37.1, %for.body19.for.cond.cleanup18_crit_edge ]
  store i64 %mul37.phi, i64* %src_begin, align 8, !tbaa !242
  %dimensions39 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 5
  %62 = load i32, i32* %dimensions39, align 4, !tbaa !183
  %cmp40.not = icmp eq i32 %32, %62
  br i1 %cmp40.not, label %lor.lhs.false, label %if.then

for.body19:                                       ; preds = %for.body19.preheader, %for.body19
  %indvars.iv284 = phi i64 [ %indvars.iv.next285, %for.body19 ], [ %indvars.iv284.ph, %for.body19.preheader ]
  %63 = phi i64 [ %add, %for.body19 ], [ %.ph, %for.body19.preheader ]
  %stride = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %33, i64 %indvars.iv284, i32 2
  %64 = load i32, i32* %stride, align 4, !tbaa !185
  %conv22 = sext i32 %64 to i64
  %min = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %34, i64 %indvars.iv284, i32 0
  %65 = load i32, i32* %min, align 4, !tbaa !221
  %min29 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %33, i64 %indvars.iv284, i32 0
  %66 = load i32, i32* %min29, align 4, !tbaa !221
  %sub = sub nsw i32 %65, %66
  %conv30 = sext i32 %sub to i64
  %mul = mul nsw i64 %conv30, %conv22
  %add = add i64 %mul, %63
  %indvars.iv.next285 = add nuw nsw i64 %indvars.iv284, 1
  %exitcond286.not = icmp eq i64 %indvars.iv.next285, %wide.trip.count
  br i1 %exitcond286.not, label %for.body19.for.cond.cleanup18_crit_edge, label %for.body19, !llvm.loop !244

for.body19.for.cond.cleanup18_crit_edge:          ; preds = %for.body19
  %mul37.1 = mul i64 %add, %conv
  br label %for.cond.cleanup18

lor.lhs.false:                                    ; preds = %for.cond.cleanup18
  %bits.i253 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 4, i32 1
  %67 = load i8, i8* %bits.i253, align 1, !tbaa !144
  %conv.i254 = zext i8 %67 to i32
  %add.i255 = add nuw nsw i32 %conv.i254, 7
  %div.i256 = lshr i32 %add.i255, 3
  %cmp45.not = icmp ne i32 %div.i, %div.i256
  %cmp48 = icmp sgt i32 %32, 16
  %or.cond261 = or i1 %cmp48, %cmp45.not
  br i1 %or.cond261, label %if.then, label %if.end

if.then:                                          ; preds = %lor.lhs.false, %for.cond.cleanup18
  %68 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %agg.result to i8*
  tail call void @llvm.memset.p0i8.i64(i8* nonnull align 8 dereferenceable(416) %68, i8 0, i64 416, i1 false)
  br label %cleanup

if.end:                                           ; preds = %lor.lhs.false
  %cmp50 = icmp eq i32 %div.i, 0
  br i1 %cmp50, label %if.then51, label %for.cond54.preheader

for.cond54.preheader:                             ; preds = %if.end
  br i1 %cmp17272, label %for.body58.lr.ph, label %while.end

for.body58.lr.ph:                                 ; preds = %for.cond54.preheader
  %dim60 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 6
  %69 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim60, align 8, !tbaa !184
  %dim70 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 6
  %70 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim70, align 8, !tbaa !184
  %wide.trip.count282 = zext i32 %32 to i64
  br label %for.body58

if.then51:                                        ; preds = %if.end
  %71 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %agg.result to i8*
  tail call void @llvm.memset.p0i8.i64(i8* nonnull align 8 dereferenceable(416) %71, i8 0, i64 416, i1 false)
  br label %cleanup

while.cond.preheader:                             ; preds = %for.cond.cleanup94
  %.pre = load i64, i64* %chunk_size, align 8, !tbaa !240
  %.pre290 = load i64, i64* %arrayidx12, align 8, !tbaa !22
  %cmp139263 = icmp eq i64 %.pre, %.pre290
  br i1 %cmp139263, label %land.rhs.lr.ph, label %while.end

land.rhs.lr.ph:                                   ; preds = %while.cond.preheader
  %.pre291 = load i64, i64* %arrayidx14, align 8, !tbaa !22
  %72 = bitcast i64* %arrayidx.1 to <2 x i64>*
  %73 = bitcast i64* %arrayidx to <2 x i64>*
  %74 = bitcast i64* %arrayidx12.1 to <2 x i64>*
  %75 = bitcast i64* %arrayidx12 to <2 x i64>*
  %76 = bitcast i64* %arrayidx14.1 to <2 x i64>*
  %77 = bitcast i64* %arrayidx14 to <2 x i64>*
  %78 = bitcast i64* %arrayidx.3 to <2 x i64>*
  %79 = bitcast i64* %arrayidx.2 to <2 x i64>*
  %80 = bitcast i64* %arrayidx12.3 to <2 x i64>*
  %81 = bitcast i64* %arrayidx12.2 to <2 x i64>*
  %82 = bitcast i64* %arrayidx14.3 to <2 x i64>*
  %83 = bitcast i64* %arrayidx14.2 to <2 x i64>*
  %84 = bitcast i64* %arrayidx.5 to <2 x i64>*
  %85 = bitcast i64* %arrayidx.4 to <2 x i64>*
  %86 = bitcast i64* %arrayidx12.5 to <2 x i64>*
  %87 = bitcast i64* %arrayidx12.4 to <2 x i64>*
  %88 = bitcast i64* %arrayidx14.5 to <2 x i64>*
  %89 = bitcast i64* %arrayidx14.4 to <2 x i64>*
  %90 = bitcast i64* %arrayidx.7 to <2 x i64>*
  %91 = bitcast i64* %arrayidx.6 to <2 x i64>*
  %92 = bitcast i64* %arrayidx12.7 to <2 x i64>*
  %93 = bitcast i64* %arrayidx12.6 to <2 x i64>*
  %94 = bitcast i64* %arrayidx14.7 to <2 x i64>*
  %95 = bitcast i64* %arrayidx14.6 to <2 x i64>*
  %96 = bitcast i64* %arrayidx.9 to <2 x i64>*
  %97 = bitcast i64* %arrayidx.8 to <2 x i64>*
  %98 = bitcast i64* %arrayidx12.9 to <2 x i64>*
  %99 = bitcast i64* %arrayidx12.8 to <2 x i64>*
  %100 = bitcast i64* %arrayidx14.9 to <2 x i64>*
  %101 = bitcast i64* %arrayidx14.8 to <2 x i64>*
  %102 = bitcast i64* %arrayidx.11 to <2 x i64>*
  %103 = bitcast i64* %arrayidx.10 to <2 x i64>*
  %104 = bitcast i64* %arrayidx12.11 to <2 x i64>*
  %105 = bitcast i64* %arrayidx12.10 to <2 x i64>*
  %106 = bitcast i64* %arrayidx14.11 to <2 x i64>*
  %107 = bitcast i64* %arrayidx14.10 to <2 x i64>*
  %108 = bitcast i64* %arrayidx.13 to <2 x i64>*
  %109 = bitcast i64* %arrayidx.12 to <2 x i64>*
  %110 = bitcast i64* %arrayidx12.13 to <2 x i64>*
  %111 = bitcast i64* %arrayidx12.12 to <2 x i64>*
  %112 = bitcast i64* %arrayidx14.13 to <2 x i64>*
  %113 = bitcast i64* %arrayidx14.12 to <2 x i64>*
  br label %land.rhs

for.body58:                                       ; preds = %for.cond.cleanup94, %for.body58.lr.ph
  %indvars.iv278 = phi i64 [ 0, %for.body58.lr.ph ], [ %indvars.iv.next279, %for.cond.cleanup94 ]
  %stride63 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %69, i64 %indvars.iv278, i32 2
  %114 = load i32, i32* %stride63, align 4, !tbaa !185
  %conv64 = sext i32 %114 to i64
  %mul68 = mul nsw i64 %conv64, %conv
  %stride73 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %70, i64 %indvars.iv278, i32 2
  %115 = load i32, i32* %stride73, align 4, !tbaa !185
  %conv74 = sext i32 %115 to i64
  %mul78 = mul nsw i64 %conv74, %conv
  %cmp80264.not = icmp eq i64 %indvars.iv278, 0
  br i1 %cmp80264.not, label %for.end91, label %for.body81.lr.ph

for.body81.lr.ph:                                 ; preds = %for.body58
  %cmp86.not = icmp eq i64 %mul68, 0
  br i1 %cmp86.not, label %for.body81.preheader, label %for.body81.us

for.body81.preheader:                             ; preds = %for.body81.lr.ph
  %116 = trunc i64 %indvars.iv278 to i32
  br label %for.end91

for.body81.us:                                    ; preds = %for.body81.lr.ph, %for.inc89.us
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.inc89.us ], [ 0, %for.body81.lr.ph ]
  %arrayidx84.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 %indvars.iv
  %117 = load i64, i64* %arrayidx84.us, align 8, !tbaa !22
  %cmp85.us = icmp ult i64 %mul68, %117
  br i1 %cmp85.us, label %for.end91.loopexit, label %for.inc89.us

for.inc89.us:                                     ; preds = %for.body81.us
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %indvars.iv278
  br i1 %exitcond.not, label %for.end91.loopexit, label %for.body81.us, !llvm.loop !245

for.end91.loopexit:                               ; preds = %for.inc89.us, %for.body81.us
  %insert.0.lcssa.ph.in = phi i64 [ %indvars.iv278, %for.inc89.us ], [ %indvars.iv, %for.body81.us ]
  %insert.0.lcssa.ph = trunc i64 %insert.0.lcssa.ph.in to i32
  br label %for.end91

for.end91:                                        ; preds = %for.end91.loopexit, %for.body81.preheader, %for.body58
  %insert.0.lcssa = phi i32 [ 0, %for.body58 ], [ %insert.0.lcssa.ph, %for.end91.loopexit ], [ %116, %for.body81.preheader ]
  %118 = zext i32 %insert.0.lcssa to i64
  %cmp93267 = icmp ugt i64 %indvars.iv278, %118
  br i1 %cmp93267, label %for.body95.preheader, label %for.cond.cleanup94

for.body95.preheader:                             ; preds = %for.end91
  %119 = sext i32 %insert.0.lcssa to i64
  br label %for.body95

for.cond.cleanup94:                               ; preds = %for.body95, %for.end91
  %extent122 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %69, i64 %indvars.iv278, i32 1
  %120 = load i32, i32* %extent122, align 4, !tbaa !189
  %conv123 = sext i32 %120 to i64
  %arrayidx126 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 %118
  store i64 %conv123, i64* %arrayidx126, align 8, !tbaa !22
  %arrayidx129 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 %118
  store i64 %mul68, i64* %arrayidx129, align 8, !tbaa !22
  %arrayidx132 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 %118
  store i64 %mul78, i64* %arrayidx132, align 8, !tbaa !22
  %indvars.iv.next279 = add nuw nsw i64 %indvars.iv278, 1
  %exitcond283.not = icmp eq i64 %indvars.iv.next279, %wide.trip.count282
  br i1 %exitcond283.not, label %while.cond.preheader, label %for.body58, !llvm.loop !246

for.body95:                                       ; preds = %for.body95, %for.body95.preheader
  %indvars.iv280 = phi i64 [ %indvars.iv278, %for.body95.preheader ], [ %indvars.iv.next281, %for.body95 ]
  %indvars.iv.next281 = add nsw i64 %indvars.iv280, -1
  %arrayidx99 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 %indvars.iv.next281
  %121 = load i64, i64* %arrayidx99, align 8, !tbaa !22
  %arrayidx102 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 3, i64 %indvars.iv280
  store i64 %121, i64* %arrayidx102, align 8, !tbaa !22
  %arrayidx106 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 %indvars.iv.next281
  %122 = load i64, i64* %arrayidx106, align 8, !tbaa !22
  %arrayidx109 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 5, i64 %indvars.iv280
  store i64 %122, i64* %arrayidx109, align 8, !tbaa !22
  %arrayidx113 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 %indvars.iv.next281
  %123 = load i64, i64* %arrayidx113, align 8, !tbaa !22
  %arrayidx116 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %c, i64 0, i32 4, i64 %indvars.iv280
  store i64 %123, i64* %arrayidx116, align 8, !tbaa !22
  %cmp93 = icmp sgt i64 %indvars.iv.next281, %119
  br i1 %cmp93, label %for.body95, label %for.cond.cleanup94, !llvm.loop !247

land.rhs:                                         ; preds = %while.body, %land.rhs.lr.ph
  %124 = phi i64 [ %.pre291, %land.rhs.lr.ph ], [ %152, %while.body ]
  %125 = phi i64 [ %.pre, %land.rhs.lr.ph ], [ %mul147, %while.body ]
  %cmp143 = icmp eq i64 %125, %124
  br i1 %cmp143, label %while.body, label %while.end

while.body:                                       ; preds = %land.rhs
  %126 = load i64, i64* %arrayidx, align 8, !tbaa !22
  %mul147 = mul i64 %126, %124
  store i64 %mul147, i64* %chunk_size, align 8, !tbaa !240
  %127 = load <2 x i64>, <2 x i64>* %72, align 8, !tbaa !22
  store <2 x i64> %127, <2 x i64>* %73, align 8, !tbaa !22
  %128 = load <2 x i64>, <2 x i64>* %74, align 8, !tbaa !22
  store <2 x i64> %128, <2 x i64>* %75, align 8, !tbaa !22
  %129 = load <2 x i64>, <2 x i64>* %76, align 8, !tbaa !22
  store <2 x i64> %129, <2 x i64>* %77, align 8, !tbaa !22
  %130 = load <2 x i64>, <2 x i64>* %78, align 8, !tbaa !22
  store <2 x i64> %130, <2 x i64>* %79, align 8, !tbaa !22
  %131 = load <2 x i64>, <2 x i64>* %80, align 8, !tbaa !22
  store <2 x i64> %131, <2 x i64>* %81, align 8, !tbaa !22
  %132 = load <2 x i64>, <2 x i64>* %82, align 8, !tbaa !22
  store <2 x i64> %132, <2 x i64>* %83, align 8, !tbaa !22
  %133 = load <2 x i64>, <2 x i64>* %84, align 8, !tbaa !22
  store <2 x i64> %133, <2 x i64>* %85, align 8, !tbaa !22
  %134 = load <2 x i64>, <2 x i64>* %86, align 8, !tbaa !22
  store <2 x i64> %134, <2 x i64>* %87, align 8, !tbaa !22
  %135 = load <2 x i64>, <2 x i64>* %88, align 8, !tbaa !22
  store <2 x i64> %135, <2 x i64>* %89, align 8, !tbaa !22
  %136 = load <2 x i64>, <2 x i64>* %90, align 8, !tbaa !22
  store <2 x i64> %136, <2 x i64>* %91, align 8, !tbaa !22
  %137 = load <2 x i64>, <2 x i64>* %92, align 8, !tbaa !22
  store <2 x i64> %137, <2 x i64>* %93, align 8, !tbaa !22
  %138 = load <2 x i64>, <2 x i64>* %94, align 8, !tbaa !22
  store <2 x i64> %138, <2 x i64>* %95, align 8, !tbaa !22
  %139 = load <2 x i64>, <2 x i64>* %96, align 8, !tbaa !22
  store <2 x i64> %139, <2 x i64>* %97, align 8, !tbaa !22
  %140 = load <2 x i64>, <2 x i64>* %98, align 8, !tbaa !22
  store <2 x i64> %140, <2 x i64>* %99, align 8, !tbaa !22
  %141 = load <2 x i64>, <2 x i64>* %100, align 8, !tbaa !22
  store <2 x i64> %141, <2 x i64>* %101, align 8, !tbaa !22
  %142 = load <2 x i64>, <2 x i64>* %102, align 8, !tbaa !22
  store <2 x i64> %142, <2 x i64>* %103, align 8, !tbaa !22
  %143 = load <2 x i64>, <2 x i64>* %104, align 8, !tbaa !22
  store <2 x i64> %143, <2 x i64>* %105, align 8, !tbaa !22
  %144 = load <2 x i64>, <2 x i64>* %106, align 8, !tbaa !22
  store <2 x i64> %144, <2 x i64>* %107, align 8, !tbaa !22
  %145 = load <2 x i64>, <2 x i64>* %108, align 8, !tbaa !22
  store <2 x i64> %145, <2 x i64>* %109, align 8, !tbaa !22
  %146 = load <2 x i64>, <2 x i64>* %110, align 8, !tbaa !22
  store <2 x i64> %146, <2 x i64>* %111, align 8, !tbaa !22
  %147 = load <2 x i64>, <2 x i64>* %112, align 8, !tbaa !22
  store <2 x i64> %147, <2 x i64>* %113, align 8, !tbaa !22
  %148 = load i64, i64* %arrayidx.15, align 8, !tbaa !22
  store i64 %148, i64* %arrayidx.14, align 8, !tbaa !22
  %149 = load i64, i64* %arrayidx12.15, align 8, !tbaa !22
  store i64 %149, i64* %arrayidx12.14, align 8, !tbaa !22
  %150 = load i64, i64* %arrayidx14.15, align 8, !tbaa !22
  store i64 %150, i64* %arrayidx14.14, align 8, !tbaa !22
  store i64 1, i64* %arrayidx.15, align 8, !tbaa !22
  store i64 0, i64* %arrayidx12.15, align 8, !tbaa !22
  store i64 0, i64* %arrayidx14.15, align 8, !tbaa !22
  %151 = extractelement <2 x i64> %128, i32 0
  %cmp139 = icmp eq i64 %mul147, %151
  %152 = extractelement <2 x i64> %129, i32 0
  br i1 %cmp139, label %land.rhs, label %while.end, !llvm.loop !248

while.end:                                        ; preds = %while.body, %land.rhs, %while.cond.preheader, %for.cond54.preheader
  %153 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %agg.result to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 dereferenceable(416) %153, i8* nonnull align 8 dereferenceable(416) %0, i64 416, i1 false), !tbaa.struct !249
  br label %cleanup

cleanup:                                          ; preds = %while.end, %if.then51, %if.then
  call void @llvm.lifetime.end.p0i8(i64 416, i8* nonnull %0) #11
  ret void
}

; Function Attrs: argmemonly nofree nosync nounwind willreturn writeonly
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #5

; Function Attrs: nounwind mustprogress
define linkonce zeroext i1 @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m(i8* %key1, i8* %key2, i64 %key_size) local_unnamed_addr #0 {
entry:
  %call = tail call i32 @memcmp(i8* %key1, i8* %key2, i64 %key_size) #15
  %cmp = icmp eq i32 %call, 0
  ret i1 %cmp
}

declare i32 @memcmp(i8*, i8*, i64) local_unnamed_addr #1

; Function Attrs: nounwind mustprogress
define linkonce zeroext i1 @_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t(%struct.halide_buffer_t* %buf, %struct.halide_dimension_t* %shape) local_unnamed_addr #0 {
entry:
  %dimensions = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 5
  %0 = load i32, i32* %dimensions, align 4, !tbaa !183
  %cmp9 = icmp sgt i32 %0, 0
  br i1 %cmp9, label %for.body.lr.ph, label %return

for.body.lr.ph:                                   ; preds = %entry
  %dim = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 6
  %1 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim, align 8, !tbaa !184
  %2 = zext i32 %0 to i64
  br label %for.body

for.cond:                                         ; preds = %_ZNK18halide_dimension_tneERKS_.exit
  %exitcond.not = icmp eq i64 %indvars.iv.next, %2
  br i1 %exitcond.not, label %return, label %for.body, !llvm.loop !250

for.body:                                         ; preds = %for.cond, %for.body.lr.ph
  %indvars.iv = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next, %for.cond ]
  %min.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %1, i64 %indvars.iv, i32 0
  %3 = load i32, i32* %min.i.i, align 4, !tbaa !221
  %min2.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %shape, i64 %indvars.iv, i32 0
  %4 = load i32, i32* %min2.i.i, align 4, !tbaa !221
  %cmp.i.i = icmp eq i32 %3, %4
  br i1 %cmp.i.i, label %land.lhs.true.i.i, label %return

land.lhs.true.i.i:                                ; preds = %for.body
  %extent.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %1, i64 %indvars.iv, i32 1
  %5 = load i32, i32* %extent.i.i, align 4, !tbaa !189
  %extent3.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %shape, i64 %indvars.iv, i32 1
  %6 = load i32, i32* %extent3.i.i, align 4, !tbaa !189
  %cmp4.i.i = icmp eq i32 %5, %6
  br i1 %cmp4.i.i, label %land.lhs.true5.i.i, label %return

land.lhs.true5.i.i:                               ; preds = %land.lhs.true.i.i
  %stride.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %1, i64 %indvars.iv, i32 2
  %7 = load i32, i32* %stride.i.i, align 4, !tbaa !185
  %stride6.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %shape, i64 %indvars.iv, i32 2
  %8 = load i32, i32* %stride6.i.i, align 4, !tbaa !185
  %cmp7.i.i = icmp eq i32 %7, %8
  br i1 %cmp7.i.i, label %_ZNK18halide_dimension_tneERKS_.exit, label %return

_ZNK18halide_dimension_tneERKS_.exit:             ; preds = %land.lhs.true5.i.i
  %flags.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %1, i64 %indvars.iv, i32 3
  %9 = load i32, i32* %flags.i.i, align 4, !tbaa !251
  %flags8.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %shape, i64 %indvars.iv, i32 3
  %10 = load i32, i32* %flags8.i.i, align 4, !tbaa !251
  %cmp9.i.i.not = icmp eq i32 %9, %10
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  br i1 %cmp9.i.i.not, label %for.cond, label %return

return:                                           ; preds = %_ZNK18halide_dimension_tneERKS_.exit, %land.lhs.true5.i.i, %land.lhs.true.i.i, %for.body, %for.cond, %entry
  %cmp.lcssa = phi i1 [ true, %entry ], [ false, %_ZNK18halide_dimension_tneERKS_.exit ], [ true, %for.cond ], [ false, %land.lhs.true5.i.i ], [ false, %land.lhs.true.i.i ], [ false, %for.body ]
  ret i1 %cmp.lcssa
}

; Function Attrs: nounwind mustprogress
define linkonce zeroext i1 @_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by(%"struct.Halide::Runtime::Internal::CacheEntry"* nonnull dereferenceable(96) %this, i8* %cache_key, i64 %cache_key_size, i32 %key_hash, %struct.halide_buffer_t* %computed_bounds_buf, i32 %tuples, %struct.halide_buffer_t** %tuple_buffers, i1 zeroext %has_eviction_key_arg, i64 %eviction_key_arg) local_unnamed_addr #0 align 2 {
entry:
  %frombool = zext i1 %has_eviction_key_arg to i8
  %0 = bitcast %"struct.Halide::Runtime::Internal::CacheEntry"* %this to <2 x %"struct.Halide::Runtime::Internal::CacheEntry"*>*
  store <2 x %"struct.Halide::Runtime::Internal::CacheEntry"*> zeroinitializer, <2 x %"struct.Halide::Runtime::Internal::CacheEntry"*>* %0, align 8, !tbaa !14
  %less_recent = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::CacheEntry"* null, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent, align 8, !tbaa !252
  %key_size = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 4
  store i64 %cache_key_size, i64* %key_size, align 8, !tbaa !253
  %hash = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 6
  store i32 %key_hash, i32* %hash, align 8, !tbaa !254
  %in_use_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 7
  store i32 0, i32* %in_use_count, align 4, !tbaa !255
  %tuple_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 8
  store i32 %tuples, i32* %tuple_count, align 8, !tbaa !232
  %dimensions = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %computed_bounds_buf, i64 0, i32 5
  %1 = load i32, i32* %dimensions, align 4, !tbaa !183
  %dimensions2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 9
  store i32 %1, i32* %dimensions2, align 4, !tbaa !256
  %conv = zext i32 %tuples to i64
  %mul = mul nuw nsw i64 %conv, 56
  %conv5 = sext i32 %1 to i64
  %add8 = add i32 %tuples, 1
  %conv9 = zext i32 %add8 to i64
  %mul6 = shl nuw nsw i64 %conv9, 4
  %mul10 = mul i64 %mul6, %conv5
  %add11 = add i64 %mul10, %mul
  %add13 = add i64 %add11, %cache_key_size
  %call = tail call i8* @halide_malloc(i8* null, i64 %add13) #15
  %metadata_storage = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 3
  store i8* %call, i8** %metadata_storage, align 8, !tbaa !233
  %tobool.not = icmp eq i8* %call, null
  br i1 %tobool.not, label %cleanup, label %if.end

if.end:                                           ; preds = %entry
  %buf = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 11
  %2 = bitcast %struct.halide_buffer_t** %buf to i8**
  store i8* %call, i8** %2, align 8, !tbaa !234
  %add.ptr = getelementptr inbounds i8, i8* %call, i64 %mul
  %computed_bounds = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 10
  %3 = bitcast %struct.halide_dimension_t** %computed_bounds to i8**
  store i8* %add.ptr, i8** %3, align 8, !tbaa !257
  %add.ptr18 = getelementptr inbounds i8, i8* %call, i64 %add11
  %key = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 5
  store i8* %add.ptr18, i8** %key, align 8, !tbaa !258
  %4 = load i64, i64* %key_size, align 8, !tbaa !253
  %cmp117.not = icmp eq i64 %4, 0
  br i1 %cmp117.not, label %for.cond23.preheader, label %for.body.preheader

for.body.preheader:                               ; preds = %if.end
  %5 = load i8, i8* %cache_key, align 1, !tbaa !18
  store i8 %5, i8* %add.ptr18, align 1, !tbaa !18
  %6 = load i64, i64* %key_size, align 8, !tbaa !253
  %cmp130 = icmp ugt i64 %6, 1
  br i1 %cmp130, label %for.body.for.body_crit_edge, label %for.cond23.preheader, !llvm.loop !259

for.cond23.preheader:                             ; preds = %for.body.for.body_crit_edge, %for.body.preheader, %if.end
  %7 = load i32, i32* %dimensions2, align 4, !tbaa !256
  %cmp25115 = icmp sgt i32 %7, 0
  br i1 %cmp25115, label %for.body27.lr.ph, label %for.cond36.preheader

for.body27.lr.ph:                                 ; preds = %for.cond23.preheader
  %dim = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %computed_bounds_buf, i64 0, i32 6
  br label %for.body27

for.body.for.body_crit_edge:                      ; preds = %for.body.preheader, %for.body.for.body_crit_edge
  %inc131 = phi i64 [ %inc, %for.body.for.body_crit_edge ], [ 1, %for.body.preheader ]
  %.pre = load i8*, i8** %key, align 8, !tbaa !258
  %arrayidx = getelementptr inbounds i8, i8* %cache_key, i64 %inc131
  %8 = load i8, i8* %arrayidx, align 1, !tbaa !18
  %arrayidx21 = getelementptr inbounds i8, i8* %.pre, i64 %inc131
  store i8 %8, i8* %arrayidx21, align 1, !tbaa !18
  %inc = add nuw i64 %inc131, 1
  %9 = load i64, i64* %key_size, align 8, !tbaa !253
  %cmp = icmp ult i64 %inc, %9
  br i1 %cmp, label %for.body.for.body_crit_edge, label %for.cond23.preheader, !llvm.loop !259

for.cond36.preheader:                             ; preds = %for.body27, %for.cond23.preheader
  %10 = load i32, i32* %tuple_count, align 8, !tbaa !232
  %cmp38113.not = icmp eq i32 %10, 0
  br i1 %cmp38113.not, label %for.cond.cleanup39, label %for.body40

for.body27:                                       ; preds = %for.body27, %for.body27.lr.ph
  %indvars.iv121 = phi i64 [ 0, %for.body27.lr.ph ], [ %indvars.iv.next122, %for.body27 ]
  %11 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim, align 8, !tbaa !184
  %arrayidx28 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %11, i64 %indvars.iv121
  %12 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %computed_bounds, align 8, !tbaa !257
  %arrayidx31 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %12, i64 %indvars.iv121
  %13 = bitcast %struct.halide_dimension_t* %arrayidx31 to i8*
  %14 = bitcast %struct.halide_dimension_t* %arrayidx28 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 4 dereferenceable(16) %13, i8* nonnull align 4 dereferenceable(16) %14, i64 16, i1 false), !tbaa.struct !188
  %indvars.iv.next122 = add nuw nsw i64 %indvars.iv121, 1
  %15 = load i32, i32* %dimensions2, align 4, !tbaa !256
  %16 = sext i32 %15 to i64
  %cmp25 = icmp slt i64 %indvars.iv.next122, %16
  br i1 %cmp25, label %for.body27, label %for.cond36.preheader, !llvm.loop !260

for.cond36.loopexit:                              ; preds = %for.body59.for.body59_crit_edge, %for.body59.preheader, %for.body40
  %17 = load i32, i32* %tuple_count, align 8, !tbaa !232
  %18 = zext i32 %17 to i64
  %cmp38 = icmp ult i64 %indvars.iv.next120, %18
  br i1 %cmp38, label %for.body40, label %for.cond.cleanup39, !llvm.loop !261

for.cond.cleanup39:                               ; preds = %for.cond36.loopexit, %for.cond36.preheader
  %has_eviction_key = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 13
  store i8 %frombool, i8* %has_eviction_key, align 8, !tbaa !262
  %eviction_key = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %this, i64 0, i32 12
  store i64 %eviction_key_arg, i64* %eviction_key, align 8, !tbaa !263
  br label %cleanup

for.body40:                                       ; preds = %for.cond36.preheader, %for.cond36.loopexit
  %indvars.iv119 = phi i64 [ %indvars.iv.next120, %for.cond36.loopexit ], [ 0, %for.cond36.preheader ]
  %arrayidx42 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv119
  %19 = bitcast %struct.halide_buffer_t** %arrayidx42 to i8**
  %20 = load i8*, i8** %19, align 8, !tbaa !14
  %21 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf, align 8, !tbaa !234
  %arrayidx45 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %21, i64 %indvars.iv119
  %22 = bitcast %struct.halide_buffer_t* %arrayidx45 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 dereferenceable(56) %22, i8* nonnull align 8 dereferenceable(56) %20, i64 56, i1 false), !tbaa.struct !264
  %23 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %computed_bounds, align 8, !tbaa !257
  %indvars.iv.next120 = add nuw nsw i64 %indvars.iv119, 1
  %24 = load i32, i32* %dimensions2, align 4, !tbaa !256
  %25 = trunc i64 %indvars.iv.next120 to i32
  %mul49 = mul i32 %24, %25
  %idx.ext = zext i32 %mul49 to i64
  %add.ptr50 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %23, i64 %idx.ext
  %26 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf, align 8, !tbaa !234
  %dim54 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %26, i64 %indvars.iv119, i32 6
  store %struct.halide_dimension_t* %add.ptr50, %struct.halide_dimension_t** %dim54, align 8, !tbaa !184
  %cmp57111 = icmp sgt i32 %24, 0
  br i1 %cmp57111, label %for.body59.preheader, label %for.cond36.loopexit

for.body59.preheader:                             ; preds = %for.body40
  %27 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx42, align 8, !tbaa !14
  %dim62125 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %27, i64 0, i32 6
  %28 = bitcast %struct.halide_dimension_t** %dim62125 to i8**
  %29 = load i8*, i8** %28, align 8, !tbaa !184
  %30 = bitcast %struct.halide_dimension_t* %add.ptr50 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 4 dereferenceable(16) %30, i8* nonnull align 4 dereferenceable(16) %29, i64 16, i1 false), !tbaa.struct !188
  %31 = load i32, i32* %dimensions2, align 4, !tbaa !256
  %cmp57128 = icmp sgt i32 %31, 1
  br i1 %cmp57128, label %for.body59.for.body59_crit_edge, label %for.cond36.loopexit, !llvm.loop !266

for.body59.for.body59_crit_edge:                  ; preds = %for.body59.preheader, %for.body59.for.body59_crit_edge
  %indvars.iv.next129 = phi i64 [ %indvars.iv.next, %for.body59.for.body59_crit_edge ], [ 1, %for.body59.preheader ]
  %.pre123 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf, align 8, !tbaa !234
  %dim68.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %.pre123, i64 %indvars.iv119, i32 6
  %.pre124 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim68.phi.trans.insert, align 8, !tbaa !184
  %32 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx42, align 8, !tbaa !14
  %dim62 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %32, i64 0, i32 6
  %33 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim62, align 8, !tbaa !184
  %arrayidx64 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %33, i64 %indvars.iv.next129
  %arrayidx70 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %.pre124, i64 %indvars.iv.next129
  %34 = bitcast %struct.halide_dimension_t* %arrayidx70 to i8*
  %35 = bitcast %struct.halide_dimension_t* %arrayidx64 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 4 dereferenceable(16) %34, i8* nonnull align 4 dereferenceable(16) %35, i64 16, i1 false), !tbaa.struct !188
  %indvars.iv.next = add nuw nsw i64 %indvars.iv.next129, 1
  %36 = load i32, i32* %dimensions2, align 4, !tbaa !256
  %37 = sext i32 %36 to i64
  %cmp57 = icmp slt i64 %indvars.iv.next, %37
  br i1 %cmp57, label %for.body59.for.body59_crit_edge, label %for.cond36.loopexit, !llvm.loop !266

cleanup:                                          ; preds = %for.cond.cleanup39, %entry
  %38 = xor i1 %tobool.not, true
  ret i1 %38
}

; Function Attrs: nounwind mustprogress
define linkonce i32 @_ZN6Halide7Runtime8Internal8djb_hashEPKhm(i8* %key, i64 %key_size) local_unnamed_addr #0 {
entry:
  %cmp8.not = icmp eq i64 %key_size, 0
  br i1 %cmp8.not, label %for.cond.cleanup, label %entry.for.body_crit_edge

entry.for.body_crit_edge:                         ; preds = %entry
  %inc.1 = add nuw i64 0, 1
  %arrayidx.1 = getelementptr inbounds i8, i8* %key, i64 0
  %add.1 = mul i32 5381, 33
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.body, %entry
  %h.0.lcssa = phi i32 [ 5381, %entry ], [ %add1, %for.body ]
  ret i32 %h.0.lcssa

for.body:                                         ; preds = %entry.for.body_crit_edge, %for.body.for.body_crit_edge
  %add.phi = phi i32 [ %add.0, %for.body.for.body_crit_edge ], [ %add.1, %entry.for.body_crit_edge ]
  %arrayidx.phi = phi i8* [ %arrayidx.0, %for.body.for.body_crit_edge ], [ %arrayidx.1, %entry.for.body_crit_edge ]
  %inc.phi = phi i64 [ %inc.0, %for.body.for.body_crit_edge ], [ %inc.1, %entry.for.body_crit_edge ]
  %0 = load i8, i8* %arrayidx.phi, align 1, !tbaa !18
  %conv = zext i8 %0 to i32
  %add1 = add i32 %add.phi, %conv
  %exitcond.not = icmp eq i64 %inc.phi, %key_size
  br i1 %exitcond.not, label %for.cond.cleanup, label %for.body.for.body_crit_edge, !llvm.loop !267

for.body.for.body_crit_edge:                      ; preds = %for.body
  %inc.0 = add nuw i64 %inc.phi, 1
  %arrayidx.0 = getelementptr inbounds i8, i8* %key, i64 %inc.phi
  %add.0 = mul i32 %add1, 33
  br label %for.body
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal11prune_cacheEv() local_unnamed_addr #0 {
entry:
  %0 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE, align 8, !tbaa !14
  %1 = load i64, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  %2 = load i64, i64* @_ZN6Halide7Runtime8Internal14max_cache_sizeE, align 8, !tbaa !22
  %cmp84 = icmp sgt i64 %1, %2
  %cmp185 = icmp ne %"struct.Halide::Runtime::Internal::CacheEntry"* %0, null
  %3 = and i1 %cmp185, %cmp84
  br i1 %3, label %while.body, label %while.end42

while.body:                                       ; preds = %entry, %if.end41
  %4 = phi i64 [ %74, %if.end41 ], [ %2, %entry ]
  %5 = phi i64 [ %75, %if.end41 ], [ %1, %entry ]
  %prune_candidate.086 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %6, %if.end41 ], [ %0, %entry ]
  %more_recent2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 1
  %6 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent2, align 8, !tbaa !268
  %in_use_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 7
  %7 = load i32, i32* %in_use_count, align 4, !tbaa !255
  %cmp3 = icmp eq i32 %7, 0
  br i1 %cmp3, label %if.then, label %if.end41

if.then:                                          ; preds = %while.body
  %hash = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 6
  %8 = load i32, i32* %hash, align 8, !tbaa !254
  %9 = and i32 %8, 255
  %idxprom = zext i32 %9 to i64
  %arrayidx = getelementptr inbounds [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*], [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*]* @_ZN6Halide7Runtime8Internal13cache_entriesE, i64 0, i64 %idxprom
  %10 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %arrayidx, align 8, !tbaa !14
  %cmp5 = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %10, %prune_candidate.086
  br i1 %cmp5, label %if.then6, label %while.cond9

if.then6:                                         ; preds = %if.then
  %next = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 0
  %11 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next, align 8, !tbaa !229
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %11, %"struct.Halide::Runtime::Internal::CacheEntry"** %arrayidx, align 8, !tbaa !14
  br label %if.end21

while.cond9:                                      ; preds = %if.then, %land.rhs11
  %prev_hash_entry.0 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %12, %land.rhs11 ], [ %10, %if.then ]
  %cmp10.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %prev_hash_entry.0, null
  br i1 %cmp10.not, label %if.then18, label %land.rhs11

land.rhs11:                                       ; preds = %while.cond9
  %next12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prev_hash_entry.0, i64 0, i32 0
  %12 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next12, align 8, !tbaa !229
  %cmp13.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %12, %prune_candidate.086
  br i1 %cmp13.not, label %do.end, label %while.cond9, !llvm.loop !269

if.then18:                                        ; preds = %while.cond9
  tail call void @halide_print(i8* null, i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.2.42, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %land.rhs11, %if.then18
  %next19 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 0
  %13 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next19, align 8, !tbaa !229
  %next20 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prev_hash_entry.0, i64 0, i32 0
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %13, %"struct.Halide::Runtime::Internal::CacheEntry"** %next20, align 8, !tbaa !229
  br label %if.end21

if.end21:                                         ; preds = %do.end, %if.then6
  %14 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE, align 8, !tbaa !14
  %cmp22 = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %14, %prune_candidate.086
  br i1 %cmp22, label %if.then23, label %if.end24

if.then23:                                        ; preds = %if.end21
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %6, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE, align 8, !tbaa !14
  br label %if.end24

if.end24:                                         ; preds = %if.then23, %if.end21
  %cmp25.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %6, null
  br i1 %cmp25.not, label %if.end28, label %if.then26

if.then26:                                        ; preds = %if.end24
  %less_recent = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 2
  %15 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent, align 8, !tbaa !252
  %less_recent27 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %6, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %15, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent27, align 8, !tbaa !252
  br label %if.end28

if.end28:                                         ; preds = %if.then26, %if.end24
  %16 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE, align 8, !tbaa !14
  %cmp29 = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %16, %prune_candidate.086
  %less_recent31 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 2
  %17 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent31, align 8, !tbaa !252
  br i1 %cmp29, label %if.then30, label %if.end32

if.then30:                                        ; preds = %if.end28
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %17, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE, align 8, !tbaa !14
  br label %if.end32

if.end32:                                         ; preds = %if.then30, %if.end28
  %cmp34.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %17, null
  br i1 %cmp34.not, label %if.end37, label %if.then35

if.then35:                                        ; preds = %if.end32
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %6, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent31, align 8, !tbaa !252
  br label %if.end37

if.end37:                                         ; preds = %if.then35, %if.end32
  %tuple_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 8
  %18 = load i32, i32* %tuple_count, align 8, !tbaa !232
  %cmp3882.not = icmp eq i32 %18, 0
  br i1 %cmp3882.not, label %for.cond.cleanup, label %for.body.lr.ph

for.body.lr.ph:                                   ; preds = %if.end37
  %buf = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086, i64 0, i32 11
  %19 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf, align 8, !tbaa !234
  %_ZN6Halide7Runtime8Internal18current_cache_sizeE.promoted = load i64, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  %wide.trip.count = zext i32 %18 to i64
  br label %for.body

for.cond.for.cond.cleanup_crit_edge:              ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit
  store i64 %sub, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  br label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %for.cond.for.cond.cleanup_crit_edge, %if.end37
  tail call void @_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv(%"struct.Halide::Runtime::Internal::CacheEntry"* nonnull dereferenceable(96) %prune_candidate.086) #16
  %20 = bitcast %"struct.Halide::Runtime::Internal::CacheEntry"* %prune_candidate.086 to i8*
  tail call void @halide_free(i8* null, i8* nonnull %20) #15
  %.pre92 = load i64, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  %.pre93 = load i64, i64* @_ZN6Halide7Runtime8Internal14max_cache_sizeE, align 8, !tbaa !22
  br label %if.end41

for.body:                                         ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit, %for.body.lr.ph
  %indvars.iv = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next, %_ZNK15halide_buffer_t13size_in_bytesEv.exit ]
  %sub89 = phi i64 [ %_ZN6Halide7Runtime8Internal18current_cache_sizeE.promoted, %for.body.lr.ph ], [ %sub, %_ZNK15halide_buffer_t13size_in_bytesEv.exit ]
  %dimensions.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %19, i64 %indvars.iv, i32 5
  %21 = load i32, i32* %dimensions.i.i, align 4, !tbaa !183
  %cmp19.i.i = icmp sgt i32 %21, 0
  br i1 %cmp19.i.i, label %for.body.lr.ph.i.i, label %for.body._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge

for.body._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge: ; preds = %for.body
  %sub.i.neg.0 = add i64 0, -1
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

for.body.lr.ph.i.i:                               ; preds = %for.body
  %dim.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %19, i64 %indvars.iv, i32 6
  %22 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i.i, align 8, !tbaa !184
  %wide.trip.count.i.i = zext i32 %21 to i64
  %min.iters.check28 = icmp ult i32 %21, 3
  br i1 %min.iters.check28, label %for.body.i.i.preheader, label %vector.ph29

vector.ph29:                                      ; preds = %for.body.lr.ph.i.i
  %n.vec31 = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body27

vector.body27:                                    ; preds = %pred.load.continue43, %vector.ph29
  %index32 = phi i64 [ 0, %vector.ph29 ], [ %index.next33, %pred.load.continue43 ]
  %vec.phi38 = phi i64 [ 0, %vector.ph29 ], [ %predphi44, %pred.load.continue43 ]
  %vec.phi39 = phi i64 [ 0, %vector.ph29 ], [ %predphi45, %pred.load.continue43 ]
  %induction37 = or i64 %index32, 1
  %23 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %index32, i32 2
  %24 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %induction37, i32 2
  %25 = load i32, i32* %23, align 4, !tbaa !185
  %26 = load i32, i32* %24, align 4, !tbaa !185
  %27 = icmp sgt i32 %25, 0
  %28 = icmp sgt i32 %26, 0
  %29 = zext i32 %25 to i64
  %30 = zext i32 %26 to i64
  br i1 %27, label %pred.load.if40, label %pred.load.continue41

pred.load.if40:                                   ; preds = %vector.body27
  %31 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %index32, i32 1
  %32 = load i32, i32* %31, align 4, !tbaa !189
  br label %pred.load.continue41

pred.load.continue41:                             ; preds = %pred.load.if40, %vector.body27
  %33 = phi i32 [ poison, %vector.body27 ], [ %32, %pred.load.if40 ]
  br i1 %28, label %pred.load.if42, label %pred.load.continue43

pred.load.if42:                                   ; preds = %pred.load.continue41
  %34 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %induction37, i32 1
  %35 = load i32, i32* %34, align 4, !tbaa !189
  br label %pred.load.continue43

pred.load.continue43:                             ; preds = %pred.load.if42, %pred.load.continue41
  %36 = phi i32 [ poison, %pred.load.continue41 ], [ %35, %pred.load.if42 ]
  %37 = add nsw i32 %33, -1
  %38 = add nsw i32 %36, -1
  %39 = sext i32 %37 to i64
  %40 = sext i32 %38 to i64
  %41 = mul nsw i64 %39, %29
  %42 = mul nsw i64 %40, %30
  %43 = select i1 %27, i64 %41, i64 0
  %predphi44 = add i64 %vec.phi38, %43
  %44 = select i1 %28, i64 %42, i64 0
  %predphi45 = add i64 %vec.phi39, %44
  %index.next33 = add i64 %index32, 2
  %45 = icmp eq i64 %index.next33, %n.vec31
  br i1 %45, label %middle.block25, label %vector.body27, !llvm.loop !270

middle.block25:                                   ; preds = %pred.load.continue43
  %bin.rdx46 = add i64 %predphi45, %predphi44
  %cmp.n35 = icmp eq i64 %n.vec31, %wide.trip.count.i.i
  br i1 %cmp.n35, label %for.body.i13.i.preheader, label %for.body.i.i.preheader

for.body.i.i.preheader:                           ; preds = %for.body.lr.ph.i.i, %middle.block25
  %indvars.iv.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %n.vec31, %middle.block25 ]
  %index.021.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %bin.rdx46, %middle.block25 ]
  br label %for.body.i.i

for.body.i.i:                                     ; preds = %for.body.i.i.preheader, %if.end.i.i
  %indvars.iv.i.i = phi i64 [ %indvars.iv.next.i.i, %if.end.i.i ], [ %indvars.iv.i.i.ph, %for.body.i.i.preheader ]
  %index.021.i.i = phi i64 [ %index.1.i.i, %if.end.i.i ], [ %index.021.i.i.ph, %for.body.i.i.preheader ]
  %stride2.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %indvars.iv.i.i, i32 2
  %46 = load i32, i32* %stride2.i.i, align 4, !tbaa !185
  %cmp3.i.i = icmp sgt i32 %46, 0
  br i1 %cmp3.i.i, label %if.then.i.i, label %if.end.i.i

if.then.i.i:                                      ; preds = %for.body.i.i
  %conv18.i.i = zext i32 %46 to i64
  %extent.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %indvars.iv.i.i, i32 1
  %47 = load i32, i32* %extent.i.i, align 4, !tbaa !189
  %sub.i.i = add nsw i32 %47, -1
  %conv7.i.i = sext i32 %sub.i.i to i64
  %mul.i.i = mul nsw i64 %conv7.i.i, %conv18.i.i
  %add.i.i = add nsw i64 %mul.i.i, %index.021.i.i
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %for.body.i.i
  %index.1.i.i = phi i64 [ %add.i.i, %if.then.i.i ], [ %index.021.i.i, %for.body.i.i ]
  %indvars.iv.next.i.i = add nuw nsw i64 %indvars.iv.i.i, 1
  %exitcond.not.i.i = icmp eq i64 %indvars.iv.next.i.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i.i, label %for.body.i13.i.preheader, label %for.body.i.i, !llvm.loop !271

for.body.i13.i.preheader:                         ; preds = %if.end.i.i, %middle.block25
  %index.1.i.i.lcssa = phi i64 [ %bin.rdx46, %middle.block25 ], [ %index.1.i.i, %if.end.i.i ]
  %min.iters.check = icmp ult i32 %21, 3
  br i1 %min.iters.check, label %for.body.i13.i.preheader48, label %vector.ph

vector.ph:                                        ; preds = %for.body.i13.i.preheader
  %n.vec = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body

vector.body:                                      ; preds = %pred.load.continue23, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %pred.load.continue23 ]
  %vec.phi = phi i64 [ 0, %vector.ph ], [ %predphi, %pred.load.continue23 ]
  %vec.phi21 = phi i64 [ 0, %vector.ph ], [ %predphi24, %pred.load.continue23 ]
  %induction20 = or i64 %index, 1
  %48 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %index, i32 2
  %49 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %induction20, i32 2
  %50 = load i32, i32* %48, align 4, !tbaa !185
  %51 = load i32, i32* %49, align 4, !tbaa !185
  %52 = icmp slt i32 %50, 0
  %53 = icmp slt i32 %51, 0
  %54 = sext i32 %50 to i64
  %55 = sext i32 %51 to i64
  br i1 %52, label %pred.load.if, label %pred.load.continue

pred.load.if:                                     ; preds = %vector.body
  %56 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %index, i32 1
  %57 = load i32, i32* %56, align 4, !tbaa !189
  br label %pred.load.continue

pred.load.continue:                               ; preds = %pred.load.if, %vector.body
  %58 = phi i32 [ poison, %vector.body ], [ %57, %pred.load.if ]
  br i1 %53, label %pred.load.if22, label %pred.load.continue23

pred.load.if22:                                   ; preds = %pred.load.continue
  %59 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %induction20, i32 1
  %60 = load i32, i32* %59, align 4, !tbaa !189
  br label %pred.load.continue23

pred.load.continue23:                             ; preds = %pred.load.if22, %pred.load.continue
  %61 = phi i32 [ poison, %pred.load.continue ], [ %60, %pred.load.if22 ]
  %62 = add nsw i32 %58, -1
  %63 = add nsw i32 %61, -1
  %64 = sext i32 %62 to i64
  %65 = sext i32 %63 to i64
  %66 = mul nsw i64 %64, %54
  %67 = mul nsw i64 %65, %55
  %68 = select i1 %52, i64 %66, i64 0
  %predphi = add i64 %vec.phi, %68
  %69 = select i1 %53, i64 %67, i64 0
  %predphi24 = add i64 %vec.phi21, %69
  %index.next = add i64 %index, 2
  %70 = icmp eq i64 %index.next, %n.vec
  br i1 %70, label %middle.block, label %vector.body, !llvm.loop !272

middle.block:                                     ; preds = %pred.load.continue23
  %bin.rdx = add i64 %predphi24, %predphi
  %cmp.n = icmp eq i64 %n.vec, %wide.trip.count.i.i
  br i1 %cmp.n, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i.preheader48

for.body.i13.i.preheader48:                       ; preds = %for.body.i13.i.preheader, %middle.block
  %indvars.iv.i10.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %n.vec, %middle.block ]
  %index.018.i.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %bin.rdx, %middle.block ]
  br label %for.body.i13.i

for.body.i13.i:                                   ; preds = %for.body.i13.i.preheader48, %if.end.i24.i
  %indvars.iv.i10.i = phi i64 [ %indvars.iv.next.i22.i, %if.end.i24.i ], [ %indvars.iv.i10.i.ph, %for.body.i13.i.preheader48 ]
  %index.018.i.i = phi i64 [ %index.1.i21.i, %if.end.i24.i ], [ %index.018.i.i.ph, %for.body.i13.i.preheader48 ]
  %stride2.i11.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %indvars.iv.i10.i, i32 2
  %71 = load i32, i32* %stride2.i11.i, align 4, !tbaa !185
  %cmp3.i12.i = icmp slt i32 %71, 0
  br i1 %cmp3.i12.i, label %if.then.i20.i, label %if.end.i24.i

if.then.i20.i:                                    ; preds = %for.body.i13.i
  %conv.i14.i = sext i32 %71 to i64
  %extent.i15.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %22, i64 %indvars.iv.i10.i, i32 1
  %72 = load i32, i32* %extent.i15.i, align 4, !tbaa !189
  %sub.i16.i = add nsw i32 %72, -1
  %conv7.i17.i = sext i32 %sub.i16.i to i64
  %mul.i18.i = mul nsw i64 %conv7.i17.i, %conv.i14.i
  %add.i19.i = add nsw i64 %mul.i18.i, %index.018.i.i
  br label %if.end.i24.i

if.end.i24.i:                                     ; preds = %if.then.i20.i, %for.body.i13.i
  %index.1.i21.i = phi i64 [ %add.i19.i, %if.then.i20.i ], [ %index.018.i.i, %for.body.i13.i ]
  %indvars.iv.next.i22.i = add nuw nsw i64 %indvars.iv.i10.i, 1
  %exitcond.not.i23.i = icmp eq i64 %indvars.iv.next.i22.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i23.i, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i, !llvm.loop !273

_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit: ; preds = %if.end.i24.i, %middle.block
  %index.1.i21.i.lcssa = phi i64 [ %bin.rdx, %middle.block ], [ %index.1.i21.i, %if.end.i24.i ]
  %add8.i.i.neg = xor i64 %index.1.i.i.lcssa, -1
  %sub.i.neg.1 = add i64 %index.1.i21.i.lcssa, %add8.i.i.neg
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

_ZNK15halide_buffer_t13size_in_bytesEv.exit:      ; preds = %for.body._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
  %sub.i.neg.phi = phi i64 [ %sub.i.neg.0, %for.body._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %sub.i.neg.1, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %bits.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %19, i64 %indvars.iv, i32 4, i32 1
  %73 = load i8, i8* %bits.i.i, align 1, !tbaa !144
  %conv.i.i = zext i8 %73 to i64
  %add.i4.i = add nuw nsw i64 %conv.i.i, 7
  %div.i.i = lshr i64 %add.i4.i, 3
  %mul.i.neg = mul i64 %div.i.i, %sub.i.neg.phi
  %sub = add i64 %mul.i.neg, %sub89
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %for.cond.for.cond.cleanup_crit_edge, label %for.body, !llvm.loop !274

if.end41:                                         ; preds = %for.cond.cleanup, %while.body
  %74 = phi i64 [ %.pre93, %for.cond.cleanup ], [ %4, %while.body ]
  %75 = phi i64 [ %.pre92, %for.cond.cleanup ], [ %5, %while.body ]
  %cmp = icmp sgt i64 %75, %74
  %cmp1 = icmp ne %"struct.Halide::Runtime::Internal::CacheEntry"* %6, null
  %76 = and i1 %cmp1, %cmp
  br i1 %76, label %while.body, label %while.end42, !llvm.loop !275

while.end42:                                      ; preds = %if.end41, %entry
  ret void
}

; Function Attrs: nounwind
define weak void @halide_memoization_cache_set_size(i64 %size) local_unnamed_addr #4 {
entry:
  %cmp = icmp eq i64 %size, 0
  %spec.store.select = select i1 %cmp, i64 1048576, i64 %size
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  store i64 %spec.store.select, i64* @_ZN6Halide7Runtime8Internal14max_cache_sizeE, align 8, !tbaa !22
  tail call void @_ZN6Halide7Runtime8Internal11prune_cacheEv() #16
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_memoization_cache_lookup(i8* %user_context, i8* %cache_key, i32 %size, %struct.halide_buffer_t* %computed_bounds, i32 %tuple_count, %struct.halide_buffer_t** %tuple_buffers) local_unnamed_addr #4 {
entry:
  %conv = sext i32 %size to i64
  %call = tail call i32 @_ZN6Halide7Runtime8Internal8djb_hashEPKhm(i8* %cache_key, i64 %conv) #16
  %0 = and i32 %call, 255
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  %idxprom = zext i32 %0 to i64
  %arrayidx = getelementptr inbounds [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*], [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*]* @_ZN6Halide7Runtime8Internal13cache_entriesE, i64 0, i64 %idxprom
  %entry3.0220 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %arrayidx, align 8, !tbaa !14
  %cmp.not221 = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0220, null
  br i1 %cmp.not221, label %for.cond75.preheader, label %while.body.lr.ph

while.body.lr.ph:                                 ; preds = %entry
  %cmp16215 = icmp sgt i32 %tuple_count, 0
  %1 = sext i32 %tuple_count to i64
  br i1 %cmp16215, label %while.body.us, label %while.body

while.body.us:                                    ; preds = %while.body.lr.ph, %if.end73.us
  %entry3.0222.us = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %entry3.0.us, %if.end73.us ], [ %entry3.0220, %while.body.lr.ph ]
  %hash.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222.us, i64 0, i32 6
  %2 = load i32, i32* %hash.us, align 8, !tbaa !254
  %cmp4.us = icmp eq i32 %2, %call
  br i1 %cmp4.us, label %land.lhs.true.us, label %if.end73.us

land.lhs.true.us:                                 ; preds = %while.body.us
  %key_size.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222.us, i64 0, i32 4
  %3 = load i64, i64* %key_size.us, align 8, !tbaa !253
  %cmp6.us = icmp eq i64 %3, %conv
  br i1 %cmp6.us, label %land.lhs.true7.us, label %if.end73.us

land.lhs.true7.us:                                ; preds = %land.lhs.true.us
  %key.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222.us, i64 0, i32 5
  %4 = load i8*, i8** %key.us, align 8, !tbaa !258
  %call9.us = tail call zeroext i1 @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m(i8* %4, i8* %cache_key, i64 %conv) #16
  br i1 %call9.us, label %land.lhs.true10.us, label %if.end73.us

land.lhs.true10.us:                               ; preds = %land.lhs.true7.us
  %computed_bounds11.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222.us, i64 0, i32 10
  %5 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %computed_bounds11.us, align 8, !tbaa !257
  %call12.us = tail call zeroext i1 @_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t(%struct.halide_buffer_t* %computed_bounds, %struct.halide_dimension_t* %5) #16
  br i1 %call12.us, label %land.lhs.true13.us, label %if.end73.us

land.lhs.true13.us:                               ; preds = %land.lhs.true10.us
  %tuple_count14.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222.us, i64 0, i32 8
  %6 = load i32, i32* %tuple_count14.us, align 8, !tbaa !232
  %cmp15.us = icmp eq i32 %6, %tuple_count
  br i1 %cmp15.us, label %for.cond.preheader.us, label %if.end73.us

for.cond.preheader.us:                            ; preds = %land.lhs.true13.us
  %buf.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222.us, i64 0, i32 11
  br label %for.body.us

for.body.us:                                      ; preds = %for.body.us, %for.cond.preheader.us
  %indvars.iv226.us = phi i64 [ 0, %for.cond.preheader.us ], [ %indvars.iv.next227.us, %for.body.us ]
  %arrayidx18.us = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv226.us
  %7 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx18.us, align 8, !tbaa !14
  %8 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf.us, align 8, !tbaa !234
  %dim.us = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %8, i64 %indvars.iv226.us, i32 6
  %9 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.us, align 8, !tbaa !184
  %call21.us = tail call zeroext i1 @_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t(%struct.halide_buffer_t* %7, %struct.halide_dimension_t* %9) #16
  %indvars.iv.next227.us = add nuw nsw i64 %indvars.iv226.us, 1
  %cmp16.us = icmp slt i64 %indvars.iv.next227.us, %1
  %10 = and i1 %cmp16.us, %call21.us
  br i1 %10, label %for.body.us, label %for.cond.cleanup.us, !llvm.loop !276

for.cond.cleanup.us:                              ; preds = %for.body.us
  br i1 %call21.us, label %if.then23, label %if.end73.us

if.end73.us:                                      ; preds = %for.cond.cleanup.us, %land.lhs.true13.us, %land.lhs.true10.us, %land.lhs.true7.us, %land.lhs.true.us, %while.body.us
  %next.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222.us, i64 0, i32 0
  %entry3.0.us = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next.us, align 8, !tbaa !14
  %cmp.not.us = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0.us, null
  br i1 %cmp.not.us, label %for.cond75.preheader, label %while.body.us, !llvm.loop !277

for.cond75.preheader:                             ; preds = %if.end73, %if.end73.us, %entry
  %cmp76210 = icmp sgt i32 %tuple_count, 0
  br i1 %cmp76210, label %for.body78.preheader, label %cleanup119

for.body78.preheader:                             ; preds = %for.cond75.preheader
  %wide.trip.count = zext i32 %tuple_count to i64
  br label %for.body78

while.body:                                       ; preds = %while.body.lr.ph, %if.end73
  %entry3.0222 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %entry3.0, %if.end73 ], [ %entry3.0220, %while.body.lr.ph ]
  %hash = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222, i64 0, i32 6
  %11 = load i32, i32* %hash, align 8, !tbaa !254
  %cmp4 = icmp eq i32 %11, %call
  br i1 %cmp4, label %land.lhs.true, label %if.end73

land.lhs.true:                                    ; preds = %while.body
  %key_size = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222, i64 0, i32 4
  %12 = load i64, i64* %key_size, align 8, !tbaa !253
  %cmp6 = icmp eq i64 %12, %conv
  br i1 %cmp6, label %land.lhs.true7, label %if.end73

land.lhs.true7:                                   ; preds = %land.lhs.true
  %key = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222, i64 0, i32 5
  %13 = load i8*, i8** %key, align 8, !tbaa !258
  %call9 = tail call zeroext i1 @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m(i8* %13, i8* %cache_key, i64 %conv) #16
  br i1 %call9, label %land.lhs.true10, label %if.end73

land.lhs.true10:                                  ; preds = %land.lhs.true7
  %computed_bounds11 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222, i64 0, i32 10
  %14 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %computed_bounds11, align 8, !tbaa !257
  %call12 = tail call zeroext i1 @_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t(%struct.halide_buffer_t* %computed_bounds, %struct.halide_dimension_t* %14) #16
  br i1 %call12, label %land.lhs.true13, label %if.end73

land.lhs.true13:                                  ; preds = %land.lhs.true10
  %tuple_count14 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222, i64 0, i32 8
  %15 = load i32, i32* %tuple_count14, align 8, !tbaa !232
  %cmp15 = icmp eq i32 %15, %tuple_count
  br i1 %cmp15, label %if.then23, label %if.end73

if.then23:                                        ; preds = %land.lhs.true13, %for.cond.cleanup.us
  %.us-phi = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %entry3.0222.us, %for.cond.cleanup.us ], [ %entry3.0222, %land.lhs.true13 ]
  %16 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE, align 8, !tbaa !14
  %cmp24.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %.us-phi, %16
  br i1 %cmp24.not, label %if.end57, label %do.body

do.body:                                          ; preds = %if.then23
  %more_recent = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %.us-phi, i64 0, i32 1
  %17 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent, align 8, !tbaa !268
  %cmp26.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %17, null
  br i1 %cmp26.not, label %if.then27, label %do.end

if.then27:                                        ; preds = %do.body
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([129 x i8], [129 x i8]* @.str.3.43, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %if.then27, %do.body
  %less_recent = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %.us-phi, i64 0, i32 2
  %18 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent, align 8, !tbaa !252
  %cmp28.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %18, null
  br i1 %cmp28.not, label %do.body33, label %if.then29

if.then29:                                        ; preds = %do.end
  %19 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent, align 8, !tbaa !268
  %more_recent32 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %18, i64 0, i32 1
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %19, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent32, align 8, !tbaa !268
  %.pr = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent, align 8, !tbaa !268
  br label %do.body41

do.body33:                                        ; preds = %do.end
  %20 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE, align 8, !tbaa !14
  %cmp34 = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %20, %.us-phi
  br i1 %cmp34, label %do.end38, label %if.then35

if.then35:                                        ; preds = %do.body33
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.4.44, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end38

do.end38:                                         ; preds = %if.then35, %do.body33
  %21 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent, align 8, !tbaa !268
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %21, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE, align 8, !tbaa !14
  br label %do.body41

do.body41:                                        ; preds = %do.end38, %if.then29
  %22 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %.pr, %if.then29 ], [ %21, %do.end38 ]
  %cmp43.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %22, null
  br i1 %cmp43.not, label %if.then44, label %do.end47

if.then44:                                        ; preds = %do.body41
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([129 x i8], [129 x i8]* @.str.5.45, i64 0, i64 0)) #15
  tail call void @abort() #15
  %.pre = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent, align 8, !tbaa !268
  br label %do.end47

do.end47:                                         ; preds = %if.then44, %do.body41
  %23 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %.pre, %if.then44 ], [ %22, %do.body41 ]
  %24 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent, align 8, !tbaa !252
  %less_recent50 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %23, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %24, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent50, align 8, !tbaa !252
  store %"struct.Halide::Runtime::Internal::CacheEntry"* null, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent, align 8, !tbaa !268
  %25 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE, align 8, !tbaa !14
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %25, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent, align 8, !tbaa !252
  %cmp53.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %25, null
  br i1 %cmp53.not, label %if.end56, label %if.then54

if.then54:                                        ; preds = %do.end47
  %more_recent55 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %25, i64 0, i32 1
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %.us-phi, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent55, align 8, !tbaa !268
  br label %if.end56

if.end56:                                         ; preds = %if.then54, %do.end47
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %.us-phi, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE, align 8, !tbaa !14
  br label %if.end57

if.end57:                                         ; preds = %if.end56, %if.then23
  br i1 %cmp16215, label %for.body62.lr.ph, label %cleanup119.loopexit223

for.body62.lr.ph:                                 ; preds = %if.end57
  %buf66 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %.us-phi, i64 0, i32 11
  %wide.trip.count230 = zext i32 %tuple_count to i64
  br label %for.body62

for.body62:                                       ; preds = %for.body62, %for.body62.lr.ph
  %indvars.iv228 = phi i64 [ 0, %for.body62.lr.ph ], [ %indvars.iv.next229, %for.body62 ]
  %arrayidx65 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv228
  %26 = bitcast %struct.halide_buffer_t** %arrayidx65 to i8**
  %27 = load i8*, i8** %26, align 8, !tbaa !14
  %28 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf66, align 8, !tbaa !234
  %arrayidx68 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %28, i64 %indvars.iv228
  %29 = bitcast %struct.halide_buffer_t* %arrayidx68 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 dereferenceable(56) %27, i8* nonnull align 8 dereferenceable(56) %29, i64 56, i1 false), !tbaa.struct !264
  %indvars.iv.next229 = add nuw nsw i64 %indvars.iv228, 1
  %exitcond231.not = icmp eq i64 %indvars.iv.next229, %wide.trip.count230
  br i1 %exitcond231.not, label %cleanup119.loopexit223, label %for.body62, !llvm.loop !278

if.end73:                                         ; preds = %land.lhs.true13, %land.lhs.true10, %land.lhs.true7, %land.lhs.true, %while.body
  %next = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0222, i64 0, i32 0
  %entry3.0 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next, align 8, !tbaa !14
  %cmp.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %entry3.0, null
  br i1 %cmp.not, label %for.cond75.preheader, label %while.body, !llvm.loop !277

for.body78:                                       ; preds = %for.inc114, %for.body78.preheader
  %indvars.iv = phi i64 [ 0, %for.body78.preheader ], [ %indvars.iv.next, %for.inc114 ]
  %arrayidx81 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv
  %30 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx81, align 8, !tbaa !14
  %dimensions.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %30, i64 0, i32 5
  %31 = load i32, i32* %dimensions.i.i, align 4, !tbaa !183
  %cmp19.i.i = icmp sgt i32 %31, 0
  br i1 %cmp19.i.i, label %for.body.lr.ph.i.i, label %for.body78._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge

for.body78._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge: ; preds = %for.body78
  %sub.i.0 = sub nsw i64 1, 0
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

for.body.lr.ph.i.i:                               ; preds = %for.body78
  %dim.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %30, i64 0, i32 6
  %32 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i.i, align 8, !tbaa !184
  %wide.trip.count.i.i = zext i32 %31 to i64
  %min.iters.check38 = icmp ult i32 %31, 3
  br i1 %min.iters.check38, label %for.body.i.i.preheader, label %vector.ph39

vector.ph39:                                      ; preds = %for.body.lr.ph.i.i
  %n.vec41 = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body37

vector.body37:                                    ; preds = %pred.load.continue53, %vector.ph39
  %index42 = phi i64 [ 0, %vector.ph39 ], [ %index.next43, %pred.load.continue53 ]
  %vec.phi48 = phi i64 [ 0, %vector.ph39 ], [ %predphi54, %pred.load.continue53 ]
  %vec.phi49 = phi i64 [ 0, %vector.ph39 ], [ %predphi55, %pred.load.continue53 ]
  %induction47 = or i64 %index42, 1
  %33 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %index42, i32 2
  %34 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %induction47, i32 2
  %35 = load i32, i32* %33, align 4, !tbaa !185
  %36 = load i32, i32* %34, align 4, !tbaa !185
  %37 = icmp sgt i32 %35, 0
  %38 = icmp sgt i32 %36, 0
  %39 = zext i32 %35 to i64
  %40 = zext i32 %36 to i64
  br i1 %37, label %pred.load.if50, label %pred.load.continue51

pred.load.if50:                                   ; preds = %vector.body37
  %41 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %index42, i32 1
  %42 = load i32, i32* %41, align 4, !tbaa !189
  br label %pred.load.continue51

pred.load.continue51:                             ; preds = %pred.load.if50, %vector.body37
  %43 = phi i32 [ poison, %vector.body37 ], [ %42, %pred.load.if50 ]
  br i1 %38, label %pred.load.if52, label %pred.load.continue53

pred.load.if52:                                   ; preds = %pred.load.continue51
  %44 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %induction47, i32 1
  %45 = load i32, i32* %44, align 4, !tbaa !189
  br label %pred.load.continue53

pred.load.continue53:                             ; preds = %pred.load.if52, %pred.load.continue51
  %46 = phi i32 [ poison, %pred.load.continue51 ], [ %45, %pred.load.if52 ]
  %47 = add nsw i32 %43, -1
  %48 = add nsw i32 %46, -1
  %49 = sext i32 %47 to i64
  %50 = sext i32 %48 to i64
  %51 = mul nsw i64 %49, %39
  %52 = mul nsw i64 %50, %40
  %53 = select i1 %37, i64 %51, i64 0
  %predphi54 = add i64 %vec.phi48, %53
  %54 = select i1 %38, i64 %52, i64 0
  %predphi55 = add i64 %vec.phi49, %54
  %index.next43 = add i64 %index42, 2
  %55 = icmp eq i64 %index.next43, %n.vec41
  br i1 %55, label %middle.block35, label %vector.body37, !llvm.loop !279

middle.block35:                                   ; preds = %pred.load.continue53
  %bin.rdx56 = add i64 %predphi55, %predphi54
  %cmp.n45 = icmp eq i64 %n.vec41, %wide.trip.count.i.i
  br i1 %cmp.n45, label %for.body.i13.i.preheader, label %for.body.i.i.preheader

for.body.i.i.preheader:                           ; preds = %for.body.lr.ph.i.i, %middle.block35
  %indvars.iv.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %n.vec41, %middle.block35 ]
  %index.021.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %bin.rdx56, %middle.block35 ]
  br label %for.body.i.i

for.body.i.i:                                     ; preds = %for.body.i.i.preheader, %if.end.i.i
  %indvars.iv.i.i = phi i64 [ %indvars.iv.next.i.i, %if.end.i.i ], [ %indvars.iv.i.i.ph, %for.body.i.i.preheader ]
  %index.021.i.i = phi i64 [ %index.1.i.i, %if.end.i.i ], [ %index.021.i.i.ph, %for.body.i.i.preheader ]
  %stride2.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %indvars.iv.i.i, i32 2
  %56 = load i32, i32* %stride2.i.i, align 4, !tbaa !185
  %cmp3.i.i = icmp sgt i32 %56, 0
  br i1 %cmp3.i.i, label %if.then.i.i, label %if.end.i.i

if.then.i.i:                                      ; preds = %for.body.i.i
  %conv18.i.i = zext i32 %56 to i64
  %extent.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %indvars.iv.i.i, i32 1
  %57 = load i32, i32* %extent.i.i, align 4, !tbaa !189
  %sub.i.i = add nsw i32 %57, -1
  %conv7.i.i = sext i32 %sub.i.i to i64
  %mul.i.i = mul nsw i64 %conv7.i.i, %conv18.i.i
  %add.i.i = add nsw i64 %mul.i.i, %index.021.i.i
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %for.body.i.i
  %index.1.i.i = phi i64 [ %add.i.i, %if.then.i.i ], [ %index.021.i.i, %for.body.i.i ]
  %indvars.iv.next.i.i = add nuw nsw i64 %indvars.iv.i.i, 1
  %exitcond.not.i.i = icmp eq i64 %indvars.iv.next.i.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i.i, label %for.body.i13.i.preheader, label %for.body.i.i, !llvm.loop !280

for.body.i13.i.preheader:                         ; preds = %if.end.i.i, %middle.block35
  %index.1.i.i.lcssa = phi i64 [ %bin.rdx56, %middle.block35 ], [ %index.1.i.i, %if.end.i.i ]
  %min.iters.check = icmp ult i32 %31, 3
  br i1 %min.iters.check, label %for.body.i13.i.preheader58, label %vector.ph

vector.ph:                                        ; preds = %for.body.i13.i.preheader
  %n.vec = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body

vector.body:                                      ; preds = %pred.load.continue33, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %pred.load.continue33 ]
  %vec.phi = phi i64 [ 0, %vector.ph ], [ %predphi, %pred.load.continue33 ]
  %vec.phi31 = phi i64 [ 0, %vector.ph ], [ %predphi34, %pred.load.continue33 ]
  %induction30 = or i64 %index, 1
  %58 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %index, i32 2
  %59 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %induction30, i32 2
  %60 = load i32, i32* %58, align 4, !tbaa !185
  %61 = load i32, i32* %59, align 4, !tbaa !185
  %62 = icmp slt i32 %60, 0
  %63 = icmp slt i32 %61, 0
  %64 = sext i32 %60 to i64
  %65 = sext i32 %61 to i64
  br i1 %62, label %pred.load.if, label %pred.load.continue

pred.load.if:                                     ; preds = %vector.body
  %66 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %index, i32 1
  %67 = load i32, i32* %66, align 4, !tbaa !189
  br label %pred.load.continue

pred.load.continue:                               ; preds = %pred.load.if, %vector.body
  %68 = phi i32 [ poison, %vector.body ], [ %67, %pred.load.if ]
  br i1 %63, label %pred.load.if32, label %pred.load.continue33

pred.load.if32:                                   ; preds = %pred.load.continue
  %69 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %induction30, i32 1
  %70 = load i32, i32* %69, align 4, !tbaa !189
  br label %pred.load.continue33

pred.load.continue33:                             ; preds = %pred.load.if32, %pred.load.continue
  %71 = phi i32 [ poison, %pred.load.continue ], [ %70, %pred.load.if32 ]
  %72 = add nsw i32 %68, -1
  %73 = add nsw i32 %71, -1
  %74 = sext i32 %72 to i64
  %75 = sext i32 %73 to i64
  %76 = mul nsw i64 %74, %64
  %77 = mul nsw i64 %75, %65
  %78 = select i1 %62, i64 %76, i64 0
  %predphi = add i64 %vec.phi, %78
  %79 = select i1 %63, i64 %77, i64 0
  %predphi34 = add i64 %vec.phi31, %79
  %index.next = add i64 %index, 2
  %80 = icmp eq i64 %index.next, %n.vec
  br i1 %80, label %middle.block, label %vector.body, !llvm.loop !281

middle.block:                                     ; preds = %pred.load.continue33
  %bin.rdx = add i64 %predphi34, %predphi
  %cmp.n = icmp eq i64 %n.vec, %wide.trip.count.i.i
  br i1 %cmp.n, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i.preheader58

for.body.i13.i.preheader58:                       ; preds = %for.body.i13.i.preheader, %middle.block
  %indvars.iv.i10.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %n.vec, %middle.block ]
  %index.018.i.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %bin.rdx, %middle.block ]
  br label %for.body.i13.i

for.body.i13.i:                                   ; preds = %for.body.i13.i.preheader58, %if.end.i24.i
  %indvars.iv.i10.i = phi i64 [ %indvars.iv.next.i22.i, %if.end.i24.i ], [ %indvars.iv.i10.i.ph, %for.body.i13.i.preheader58 ]
  %index.018.i.i = phi i64 [ %index.1.i21.i, %if.end.i24.i ], [ %index.018.i.i.ph, %for.body.i13.i.preheader58 ]
  %stride2.i11.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %indvars.iv.i10.i, i32 2
  %81 = load i32, i32* %stride2.i11.i, align 4, !tbaa !185
  %cmp3.i12.i = icmp slt i32 %81, 0
  br i1 %cmp3.i12.i, label %if.then.i20.i, label %if.end.i24.i

if.then.i20.i:                                    ; preds = %for.body.i13.i
  %conv.i14.i = sext i32 %81 to i64
  %extent.i15.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %32, i64 %indvars.iv.i10.i, i32 1
  %82 = load i32, i32* %extent.i15.i, align 4, !tbaa !189
  %sub.i16.i = add nsw i32 %82, -1
  %conv7.i17.i = sext i32 %sub.i16.i to i64
  %mul.i18.i = mul nsw i64 %conv7.i17.i, %conv.i14.i
  %add.i19.i = add nsw i64 %mul.i18.i, %index.018.i.i
  br label %if.end.i24.i

if.end.i24.i:                                     ; preds = %if.then.i20.i, %for.body.i13.i
  %index.1.i21.i = phi i64 [ %add.i19.i, %if.then.i20.i ], [ %index.018.i.i, %for.body.i13.i ]
  %indvars.iv.next.i22.i = add nuw nsw i64 %indvars.iv.i10.i, 1
  %exitcond.not.i23.i = icmp eq i64 %indvars.iv.next.i22.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i23.i, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i, !llvm.loop !282

_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit: ; preds = %if.end.i24.i, %middle.block
  %index.1.i21.i.lcssa = phi i64 [ %bin.rdx, %middle.block ], [ %index.1.i21.i, %if.end.i24.i ]
  %add8.i.i = add nsw i64 %index.1.i.i.lcssa, 1
  %sub.i.1 = sub nsw i64 %add8.i.i, %index.1.i21.i.lcssa
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

_ZNK15halide_buffer_t13size_in_bytesEv.exit:      ; preds = %for.body78._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
  %add8.i27.i = phi i64 [ 1, %for.body78._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %add8.i.i, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %sub.i.phi = phi i64 [ %sub.i.0, %for.body78._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %sub.i.1, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %bits.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %30, i64 0, i32 4, i32 1
  %83 = load i8, i8* %bits.i.i, align 1, !tbaa !144
  %conv.i.i = zext i8 %83 to i64
  %add.i4.i = add nuw nsw i64 %conv.i.i, 7
  %div.i.i = lshr i64 %add.i4.i, 3
  %mul.i = mul i64 %div.i.i, %sub.i.phi
  %add84 = add i64 %mul.i, 32
  %call85 = tail call i8* @halide_malloc(i8* %user_context, i64 %add84) #15
  %host = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %30, i64 0, i32 2
  store i8* %call85, i8** %host, align 8, !tbaa !180
  %cmp87 = icmp eq i8* %call85, null
  br i1 %cmp87, label %for.cond89.preheader, label %for.inc114

for.cond89.preheader:                             ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit
  %cmp90208.not = icmp eq i64 %indvars.iv, 0
  br i1 %cmp90208.not, label %cleanup119, label %for.body92

for.body92:                                       ; preds = %for.cond89.preheader, %for.body92
  %indvars.iv224 = phi i64 [ %indvars.iv.next225, %for.body92 ], [ %indvars.iv, %for.cond89.preheader ]
  %sub = add nuw nsw i64 %indvars.iv224, 4294967295
  %84 = and i64 %sub, 4294967295
  %arrayidx94 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %84
  %85 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx94, align 8, !tbaa !14
  %host95 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %85, i64 0, i32 2
  %86 = load i8*, i8** %host95, align 8, !tbaa !180
  %call96 = tail call %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* %86) #16
  %87 = bitcast %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call96 to i8*
  tail call void @halide_free(i8* %user_context, i8* %87) #15
  %88 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx94, align 8, !tbaa !14
  %host100 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %88, i64 0, i32 2
  store i8* null, i8** %host100, align 8, !tbaa !180
  %89 = icmp sgt i64 %indvars.iv224, 1
  %indvars.iv.next225 = add nsw i64 %indvars.iv224, -1
  br i1 %89, label %for.body92, label %cleanup119, !llvm.loop !283

for.inc114:                                       ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit
  %add.ptr = getelementptr inbounds i8, i8* %call85, i64 32
  store i8* %add.ptr, i8** %host, align 8, !tbaa !180
  %call108 = tail call %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* nonnull %add.ptr) #16
  %hash109 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheBlockHeader", %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call108, i64 0, i32 1
  store i32 %call, i32* %hash109, align 8, !tbaa !284
  %entry110 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheBlockHeader", %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call108, i64 0, i32 0
  store %"struct.Halide::Runtime::Internal::CacheEntry"* null, %"struct.Halide::Runtime::Internal::CacheEntry"** %entry110, align 8, !tbaa !286
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %cleanup119, label %for.body78, !llvm.loop !287

cleanup119.loopexit223:                           ; preds = %for.body62, %if.end57
  %in_use_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %.us-phi, i64 0, i32 7
  %90 = load i32, i32* %in_use_count, align 4, !tbaa !255
  %add = add i32 %90, %tuple_count
  store i32 %add, i32* %in_use_count, align 4, !tbaa !255
  br label %cleanup119

cleanup119:                                       ; preds = %for.inc114, %for.body92, %cleanup119.loopexit223, %for.cond89.preheader, %for.cond75.preheader
  %retval.6 = phi i32 [ 1, %for.cond75.preheader ], [ 0, %cleanup119.loopexit223 ], [ -1, %for.cond89.preheader ], [ -1, %for.body92 ], [ 1, %for.inc114 ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  ret i32 %retval.6
}

; Function Attrs: nounwind
define weak i32 @halide_memoization_cache_store(i8* %user_context, i8* %cache_key, i32 %size, %struct.halide_buffer_t* %computed_bounds, i32 %tuple_count, %struct.halide_buffer_t** %tuple_buffers, i1 zeroext %has_eviction_key, i64 %eviction_key) local_unnamed_addr #4 {
entry:
  %0 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, align 8, !tbaa !14
  %host = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %0, i64 0, i32 2
  %1 = load i8*, i8** %host, align 8, !tbaa !180
  %call = tail call %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* %1) #16
  %hash = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheBlockHeader", %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call, i64 0, i32 1
  %2 = load i32, i32* %hash, align 8, !tbaa !284
  %3 = and i32 %2, 255
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  %idxprom = zext i32 %3 to i64
  %arrayidx7 = getelementptr inbounds [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*], [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*]* @_ZN6Halide7Runtime8Internal13cache_entriesE, i64 0, i64 %idxprom
  %entry6.0228 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %arrayidx7, align 8, !tbaa !14
  %cmp.not229 = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0228, null
  br i1 %cmp.not229, label %for.cond61.preheader, label %while.body.lr.ph

while.body.lr.ph:                                 ; preds = %entry
  %conv10 = sext i32 %size to i64
  %cmp22221 = icmp sgt i32 %tuple_count, 0
  br i1 %cmp22221, label %while.body.us.preheader, label %while.body

while.body.us.preheader:                          ; preds = %while.body.lr.ph
  %4 = zext i32 %tuple_count to i64
  br label %while.body.us

while.body.us:                                    ; preds = %if.end59.us, %while.body.us.preheader
  %entry6.0230.us = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %entry6.0.us, %if.end59.us ], [ %entry6.0228, %while.body.us.preheader ]
  %hash8.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230.us, i64 0, i32 6
  %5 = load i32, i32* %hash8.us, align 8, !tbaa !254
  %cmp9.us = icmp eq i32 %5, %2
  br i1 %cmp9.us, label %land.lhs.true.us, label %if.end59.us

land.lhs.true.us:                                 ; preds = %while.body.us
  %key_size.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230.us, i64 0, i32 4
  %6 = load i64, i64* %key_size.us, align 8, !tbaa !253
  %cmp11.us = icmp eq i64 %6, %conv10
  br i1 %cmp11.us, label %land.lhs.true12.us, label %if.end59.us

land.lhs.true12.us:                               ; preds = %land.lhs.true.us
  %key.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230.us, i64 0, i32 5
  %7 = load i8*, i8** %key.us, align 8, !tbaa !258
  %call14.us = tail call zeroext i1 @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m(i8* %7, i8* %cache_key, i64 %conv10) #16
  br i1 %call14.us, label %land.lhs.true15.us, label %if.end59.us

land.lhs.true15.us:                               ; preds = %land.lhs.true12.us
  %computed_bounds16.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230.us, i64 0, i32 10
  %8 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %computed_bounds16.us, align 8, !tbaa !257
  %call17.us = tail call zeroext i1 @_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t(%struct.halide_buffer_t* %computed_bounds, %struct.halide_dimension_t* %8) #16
  br i1 %call17.us, label %land.lhs.true18.us, label %if.end59.us

land.lhs.true18.us:                               ; preds = %land.lhs.true15.us
  %tuple_count19.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230.us, i64 0, i32 8
  %9 = load i32, i32* %tuple_count19.us, align 8, !tbaa !232
  %cmp20.us = icmp eq i32 %9, %tuple_count
  br i1 %cmp20.us, label %for.body.lr.ph.us, label %if.end59.us

for.cond.cleanup.us:                              ; preds = %for.body.us
  br i1 %call30.us, label %do.body.us, label %if.end59.us

do.body.us:                                       ; preds = %for.cond.cleanup.us
  %10 = and i8 %spec.select.us, 1
  %tobool41.not.us = icmp eq i8 %10, 0
  br i1 %tobool41.not.us, label %if.then42.us, label %for.body48.us.preheader

for.body48.us.preheader:                          ; preds = %if.then42.us, %do.body.us
  %indvars.iv.next245.1 = add nuw nsw i64 0, 1
  %arrayidx50.us.1 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 0
  br label %for.body48.us

if.then42.us:                                     ; preds = %do.body.us
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([122 x i8], [122 x i8]* @.str.9.46, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %for.body48.us.preheader

if.end59.us:                                      ; preds = %for.cond.cleanup.us, %land.lhs.true18.us, %land.lhs.true15.us, %land.lhs.true12.us, %land.lhs.true.us, %while.body.us
  %next.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230.us, i64 0, i32 0
  %entry6.0.us = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next.us, align 8, !tbaa !14
  %cmp.not.us = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0.us, null
  br i1 %cmp.not.us, label %for.cond61.preheader, label %while.body.us, !llvm.loop !288

for.body48.us:                                    ; preds = %for.body48.us.for.body48.us_crit_edge, %for.body48.us.preheader
  %arrayidx50.us.phi = phi %struct.halide_buffer_t** [ %arrayidx50.us.0, %for.body48.us.for.body48.us_crit_edge ], [ %arrayidx50.us.1, %for.body48.us.preheader ]
  %indvars.iv.next245.phi = phi i64 [ %indvars.iv.next245.0, %for.body48.us.for.body48.us_crit_edge ], [ %indvars.iv.next245.1, %for.body48.us.preheader ]
  %11 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx50.us.phi, align 8, !tbaa !14
  %host51.us = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %11, i64 0, i32 2
  %12 = load i8*, i8** %host51.us, align 8, !tbaa !180
  %call52.us = tail call %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* %12) #16
  %entry53.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheBlockHeader", %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call52.us, i64 0, i32 0
  store %"struct.Halide::Runtime::Internal::CacheEntry"* null, %"struct.Halide::Runtime::Internal::CacheEntry"** %entry53.us, align 8, !tbaa !286
  %exitcond247.not = icmp eq i64 %indvars.iv.next245.phi, %4
  br i1 %exitcond247.not, label %cleanup132, label %for.body48.us.for.body48.us_crit_edge, !llvm.loop !289

for.body48.us.for.body48.us_crit_edge:            ; preds = %for.body48.us
  %indvars.iv.next245.0 = add nuw nsw i64 %indvars.iv.next245.phi, 1
  %arrayidx50.us.0 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv.next245.phi
  br label %for.body48.us

for.body.us:                                      ; preds = %for.body.lr.ph.us, %for.body.us
  %13 = phi %struct.halide_buffer_t* [ %.pre, %for.body.lr.ph.us ], [ %16, %for.body.us ]
  %indvars.iv242 = phi i64 [ 0, %for.body.lr.ph.us ], [ %indvars.iv.next243, %for.body.us ]
  %no_host_pointers_equal.0222.us = phi i8 [ 1, %for.body.lr.ph.us ], [ %spec.select.us, %for.body.us ]
  %arrayidx24.us = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv242
  %14 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx24.us, align 8, !tbaa !14
  %dim.us = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %13, i64 %indvars.iv242, i32 6
  %15 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.us, align 8, !tbaa !184
  %call30.us = tail call zeroext i1 @_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t(%struct.halide_buffer_t* %14, %struct.halide_dimension_t* %15) #16
  %16 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf27.us, align 8, !tbaa !234
  %host35.us = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %16, i64 %indvars.iv242, i32 2
  %17 = load i8*, i8** %host35.us, align 8, !tbaa !180
  %host36.us = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %14, i64 0, i32 2
  %18 = load i8*, i8** %host36.us, align 8, !tbaa !180
  %cmp37.us = icmp eq i8* %17, %18
  %spec.select.us = select i1 %cmp37.us, i8 0, i8 %no_host_pointers_equal.0222.us
  %indvars.iv.next243 = add nuw nsw i64 %indvars.iv242, 1
  %cmp22.us = icmp ult i64 %indvars.iv.next243, %4
  %19 = and i1 %cmp22.us, %call30.us
  br i1 %19, label %for.body.us, label %for.cond.cleanup.us, !llvm.loop !290

for.body.lr.ph.us:                                ; preds = %land.lhs.true18.us
  %buf27.us = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230.us, i64 0, i32 11
  %.pre = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %buf27.us, align 8, !tbaa !234
  br label %for.body.us

for.cond61.preheader:                             ; preds = %if.end59, %if.end59.us, %entry
  %cmp62218 = icmp sgt i32 %tuple_count, 0
  br i1 %cmp62218, label %for.body64.preheader, label %for.cond.cleanup63

for.body64.preheader:                             ; preds = %for.cond61.preheader
  %wide.trip.count240 = zext i32 %tuple_count to i64
  br label %for.body64

while.body:                                       ; preds = %while.body.lr.ph, %if.end59
  %entry6.0230 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %entry6.0, %if.end59 ], [ %entry6.0228, %while.body.lr.ph ]
  %hash8 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230, i64 0, i32 6
  %20 = load i32, i32* %hash8, align 8, !tbaa !254
  %cmp9 = icmp eq i32 %20, %2
  br i1 %cmp9, label %land.lhs.true, label %if.end59

land.lhs.true:                                    ; preds = %while.body
  %key_size = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230, i64 0, i32 4
  %21 = load i64, i64* %key_size, align 8, !tbaa !253
  %cmp11 = icmp eq i64 %21, %conv10
  br i1 %cmp11, label %land.lhs.true12, label %if.end59

land.lhs.true12:                                  ; preds = %land.lhs.true
  %key = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230, i64 0, i32 5
  %22 = load i8*, i8** %key, align 8, !tbaa !258
  %call14 = tail call zeroext i1 @_ZN6Halide7Runtime8Internal10keys_equalEPKhS3_m(i8* %22, i8* %cache_key, i64 %conv10) #16
  br i1 %call14, label %land.lhs.true15, label %if.end59

land.lhs.true15:                                  ; preds = %land.lhs.true12
  %computed_bounds16 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230, i64 0, i32 10
  %23 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %computed_bounds16, align 8, !tbaa !257
  %call17 = tail call zeroext i1 @_ZN6Halide7Runtime8Internal16buffer_has_shapeEPK15halide_buffer_tPK18halide_dimension_t(%struct.halide_buffer_t* %computed_bounds, %struct.halide_dimension_t* %23) #16
  br i1 %call17, label %land.lhs.true18, label %if.end59

land.lhs.true18:                                  ; preds = %land.lhs.true15
  %tuple_count19 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230, i64 0, i32 8
  %24 = load i32, i32* %tuple_count19, align 8, !tbaa !232
  %cmp20 = icmp eq i32 %24, %tuple_count
  br i1 %cmp20, label %cleanup132, label %if.end59

if.end59:                                         ; preds = %land.lhs.true18, %land.lhs.true15, %land.lhs.true12, %land.lhs.true, %while.body
  %next = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0230, i64 0, i32 0
  %entry6.0 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next, align 8, !tbaa !14
  %cmp.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %entry6.0, null
  br i1 %cmp.not, label %for.cond61.preheader, label %while.body, !llvm.loop !288

for.cond.cleanup63:                               ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit, %for.cond61.preheader
  %added_size.0.lcssa = phi i64 [ 0, %for.cond61.preheader ], [ %add, %_ZNK15halide_buffer_t13size_in_bytesEv.exit ]
  %25 = load i64, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  %add73 = add i64 %25, %added_size.0.lcssa
  store i64 %add73, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  tail call void @_ZN6Halide7Runtime8Internal11prune_cacheEv() #16
  %call74 = tail call i8* @halide_malloc(i8* null, i64 96) #15
  %tobool75.not = icmp eq i8* %call74, null
  br i1 %tobool75.not, label %if.then83, label %if.then76

for.body64:                                       ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit, %for.body64.preheader
  %indvars.iv238 = phi i64 [ 0, %for.body64.preheader ], [ %indvars.iv.next239, %_ZNK15halide_buffer_t13size_in_bytesEv.exit ]
  %added_size.0219 = phi i64 [ 0, %for.body64.preheader ], [ %add, %_ZNK15halide_buffer_t13size_in_bytesEv.exit ]
  %arrayidx67 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv238
  %26 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx67, align 8, !tbaa !14
  %dimensions.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %26, i64 0, i32 5
  %27 = load i32, i32* %dimensions.i.i, align 4, !tbaa !183
  %cmp19.i.i = icmp sgt i32 %27, 0
  br i1 %cmp19.i.i, label %for.body.lr.ph.i.i, label %for.body64._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge

for.body64._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge: ; preds = %for.body64
  %sub.i.0 = sub nsw i64 1, 0
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

for.body.lr.ph.i.i:                               ; preds = %for.body64
  %dim.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %26, i64 0, i32 6
  %28 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i.i, align 8, !tbaa !184
  %wide.trip.count.i.i = zext i32 %27 to i64
  %min.iters.check24 = icmp ult i32 %27, 3
  br i1 %min.iters.check24, label %for.body.i.i.preheader, label %vector.ph25

vector.ph25:                                      ; preds = %for.body.lr.ph.i.i
  %n.vec27 = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body23

vector.body23:                                    ; preds = %pred.load.continue39, %vector.ph25
  %index28 = phi i64 [ 0, %vector.ph25 ], [ %index.next29, %pred.load.continue39 ]
  %vec.phi34 = phi i64 [ 0, %vector.ph25 ], [ %predphi40, %pred.load.continue39 ]
  %vec.phi35 = phi i64 [ 0, %vector.ph25 ], [ %predphi41, %pred.load.continue39 ]
  %induction33 = or i64 %index28, 1
  %29 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %index28, i32 2
  %30 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %induction33, i32 2
  %31 = load i32, i32* %29, align 4, !tbaa !185
  %32 = load i32, i32* %30, align 4, !tbaa !185
  %33 = icmp sgt i32 %31, 0
  %34 = icmp sgt i32 %32, 0
  %35 = zext i32 %31 to i64
  %36 = zext i32 %32 to i64
  br i1 %33, label %pred.load.if36, label %pred.load.continue37

pred.load.if36:                                   ; preds = %vector.body23
  %37 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %index28, i32 1
  %38 = load i32, i32* %37, align 4, !tbaa !189
  br label %pred.load.continue37

pred.load.continue37:                             ; preds = %pred.load.if36, %vector.body23
  %39 = phi i32 [ poison, %vector.body23 ], [ %38, %pred.load.if36 ]
  br i1 %34, label %pred.load.if38, label %pred.load.continue39

pred.load.if38:                                   ; preds = %pred.load.continue37
  %40 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %induction33, i32 1
  %41 = load i32, i32* %40, align 4, !tbaa !189
  br label %pred.load.continue39

pred.load.continue39:                             ; preds = %pred.load.if38, %pred.load.continue37
  %42 = phi i32 [ poison, %pred.load.continue37 ], [ %41, %pred.load.if38 ]
  %43 = add nsw i32 %39, -1
  %44 = add nsw i32 %42, -1
  %45 = sext i32 %43 to i64
  %46 = sext i32 %44 to i64
  %47 = mul nsw i64 %45, %35
  %48 = mul nsw i64 %46, %36
  %49 = select i1 %33, i64 %47, i64 0
  %predphi40 = add i64 %vec.phi34, %49
  %50 = select i1 %34, i64 %48, i64 0
  %predphi41 = add i64 %vec.phi35, %50
  %index.next29 = add i64 %index28, 2
  %51 = icmp eq i64 %index.next29, %n.vec27
  br i1 %51, label %middle.block21, label %vector.body23, !llvm.loop !291

middle.block21:                                   ; preds = %pred.load.continue39
  %bin.rdx42 = add i64 %predphi41, %predphi40
  %cmp.n31 = icmp eq i64 %n.vec27, %wide.trip.count.i.i
  br i1 %cmp.n31, label %for.body.i13.i.preheader, label %for.body.i.i.preheader

for.body.i.i.preheader:                           ; preds = %for.body.lr.ph.i.i, %middle.block21
  %indvars.iv.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %n.vec27, %middle.block21 ]
  %index.021.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %bin.rdx42, %middle.block21 ]
  br label %for.body.i.i

for.body.i.i:                                     ; preds = %for.body.i.i.preheader, %if.end.i.i
  %indvars.iv.i.i = phi i64 [ %indvars.iv.next.i.i, %if.end.i.i ], [ %indvars.iv.i.i.ph, %for.body.i.i.preheader ]
  %index.021.i.i = phi i64 [ %index.1.i.i, %if.end.i.i ], [ %index.021.i.i.ph, %for.body.i.i.preheader ]
  %stride2.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %indvars.iv.i.i, i32 2
  %52 = load i32, i32* %stride2.i.i, align 4, !tbaa !185
  %cmp3.i.i = icmp sgt i32 %52, 0
  br i1 %cmp3.i.i, label %if.then.i.i, label %if.end.i.i

if.then.i.i:                                      ; preds = %for.body.i.i
  %conv18.i.i = zext i32 %52 to i64
  %extent.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %indvars.iv.i.i, i32 1
  %53 = load i32, i32* %extent.i.i, align 4, !tbaa !189
  %sub.i.i = add nsw i32 %53, -1
  %conv7.i.i = sext i32 %sub.i.i to i64
  %mul.i.i = mul nsw i64 %conv7.i.i, %conv18.i.i
  %add.i.i = add nsw i64 %mul.i.i, %index.021.i.i
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %for.body.i.i
  %index.1.i.i = phi i64 [ %add.i.i, %if.then.i.i ], [ %index.021.i.i, %for.body.i.i ]
  %indvars.iv.next.i.i = add nuw nsw i64 %indvars.iv.i.i, 1
  %exitcond.not.i.i = icmp eq i64 %indvars.iv.next.i.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i.i, label %for.body.i13.i.preheader, label %for.body.i.i, !llvm.loop !292

for.body.i13.i.preheader:                         ; preds = %if.end.i.i, %middle.block21
  %index.1.i.i.lcssa = phi i64 [ %bin.rdx42, %middle.block21 ], [ %index.1.i.i, %if.end.i.i ]
  %min.iters.check = icmp ult i32 %27, 3
  br i1 %min.iters.check, label %for.body.i13.i.preheader44, label %vector.ph

vector.ph:                                        ; preds = %for.body.i13.i.preheader
  %n.vec = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body

vector.body:                                      ; preds = %pred.load.continue19, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %pred.load.continue19 ]
  %vec.phi = phi i64 [ 0, %vector.ph ], [ %predphi, %pred.load.continue19 ]
  %vec.phi17 = phi i64 [ 0, %vector.ph ], [ %predphi20, %pred.load.continue19 ]
  %induction16 = or i64 %index, 1
  %54 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %index, i32 2
  %55 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %induction16, i32 2
  %56 = load i32, i32* %54, align 4, !tbaa !185
  %57 = load i32, i32* %55, align 4, !tbaa !185
  %58 = icmp slt i32 %56, 0
  %59 = icmp slt i32 %57, 0
  %60 = sext i32 %56 to i64
  %61 = sext i32 %57 to i64
  br i1 %58, label %pred.load.if, label %pred.load.continue

pred.load.if:                                     ; preds = %vector.body
  %62 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %index, i32 1
  %63 = load i32, i32* %62, align 4, !tbaa !189
  br label %pred.load.continue

pred.load.continue:                               ; preds = %pred.load.if, %vector.body
  %64 = phi i32 [ poison, %vector.body ], [ %63, %pred.load.if ]
  br i1 %59, label %pred.load.if18, label %pred.load.continue19

pred.load.if18:                                   ; preds = %pred.load.continue
  %65 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %induction16, i32 1
  %66 = load i32, i32* %65, align 4, !tbaa !189
  br label %pred.load.continue19

pred.load.continue19:                             ; preds = %pred.load.if18, %pred.load.continue
  %67 = phi i32 [ poison, %pred.load.continue ], [ %66, %pred.load.if18 ]
  %68 = add nsw i32 %64, -1
  %69 = add nsw i32 %67, -1
  %70 = sext i32 %68 to i64
  %71 = sext i32 %69 to i64
  %72 = mul nsw i64 %70, %60
  %73 = mul nsw i64 %71, %61
  %74 = select i1 %58, i64 %72, i64 0
  %predphi = add i64 %vec.phi, %74
  %75 = select i1 %59, i64 %73, i64 0
  %predphi20 = add i64 %vec.phi17, %75
  %index.next = add i64 %index, 2
  %76 = icmp eq i64 %index.next, %n.vec
  br i1 %76, label %middle.block, label %vector.body, !llvm.loop !293

middle.block:                                     ; preds = %pred.load.continue19
  %bin.rdx = add i64 %predphi20, %predphi
  %cmp.n = icmp eq i64 %n.vec, %wide.trip.count.i.i
  br i1 %cmp.n, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i.preheader44

for.body.i13.i.preheader44:                       ; preds = %for.body.i13.i.preheader, %middle.block
  %indvars.iv.i10.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %n.vec, %middle.block ]
  %index.018.i.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %bin.rdx, %middle.block ]
  br label %for.body.i13.i

for.body.i13.i:                                   ; preds = %for.body.i13.i.preheader44, %if.end.i24.i
  %indvars.iv.i10.i = phi i64 [ %indvars.iv.next.i22.i, %if.end.i24.i ], [ %indvars.iv.i10.i.ph, %for.body.i13.i.preheader44 ]
  %index.018.i.i = phi i64 [ %index.1.i21.i, %if.end.i24.i ], [ %index.018.i.i.ph, %for.body.i13.i.preheader44 ]
  %stride2.i11.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %indvars.iv.i10.i, i32 2
  %77 = load i32, i32* %stride2.i11.i, align 4, !tbaa !185
  %cmp3.i12.i = icmp slt i32 %77, 0
  br i1 %cmp3.i12.i, label %if.then.i20.i, label %if.end.i24.i

if.then.i20.i:                                    ; preds = %for.body.i13.i
  %conv.i14.i = sext i32 %77 to i64
  %extent.i15.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %28, i64 %indvars.iv.i10.i, i32 1
  %78 = load i32, i32* %extent.i15.i, align 4, !tbaa !189
  %sub.i16.i = add nsw i32 %78, -1
  %conv7.i17.i = sext i32 %sub.i16.i to i64
  %mul.i18.i = mul nsw i64 %conv7.i17.i, %conv.i14.i
  %add.i19.i = add nsw i64 %mul.i18.i, %index.018.i.i
  br label %if.end.i24.i

if.end.i24.i:                                     ; preds = %if.then.i20.i, %for.body.i13.i
  %index.1.i21.i = phi i64 [ %add.i19.i, %if.then.i20.i ], [ %index.018.i.i, %for.body.i13.i ]
  %indvars.iv.next.i22.i = add nuw nsw i64 %indvars.iv.i10.i, 1
  %exitcond.not.i23.i = icmp eq i64 %indvars.iv.next.i22.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i23.i, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i, !llvm.loop !294

_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit: ; preds = %if.end.i24.i, %middle.block
  %index.1.i21.i.lcssa = phi i64 [ %bin.rdx, %middle.block ], [ %index.1.i21.i, %if.end.i24.i ]
  %add8.i.i = add nsw i64 %index.1.i.i.lcssa, 1
  %sub.i.1 = sub nsw i64 %add8.i.i, %index.1.i21.i.lcssa
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

_ZNK15halide_buffer_t13size_in_bytesEv.exit:      ; preds = %for.body64._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
  %add8.i27.i = phi i64 [ 1, %for.body64._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %add8.i.i, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %sub.i.phi = phi i64 [ %sub.i.0, %for.body64._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %sub.i.1, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %bits.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %26, i64 0, i32 4, i32 1
  %79 = load i8, i8* %bits.i.i, align 1, !tbaa !144
  %conv.i.i = zext i8 %79 to i64
  %add.i4.i = add nuw nsw i64 %conv.i.i, 7
  %div.i.i = lshr i64 %add.i4.i, 3
  %mul.i = mul i64 %div.i.i, %sub.i.phi
  %add = add i64 %mul.i, %added_size.0219
  %indvars.iv.next239 = add nuw nsw i64 %indvars.iv238, 1
  %exitcond241.not = icmp eq i64 %indvars.iv.next239, %wide.trip.count240
  br i1 %exitcond241.not, label %for.cond.cleanup63, label %for.body64, !llvm.loop !295

if.then76:                                        ; preds = %for.cond.cleanup63
  %80 = bitcast i8* %call74 to %"struct.Halide::Runtime::Internal::CacheEntry"*
  %conv77 = sext i32 %size to i64
  %call79 = tail call zeroext i1 @_ZN6Halide7Runtime8Internal10CacheEntry4initEPKhmjPK15halide_buffer_tiPPS5_by(%"struct.Halide::Runtime::Internal::CacheEntry"* nonnull dereferenceable(96) %80, i8* %cache_key, i64 %conv77, i32 %2, %struct.halide_buffer_t* %computed_bounds, i32 %tuple_count, %struct.halide_buffer_t** nonnull %tuple_buffers, i1 zeroext %has_eviction_key, i64 %eviction_key) #16
  br i1 %call79, label %if.end101, label %if.then83

if.then83:                                        ; preds = %if.then76, %for.cond.cleanup63
  %81 = load i64, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  %sub = sub i64 %81, %added_size.0.lcssa
  store i64 %sub, i64* @_ZN6Halide7Runtime8Internal18current_cache_sizeE, align 8, !tbaa !22
  br i1 %cmp62218, label %for.body88.preheader, label %for.cond.cleanup87

for.body88.preheader:                             ; preds = %if.then83
  %wide.trip.count = zext i32 %tuple_count to i64
  %indvars.iv.next.0 = add nuw nsw i64 0, 1
  %arrayidx90.0 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 0
  br label %for.body88

for.cond.cleanup87:                               ; preds = %for.body88, %if.then83
  br i1 %tobool75.not, label %cleanup132, label %if.then99

for.body88:                                       ; preds = %for.body88.for.body88_crit_edge, %for.body88.preheader
  %arrayidx90.phi = phi %struct.halide_buffer_t** [ %arrayidx90.0, %for.body88.preheader ], [ %arrayidx90.1, %for.body88.for.body88_crit_edge ]
  %indvars.iv.next.phi = phi i64 [ %indvars.iv.next.0, %for.body88.preheader ], [ %indvars.iv.next.1, %for.body88.for.body88_crit_edge ]
  %82 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx90.phi, align 8, !tbaa !14
  %host91 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %82, i64 0, i32 2
  %83 = load i8*, i8** %host91, align 8, !tbaa !180
  %call92 = tail call %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* %83) #16
  %entry93 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheBlockHeader", %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call92, i64 0, i32 0
  store %"struct.Halide::Runtime::Internal::CacheEntry"* null, %"struct.Halide::Runtime::Internal::CacheEntry"** %entry93, align 8, !tbaa !286
  %exitcond.not = icmp eq i64 %indvars.iv.next.phi, %wide.trip.count
  br i1 %exitcond.not, label %for.cond.cleanup87, label %for.body88.for.body88_crit_edge, !llvm.loop !296

for.body88.for.body88_crit_edge:                  ; preds = %for.body88
  %indvars.iv.next.1 = add nuw nsw i64 %indvars.iv.next.phi, 1
  %arrayidx90.1 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv.next.phi
  br label %for.body88

if.then99:                                        ; preds = %for.cond.cleanup87
  tail call void @halide_free(i8* %user_context, i8* nonnull %call74) #15
  br label %cleanup132

if.end101:                                        ; preds = %if.then76
  %84 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %arrayidx7, align 8, !tbaa !14
  %next104 = bitcast i8* %call74 to %"struct.Halide::Runtime::Internal::CacheEntry"**
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %84, %"struct.Halide::Runtime::Internal::CacheEntry"** %next104, align 8, !tbaa !229
  %85 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE, align 8, !tbaa !14
  %less_recent = getelementptr inbounds i8, i8* %call74, i64 16
  %86 = bitcast i8* %less_recent to %"struct.Halide::Runtime::Internal::CacheEntry"**
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %85, %"struct.Halide::Runtime::Internal::CacheEntry"** %86, align 8, !tbaa !252
  %cmp105.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %85, null
  br i1 %cmp105.not, label %if.end107, label %if.then106

if.then106:                                       ; preds = %if.end101
  %more_recent = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %85, i64 0, i32 1
  %87 = bitcast %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent to i8**
  store i8* %call74, i8** %87, align 8, !tbaa !268
  br label %if.end107

if.end107:                                        ; preds = %if.then106, %if.end101
  store i8* %call74, i8** bitcast (%"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE to i8**), align 8, !tbaa !14
  %88 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE, align 8, !tbaa !14
  %cmp108 = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %88, null
  br i1 %cmp108, label %if.then109, label %if.end110

if.then109:                                       ; preds = %if.end107
  store i8* %call74, i8** bitcast (%"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE to i8**), align 8, !tbaa !14
  br label %if.end110

if.end110:                                        ; preds = %if.then109, %if.end107
  %89 = bitcast %"struct.Halide::Runtime::Internal::CacheEntry"** %arrayidx7 to i8**
  store i8* %call74, i8** %89, align 8, !tbaa !14
  %in_use_count = getelementptr inbounds i8, i8* %call74, i64 52
  %90 = bitcast i8* %in_use_count to i32*
  store i32 %tuple_count, i32* %90, align 4, !tbaa !255
  br i1 %cmp62218, label %for.body117.preheader, label %cleanup132

for.body117.preheader:                            ; preds = %if.end110
  %wide.trip.count236 = zext i32 %tuple_count to i64
  %indvars.iv.next235.0 = add nuw nsw i64 0, 1
  %arrayidx119.0 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 0
  br label %for.body117

for.body117:                                      ; preds = %for.body117.for.body117_crit_edge, %for.body117.preheader
  %arrayidx119.phi = phi %struct.halide_buffer_t** [ %arrayidx119.0, %for.body117.preheader ], [ %arrayidx119.1, %for.body117.for.body117_crit_edge ]
  %indvars.iv.next235.phi = phi i64 [ %indvars.iv.next235.0, %for.body117.preheader ], [ %indvars.iv.next235.1, %for.body117.for.body117_crit_edge ]
  %91 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %arrayidx119.phi, align 8, !tbaa !14
  %host120 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %91, i64 0, i32 2
  %92 = load i8*, i8** %host120, align 8, !tbaa !180
  %call121 = tail call %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* %92) #16
  %93 = bitcast %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call121 to i8**
  store i8* %call74, i8** %93, align 8, !tbaa !286
  %exitcond237.not = icmp eq i64 %indvars.iv.next235.phi, %wide.trip.count236
  br i1 %exitcond237.not, label %cleanup132, label %for.body117.for.body117_crit_edge, !llvm.loop !297

for.body117.for.body117_crit_edge:                ; preds = %for.body117
  %indvars.iv.next235.1 = add nuw nsw i64 %indvars.iv.next235.phi, 1
  %arrayidx119.1 = getelementptr inbounds %struct.halide_buffer_t*, %struct.halide_buffer_t** %tuple_buffers, i64 %indvars.iv.next235.phi
  br label %for.body117

cleanup132:                                       ; preds = %land.lhs.true18, %for.body48.us, %for.body117, %if.end110, %if.then99, %for.cond.cleanup87
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  ret i32 0
}

; Function Attrs: nounwind
define weak void @halide_memoization_cache_release(i8* %user_context, i8* %host) local_unnamed_addr #4 {
entry:
  %call = tail call %"struct.Halide::Runtime::Internal::CacheBlockHeader"* @_ZN6Halide7Runtime8Internal21get_pointer_to_headerEPh(i8* %host) #16
  %entry2 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheBlockHeader", %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call, i64 0, i32 0
  %0 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %entry2, align 8, !tbaa !286
  %cmp = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %0, null
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %1 = bitcast %"struct.Halide::Runtime::Internal::CacheBlockHeader"* %call to i8*
  tail call void @halide_free(i8* %user_context, i8* %1) #15
  br label %if.end6

if.else:                                          ; preds = %entry
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  %in_use_count = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %0, i64 0, i32 7
  %2 = load i32, i32* %in_use_count, align 4, !tbaa !255
  %cmp3.not = icmp eq i32 %2, 0
  br i1 %cmp3.not, label %if.then4, label %do.end

if.then4:                                         ; preds = %if.else
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([123 x i8], [123 x i8]* @.str.12.47, i64 0, i64 0)) #15
  tail call void @abort() #15
  %.pre = load i32, i32* %in_use_count, align 4, !tbaa !255
  br label %do.end

do.end:                                           ; preds = %if.then4, %if.else
  %3 = phi i32 [ %.pre, %if.then4 ], [ %2, %if.else ]
  %dec = add i32 %3, -1
  store i32 %dec, i32* %in_use_count, align 4, !tbaa !255
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  br label %if.end6

if.end6:                                          ; preds = %do.end, %if.then
  ret void
}

; Function Attrs: nounwind
define weak void @halide_memoization_cache_evict(i8* %user_context, i64 %eviction_key) local_unnamed_addr #4 {
entry:
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  br label %for.body

for.cond.cleanup:                                 ; preds = %if.end25
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal16memoization_lockE) #15
  ret void

for.body:                                         ; preds = %if.end25, %entry
  %__begin1.059 = phi %"struct.Halide::Runtime::Internal::CacheEntry"** [ getelementptr inbounds ([256 x %"struct.Halide::Runtime::Internal::CacheEntry"*], [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*]* @_ZN6Halide7Runtime8Internal13cache_entriesE, i64 0, i64 0), %entry ], [ %incdec.ptr, %if.end25 ]
  %0 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %__begin1.059, align 8, !tbaa !14
  %cmp2.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %0, null
  br i1 %cmp2.not, label %if.end25, label %while.body

while.body:                                       ; preds = %for.body, %if.end24
  %prev.058 = phi %"struct.Halide::Runtime::Internal::CacheEntry"** [ %prev.1, %if.end24 ], [ %__begin1.059, %for.body ]
  %entry1.056 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %1, %if.end24 ], [ %0, %for.body ]
  %next4 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry1.056, i64 0, i32 0
  %1 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %next4, align 8, !tbaa !229
  %has_eviction_key = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry1.056, i64 0, i32 13
  %2 = load i8, i8* %has_eviction_key, align 8, !tbaa !262, !range !21
  %tobool.not = icmp eq i8 %2, 0
  br i1 %tobool.not, label %if.end24, label %land.lhs.true

land.lhs.true:                                    ; preds = %while.body
  %eviction_key5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry1.056, i64 0, i32 12
  %3 = load i64, i64* %eviction_key5, align 8, !tbaa !263
  %cmp6 = icmp eq i64 %3, %eviction_key
  br i1 %cmp6, label %if.then7, label %if.end24

if.then7:                                         ; preds = %land.lhs.true
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %1, %"struct.Halide::Runtime::Internal::CacheEntry"** %prev.058, align 8, !tbaa !14
  %more_recent = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry1.056, i64 0, i32 1
  %4 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent, align 8, !tbaa !268
  %cmp8.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %4, null
  %less_recent12 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %entry1.056, i64 0, i32 2
  %5 = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent12, align 8, !tbaa !252
  br i1 %cmp8.not, label %if.else, label %if.then9

if.then9:                                         ; preds = %if.then7
  %less_recent11 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %4, i64 0, i32 2
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %5, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent11, align 8, !tbaa !252
  %.pre = load %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %less_recent12, align 8, !tbaa !252
  br label %if.end

if.else:                                          ; preds = %if.then7
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %5, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal18most_recently_usedE, align 8, !tbaa !14
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then9
  %6 = phi %"struct.Halide::Runtime::Internal::CacheEntry"* [ %5, %if.else ], [ %.pre, %if.then9 ]
  %cmp14.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %6, null
  %more_recent18 = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry", %"struct.Halide::Runtime::Internal::CacheEntry"* %6, i64 0, i32 1
  %_ZN6Halide7Runtime8Internal19least_recently_usedE.sink = select i1 %cmp14.not, %"struct.Halide::Runtime::Internal::CacheEntry"** @_ZN6Halide7Runtime8Internal19least_recently_usedE, %"struct.Halide::Runtime::Internal::CacheEntry"** %more_recent18
  store %"struct.Halide::Runtime::Internal::CacheEntry"* %4, %"struct.Halide::Runtime::Internal::CacheEntry"** %_ZN6Halide7Runtime8Internal19least_recently_usedE.sink, align 8, !tbaa !14
  tail call void @_ZN6Halide7Runtime8Internal10CacheEntry7destroyEv(%"struct.Halide::Runtime::Internal::CacheEntry"* nonnull dereferenceable(96) %entry1.056) #16
  %7 = bitcast %"struct.Halide::Runtime::Internal::CacheEntry"* %entry1.056 to i8*
  tail call void @halide_free(i8* %user_context, i8* nonnull %7) #15
  br label %if.end24

if.end24:                                         ; preds = %if.end, %land.lhs.true, %while.body
  %prev.1 = phi %"struct.Halide::Runtime::Internal::CacheEntry"** [ %prev.058, %if.end ], [ %next4, %land.lhs.true ], [ %next4, %while.body ]
  %cmp3.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"* %1, null
  br i1 %cmp3.not, label %if.end25, label %while.body, !llvm.loop !298

if.end25:                                         ; preds = %if.end24, %for.body
  %incdec.ptr = getelementptr inbounds %"struct.Halide::Runtime::Internal::CacheEntry"*, %"struct.Halide::Runtime::Internal::CacheEntry"** %__begin1.059, i64 1
  %cmp.not = icmp eq %"struct.Halide::Runtime::Internal::CacheEntry"** %incdec.ptr, getelementptr inbounds ([256 x %"struct.Halide::Runtime::Internal::CacheEntry"*], [256 x %"struct.Halide::Runtime::Internal::CacheEntry"*]* @_ZN6Halide7Runtime8Internal13cache_entriesE, i64 1, i64 0)
  br i1 %cmp.not, label %for.cond.cleanup, label %for.body
}

; Function Attrs: nounwind
define weak i8* @halide_string_to_string(i8* %dst, i8* %end, i8* %arg) local_unnamed_addr #4 {
entry:
  %cmp.not = icmp ult i8* %dst, %end
  br i1 %cmp.not, label %if.end, label %return

if.end:                                           ; preds = %entry
  %tobool.not = icmp eq i8* %arg, null
  %spec.select = select i1 %tobool.not, i8* getelementptr inbounds ([10 x i8], [10 x i8]* @.str.50, i64 0, i64 0), i8* %arg
  br label %if.end5

if.then4:                                         ; preds = %if.end8
  store i8 0, i8* %dst.addr.023, align 1, !tbaa !18
  br label %return

if.end5:                                          ; preds = %if.end8, %if.end
  %arg.addr.124 = phi i8* [ %spec.select, %if.end ], [ %incdec.ptr9, %if.end8 ]
  %dst.addr.023 = phi i8* [ %dst, %if.end ], [ %incdec.ptr, %if.end8 ]
  %0 = load i8, i8* %arg.addr.124, align 1, !tbaa !18
  store i8 %0, i8* %dst.addr.023, align 1, !tbaa !18
  %cmp6 = icmp eq i8 %0, 0
  br i1 %cmp6, label %return, label %if.end8

if.end8:                                          ; preds = %if.end5
  %incdec.ptr = getelementptr inbounds i8, i8* %dst.addr.023, i64 1
  %incdec.ptr9 = getelementptr inbounds i8, i8* %arg.addr.124, i64 1
  %cmp3 = icmp eq i8* %incdec.ptr, %end
  br i1 %cmp3, label %if.then4, label %if.end5

return:                                           ; preds = %if.end5, %if.then4, %entry
  %retval.0 = phi i8* [ %end, %if.then4 ], [ %dst, %entry ], [ %dst.addr.023, %if.end5 ]
  ret i8* %retval.0
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_uint64_to_string(i8* %dst, i8* %end, i64 %arg, i32 %min_digits) local_unnamed_addr #0 {
entry:
  %buf = alloca [32 x i8], align 1
  %0 = getelementptr inbounds [32 x i8], [32 x i8]* %buf, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %0) #11
  %arrayidx = getelementptr inbounds [32 x i8], [32 x i8]* %buf, i64 0, i64 31
  store i8 0, i8* %arrayidx, align 1, !tbaa !18
  %add.ptr = getelementptr inbounds [32 x i8], [32 x i8]* %buf, i64 0, i64 30
  %cmp13 = icmp sgt i32 %min_digits, 0
  %tobool14 = icmp ne i64 %arg, 0
  %1 = or i1 %tobool14, %cmp13
  br i1 %1, label %entry.for.body_crit_edge, label %for.cond.cleanup

entry.for.body_crit_edge:                         ; preds = %entry
  %inc.1 = add nuw nsw i32 0, 1
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.body, %entry
  %digits.0.lcssa = phi i8* [ %add.ptr, %entry ], [ %incdec.ptr, %for.body ]
  %incdec.ptr1 = getelementptr inbounds i8, i8* %digits.0.lcssa, i64 1
  %call = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* nonnull %incdec.ptr1) #16
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %0) #11
  ret i8* %call

for.body:                                         ; preds = %entry.for.body_crit_edge, %for.body.for.body_crit_edge
  %arg.addr.017 = phi i64 [ %div, %for.body.for.body_crit_edge ], [ %arg, %entry.for.body_crit_edge ]
  %digits.016 = phi i8* [ %incdec.ptr, %for.body.for.body_crit_edge ], [ %add.ptr, %entry.for.body_crit_edge ]
  %inc.phi = phi i32 [ %inc.0, %for.body.for.body_crit_edge ], [ %inc.1, %entry.for.body_crit_edge ]
  %div = udiv i64 %arg.addr.017, 10
  %mul.neg = mul i64 %div, -10
  %sub = add i64 %mul.neg, %arg.addr.017
  %2 = trunc i64 %sub to i8
  %conv = add i8 %2, 48
  store i8 %conv, i8* %digits.016, align 1, !tbaa !18
  %incdec.ptr = getelementptr inbounds i8, i8* %digits.016, i64 -1
  %cmp = icmp slt i32 %inc.phi, %min_digits
  %3 = icmp ugt i64 %arg.addr.017, 9
  %4 = or i1 %3, %cmp
  br i1 %4, label %for.body.for.body_crit_edge, label %for.cond.cleanup, !llvm.loop !299

for.body.for.body_crit_edge:                      ; preds = %for.body
  %inc.0 = add nuw nsw i32 %inc.phi, 1
  br label %for.body
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_int64_to_string(i8* %dst, i8* %end, i64 %arg, i32 %min_digits) local_unnamed_addr #0 {
entry:
  %cmp = icmp slt i64 %arg, 0
  %cmp1 = icmp ult i8* %dst, %end
  %or.cond = and i1 %cmp1, %cmp
  br i1 %or.cond, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  %incdec.ptr = getelementptr inbounds i8, i8* %dst, i64 1
  store i8 45, i8* %dst, align 1, !tbaa !18
  %sub = sub nsw i64 0, %arg
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  %arg.addr.0 = phi i64 [ %sub, %if.then ], [ %arg, %entry ]
  %dst.addr.0 = phi i8* [ %incdec.ptr, %if.then ], [ %dst, %entry ]
  %call = tail call i8* @halide_uint64_to_string(i8* %dst.addr.0, i8* %end, i64 %arg.addr.0, i32 %min_digits) #16
  ret i8* %call
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_double_to_string(i8* %dst, i8* %end, double %arg, i32 %scientific) local_unnamed_addr #0 {
entry:
  %arg.addr = alloca double, align 8
  %bits = alloca i64, align 8
  %buf = alloca [512 x i8], align 1
  store double %arg, double* %arg.addr, align 8, !tbaa !172
  %0 = bitcast i64* %bits to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %0) #11
  store i64 0, i64* %bits, align 8, !tbaa !22
  %1 = bitcast double* %arg.addr to i8*
  %call = call i8* @memcpy(i8* nonnull %0, i8* nonnull %1, i64 8) #15
  %2 = load i64, i64* %bits, align 8, !tbaa !22
  %and = and i64 %2, 4503599627370495
  %shr = lshr i64 %2, 52
  %3 = trunc i64 %shr to i32
  %conv = and i32 %3, 2047
  %cmp = icmp eq i32 %conv, 2047
  br i1 %cmp, label %if.then, label %if.else15

if.then:                                          ; preds = %entry
  %tobool.not = icmp eq i64 %and, 0
  %tobool10.not = icmp sgt i64 %2, -1
  br i1 %tobool.not, label %if.else9, label %if.then4

if.then4:                                         ; preds = %if.then
  br i1 %tobool10.not, label %if.else, label %if.then6

if.then6:                                         ; preds = %if.then4
  %call7 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.1.57, i64 0, i64 0)) #16
  br label %cleanup147

if.else:                                          ; preds = %if.then4
  %call8 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.2.58, i64 0, i64 0)) #16
  br label %cleanup147

if.else9:                                         ; preds = %if.then
  br i1 %tobool10.not, label %if.else13, label %if.then11

if.then11:                                        ; preds = %if.else9
  %call12 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.3.59, i64 0, i64 0)) #16
  br label %cleanup147

if.else13:                                        ; preds = %if.else9
  %call14 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.4.60, i64 0, i64 0)) #16
  br label %cleanup147

if.else15:                                        ; preds = %entry
  %cmp16 = icmp eq i32 %conv, 0
  %cmp17 = icmp eq i64 %and, 0
  %or.cond = and i1 %cmp17, %cmp16
  br i1 %or.cond, label %if.then18, label %if.end32

if.then18:                                        ; preds = %if.else15
  %tobool19.not = icmp eq i32 %scientific, 0
  %tobool27.not = icmp sgt i64 %2, -1
  br i1 %tobool19.not, label %if.else26, label %if.then20

if.then20:                                        ; preds = %if.then18
  br i1 %tobool27.not, label %if.else24, label %if.then22

if.then22:                                        ; preds = %if.then20
  %call23 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([14 x i8], [14 x i8]* @.str.5.61, i64 0, i64 0)) #16
  br label %cleanup147

if.else24:                                        ; preds = %if.then20
  %call25 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([13 x i8], [13 x i8]* @.str.6.62, i64 0, i64 0)) #16
  br label %cleanup147

if.else26:                                        ; preds = %if.then18
  br i1 %tobool27.not, label %if.else30, label %if.then28

if.then28:                                        ; preds = %if.else26
  %call29 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([10 x i8], [10 x i8]* @.str.7.63, i64 0, i64 0)) #16
  br label %cleanup147

if.else30:                                        ; preds = %if.else26
  %call31 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([9 x i8], [9 x i8]* @.str.8.64, i64 0, i64 0)) #16
  br label %cleanup147

if.end32:                                         ; preds = %if.else15
  %tobool33.not = icmp sgt i64 %2, -1
  br i1 %tobool33.not, label %if.end36, label %if.then34

if.then34:                                        ; preds = %if.end32
  %call35 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.9.65, i64 0, i64 0)) #16
  %4 = load double, double* %arg.addr, align 8, !tbaa !172
  %fneg = fneg double %4
  store double %fneg, double* %arg.addr, align 8, !tbaa !172
  br label %if.end36

if.end36:                                         ; preds = %if.then34, %if.end32
  %dst.addr.0 = phi i8* [ %call35, %if.then34 ], [ %dst, %if.end32 ]
  %tobool37.not = icmp eq i32 %scientific, 0
  br i1 %tobool37.not, label %if.else61, label %while.condthread-pre-split

while.condthread-pre-split:                       ; preds = %if.end36
  %.pr = load double, double* %arg.addr, align 8, !tbaa !172
  %cmp39276 = fcmp olt double %.pr, 1.000000e+00
  br i1 %cmp39276, label %while.condthread-pre-split.while.body_crit_edge, label %while.cond40thread-pre-split

while.condthread-pre-split.while.body_crit_edge:  ; preds = %while.condthread-pre-split
  %dec.1 = add nsw i32 0, -1
  br label %while.body

while.body:                                       ; preds = %while.condthread-pre-split.while.body_crit_edge, %while.body.while.body_crit_edge
  %dec.phi = phi i32 [ %dec.0, %while.body.while.body_crit_edge ], [ %dec.1, %while.condthread-pre-split.while.body_crit_edge ]
  %5 = phi double [ %mul, %while.body.while.body_crit_edge ], [ %.pr, %while.condthread-pre-split.while.body_crit_edge ]
  %mul = fmul double %5, 1.000000e+01
  %cmp39 = fcmp olt double %mul, 1.000000e+00
  br i1 %cmp39, label %while.body.while.body_crit_edge, label %while.cond.while.cond40thread-pre-split_crit_edge, !llvm.loop !300

while.body.while.body_crit_edge:                  ; preds = %while.body
  %dec.0 = add nsw i32 %dec.phi, -1
  br label %while.body

while.cond.while.cond40thread-pre-split_crit_edge: ; preds = %while.body
  store double %mul, double* %arg.addr, align 8, !tbaa !172
  br label %while.cond40thread-pre-split

while.cond40thread-pre-split:                     ; preds = %while.cond.while.cond40thread-pre-split_crit_edge, %while.condthread-pre-split
  %.pr261 = phi double [ %mul, %while.cond.while.cond40thread-pre-split_crit_edge ], [ %.pr, %while.condthread-pre-split ]
  %exponent_base_10.0.lcssa = phi i32 [ %dec.phi, %while.cond.while.cond40thread-pre-split_crit_edge ], [ 0, %while.condthread-pre-split ]
  %cmp41272 = fcmp ult double %.pr261, 1.000000e+01
  br i1 %cmp41272, label %while.end43, label %while.body42

while.body42:                                     ; preds = %while.cond40thread-pre-split, %while.body42
  %exponent_base_10.1273 = phi i32 [ %inc, %while.body42 ], [ %exponent_base_10.0.lcssa, %while.cond40thread-pre-split ]
  %6 = phi double [ %div, %while.body42 ], [ %.pr261, %while.cond40thread-pre-split ]
  %div = fdiv double %6, 1.000000e+01
  %inc = add nsw i32 %exponent_base_10.1273, 1
  %cmp41 = fcmp ult double %div, 1.000000e+01
  br i1 %cmp41, label %while.cond40.while.end43_crit_edge, label %while.body42, !llvm.loop !301

while.cond40.while.end43_crit_edge:               ; preds = %while.body42
  store double %div, double* %arg.addr, align 8, !tbaa !172
  br label %while.end43

while.end43:                                      ; preds = %while.cond40.while.end43_crit_edge, %while.cond40thread-pre-split
  %.lcssa = phi double [ %div, %while.cond40.while.end43_crit_edge ], [ %.pr261, %while.cond40thread-pre-split ]
  %exponent_base_10.1.lcssa = phi i32 [ %inc, %while.cond40.while.end43_crit_edge ], [ %exponent_base_10.0.lcssa, %while.cond40thread-pre-split ]
  %mul44 = fmul double %.lcssa, 1.000000e+06
  %add = fadd double %mul44, 5.000000e-01
  %conv45 = fptoui double %add to i64
  %div46 = udiv i64 %conv45, 1000000
  %mul47.neg = mul i64 %div46, -1000000
  %sub48 = add i64 %mul47.neg, %conv45
  %call49 = call i8* @halide_int64_to_string(i8* %dst.addr.0, i8* %end, i64 %div46, i32 1) #16
  %call50 = call i8* @halide_string_to_string(i8* %call49, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.141, i64 0, i64 0)) #16
  %call51 = call i8* @halide_int64_to_string(i8* %call50, i8* %end, i64 %sub48, i32 6) #16
  %cmp52 = icmp sgt i32 %exponent_base_10.1.lcssa, -1
  br i1 %cmp52, label %if.then53, label %if.else55

if.then53:                                        ; preds = %while.end43
  %call54 = call i8* @halide_string_to_string(i8* %call51, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.11.67, i64 0, i64 0)) #16
  br label %if.end58

if.else55:                                        ; preds = %while.end43
  %call56 = call i8* @halide_string_to_string(i8* %call51, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.12.68, i64 0, i64 0)) #16
  %sub57 = sub nsw i32 0, %exponent_base_10.1.lcssa
  br label %if.end58

if.end58:                                         ; preds = %if.else55, %if.then53
  %exponent_base_10.2 = phi i32 [ %exponent_base_10.1.lcssa, %if.then53 ], [ %sub57, %if.else55 ]
  %dst.addr.1 = phi i8* [ %call54, %if.then53 ], [ %call56, %if.else55 ]
  %conv59262 = zext i32 %exponent_base_10.2 to i64
  %call60 = call i8* @halide_int64_to_string(i8* %dst.addr.1, i8* %end, i64 %conv59262, i32 2) #16
  br label %cleanup147

if.else61:                                        ; preds = %if.end36
  br i1 %cmp16, label %if.then63, label %if.end65

if.then63:                                        ; preds = %if.else61
  %call64 = call i8* @halide_double_to_string(i8* %dst.addr.0, i8* %end, double 0.000000e+00, i32 0) #16
  br label %cleanup147

if.end65:                                         ; preds = %if.else61
  %add67 = or i64 %and, 4503599627370496
  %sub69 = add nsw i32 %conv, -1075
  %cmp70 = icmp ult i32 %conv, 1075
  br i1 %cmp70, label %if.then71, label %if.end104

if.then71:                                        ; preds = %if.end65
  %cmp72 = icmp ult i32 %conv, 1023
  %sub76 = sub nuw nsw i32 1075, %conv
  %sh_prom = zext i32 %sub76 to i64
  %shr77 = lshr i64 %add67, %sh_prom
  %shl80 = shl i64 %shr77, %sh_prom
  %integer_part.0 = select i1 %cmp72, i64 0, i64 %shr77
  %sub81 = select i1 %cmp72, i64 0, i64 %shl80
  %f.0.in = sub i64 %add67, %sub81
  %f.0 = uitofp i64 %f.0.in to double
  %conv84258 = zext i32 %sub69 to i64
  %shl85 = shl i64 %conv84258, 52
  %add87 = add i64 %shl85, 4696837146684686336
  %7 = bitcast i64 %add87 to double
  %mul89 = fmul double %7, %f.0
  %add90 = fadd double %mul89, 5.000000e-01
  %conv91 = fptoui double %add90 to i64
  %conv92 = uitofp i64 %conv91 to double
  %cmp93 = fcmp oeq double %add90, %conv92
  %and95 = and i64 %conv91, 1
  %tobool96.not = icmp ne i64 %and95, 0
  %not.or.cond259 = and i1 %cmp93, %tobool96.not
  %dec98 = sext i1 %not.or.cond259 to i64
  %fractional_part.0 = add i64 %dec98, %conv91
  %cmp100 = icmp eq i64 %fractional_part.0, 1000000
  %inc102 = zext i1 %cmp100 to i64
  %spec.select = add nuw i64 %integer_part.0, %inc102
  %spec.select260 = select i1 %cmp100, i64 0, i64 %fractional_part.0
  br label %if.end104

if.end104:                                        ; preds = %if.then71, %if.end65
  %integer_part.2 = phi i64 [ %spec.select, %if.then71 ], [ %add67, %if.end65 ]
  %integer_exponent.0 = phi i32 [ 0, %if.then71 ], [ %sub69, %if.end65 ]
  %fractional_part.2 = phi i64 [ %spec.select260, %if.then71 ], [ 0, %if.end65 ]
  %8 = getelementptr inbounds [512 x i8], [512 x i8]* %buf, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %8) #11
  %add.ptr = getelementptr inbounds [512 x i8], [512 x i8]* %buf, i64 0, i64 512
  %add.ptr105 = getelementptr inbounds [512 x i8], [512 x i8]* %buf, i64 0, i64 480
  %call108 = call i8* @halide_int64_to_string(i8* nonnull %add.ptr105, i8* nonnull %add.ptr, i64 %integer_part.2, i32 1) #16
  %cmp109267 = icmp sgt i32 %integer_exponent.0, 0
  br i1 %cmp109267, label %for.cond111.preheader, label %for.cond.cleanup

for.cond111.preheader:                            ; preds = %if.end104, %if.end137
  %i.0270 = phi i32 [ %inc139, %if.end137 ], [ 0, %if.end104 ]
  %int_part_ptr.0268 = phi i8* [ %int_part_ptr.1, %if.end137 ], [ %add.ptr105, %if.end104 ]
  %cmp113.not264 = icmp eq i8* %call108, %int_part_ptr.0268
  br i1 %cmp113.not264, label %if.end137, label %for.body115

for.cond.cleanup:                                 ; preds = %if.end137, %if.end104
  %int_part_ptr.0.lcssa = phi i8* [ %add.ptr105, %if.end104 ], [ %int_part_ptr.1, %if.end137 ]
  %call141 = call i8* @halide_string_to_string(i8* %dst.addr.0, i8* %end, i8* %int_part_ptr.0.lcssa) #16
  %call142 = call i8* @halide_string_to_string(i8* %call141, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.141, i64 0, i64 0)) #16
  %call143 = call i8* @halide_int64_to_string(i8* %call142, i8* %end, i64 %fractional_part.2, i32 6) #16
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %8) #11
  br label %cleanup147

for.cond.cleanup114:                              ; preds = %for.body115
  br i1 %cmp124, label %if.then135, label %if.end137

for.body115:                                      ; preds = %for.cond111.preheader, %for.body115
  %p.0266.pn = phi i8* [ %p.0266, %for.body115 ], [ %call108, %for.cond111.preheader ]
  %carry.0265 = phi i32 [ %carry.1, %for.body115 ], [ 0, %for.cond111.preheader ]
  %p.0266 = getelementptr inbounds i8, i8* %p.0266.pn, i64 -1
  %9 = load i8, i8* %p.0266, align 1, !tbaa !18
  %sub117 = add i8 %9, -48
  %conv119 = sext i8 %sub117 to i32
  %mul120 = shl nsw i32 %conv119, 1
  %add121 = or i32 %mul120, %carry.0265
  %10 = trunc i32 %add121 to i8
  %cmp124 = icmp sgt i8 %10, 9
  %sub127 = add nsw i32 %add121, 246
  %carry.1 = zext i1 %cmp124 to i32
  %new_digit.0.in = select i1 %cmp124, i32 %sub127, i32 %add121
  %11 = trunc i32 %new_digit.0.in to i8
  %conv133 = add i8 %11, 48
  store i8 %conv133, i8* %p.0266, align 1, !tbaa !18
  %cmp113.not = icmp eq i8* %p.0266, %int_part_ptr.0268
  br i1 %cmp113.not, label %for.cond.cleanup114, label %for.body115, !llvm.loop !302

if.then135:                                       ; preds = %for.cond.cleanup114
  %incdec.ptr136 = getelementptr inbounds i8, i8* %int_part_ptr.0268, i64 -1
  store i8 49, i8* %incdec.ptr136, align 1, !tbaa !18
  br label %if.end137

if.end137:                                        ; preds = %if.then135, %for.cond.cleanup114, %for.cond111.preheader
  %int_part_ptr.1 = phi i8* [ %incdec.ptr136, %if.then135 ], [ %int_part_ptr.0268, %for.cond.cleanup114 ], [ %call108, %for.cond111.preheader ]
  %inc139 = add nuw nsw i32 %i.0270, 1
  %exitcond.not = icmp eq i32 %inc139, %integer_exponent.0
  br i1 %exitcond.not, label %for.cond.cleanup, label %for.cond111.preheader, !llvm.loop !303

cleanup147:                                       ; preds = %for.cond.cleanup, %if.then63, %if.end58, %if.else30, %if.then28, %if.else24, %if.then22, %if.else13, %if.then11, %if.else, %if.then6
  %retval.1 = phi i8* [ %call7, %if.then6 ], [ %call8, %if.else ], [ %call12, %if.then11 ], [ %call14, %if.else13 ], [ %call23, %if.then22 ], [ %call25, %if.else24 ], [ %call29, %if.then28 ], [ %call31, %if.else30 ], [ %call64, %if.then63 ], [ %call60, %if.end58 ], [ %call143, %for.cond.cleanup ]
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %0) #11
  ret i8* %retval.1
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_pointer_to_string(i8* %dst, i8* %end, i8* %arg) local_unnamed_addr #0 {
entry:
  %buf = alloca [20 x i8], align 1
  %0 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 0
  call void @llvm.lifetime.start.p0i8(i64 20, i8* nonnull %0) #11
  call void @llvm.memset.p0i8.i64(i8* nonnull align 1 dereferenceable(20) %0, i8 0, i64 20, i1 false)
  %add.ptr = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 18
  %1 = ptrtoint i8* %arg to i64
  %and = and i64 %1, 15
  %arrayidx = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and
  %2 = load i8, i8* %arrayidx, align 1, !tbaa !18
  %incdec.ptr = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 17
  store i8 %2, i8* %add.ptr, align 1, !tbaa !18
  %shr = lshr i64 %1, 4
  %tobool.not = icmp eq i64 %shr, 0
  br i1 %tobool.not, label %cleanup, label %for.cond

for.cond:                                         ; preds = %entry
  %and.1 = and i64 %shr, 15
  %arrayidx.1 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.1
  %3 = load i8, i8* %arrayidx.1, align 1, !tbaa !18
  %incdec.ptr.1 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 16
  store i8 %3, i8* %incdec.ptr, align 1, !tbaa !18
  %shr.1 = lshr i64 %1, 8
  %tobool.not.1 = icmp eq i64 %shr.1, 0
  br i1 %tobool.not.1, label %cleanup, label %for.cond.1

cleanup:                                          ; preds = %for.cond.14, %for.cond.13, %for.cond.12, %for.cond.11, %for.cond.10, %for.cond.9, %for.cond.8, %for.cond.7, %for.cond.6, %for.cond.5, %for.cond.4, %for.cond.3, %for.cond.2, %for.cond.1, %for.cond, %entry
  %buf_ptr.016.lcssa = phi i8* [ %add.ptr, %entry ], [ %incdec.ptr, %for.cond ], [ %incdec.ptr.1, %for.cond.1 ], [ %incdec.ptr.2, %for.cond.2 ], [ %incdec.ptr.3, %for.cond.3 ], [ %incdec.ptr.4, %for.cond.4 ], [ %incdec.ptr.5, %for.cond.5 ], [ %incdec.ptr.6, %for.cond.6 ], [ %incdec.ptr.7, %for.cond.7 ], [ %incdec.ptr.8, %for.cond.8 ], [ %incdec.ptr.9, %for.cond.9 ], [ %incdec.ptr.10, %for.cond.10 ], [ %incdec.ptr.11, %for.cond.11 ], [ %incdec.ptr.12, %for.cond.12 ], [ %incdec.ptr.13, %for.cond.13 ], [ %incdec.ptr.14, %for.cond.14 ]
  %incdec.ptr.lcssa = phi i8* [ %incdec.ptr, %entry ], [ %incdec.ptr.1, %for.cond ], [ %incdec.ptr.2, %for.cond.1 ], [ %incdec.ptr.3, %for.cond.2 ], [ %incdec.ptr.4, %for.cond.3 ], [ %incdec.ptr.5, %for.cond.4 ], [ %incdec.ptr.6, %for.cond.5 ], [ %incdec.ptr.7, %for.cond.6 ], [ %incdec.ptr.8, %for.cond.7 ], [ %incdec.ptr.9, %for.cond.8 ], [ %incdec.ptr.10, %for.cond.9 ], [ %incdec.ptr.11, %for.cond.10 ], [ %incdec.ptr.12, %for.cond.11 ], [ %incdec.ptr.13, %for.cond.12 ], [ %incdec.ptr.14, %for.cond.13 ], [ %incdec.ptr.15, %for.cond.14 ]
  %incdec.ptr1 = getelementptr inbounds i8, i8* %buf_ptr.016.lcssa, i64 -2
  store i8 120, i8* %incdec.ptr.lcssa, align 1, !tbaa !18
  store i8 48, i8* %incdec.ptr1, align 1, !tbaa !18
  %call = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* nonnull %incdec.ptr1) #16
  call void @llvm.lifetime.end.p0i8(i64 20, i8* nonnull %0) #11
  ret i8* %call

for.cond.1:                                       ; preds = %for.cond
  %and.2 = and i64 %shr.1, 15
  %arrayidx.2 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.2
  %4 = load i8, i8* %arrayidx.2, align 1, !tbaa !18
  %incdec.ptr.2 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 15
  store i8 %4, i8* %incdec.ptr.1, align 1, !tbaa !18
  %shr.2 = lshr i64 %1, 12
  %tobool.not.2 = icmp eq i64 %shr.2, 0
  br i1 %tobool.not.2, label %cleanup, label %for.cond.2

for.cond.2:                                       ; preds = %for.cond.1
  %and.3 = and i64 %shr.2, 15
  %arrayidx.3 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.3
  %5 = load i8, i8* %arrayidx.3, align 1, !tbaa !18
  %incdec.ptr.3 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 14
  store i8 %5, i8* %incdec.ptr.2, align 1, !tbaa !18
  %shr.3 = lshr i64 %1, 16
  %tobool.not.3 = icmp eq i64 %shr.3, 0
  br i1 %tobool.not.3, label %cleanup, label %for.cond.3

for.cond.3:                                       ; preds = %for.cond.2
  %and.4 = and i64 %shr.3, 15
  %arrayidx.4 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.4
  %6 = load i8, i8* %arrayidx.4, align 1, !tbaa !18
  %incdec.ptr.4 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 13
  store i8 %6, i8* %incdec.ptr.3, align 1, !tbaa !18
  %shr.4 = lshr i64 %1, 20
  %tobool.not.4 = icmp eq i64 %shr.4, 0
  br i1 %tobool.not.4, label %cleanup, label %for.cond.4

for.cond.4:                                       ; preds = %for.cond.3
  %and.5 = and i64 %shr.4, 15
  %arrayidx.5 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.5
  %7 = load i8, i8* %arrayidx.5, align 1, !tbaa !18
  %incdec.ptr.5 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 12
  store i8 %7, i8* %incdec.ptr.4, align 1, !tbaa !18
  %shr.5 = lshr i64 %1, 24
  %tobool.not.5 = icmp eq i64 %shr.5, 0
  br i1 %tobool.not.5, label %cleanup, label %for.cond.5

for.cond.5:                                       ; preds = %for.cond.4
  %and.6 = and i64 %shr.5, 15
  %arrayidx.6 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.6
  %8 = load i8, i8* %arrayidx.6, align 1, !tbaa !18
  %incdec.ptr.6 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 11
  store i8 %8, i8* %incdec.ptr.5, align 1, !tbaa !18
  %shr.6 = lshr i64 %1, 28
  %tobool.not.6 = icmp eq i64 %shr.6, 0
  br i1 %tobool.not.6, label %cleanup, label %for.cond.6

for.cond.6:                                       ; preds = %for.cond.5
  %and.7 = and i64 %shr.6, 15
  %arrayidx.7 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.7
  %9 = load i8, i8* %arrayidx.7, align 1, !tbaa !18
  %incdec.ptr.7 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 10
  store i8 %9, i8* %incdec.ptr.6, align 1, !tbaa !18
  %shr.7 = lshr i64 %1, 32
  %tobool.not.7 = icmp eq i64 %shr.7, 0
  br i1 %tobool.not.7, label %cleanup, label %for.cond.7

for.cond.7:                                       ; preds = %for.cond.6
  %and.8 = and i64 %shr.7, 15
  %arrayidx.8 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.8
  %10 = load i8, i8* %arrayidx.8, align 1, !tbaa !18
  %incdec.ptr.8 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 9
  store i8 %10, i8* %incdec.ptr.7, align 1, !tbaa !18
  %shr.8 = lshr i64 %1, 36
  %tobool.not.8 = icmp eq i64 %shr.8, 0
  br i1 %tobool.not.8, label %cleanup, label %for.cond.8

for.cond.8:                                       ; preds = %for.cond.7
  %and.9 = and i64 %shr.8, 15
  %arrayidx.9 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.9
  %11 = load i8, i8* %arrayidx.9, align 1, !tbaa !18
  %incdec.ptr.9 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 8
  store i8 %11, i8* %incdec.ptr.8, align 1, !tbaa !18
  %shr.9 = lshr i64 %1, 40
  %tobool.not.9 = icmp eq i64 %shr.9, 0
  br i1 %tobool.not.9, label %cleanup, label %for.cond.9

for.cond.9:                                       ; preds = %for.cond.8
  %and.10 = and i64 %shr.9, 15
  %arrayidx.10 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.10
  %12 = load i8, i8* %arrayidx.10, align 1, !tbaa !18
  %incdec.ptr.10 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 7
  store i8 %12, i8* %incdec.ptr.9, align 1, !tbaa !18
  %shr.10 = lshr i64 %1, 44
  %tobool.not.10 = icmp eq i64 %shr.10, 0
  br i1 %tobool.not.10, label %cleanup, label %for.cond.10

for.cond.10:                                      ; preds = %for.cond.9
  %and.11 = and i64 %shr.10, 15
  %arrayidx.11 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.11
  %13 = load i8, i8* %arrayidx.11, align 1, !tbaa !18
  %incdec.ptr.11 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 6
  store i8 %13, i8* %incdec.ptr.10, align 1, !tbaa !18
  %shr.11 = lshr i64 %1, 48
  %tobool.not.11 = icmp eq i64 %shr.11, 0
  br i1 %tobool.not.11, label %cleanup, label %for.cond.11

for.cond.11:                                      ; preds = %for.cond.10
  %and.12 = and i64 %shr.11, 15
  %arrayidx.12 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.12
  %14 = load i8, i8* %arrayidx.12, align 1, !tbaa !18
  %incdec.ptr.12 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 5
  store i8 %14, i8* %incdec.ptr.11, align 1, !tbaa !18
  %shr.12 = lshr i64 %1, 52
  %tobool.not.12 = icmp eq i64 %shr.12, 0
  br i1 %tobool.not.12, label %cleanup, label %for.cond.12

for.cond.12:                                      ; preds = %for.cond.11
  %and.13 = and i64 %shr.12, 15
  %arrayidx.13 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.13
  %15 = load i8, i8* %arrayidx.13, align 1, !tbaa !18
  %incdec.ptr.13 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 4
  store i8 %15, i8* %incdec.ptr.12, align 1, !tbaa !18
  %shr.13 = lshr i64 %1, 56
  %tobool.not.13 = icmp eq i64 %shr.13, 0
  br i1 %tobool.not.13, label %cleanup, label %for.cond.13

for.cond.13:                                      ; preds = %for.cond.12
  %and.14 = and i64 %shr.13, 15
  %arrayidx.14 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %and.14
  %16 = load i8, i8* %arrayidx.14, align 1, !tbaa !18
  %incdec.ptr.14 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 3
  store i8 %16, i8* %incdec.ptr.13, align 1, !tbaa !18
  %shr.14 = lshr i64 %1, 60
  %tobool.not.14 = icmp eq i64 %shr.14, 0
  br i1 %tobool.not.14, label %cleanup, label %for.cond.14

for.cond.14:                                      ; preds = %for.cond.13
  %arrayidx.15 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.13.71, i64 0, i64 %shr.14
  %17 = load i8, i8* %arrayidx.15, align 1, !tbaa !18
  %incdec.ptr.15 = getelementptr inbounds [20 x i8], [20 x i8]* %buf, i64 0, i64 2
  store i8 %17, i8* %incdec.ptr.14, align 1, !tbaa !18
  br label %cleanup
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_type_to_string(i8* %dst, i8* %end, %struct.halide_type_t* %t) local_unnamed_addr #0 {
entry:
  %code = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %t, i64 0, i32 0
  %0 = load i8, i8* %code, align 2, !tbaa !304
  %1 = icmp ult i8 %0, 4
  br i1 %1, label %switch.lookup, label %sw.epilog

switch.lookup:                                    ; preds = %entry
  %2 = sext i8 %0 to i64
  %switch.gep = getelementptr inbounds [4 x i8*], [4 x i8*]* @switch.table.halide_type_to_string, i64 0, i64 %2
  %switch.load = load i8*, i8** %switch.gep, align 8
  br label %sw.epilog

sw.epilog:                                        ; preds = %entry, %switch.lookup
  %code_name.0 = phi i8* [ %switch.load, %switch.lookup ], [ getelementptr inbounds ([14 x i8], [14 x i8]* @.str.18.72, i64 0, i64 0), %entry ]
  %call = tail call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* nonnull %code_name.0) #16
  %bits = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %t, i64 0, i32 1
  %3 = load i8, i8* %bits, align 1, !tbaa !144
  %conv4 = zext i8 %3 to i64
  %call5 = tail call i8* @halide_uint64_to_string(i8* %call, i8* %end, i64 %conv4, i32 1) #16
  %lanes = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %t, i64 0, i32 2
  %4 = load i16, i16* %lanes, align 2, !tbaa !305
  %cmp.not = icmp eq i16 %4, 1
  br i1 %cmp.not, label %if.end, label %if.then

if.then:                                          ; preds = %sw.epilog
  %call7 = tail call i8* @halide_string_to_string(i8* %call5, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.19.77, i64 0, i64 0)) #16
  %5 = load i16, i16* %lanes, align 2, !tbaa !305
  %conv9 = zext i16 %5 to i64
  %call10 = tail call i8* @halide_uint64_to_string(i8* %call7, i8* %end, i64 %conv9, i32 1) #16
  br label %if.end

if.end:                                           ; preds = %if.then, %sw.epilog
  %dst.addr.0 = phi i8* [ %call10, %if.then ], [ %call5, %sw.epilog ]
  ret i8* %dst.addr.0
}

; Function Attrs: nounwind mustprogress
define weak i8* @halide_buffer_to_string(i8* %dst, i8* %end, %struct.halide_buffer_t* %buf) local_unnamed_addr #0 {
entry:
  %cmp = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  %call = tail call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str.20.78, i64 0, i64 0)) #16
  br label %return

if.end:                                           ; preds = %entry
  %call1 = tail call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str.21.79, i64 0, i64 0)) #16
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %0 = load i64, i64* %device, align 8, !tbaa !182
  %call2 = tail call i8* @halide_uint64_to_string(i8* %call1, i8* %end, i64 %0, i32 1) #16
  %call3 = tail call i8* @halide_string_to_string(i8* %call2, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #16
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %1 = bitcast %struct.halide_device_interface_t** %device_interface to i8**
  %2 = load i8*, i8** %1, align 8, !tbaa !306
  %call4 = tail call i8* @halide_pointer_to_string(i8* %call3, i8* %end, i8* %2) #16
  %call5 = tail call i8* @halide_string_to_string(i8* %call4, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #16
  %host = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 2
  %3 = load i8*, i8** %host, align 8, !tbaa !180
  %call6 = tail call i8* @halide_pointer_to_string(i8* %call5, i8* %end, i8* %3) #16
  %call7 = tail call i8* @halide_string_to_string(i8* %call6, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #16
  %flags = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %4 = load i64, i64* %flags, align 8, !tbaa !307
  %call8 = tail call i8* @halide_uint64_to_string(i8* %call7, i8* %end, i64 %4, i32 1) #16
  %call9 = tail call i8* @halide_string_to_string(i8* %call8, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #16
  %type = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 4
  %call10 = tail call i8* @halide_type_to_string(i8* %call9, i8* %end, %struct.halide_type_t* nonnull %type) #16
  %dimensions = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 5
  %5 = load i32, i32* %dimensions, align 4, !tbaa !183
  %cmp1179 = icmp sgt i32 %5, 0
  br i1 %cmp1179, label %for.body.lr.ph, label %for.cond.cleanup

for.body.lr.ph:                                   ; preds = %if.end
  %dim = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 6
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.body, %if.end
  %dst.addr.0.lcssa = phi i8* [ %call10, %if.end ], [ %call26, %for.body ]
  %call27 = tail call i8* @halide_string_to_string(i8* %dst.addr.0.lcssa, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.8.119, i64 0, i64 0)) #16
  br label %return

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %indvars.iv = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next, %for.body ]
  %dst.addr.080 = phi i8* [ %call10, %for.body.lr.ph ], [ %call26, %for.body ]
  %call12 = tail call i8* @halide_string_to_string(i8* %dst.addr.080, i8* %end, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.23.82, i64 0, i64 0)) #16
  %6 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim, align 8, !tbaa !184
  %min = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %6, i64 %indvars.iv, i32 0
  %7 = load i32, i32* %min, align 4, !tbaa !221
  %conv = sext i32 %7 to i64
  %call13 = tail call i8* @halide_int64_to_string(i8* %call12, i8* %end, i64 %conv, i32 1) #16
  %call14 = tail call i8* @halide_string_to_string(i8* %call13, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #16
  %8 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim, align 8, !tbaa !184
  %extent = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %8, i64 %indvars.iv, i32 1
  %9 = load i32, i32* %extent, align 4, !tbaa !189
  %conv18 = sext i32 %9 to i64
  %call19 = tail call i8* @halide_int64_to_string(i8* %call14, i8* %end, i64 %conv18, i32 1) #16
  %call20 = tail call i8* @halide_string_to_string(i8* %call19, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #16
  %10 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim, align 8, !tbaa !184
  %stride = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %10, i64 %indvars.iv, i32 2
  %11 = load i32, i32* %stride, align 4, !tbaa !185
  %conv24 = sext i32 %11 to i64
  %call25 = tail call i8* @halide_int64_to_string(i8* %call20, i8* %end, i64 %conv24, i32 1) #16
  %call26 = tail call i8* @halide_string_to_string(i8* %call25, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.24.83, i64 0, i64 0)) #16
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %12 = load i32, i32* %dimensions, align 4, !tbaa !183
  %13 = sext i32 %12 to i64
  %cmp11 = icmp slt i64 %indvars.iv.next, %13
  br i1 %cmp11, label %for.body, label %for.cond.cleanup, !llvm.loop !308

return:                                           ; preds = %for.cond.cleanup, %if.then
  %retval.0 = phi i8* [ %call, %if.then ], [ %call27, %for.cond.cleanup ]
  ret i8* %retval.0
}

; Function Attrs: alwaysinline nounwind willreturn mustprogress
define weak i32 @halide_malloc_alignment() local_unnamed_addr #6 {
entry:
  ret i32 32
}

; Function Attrs: nounwind
define weak i32 @halide_reuse_device_allocations(i8* %user_context, i1 zeroext %flag) local_unnamed_addr #4 {
entry:
  %frombool = zext i1 %flag to i8
  store i8 %frombool, i8* @_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE, align 1, !tbaa !19
  br i1 %flag, label %if.end5, label %if.then

if.then:                                          ; preds = %entry
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal21allocation_pools_lockE) #15
  %p.014 = load %struct.halide_device_allocation_pool*, %struct.halide_device_allocation_pool** @_ZN6Halide7Runtime8Internal23device_allocation_poolsE, align 8, !tbaa !14
  %cmp.not15 = icmp eq %struct.halide_device_allocation_pool* %p.014, null
  br i1 %cmp.not15, label %for.cond.cleanup, label %for.body

for.cond.cleanup:                                 ; preds = %for.body, %if.then
  %err.0.lcssa = phi i32 [ 0, %if.then ], [ %spec.select, %for.body ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal21allocation_pools_lockE) #15
  br label %if.end5

for.body:                                         ; preds = %if.then, %for.body
  %p.017 = phi %struct.halide_device_allocation_pool* [ %p.0, %for.body ], [ %p.014, %if.then ]
  %err.016 = phi i32 [ %spec.select, %for.body ], [ 0, %if.then ]
  %release_unused = getelementptr inbounds %struct.halide_device_allocation_pool, %struct.halide_device_allocation_pool* %p.017, i64 0, i32 0
  %0 = load i32 (i8*)*, i32 (i8*)** %release_unused, align 8, !tbaa !309
  %call = tail call i32 %0(i8* %user_context) #15
  %tobool3.not = icmp eq i32 %call, 0
  %spec.select = select i1 %tobool3.not, i32 %err.016, i32 %call
  %next = getelementptr inbounds %struct.halide_device_allocation_pool, %struct.halide_device_allocation_pool* %p.017, i64 0, i32 1
  %p.0 = load %struct.halide_device_allocation_pool*, %struct.halide_device_allocation_pool** %next, align 8, !tbaa !14
  %cmp.not = icmp eq %struct.halide_device_allocation_pool* %p.0, null
  br i1 %cmp.not, label %for.cond.cleanup, label %for.body, !llvm.loop !311

if.end5:                                          ; preds = %for.cond.cleanup, %entry
  %err.2 = phi i32 [ 0, %entry ], [ %err.0.lcssa, %for.cond.cleanup ]
  ret i32 %err.2
}

; Function Attrs: nounwind willreturn mustprogress
define weak zeroext i1 @halide_can_reuse_device_allocations(i8* %user_context) local_unnamed_addr #2 {
entry:
  %0 = load i8, i8* @_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE, align 1, !tbaa !19, !range !21
  %tobool = icmp ne i8 %0, 0
  ret i1 %tobool
}

; Function Attrs: nounwind
define weak void @halide_register_device_allocation_pool(%struct.halide_device_allocation_pool* %pool) local_unnamed_addr #4 {
entry:
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal21allocation_pools_lockE) #15
  %0 = load %struct.halide_device_allocation_pool*, %struct.halide_device_allocation_pool** @_ZN6Halide7Runtime8Internal23device_allocation_poolsE, align 8, !tbaa !14
  %next = getelementptr inbounds %struct.halide_device_allocation_pool, %struct.halide_device_allocation_pool* %pool, i64 0, i32 1
  store %struct.halide_device_allocation_pool* %0, %struct.halide_device_allocation_pool** %next, align 8, !tbaa !312
  store %struct.halide_device_allocation_pool* %pool, %struct.halide_device_allocation_pool** @_ZN6Halide7Runtime8Internal23device_allocation_poolsE, align 8, !tbaa !14
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal21allocation_pools_lockE) #15
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #0 {
entry:
  %flags.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %0 = load i64, i64* %flags.i.i, align 8, !tbaa !307
  %and.i.i = and i64 %0, 2
  %cmp.i.i.not = icmp eq i64 %and.i.i, 0
  br i1 %cmp.i.i.not, label %return, label %if.end

if.end:                                           ; preds = %entry
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %1 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %and.i.i46 = and i64 %0, 1
  %cmp.i.i47.not = icmp eq i64 %and.i.i46, 0
  br i1 %cmp.i.i47.not, label %if.end9, label %return

if.end9:                                          ; preds = %if.end
  %cmp = icmp eq %struct.halide_device_interface_t* %1, null
  br i1 %cmp, label %return, label %if.end15

if.end15:                                         ; preds = %if.end9
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %1, i64 0, i32 15
  %2 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %copy_to_host = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %2, i64 0, i32 6
  %3 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %copy_to_host, align 8, !tbaa !315
  %call16 = tail call i32 %3(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %cmp17.not = icmp eq i32 %call16, 0
  br i1 %cmp17.not, label %if.end23, label %return

if.end23:                                         ; preds = %if.end15
  %4 = load i64, i64* %flags.i.i, align 8, !tbaa !307
  %and.i.i44 = and i64 %4, -3
  store i64 %and.i.i44, i64* %flags.i.i, align 8, !tbaa !307
  %call24 = tail call i32 @halide_msan_annotate_buffer_is_initialized(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  br label %return

return:                                           ; preds = %if.end23, %if.end15, %if.end9, %if.end, %entry
  %retval.2 = phi i32 [ 0, %entry ], [ 0, %if.end23 ], [ -14, %if.end ], [ -19, %if.end9 ], [ -14, %if.end15 ]
  ret i32 %retval.2
}

; Function Attrs: nounwind mustprogress
define weak void @halide_device_release(i8* %user_context, %struct.halide_device_interface_t* %device_interface) local_unnamed_addr #0 {
entry:
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %device_interface, i64 0, i32 15
  %0 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %device_release = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %0, i64 0, i32 5
  %1 = load i32 (i8*)*, i32 (i8*)** %device_release, align 8, !tbaa !317
  %call = tail call i32 %1(i8* %user_context) #15
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_copy_to_host(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #4 {
entry:
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.str.6.88, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end16.i.split

if.end16.i.split:                                 ; preds = %if.end16.i
  %call11 = tail call i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #16
  br label %cleanup

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split, label %cleanup

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %call12 = tail call i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %user_context, %struct.halide_buffer_t* %buf) #16
  br label %cleanup

cleanup:                                          ; preds = %if.end16.i.split, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.0 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ %call11, %if.end16.i.split ], [ %call12, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  ret i32 %retval.0
}

; Function Attrs: nounwind mustprogress
define linkonce i32 @copy_to_device_already_locked(i8* %user_context, %struct.halide_buffer_t* %buf, %struct.halide_device_interface_t* %device_interface) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.7.89, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %if.end, label %cleanup

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit, %if.end16.i
  %cmp1 = icmp eq %struct.halide_device_interface_t* %device_interface, null
  br i1 %cmp1, label %if.then2, label %if.end11

if.then2:                                         ; preds = %if.end
  %device_interface5 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %4 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface5, align 8, !tbaa !306
  %cmp6 = icmp eq %struct.halide_device_interface_t* %4, null
  br i1 %cmp6, label %if.then7, label %if.end11

if.then7:                                         ; preds = %if.then2
  %call8 = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %cleanup

if.end11:                                         ; preds = %if.then2, %if.end
  %device_interface.addr.0 = phi %struct.halide_device_interface_t* [ %device_interface, %if.end ], [ %4, %if.then2 ]
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %5 = load i64, i64* %device, align 8, !tbaa !182
  %tobool.not = icmp eq i64 %5, 0
  br i1 %tobool.not, label %if.then18, label %land.lhs.true

land.lhs.true:                                    ; preds = %if.end11
  %device_interface12 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %6 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface12, align 8, !tbaa !306
  %cmp13.not = icmp eq %struct.halide_device_interface_t* %6, %device_interface.addr.0
  br i1 %cmp13.not, label %if.end27, label %if.then14

if.then14:                                        ; preds = %land.lhs.true
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([61 x i8], [61 x i8]* @.str.9.90, i64 0, i64 0)) #15
  br label %cleanup

if.then18:                                        ; preds = %if.end11
  %call19 = tail call i32 @halide_device_malloc(i8* %user_context, %struct.halide_buffer_t* nonnull %buf, %struct.halide_device_interface_t* nonnull %device_interface.addr.0) #16
  %cmp20.not = icmp eq i32 %call19, 0
  br i1 %cmp20.not, label %if.end27, label %cleanup

if.end27:                                         ; preds = %if.then18, %land.lhs.true
  %flags.i.i97 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %7 = load i64, i64* %flags.i.i97, align 8, !tbaa !307
  %and.i.i98 = and i64 %7, 1
  %cmp.i.i99.not = icmp eq i64 %and.i.i98, 0
  br i1 %cmp.i.i99.not, label %cleanup, label %if.then29

if.then29:                                        ; preds = %if.end27
  %and.i.i96 = and i64 %7, 2
  %cmp.i.i.not = icmp eq i64 %and.i.i96, 0
  br i1 %cmp.i.i.not, label %if.else, label %cleanup

if.else:                                          ; preds = %if.then29
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %device_interface.addr.0, i64 0, i32 15
  %8 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %copy_to_device = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %8, i64 0, i32 7
  %9 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %copy_to_device, align 8, !tbaa !318
  %call44 = tail call i32 %9(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %cmp45 = icmp eq i32 %call44, 0
  br i1 %cmp45, label %if.then46, label %cleanup

if.then46:                                        ; preds = %if.else
  %10 = load i64, i64* %flags.i.i97, align 8, !tbaa !307
  %and.i.i = and i64 %10, -2
  store i64 %and.i.i, i64* %flags.i.i97, align 8, !tbaa !307
  br label %cleanup

cleanup:                                          ; preds = %if.then46, %if.else, %if.then29, %if.end27, %if.then18, %if.then14, %if.then7, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.0 = phi i32 [ %call8, %if.then7 ], [ -42, %if.then14 ], [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ 0, %if.then46 ], [ %call19, %if.then18 ], [ -15, %if.then29 ], [ -15, %if.else ], [ 0, %if.end27 ]
  ret i32 %retval.0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_device_malloc(i8* %user_context, %struct.halide_buffer_t* %buf, %struct.halide_device_interface_t* %device_interface) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([21 x i8], [21 x i8]* @.str.17.91, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, label %cleanup12

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %device_interface3.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %.pre = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface3.phi.trans.insert, align 8, !tbaa !306
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, %if.end16.i
  %4 = phi %struct.halide_device_interface_t* [ %.pre, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge ], [ %0, %if.end16.i ]
  %cmp4.not = icmp eq %struct.halide_device_interface_t* %4, null
  %cmp5.not = icmp eq %struct.halide_device_interface_t* %4, %device_interface
  %or.cond = or i1 %cmp4.not, %cmp5.not
  br i1 %or.cond, label %if.end7, label %if.then6

if.then6:                                         ; preds = %if.end
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([59 x i8], [59 x i8]* @.str.20.92, i64 0, i64 0)) #15
  br label %cleanup12

if.end7:                                          ; preds = %if.end
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %device_interface, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 0
  %6 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %6() #15
  %7 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %device_malloc = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %7, i64 0, i32 2
  %8 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %device_malloc, align 8, !tbaa !320
  %call9 = tail call i32 %8(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %9 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %9, i64 0, i32 1
  %10 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %10() #15
  %tobool.not = icmp eq i32 %call9, 0
  %spec.select = select i1 %tobool.not, i32 0, i32 -16
  ret i32 %spec.select

cleanup12:                                        ; preds = %if.then6, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.1 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ -42, %if.then6 ]
  ret i32 %retval.1
}

; Function Attrs: nounwind
define weak i32 @halide_copy_to_device(i8* %user_context, %struct.halide_buffer_t* %buf, %struct.halide_device_interface_t* %device_interface) local_unnamed_addr #4 {
entry:
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  %call = tail call i32 @copy_to_device_already_locked(i8* %user_context, %struct.halide_buffer_t* %buf, %struct.halide_device_interface_t* %device_interface) #16
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_device_sync(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([19 x i8], [19 x i8]* @.str.16.93, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, label %cleanup8

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %device_interface1.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %.pre = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface1.phi.trans.insert, align 8, !tbaa !306
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, %if.end16.i
  %4 = phi %struct.halide_device_interface_t* [ %.pre, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge ], [ %0, %if.end16.i ]
  %cmp2 = icmp eq %struct.halide_device_interface_t* %4, null
  br i1 %cmp2, label %if.then3, label %if.end5

if.then3:                                         ; preds = %if.end
  %call4 = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %cleanup8

if.end5:                                          ; preds = %if.end
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %4, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %device_sync = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 4
  %6 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %device_sync, align 8, !tbaa !322
  %call6 = tail call i32 %6(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %tobool.not = icmp eq i32 %call6, 0
  %spec.select = select i1 %tobool.not, i32 0, i32 -17
  ret i32 %spec.select

cleanup8:                                         ; preds = %if.then3, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.1 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ %call4, %if.then3 ]
  ret i32 %retval.1
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_device_free(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([19 x i8], [19 x i8]* @.str.21.96, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, label %cleanup12

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %device_interface1.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %.pre = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface1.phi.trans.insert, align 8, !tbaa !306
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, %if.end16.i
  %4 = phi %struct.halide_device_interface_t* [ %.pre, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge ], [ %0, %if.end16.i ]
  %cmp2.not = icmp eq %struct.halide_device_interface_t* %4, null
  br i1 %cmp2.not, label %if.end11, label %if.then3

if.then3:                                         ; preds = %if.end
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %4, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 0
  %6 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %6() #15
  %7 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %device_free = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %7, i64 0, i32 3
  %8 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %device_free, align 8, !tbaa !323
  %call5 = tail call i32 %8(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %9 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %9, i64 0, i32 1
  %10 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %10() #15
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %11 = load i64, i64* %device, align 8, !tbaa !182
  %cmp7 = icmp eq i64 %11, 0
  br i1 %cmp7, label %do.end, label %if.then8

if.then8:                                         ; preds = %if.then3
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([127 x i8], [127 x i8]* @.str.22.97, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %if.then8, %if.then3
  %tobool.not = icmp eq i32 %call5, 0
  %spec.select = select i1 %tobool.not, i32 0, i32 -18
  ret i32 %spec.select

if.end11:                                         ; preds = %if.end
  %flags3.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %12 = load i64, i64* %flags3.i.i, align 8, !tbaa !307
  %and.i.i = and i64 %12, -3
  store i64 %and.i.i, i64* %flags3.i.i, align 8, !tbaa !307
  br label %cleanup12

cleanup12:                                        ; preds = %if.end11, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.1 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ 0, %if.end11 ]
  ret i32 %retval.1
}

; Function Attrs: nounwind mustprogress
define weak void @halide_device_free_as_destructor(i8* %user_context, i8* %obj) local_unnamed_addr #0 {
entry:
  %0 = bitcast i8* %obj to %struct.halide_buffer_t*
  %call = tail call i32 @halide_device_free(i8* %user_context, %struct.halide_buffer_t* %0) #16
  ret void
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_device_and_host_malloc(i8* %user_context, %struct.halide_buffer_t* %buf, %struct.halide_device_interface_t* %device_interface) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([30 x i8], [30 x i8]* @.str.23.98, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, label %cleanup14

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %device_interface3.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %.pre = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface3.phi.trans.insert, align 8, !tbaa !306
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, %if.end16.i
  %4 = phi %struct.halide_device_interface_t* [ %.pre, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge ], [ %0, %if.end16.i ]
  %cmp4.not = icmp eq %struct.halide_device_interface_t* %4, null
  %cmp5.not = icmp eq %struct.halide_device_interface_t* %4, %device_interface
  %or.cond = or i1 %cmp4.not, %cmp5.not
  br i1 %or.cond, label %if.end7, label %if.then6

if.then6:                                         ; preds = %if.end
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([68 x i8], [68 x i8]* @.str.25.99, i64 0, i64 0)) #15
  br label %cleanup14

if.end7:                                          ; preds = %if.end
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %device_interface, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 0
  %6 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %6() #15
  %7 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %device_and_host_malloc = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %7, i64 0, i32 8
  %8 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %device_and_host_malloc, align 8, !tbaa !324
  %call9 = tail call i32 %8(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %9 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %9, i64 0, i32 1
  %10 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %10() #15
  %cmp11.not = icmp eq i32 %call9, 0
  br i1 %cmp11.not, label %cleanup14, label %if.then12

if.then12:                                        ; preds = %if.end7
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([42 x i8], [42 x i8]* @.str.26.100, i64 0, i64 0)) #15
  br label %cleanup14

cleanup14:                                        ; preds = %if.then12, %if.end7, %if.then6, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.1 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ -42, %if.then6 ], [ -16, %if.then12 ], [ 0, %if.end7 ]
  ret i32 %retval.1
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_device_and_host_free(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([28 x i8], [28 x i8]* @.str.27.101, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, label %cleanup18

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %device_interface1.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %.pre = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface1.phi.trans.insert, align 8, !tbaa !306
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, %if.end16.i
  %4 = phi %struct.halide_device_interface_t* [ %.pre, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge ], [ %0, %if.end16.i ]
  %cmp2.not = icmp eq %struct.halide_device_interface_t* %4, null
  br i1 %cmp2.not, label %if.else11, label %if.then3

if.then3:                                         ; preds = %if.end
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %4, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 0
  %6 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %6() #15
  %7 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %device_and_host_free = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %7, i64 0, i32 9
  %8 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %device_and_host_free, align 8, !tbaa !325
  %call5 = tail call i32 %8(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %9 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %9, i64 0, i32 1
  %10 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %10() #15
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %11 = load i64, i64* %device, align 8, !tbaa !182
  %cmp7 = icmp eq i64 %11, 0
  br i1 %cmp7, label %do.end, label %if.then8

if.then8:                                         ; preds = %if.then3
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([127 x i8], [127 x i8]* @.str.28.102, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %if.then8, %if.then3
  %tobool.not = icmp eq i32 %call5, 0
  %spec.select = select i1 %tobool.not, i32 0, i32 -18
  ret i32 %spec.select

if.else11:                                        ; preds = %if.end
  %host = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 2
  %12 = load i8*, i8** %host, align 8, !tbaa !180
  %tobool12.not = icmp eq i8* %12, null
  br i1 %tobool12.not, label %if.end17, label %if.then13

if.then13:                                        ; preds = %if.else11
  tail call void @halide_free(i8* %user_context, i8* nonnull %12) #15
  store i8* null, i8** %host, align 8, !tbaa !180
  br label %if.end17

if.end17:                                         ; preds = %if.then13, %if.else11
  %flags3.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %13 = load i64, i64* %flags3.i.i, align 8, !tbaa !307
  %and.i.i = and i64 %13, -3
  store i64 %and.i.i, i64* %flags3.i.i, align 8, !tbaa !307
  br label %cleanup18

cleanup18:                                        ; preds = %if.end17, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.1 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ 0, %if.end17 ]
  ret i32 %retval.1
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_device_and_host_malloc(i8* %user_context, %struct.halide_buffer_t* %buf, %struct.halide_device_interface_t* %device_interface) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([38 x i8], [38 x i8]* @.str.29.103, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %if.end, label %cleanup13

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit, %if.end16.i
  %dimensions.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 5
  %4 = load i32, i32* %dimensions.i.i, align 4, !tbaa !183
  %cmp19.i.i = icmp sgt i32 %4, 0
  br i1 %cmp19.i.i, label %for.body.lr.ph.i.i, label %if.end._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge

if.end._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge: ; preds = %if.end
  %sub.i.0 = sub nsw i64 1, 0
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

for.body.lr.ph.i.i:                               ; preds = %if.end
  %dim.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 6
  %5 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i.i, align 8, !tbaa !184
  %wide.trip.count.i.i = zext i32 %4 to i64
  %min.iters.check = icmp ult i32 %4, 3
  br i1 %min.iters.check, label %for.body.i.i.preheader, label %vector.ph

vector.ph:                                        ; preds = %for.body.lr.ph.i.i
  %n.vec = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body

vector.body:                                      ; preds = %pred.load.continue6, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %pred.load.continue6 ]
  %vec.phi = phi i64 [ 0, %vector.ph ], [ %predphi, %pred.load.continue6 ]
  %vec.phi4 = phi i64 [ 0, %vector.ph ], [ %predphi7, %pred.load.continue6 ]
  %induction3 = or i64 %index, 1
  %6 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %index, i32 2
  %7 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %induction3, i32 2
  %8 = load i32, i32* %6, align 4, !tbaa !185
  %9 = load i32, i32* %7, align 4, !tbaa !185
  %10 = icmp sgt i32 %8, 0
  %11 = icmp sgt i32 %9, 0
  %12 = zext i32 %8 to i64
  %13 = zext i32 %9 to i64
  br i1 %10, label %pred.load.if, label %pred.load.continue

pred.load.if:                                     ; preds = %vector.body
  %14 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %index, i32 1
  %15 = load i32, i32* %14, align 4, !tbaa !189
  br label %pred.load.continue

pred.load.continue:                               ; preds = %pred.load.if, %vector.body
  %16 = phi i32 [ poison, %vector.body ], [ %15, %pred.load.if ]
  br i1 %11, label %pred.load.if5, label %pred.load.continue6

pred.load.if5:                                    ; preds = %pred.load.continue
  %17 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %induction3, i32 1
  %18 = load i32, i32* %17, align 4, !tbaa !189
  br label %pred.load.continue6

pred.load.continue6:                              ; preds = %pred.load.if5, %pred.load.continue
  %19 = phi i32 [ poison, %pred.load.continue ], [ %18, %pred.load.if5 ]
  %20 = add nsw i32 %16, -1
  %21 = add nsw i32 %19, -1
  %22 = sext i32 %20 to i64
  %23 = sext i32 %21 to i64
  %24 = mul nsw i64 %22, %12
  %25 = mul nsw i64 %23, %13
  %26 = select i1 %10, i64 %24, i64 0
  %predphi = add i64 %vec.phi, %26
  %27 = select i1 %11, i64 %25, i64 0
  %predphi7 = add i64 %vec.phi4, %27
  %index.next = add i64 %index, 2
  %28 = icmp eq i64 %index.next, %n.vec
  br i1 %28, label %middle.block, label %vector.body, !llvm.loop !326

middle.block:                                     ; preds = %pred.load.continue6
  %bin.rdx = add i64 %predphi7, %predphi
  %cmp.n = icmp eq i64 %n.vec, %wide.trip.count.i.i
  br i1 %cmp.n, label %for.body.i13.i.preheader, label %for.body.i.i.preheader

for.body.i.i.preheader:                           ; preds = %for.body.lr.ph.i.i, %middle.block
  %indvars.iv.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %n.vec, %middle.block ]
  %index.021.i.i.ph = phi i64 [ 0, %for.body.lr.ph.i.i ], [ %bin.rdx, %middle.block ]
  br label %for.body.i.i

for.body.i.i:                                     ; preds = %for.body.i.i.preheader, %if.end.i.i
  %indvars.iv.i.i = phi i64 [ %indvars.iv.next.i.i, %if.end.i.i ], [ %indvars.iv.i.i.ph, %for.body.i.i.preheader ]
  %index.021.i.i = phi i64 [ %index.1.i.i, %if.end.i.i ], [ %index.021.i.i.ph, %for.body.i.i.preheader ]
  %stride2.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %indvars.iv.i.i, i32 2
  %29 = load i32, i32* %stride2.i.i, align 4, !tbaa !185
  %cmp3.i.i = icmp sgt i32 %29, 0
  br i1 %cmp3.i.i, label %if.then.i.i, label %if.end.i.i

if.then.i.i:                                      ; preds = %for.body.i.i
  %conv18.i.i = zext i32 %29 to i64
  %extent.i.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %indvars.iv.i.i, i32 1
  %30 = load i32, i32* %extent.i.i, align 4, !tbaa !189
  %sub.i.i = add nsw i32 %30, -1
  %conv7.i.i = sext i32 %sub.i.i to i64
  %mul.i.i = mul nsw i64 %conv7.i.i, %conv18.i.i
  %add.i.i = add nsw i64 %mul.i.i, %index.021.i.i
  br label %if.end.i.i

if.end.i.i:                                       ; preds = %if.then.i.i, %for.body.i.i
  %index.1.i.i = phi i64 [ %add.i.i, %if.then.i.i ], [ %index.021.i.i, %for.body.i.i ]
  %indvars.iv.next.i.i = add nuw nsw i64 %indvars.iv.i.i, 1
  %exitcond.not.i.i = icmp eq i64 %indvars.iv.next.i.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i.i, label %for.body.i13.i.preheader, label %for.body.i.i, !llvm.loop !327

for.body.i13.i.preheader:                         ; preds = %if.end.i.i, %middle.block
  %index.1.i.i.lcssa = phi i64 [ %bin.rdx, %middle.block ], [ %index.1.i.i, %if.end.i.i ]
  %min.iters.check11 = icmp ult i32 %4, 3
  br i1 %min.iters.check11, label %for.body.i13.i.preheader31, label %vector.ph12

vector.ph12:                                      ; preds = %for.body.i13.i.preheader
  %n.vec14 = and i64 %wide.trip.count.i.i, 4294967294
  br label %vector.body10

vector.body10:                                    ; preds = %pred.load.continue26, %vector.ph12
  %index15 = phi i64 [ 0, %vector.ph12 ], [ %index.next16, %pred.load.continue26 ]
  %vec.phi21 = phi i64 [ 0, %vector.ph12 ], [ %predphi27, %pred.load.continue26 ]
  %vec.phi22 = phi i64 [ 0, %vector.ph12 ], [ %predphi28, %pred.load.continue26 ]
  %induction20 = or i64 %index15, 1
  %31 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %index15, i32 2
  %32 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %induction20, i32 2
  %33 = load i32, i32* %31, align 4, !tbaa !185
  %34 = load i32, i32* %32, align 4, !tbaa !185
  %35 = icmp slt i32 %33, 0
  %36 = icmp slt i32 %34, 0
  %37 = sext i32 %33 to i64
  %38 = sext i32 %34 to i64
  br i1 %35, label %pred.load.if23, label %pred.load.continue24

pred.load.if23:                                   ; preds = %vector.body10
  %39 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %index15, i32 1
  %40 = load i32, i32* %39, align 4, !tbaa !189
  br label %pred.load.continue24

pred.load.continue24:                             ; preds = %pred.load.if23, %vector.body10
  %41 = phi i32 [ poison, %vector.body10 ], [ %40, %pred.load.if23 ]
  br i1 %36, label %pred.load.if25, label %pred.load.continue26

pred.load.if25:                                   ; preds = %pred.load.continue24
  %42 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %induction20, i32 1
  %43 = load i32, i32* %42, align 4, !tbaa !189
  br label %pred.load.continue26

pred.load.continue26:                             ; preds = %pred.load.if25, %pred.load.continue24
  %44 = phi i32 [ poison, %pred.load.continue24 ], [ %43, %pred.load.if25 ]
  %45 = add nsw i32 %41, -1
  %46 = add nsw i32 %44, -1
  %47 = sext i32 %45 to i64
  %48 = sext i32 %46 to i64
  %49 = mul nsw i64 %47, %37
  %50 = mul nsw i64 %48, %38
  %51 = select i1 %35, i64 %49, i64 0
  %predphi27 = add i64 %vec.phi21, %51
  %52 = select i1 %36, i64 %50, i64 0
  %predphi28 = add i64 %vec.phi22, %52
  %index.next16 = add i64 %index15, 2
  %53 = icmp eq i64 %index.next16, %n.vec14
  br i1 %53, label %middle.block8, label %vector.body10, !llvm.loop !328

middle.block8:                                    ; preds = %pred.load.continue26
  %bin.rdx29 = add i64 %predphi28, %predphi27
  %cmp.n18 = icmp eq i64 %n.vec14, %wide.trip.count.i.i
  br i1 %cmp.n18, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i.preheader31

for.body.i13.i.preheader31:                       ; preds = %for.body.i13.i.preheader, %middle.block8
  %indvars.iv.i10.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %n.vec14, %middle.block8 ]
  %index.018.i.i.ph = phi i64 [ 0, %for.body.i13.i.preheader ], [ %bin.rdx29, %middle.block8 ]
  br label %for.body.i13.i

for.body.i13.i:                                   ; preds = %for.body.i13.i.preheader31, %if.end.i24.i
  %indvars.iv.i10.i = phi i64 [ %indvars.iv.next.i22.i, %if.end.i24.i ], [ %indvars.iv.i10.i.ph, %for.body.i13.i.preheader31 ]
  %index.018.i.i = phi i64 [ %index.1.i21.i, %if.end.i24.i ], [ %index.018.i.i.ph, %for.body.i13.i.preheader31 ]
  %stride2.i11.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %indvars.iv.i10.i, i32 2
  %54 = load i32, i32* %stride2.i11.i, align 4, !tbaa !185
  %cmp3.i12.i = icmp slt i32 %54, 0
  br i1 %cmp3.i12.i, label %if.then.i20.i, label %if.end.i24.i

if.then.i20.i:                                    ; preds = %for.body.i13.i
  %conv.i14.i = sext i32 %54 to i64
  %extent.i15.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %5, i64 %indvars.iv.i10.i, i32 1
  %55 = load i32, i32* %extent.i15.i, align 4, !tbaa !189
  %sub.i16.i = add nsw i32 %55, -1
  %conv7.i17.i = sext i32 %sub.i16.i to i64
  %mul.i18.i = mul nsw i64 %conv7.i17.i, %conv.i14.i
  %add.i19.i = add nsw i64 %mul.i18.i, %index.018.i.i
  br label %if.end.i24.i

if.end.i24.i:                                     ; preds = %if.then.i20.i, %for.body.i13.i
  %index.1.i21.i = phi i64 [ %add.i19.i, %if.then.i20.i ], [ %index.018.i.i, %for.body.i13.i ]
  %indvars.iv.next.i22.i = add nuw nsw i64 %indvars.iv.i10.i, 1
  %exitcond.not.i23.i = icmp eq i64 %indvars.iv.next.i22.i, %wide.trip.count.i.i
  br i1 %exitcond.not.i23.i, label %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit, label %for.body.i13.i, !llvm.loop !329

_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit: ; preds = %if.end.i24.i, %middle.block8
  %index.1.i21.i.lcssa = phi i64 [ %bin.rdx29, %middle.block8 ], [ %index.1.i21.i, %if.end.i24.i ]
  %add8.i.i = add nsw i64 %index.1.i.i.lcssa, 1
  %sub.i.1 = sub nsw i64 %add8.i.i, %index.1.i21.i.lcssa
  br label %_ZNK15halide_buffer_t13size_in_bytesEv.exit

_ZNK15halide_buffer_t13size_in_bytesEv.exit:      ; preds = %if.end._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit
  %add8.i27.i = phi i64 [ 1, %if.end._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %add8.i.i, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %sub.i.phi = phi i64 [ %sub.i.0, %if.end._ZNK15halide_buffer_t13size_in_bytesEv.exit_crit_edge ], [ %sub.i.1, %_ZNK15halide_buffer_t13size_in_bytesEv.exit.loopexit ]
  %bits.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 4, i32 1
  %56 = load i8, i8* %bits.i.i, align 1, !tbaa !144
  %conv.i.i = zext i8 %56 to i64
  %add.i4.i = add nuw nsw i64 %conv.i.i, 7
  %div.i.i = lshr i64 %add.i4.i, 3
  %mul.i = mul i64 %div.i.i, %sub.i.phi
  %call2 = tail call i8* @halide_malloc(i8* %user_context, i64 %mul.i) #15
  %host = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 2
  store i8* %call2, i8** %host, align 8, !tbaa !180
  %cmp4 = icmp eq i8* %call2, null
  br i1 %cmp4, label %cleanup13, label %if.end6

if.end6:                                          ; preds = %_ZNK15halide_buffer_t13size_in_bytesEv.exit
  %call7 = tail call i32 @halide_device_malloc(i8* %user_context, %struct.halide_buffer_t* nonnull %buf, %struct.halide_device_interface_t* %device_interface) #16
  %cmp8.not = icmp eq i32 %call7, 0
  br i1 %cmp8.not, label %cleanup13, label %if.then9

if.then9:                                         ; preds = %if.end6
  %57 = load i8*, i8** %host, align 8, !tbaa !180
  tail call void @halide_free(i8* %user_context, i8* %57) #15
  store i8* null, i8** %host, align 8, !tbaa !180
  br label %cleanup13

cleanup13:                                        ; preds = %if.then9, %if.end6, %_ZNK15halide_buffer_t13size_in_bytesEv.exit, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.1 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ -1, %_ZNK15halide_buffer_t13size_in_bytesEv.exit ], [ %call7, %if.then9 ], [ 0, %if.end6 ]
  ret i32 %retval.1
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_device_and_host_free(i8* %user_context, %struct.halide_buffer_t* %buf, %struct.halide_device_interface_t* %device_interface) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([36 x i8], [36 x i8]* @.str.30.104, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end16.i.split

if.end16.i.split:                                 ; preds = %if.end16.i
  %call11 = tail call i32 @halide_device_free(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #16
  br label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split, label %cleanup

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %call12 = tail call i32 @halide_device_free(i8* %user_context, %struct.halide_buffer_t* %buf) #16
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split, %if.end16.i.split
  %phi.call = phi i32 [ %call11, %if.end16.i.split ], [ %call12, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.split ]
  %host = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 2
  %4 = load i8*, i8** %host, align 8, !tbaa !180
  %tobool.not = icmp eq i8* %4, null
  br i1 %tobool.not, label %if.end5, label %if.then2

if.then2:                                         ; preds = %if.end
  tail call void @halide_free(i8* %user_context, i8* nonnull %4) #15
  store i8* null, i8** %host, align 8, !tbaa !180
  br label %if.end5

if.end5:                                          ; preds = %if.then2, %if.end
  %flags3.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %5 = load i64, i64* %flags3.i.i, align 8, !tbaa !307
  %and.i.i18 = and i64 %5, -4
  store i64 %and.i.i18, i64* %flags3.i.i, align 8, !tbaa !307
  br label %cleanup

cleanup:                                          ; preds = %if.end5, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.0 = phi i32 [ %phi.call, %if.end5 ], [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ]
  ret i32 %retval.0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_device_wrap_native(i8* %user_context, %struct.halide_buffer_t* %buf, i64 %handle, %struct.halide_device_interface_t* %device_interface) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([26 x i8], [26 x i8]* @.str.31.105, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, label %cleanup12

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %device_interface1.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %.pre = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface1.phi.trans.insert, align 8, !tbaa !306
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, %if.end16.i
  %4 = phi %struct.halide_device_interface_t* [ %.pre, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge ], [ %0, %if.end16.i ]
  %cmp2.not = icmp eq %struct.halide_device_interface_t* %4, null
  %cmp3.not = icmp eq %struct.halide_device_interface_t* %4, %device_interface
  %or.cond = or i1 %cmp2.not, %cmp3.not
  br i1 %or.cond, label %if.end5, label %if.then4

if.then4:                                         ; preds = %if.end
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([64 x i8], [64 x i8]* @.str.32.106, i64 0, i64 0)) #15
  br label %cleanup12

if.end5:                                          ; preds = %if.end
  %device_interface1 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %device_interface, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 0
  %6 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %6() #15
  store %struct.halide_device_interface_t* %device_interface, %struct.halide_device_interface_t** %device_interface1, align 8, !tbaa !306
  %7 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %wrap_native = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %7, i64 0, i32 14
  %8 = load i32 (i8*, %struct.halide_buffer_t*, i64)*, i32 (i8*, %struct.halide_buffer_t*, i64)** %wrap_native, align 8, !tbaa !330
  %call8 = tail call i32 %8(i8* %user_context, %struct.halide_buffer_t* nonnull %buf, i64 %handle) #15
  %9 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %9, i64 0, i32 1
  %10 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %10() #15
  %tobool.not = icmp eq i32 %call8, 0
  %spec.select = select i1 %tobool.not, i32 0, i32 -16
  ret i32 %spec.select

cleanup12:                                        ; preds = %if.then4, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.1 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ -42, %if.then4 ]
  ret i32 %retval.1
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_device_detach_native(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([28 x i8], [28 x i8]* @.str.33.107, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, label %cleanup

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %device_interface1.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %.pre = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface1.phi.trans.insert, align 8, !tbaa !306
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, %if.end16.i
  %4 = phi %struct.halide_device_interface_t* [ %.pre, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge ], [ %0, %if.end16.i ]
  %cmp2.not = icmp eq %struct.halide_device_interface_t* %4, null
  br i1 %cmp2.not, label %cleanup, label %if.then3

if.then3:                                         ; preds = %if.end
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %4, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 0
  %6 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %6() #15
  %7 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %detach_native = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %7, i64 0, i32 15
  %8 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %detach_native, align 8, !tbaa !331
  %call5 = tail call i32 %8(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  %9 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %9, i64 0, i32 1
  %10 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %10() #15
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %11 = load i64, i64* %device, align 8, !tbaa !182
  %cmp7 = icmp eq i64 %11, 0
  br i1 %cmp7, label %do.end, label %if.then8

if.then8:                                         ; preds = %if.then3
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([127 x i8], [127 x i8]* @.str.34.108, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %if.then8, %if.then3
  %tobool.not = icmp eq i32 %call5, 0
  %spec.select = select i1 %tobool.not, i32 0, i32 -33
  ret i32 %spec.select

cleanup:                                          ; preds = %if.end, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.0 = phi i32 [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ 0, %if.end ]
  ret i32 %retval.0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_device_wrap_native(i8* %user_context, %struct.halide_buffer_t* %buf, i64 %handle) local_unnamed_addr #0 {
entry:
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %0 = load i64, i64* %device, align 8, !tbaa !182
  %cmp.not = icmp eq i64 %0, 0
  br i1 %cmp.not, label %if.end, label %return

if.end:                                           ; preds = %entry
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %1 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %1, i64 0, i32 15
  %2 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %2, i64 0, i32 0
  %3 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %3() #15
  store i64 %handle, i64* %device, align 8, !tbaa !182
  br label %return

return:                                           ; preds = %if.end, %entry
  %retval.0 = phi i32 [ 0, %if.end ], [ -32, %entry ]
  ret i32 %retval.0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_device_detach_native(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #0 {
entry:
  %cmp.i = icmp eq %struct.halide_buffer_t* %buf, null
  br i1 %cmp.i, label %if.then.i, label %if.end.i

if.then.i:                                        ; preds = %entry
  %call.i = tail call i32 @halide_error_buffer_is_null(i8* %user_context, i8* getelementptr inbounds ([36 x i8], [36 x i8]* @.str.35, i64 0, i64 0)) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end.i:                                         ; preds = %entry
  %device_interface.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface.i, align 8, !tbaa !306
  %cmp4.not.i = icmp eq %struct.halide_device_interface_t* %0, null
  %device.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %1 = load i64, i64* %device.i, align 8, !tbaa !182
  %cmp5.not.i = icmp eq i64 %1, 0
  %cmp4.not.not.i = xor i1 %cmp4.not.i, true
  %brmerge.i = or i1 %cmp5.not.i, %cmp4.not.not.i
  br i1 %brmerge.i, label %if.end10.i, label %if.then8.i

if.then8.i:                                       ; preds = %if.end.i
  %call9.i = tail call i32 @halide_error_no_device_interface(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end10.i:                                       ; preds = %if.end.i
  %cmp5.not.not.i = xor i1 %cmp5.not.i, true
  %brmerge44.i = or i1 %cmp4.not.i, %cmp5.not.not.i
  br i1 %brmerge44.i, label %if.end16.i, label %if.then14.i

if.then14.i:                                      ; preds = %if.end10.i
  %call15.i = tail call i32 @halide_error_device_interface_no_device(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

if.end16.i:                                       ; preds = %if.end10.i
  %flags.i.i46.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 3
  %2 = load i64, i64* %flags.i.i46.i, align 8, !tbaa !307
  %3 = and i64 %2, 3
  %.not.i = icmp eq i64 %3, 3
  br i1 %.not.i, label %if.then24.i, label %if.end

if.then24.i:                                      ; preds = %if.end16.i
  %call25.i = tail call i32 @halide_error_host_and_device_dirty(i8* %user_context) #15
  br label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit: ; preds = %if.then24.i, %if.then14.i, %if.then8.i, %if.then.i
  %retval.2.i = phi i32 [ %call.i, %if.then.i ], [ %call15.i, %if.then14.i ], [ %call9.i, %if.then8.i ], [ %call25.i, %if.then24.i ]
  %cmp.not = icmp eq i32 %retval.2.i, 0
  br i1 %cmp.not, label %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, label %cleanup

_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge: ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %device.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %.pre = load i64, i64* %device.phi.trans.insert, align 8, !tbaa !182
  br label %if.end

if.end:                                           ; preds = %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge, %if.end16.i
  %4 = phi i64 [ %.pre, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit.if.end_crit_edge ], [ %1, %if.end16.i ]
  %cmp1 = icmp eq i64 %4, 0
  br i1 %cmp1, label %cleanup, label %if.end3

if.end3:                                          ; preds = %if.end
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %5 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %5, i64 0, i32 15
  %6 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %6, i64 0, i32 1
  %7 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %7() #15
  store i64 0, i64* %device, align 8, !tbaa !182
  store %struct.halide_device_interface_t* null, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  br label %cleanup

cleanup:                                          ; preds = %if.end3, %if.end, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit
  %retval.0 = phi i32 [ 0, %if.end3 ], [ %retval.2.i, %_ZN12_GLOBAL__N_126debug_log_and_validate_bufEPvPK15halide_buffer_tPKc.exit ], [ 0, %if.end ]
  ret i32 %retval.0
}

; Function Attrs: nounwind mustprogress
define weak void @halide_device_and_host_free_as_destructor(i8* %user_context, i8* %obj) local_unnamed_addr #0 {
entry:
  %0 = bitcast i8* %obj to %struct.halide_buffer_t*
  %call = tail call i32 @halide_device_and_host_free(i8* %user_context, %struct.halide_buffer_t* %0) #16
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak void @halide_device_host_nop_free(i8* %user_context, i8* %obj) local_unnamed_addr #2 {
entry:
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 @halide_default_buffer_copy(i8* %user_context, %struct.halide_buffer_t* %src, %struct.halide_device_interface_t* %dst_device_interface, %struct.halide_buffer_t* %dst) local_unnamed_addr #2 {
entry:
  ret i32 -39
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_buffer_copy_already_locked(i8* %user_context, %struct.halide_buffer_t* %src, %struct.halide_device_interface_t* %dst_device_interface, %struct.halide_buffer_t* %dst) local_unnamed_addr #0 {
entry:
  %c = alloca %"struct.Halide::Runtime::Internal::device_copy", align 8
  %tobool.not = icmp eq %struct.halide_device_interface_t* %dst_device_interface, null
  br i1 %tobool.not, label %if.end13, label %land.lhs.true

land.lhs.true:                                    ; preds = %entry
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 1
  %0 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %tobool1.not = icmp eq %struct.halide_device_interface_t* %0, null
  %cmp.not = icmp eq %struct.halide_device_interface_t* %0, %dst_device_interface
  %or.cond = or i1 %tobool1.not, %cmp.not
  br i1 %or.cond, label %land.lhs.true5, label %if.then

if.then:                                          ; preds = %land.lhs.true
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([64 x i8], [64 x i8]* @.str.41, i64 0, i64 0)) #15
  br label %cleanup143

land.lhs.true5:                                   ; preds = %land.lhs.true
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 0
  %1 = load i64, i64* %device, align 8, !tbaa !182
  %tobool6.not = icmp eq i64 %1, 0
  br i1 %tobool6.not, label %if.then7, label %if.end13

if.then7:                                         ; preds = %land.lhs.true5
  %call = tail call i32 @halide_device_malloc(i8* %user_context, %struct.halide_buffer_t* nonnull %dst, %struct.halide_device_interface_t* nonnull %dst_device_interface) #16
  %tobool10.not = icmp eq i32 %call, 0
  br i1 %tobool10.not, label %if.end13, label %cleanup143

if.end13:                                         ; preds = %if.then7, %land.lhs.true5, %entry
  %device14 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 0
  %2 = load i64, i64* %device14, align 8, !tbaa !182
  %cmp15.not = icmp eq i64 %2, 0
  %host22.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 2
  %.pre = load i8*, i8** %host22.phi.trans.insert, align 8, !tbaa !180
  %cmp23.not = icmp eq i8* %.pre, null
  br i1 %cmp15.not, label %land.end, label %land.rhs

land.rhs:                                         ; preds = %if.end13
  br i1 %cmp23.not, label %land.end32, label %land.end.thread264

land.end.thread264:                               ; preds = %land.rhs
  %flags.i.i243 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 3
  %3 = load i64, i64* %flags.i.i243, align 8, !tbaa !307
  %and.i.i244 = and i64 %3, 1
  %cmp.i.i.not = icmp ne i64 %and.i.i244, 0
  br label %land.rhs26

land.end:                                         ; preds = %if.end13
  br i1 %cmp23.not, label %land.end32, label %land.end.land.rhs26_crit_edge

land.end.land.rhs26_crit_edge:                    ; preds = %land.end
  %flags.i.i247.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 3
  %.pre1 = load i64, i64* %flags.i.i247.phi.trans.insert, align 8, !tbaa !307
  br label %land.rhs26

land.rhs26:                                       ; preds = %land.end.land.rhs26_crit_edge, %land.end.thread264
  %4 = phi i64 [ %3, %land.end.thread264 ], [ %.pre1, %land.end.land.rhs26_crit_edge ]
  %5 = phi i1 [ %cmp.i.i.not, %land.end.thread264 ], [ true, %land.end.land.rhs26_crit_edge ]
  %and.i.i248 = and i64 %4, 2
  %cmp.i.i249.not = icmp eq i64 %and.i.i248, 0
  br i1 %cmp.i.i249.not, label %land.end32, label %lor.rhs28

lor.rhs28:                                        ; preds = %land.rhs26
  %device_interface29 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 1
  %6 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface29, align 8, !tbaa !306
  %cmp30 = icmp ne %struct.halide_device_interface_t* %6, null
  br label %land.end32

land.end32:                                       ; preds = %lor.rhs28, %land.rhs26, %land.end, %land.rhs
  %cmp23.not263 = phi i1 [ true, %land.end ], [ false, %land.rhs26 ], [ false, %lor.rhs28 ], [ true, %land.rhs ]
  %7 = phi i1 [ true, %land.end ], [ %5, %land.rhs26 ], [ %5, %lor.rhs28 ], [ false, %land.rhs ]
  %8 = phi i1 [ true, %land.end ], [ false, %land.rhs26 ], [ %cmp30, %lor.rhs28 ], [ true, %land.rhs ]
  %host34 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 2
  %9 = load i8*, i8** %host34, align 8, !tbaa !180
  %cmp35.not = icmp eq i8* %9, null
  %cmp20.not = xor i1 %tobool.not, true
  %10 = and i1 %tobool.not, %cmp35.not
  br i1 %10, label %cleanup143, label %if.end41

if.end41:                                         ; preds = %land.end32
  %brmerge229 = or i1 %tobool.not, %7
  br i1 %brmerge229, label %if.then51, label %if.end49

if.end49:                                         ; preds = %if.end41
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %dst_device_interface, i64 0, i32 15
  %11 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %buffer_copy = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %11, i64 0, i32 10
  %12 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)** %buffer_copy, align 8, !tbaa !332
  %call48 = tail call i32 %12(i8* %user_context, %struct.halide_buffer_t* nonnull %src, %struct.halide_device_interface_t* nonnull %dst_device_interface, %struct.halide_buffer_t* nonnull %dst) #15
  %cmp50 = icmp eq i32 %call48, -42
  br i1 %cmp50, label %if.then51, label %if.end117

if.then51:                                        ; preds = %if.end49, %if.end41
  %brmerge231.demorgan = and i1 %cmp23.not263, %cmp35.not
  br i1 %brmerge231.demorgan, label %cleanup143, label %if.end58

if.end58:                                         ; preds = %if.then51
  %brmerge234 = or i1 %8, %cmp20.not
  br i1 %brmerge234, label %if.else, label %if.end117.thread258

if.end117.thread258:                              ; preds = %if.end58
  %13 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %c to i8*
  call void @llvm.lifetime.start.p0i8(i64 416, i8* nonnull %13) #11
  call void @_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b(%"struct.Halide::Runtime::Internal::device_copy"* nonnull sret(%"struct.Halide::Runtime::Internal::device_copy") align 8 %c, %struct.halide_buffer_t* nonnull %src, i1 zeroext true, %struct.halide_buffer_t* nonnull %dst, i1 zeroext true) #16
  call void @_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv(%"struct.Halide::Runtime::Internal::device_copy"* nonnull align 8 dereferenceable(416) %c, i8* %user_context) #16
  call void @llvm.lifetime.end.p0i8(i64 416, i8* nonnull %13) #11
  br label %land.lhs.true126

if.else:                                          ; preds = %if.end58
  %brmerge237 = or i1 %7, %cmp20.not
  br i1 %brmerge237, label %if.else81, label %if.then66

if.then66:                                        ; preds = %if.else
  %device_interface69 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 1
  %14 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface69, align 8, !tbaa !306
  %impl70 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %14, i64 0, i32 15
  %15 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl70, align 8, !tbaa !313
  %buffer_copy71 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %15, i64 0, i32 10
  %16 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)** %buffer_copy71, align 8, !tbaa !332
  %call72 = tail call i32 %16(i8* %user_context, %struct.halide_buffer_t* nonnull %src, %struct.halide_device_interface_t* null, %struct.halide_buffer_t* nonnull %dst) #15
  %cmp73 = icmp eq i32 %call72, -42
  br i1 %cmp73, label %if.then74, label %if.end117

if.then74:                                        ; preds = %if.then66
  %call75 = tail call i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %user_context, %struct.halide_buffer_t* nonnull %src) #16
  %tobool76.not = icmp eq i32 %call75, 0
  br i1 %tobool76.not, label %if.then77, label %cleanup143

if.then77:                                        ; preds = %if.then74
  %call78 = tail call i32 @halide_buffer_copy_already_locked(i8* %user_context, %struct.halide_buffer_t* nonnull %src, %struct.halide_device_interface_t* null, %struct.halide_buffer_t* nonnull %dst) #16
  br label %if.end117

if.else81:                                        ; preds = %if.else
  %brmerge239 = or i1 %7, %cmp35.not
  br i1 %brmerge239, label %if.else98, label %if.then85

if.then85:                                        ; preds = %if.else81
  %device_interface90 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 1
  %17 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface90, align 8, !tbaa !306
  %impl91 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %17, i64 0, i32 15
  %18 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl91, align 8, !tbaa !313
  %buffer_copy92 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %18, i64 0, i32 10
  %19 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)** %buffer_copy92, align 8, !tbaa !332
  %call93 = tail call i32 %19(i8* %user_context, %struct.halide_buffer_t* nonnull %src, %struct.halide_device_interface_t* null, %struct.halide_buffer_t* nonnull %dst) #15
  %cmp94 = icmp eq i32 %call93, 0
  br i1 %cmp94, label %if.then95, label %cleanup143

if.then95:                                        ; preds = %if.then85
  %flags.i.i245 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 3
  %20 = load i64, i64* %flags.i.i245, align 8, !tbaa !307
  %or.i.i246 = or i64 %20, 1
  store i64 %or.i.i246, i64* %flags.i.i245, align 8, !tbaa !307
  %call96 = tail call i32 @copy_to_device_already_locked(i8* %user_context, %struct.halide_buffer_t* nonnull %dst, %struct.halide_device_interface_t* %dst_device_interface) #16
  br label %if.end117

if.else98:                                        ; preds = %if.else81
  br i1 %tobool.not, label %cleanup143, label %if.then100

if.then100:                                       ; preds = %if.else98
  %call103 = tail call i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %user_context, %struct.halide_buffer_t* nonnull %src) #16
  %cmp104 = icmp eq i32 %call103, 0
  br i1 %cmp104, label %if.then105, label %cleanup143

if.then105:                                       ; preds = %if.then100
  %impl106 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %dst_device_interface, i64 0, i32 15
  %21 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl106, align 8, !tbaa !313
  %buffer_copy107 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %21, i64 0, i32 10
  %22 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)** %buffer_copy107, align 8, !tbaa !332
  %call108 = tail call i32 %22(i8* %user_context, %struct.halide_buffer_t* nonnull %src, %struct.halide_device_interface_t* nonnull %dst_device_interface, %struct.halide_buffer_t* nonnull %dst) #15
  br label %if.end117

if.end117:                                        ; preds = %if.then105, %if.then95, %if.then77, %if.then66, %if.end49
  %err.1 = phi i32 [ %call78, %if.then77 ], [ %call72, %if.then66 ], [ %call96, %if.then95 ], [ %call108, %if.then105 ], [ %call48, %if.end49 ]
  %cond = icmp eq i32 %err.1, 0
  br i1 %cond, label %land.lhs.true126, label %cleanup143

land.lhs.true126:                                 ; preds = %if.end117, %if.end117.thread258
  %cmp127.not.old = icmp eq %struct.halide_buffer_t* %dst, %src
  br i1 %cmp127.not.old, label %cleanup143, label %if.then128

if.then128:                                       ; preds = %land.lhs.true126
  %flags.i.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 3
  %23 = load i64, i64* %flags.i.i, align 8, !tbaa !307
  %or.i.i = and i64 %23, -4
  br i1 %tobool.not, label %if.else133, label %if.then130

if.then130:                                       ; preds = %if.then128
  %or.i.i242 = or i64 %or.i.i, 2
  store i64 %or.i.i242, i64* %flags.i.i, align 8, !tbaa !307
  br label %cleanup143

if.else133:                                       ; preds = %if.then128
  %and.i.i251 = or i64 %or.i.i, 1
  store i64 %and.i.i251, i64* %flags.i.i, align 8, !tbaa !307
  br label %cleanup143

cleanup143:                                       ; preds = %if.else133, %if.then130, %land.lhs.true126, %if.end117, %if.then100, %if.else98, %if.then85, %if.then74, %if.then51, %land.end32, %if.then7, %if.then
  %retval.1 = phi i32 [ -42, %if.then ], [ %call, %if.then7 ], [ -34, %land.end32 ], [ 0, %if.then130 ], [ 0, %if.else133 ], [ 0, %land.lhs.true126 ], [ -42, %if.then51 ], [ %err.1, %if.end117 ], [ -42, %if.else98 ], [ %call103, %if.then100 ], [ %call93, %if.then85 ], [ %call75, %if.then74 ]
  ret i32 %retval.1
}

; Function Attrs: nounwind
define weak i32 @halide_buffer_copy(i8* %user_context, %struct.halide_buffer_t* %src, %struct.halide_device_interface_t* %dst_device_interface, %struct.halide_buffer_t* %dst) local_unnamed_addr #4 {
entry:
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  %tobool.not = icmp eq %struct.halide_device_interface_t* %dst_device_interface, null
  br i1 %tobool.not, label %if.end, label %if.then

if.then:                                          ; preds = %entry
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %dst_device_interface, i64 0, i32 15
  %0 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %0, i64 0, i32 0
  %1 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %1() #15
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 1
  %2 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %tobool11.not = icmp eq %struct.halide_device_interface_t* %2, null
  br i1 %tobool11.not, label %if.end16, label %if.then12

if.then12:                                        ; preds = %if.end
  %impl14 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %2, i64 0, i32 15
  %3 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl14, align 8, !tbaa !313
  %use_module15 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %3, i64 0, i32 0
  %4 = load void ()*, void ()** %use_module15, align 8, !tbaa !319
  tail call void %4() #15
  br label %if.end16

if.end16:                                         ; preds = %if.then12, %if.end
  %call = tail call i32 @halide_buffer_copy_already_locked(i8* %user_context, %struct.halide_buffer_t* nonnull %src, %struct.halide_device_interface_t* %dst_device_interface, %struct.halide_buffer_t* %dst) #16
  br i1 %tobool.not, label %if.end20, label %if.then18

if.then18:                                        ; preds = %if.end16
  %impl19 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %dst_device_interface, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl19, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 1
  %6 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %6() #15
  br label %if.end20

if.end20:                                         ; preds = %if.then18, %if.end16
  %7 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %tobool22.not = icmp eq %struct.halide_device_interface_t* %7, null
  br i1 %tobool22.not, label %if.end27, label %if.then23

if.then23:                                        ; preds = %if.end20
  %impl25 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %7, i64 0, i32 15
  %8 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl25, align 8, !tbaa !313
  %release_module26 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %8, i64 0, i32 1
  %9 = load void ()*, void ()** %release_module26, align 8, !tbaa !321
  tail call void %9() #15
  br label %if.end27

if.end27:                                         ; preds = %if.then23, %if.end20
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  ret i32 %call
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_device_crop(i8* %user_context, %struct.halide_buffer_t* %src, %struct.halide_buffer_t* %dst) local_unnamed_addr #0 {
entry:
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([44 x i8], [44 x i8]* @.str.58, i64 0, i64 0)) #15
  ret i32 -40
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_device_slice(i8* %user_context, %struct.halide_buffer_t* %src, i32 %slice_dim, i32 %slice_pos, %struct.halide_buffer_t* %dst) local_unnamed_addr #0 {
entry:
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([43 x i8], [43 x i8]* @.str.59, i64 0, i64 0)) #15
  ret i32 -40
}

; Function Attrs: nounwind
define weak i32 @halide_device_crop(i8* %user_context, %struct.halide_buffer_t* %src, %struct.halide_buffer_t* %dst) local_unnamed_addr #4 {
entry:
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 0
  %0 = load i64, i64* %device, align 8, !tbaa !182
  %tobool.not = icmp eq i64 %0, 0
  br i1 %tobool.not, label %cleanup, label %if.end

if.end:                                           ; preds = %entry
  %device1 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 0
  %1 = load i64, i64* %device1, align 8, !tbaa !182
  %tobool2.not = icmp eq i64 %1, 0
  br i1 %tobool2.not, label %if.end4, label %if.then3

if.then3:                                         ; preds = %if.end
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([52 x i8], [52 x i8]* @.str.60, i64 0, i64 0)) #15
  br label %cleanup

if.end4:                                          ; preds = %if.end
  %dimensions = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 5
  %2 = load i32, i32* %dimensions, align 4, !tbaa !183
  %dimensions5 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 5
  %3 = load i32, i32* %dimensions5, align 4, !tbaa !183
  %cmp.not = icmp eq i32 %2, %3
  br i1 %cmp.not, label %if.end7, label %if.then6

if.then6:                                         ; preds = %if.end4
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([48 x i8], [48 x i8]* @.str.61, i64 0, i64 0)) #15
  br label %cleanup

if.end7:                                          ; preds = %if.end4
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 1
  %4 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %4, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 0
  %6 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %6() #15
  %7 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %impl9 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %7, i64 0, i32 15
  %8 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl9, align 8, !tbaa !313
  %device_crop = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %8, i64 0, i32 11
  %9 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*)** %device_crop, align 8, !tbaa !333
  %call = tail call i32 %9(i8* %user_context, %struct.halide_buffer_t* nonnull %src, %struct.halide_buffer_t* nonnull %dst) #15
  br label %cleanup

cleanup:                                          ; preds = %if.end7, %if.then6, %if.then3, %entry
  %retval.0 = phi i32 [ -41, %if.then3 ], [ -41, %if.then6 ], [ %call, %if.end7 ], [ 0, %entry ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  ret i32 %retval.0
}

; Function Attrs: nounwind
define weak i32 @halide_device_slice(i8* %user_context, %struct.halide_buffer_t* %src, i32 %slice_dim, i32 %slice_pos, %struct.halide_buffer_t* %dst) local_unnamed_addr #4 {
entry:
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 0
  %0 = load i64, i64* %device, align 8, !tbaa !182
  %tobool.not = icmp eq i64 %0, 0
  br i1 %tobool.not, label %cleanup, label %if.end

if.end:                                           ; preds = %entry
  %device1 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 0
  %1 = load i64, i64* %device1, align 8, !tbaa !182
  %tobool2.not = icmp eq i64 %1, 0
  br i1 %tobool2.not, label %if.end4, label %if.then3

if.then3:                                         ; preds = %if.end
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([52 x i8], [52 x i8]* @.str.60, i64 0, i64 0)) #15
  br label %cleanup

if.end4:                                          ; preds = %if.end
  %dimensions = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 5
  %2 = load i32, i32* %dimensions, align 4, !tbaa !183
  %dimensions5 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %dst, i64 0, i32 5
  %3 = load i32, i32* %dimensions5, align 4, !tbaa !183
  %add = add nsw i32 %3, 1
  %cmp.not = icmp eq i32 %2, %add
  br i1 %cmp.not, label %if.end7, label %if.then6

if.then6:                                         ; preds = %if.end4
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([52 x i8], [52 x i8]* @.str.64, i64 0, i64 0)) #15
  br label %cleanup

if.end7:                                          ; preds = %if.end4
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %src, i64 0, i32 1
  %4 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %4, i64 0, i32 15
  %5 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %use_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %5, i64 0, i32 0
  %6 = load void ()*, void ()** %use_module, align 8, !tbaa !319
  tail call void %6() #15
  %7 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %impl9 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %7, i64 0, i32 15
  %8 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl9, align 8, !tbaa !313
  %device_slice = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %8, i64 0, i32 12
  %9 = load i32 (i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*)** %device_slice, align 8, !tbaa !334
  %call = tail call i32 %9(i8* %user_context, %struct.halide_buffer_t* nonnull %src, i32 %slice_dim, i32 %slice_pos, %struct.halide_buffer_t* nonnull %dst) #15
  br label %cleanup

cleanup:                                          ; preds = %if.end7, %if.then6, %if.then3, %entry
  %retval.0 = phi i32 [ -41, %if.then3 ], [ -41, %if.then6 ], [ %call, %if.end7 ], [ 0, %entry ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  ret i32 %retval.0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_default_device_release_crop(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #0 {
entry:
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %0 = load i64, i64* %device, align 8, !tbaa !182
  %tobool.not = icmp eq i64 %0, 0
  br i1 %tobool.not, label %return, label %if.end

if.end:                                           ; preds = %entry
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([44 x i8], [44 x i8]* @.str.58, i64 0, i64 0)) #15
  br label %return

return:                                           ; preds = %if.end, %entry
  %retval.0 = phi i32 [ -40, %if.end ], [ 0, %entry ]
  ret i32 %retval.0
}

; Function Attrs: nounwind
define weak i32 @halide_device_release_crop(i8* %user_context, %struct.halide_buffer_t* %buf) local_unnamed_addr #4 {
entry:
  %device = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 0
  %0 = load i64, i64* %device, align 8, !tbaa !182
  %tobool.not = icmp eq i64 %0, 0
  br i1 %tobool.not, label %return, label %if.then

if.then:                                          ; preds = %entry
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  %device_interface = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %buf, i64 0, i32 1
  %1 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  %impl = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %1, i64 0, i32 15
  %2 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %device_release_crop = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %2, i64 0, i32 13
  %3 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %device_release_crop, align 8, !tbaa !335
  %call = tail call i32 %3(i8* %user_context, %struct.halide_buffer_t* nonnull %buf) #15
  store i64 0, i64* %device, align 8, !tbaa !182
  %4 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %impl, align 8, !tbaa !313
  %release_module = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %4, i64 0, i32 1
  %5 = load void ()*, void ()** %release_module, align 8, !tbaa !321
  tail call void %5() #15
  store %struct.halide_device_interface_t* null, %struct.halide_device_interface_t** %device_interface, align 8, !tbaa !306
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #15
  br label %return

return:                                           ; preds = %if.then, %entry
  %retval.0 = phi i32 [ %call, %if.then ], [ 0, %entry ]
  ret i32 %retval.0
}

; Function Attrs: nounwind willreturn mustprogress
define weak float @halide_float16_bits_to_float(i16 zeroext %bits) local_unnamed_addr #2 {
entry:
  %conv = zext i16 %bits to i32
  %and2 = and i32 %conv, 1023
  %and4 = lshr i32 %conv, 10
  %0 = and i32 %and4, 31
  %cmp = icmp eq i32 %0, 0
  %cmp5 = icmp ne i32 %and2, 0
  %or.cond = and i1 %cmp5, %cmp
  br i1 %or.cond, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %1 = tail call i32 @llvm.ctlz.i32(i32 %and2, i1 true), !range !336
  %sub6 = xor i32 %1, 31
  %shl7 = shl nuw i32 1, %sub6
  %neg = xor i32 %shl7, -1
  %and8 = and i32 %and2, %neg
  %sub9 = sub nsw i32 23, %sub6
  %shl10 = shl i32 %and8, %sub9
  %add11.neg = mul nsw i32 %1, -8388608
  %shl12 = add i32 %add11.neg, 1124073472
  br label %if.end28

if.else:                                          ; preds = %entry
  %shl14 = shl nuw nsw i32 %and2, 13
  br i1 %cmp, label %if.end28, label %if.else18

if.else18:                                        ; preds = %if.else
  %cmp19 = icmp eq i32 %0, 31
  br i1 %cmp19, label %if.end28, label %if.else21

if.else21:                                        ; preds = %if.else18
  %add22 = shl nuw nsw i32 %0, 23
  %phi.bo = add nuw nsw i32 %add22, 939524096
  br label %if.end28

if.end28:                                         ; preds = %if.else21, %if.else18, %if.else, %if.then
  %shl14.sink = phi i32 [ %shl12, %if.then ], [ %shl14, %if.else18 ], [ %shl14, %if.else ], [ %shl14, %if.else21 ]
  %reEncodedExponent15.0.sink = phi i32 [ %shl10, %if.then ], [ 2139095040, %if.else18 ], [ 0, %if.else ], [ %phi.bo, %if.else21 ]
  %bits.signext = sext i16 %bits to i32
  %shl = and i32 %bits.signext, -2147483648
  %or25 = or i32 %shl14.sink, %shl
  %or26 = or i32 %or25, %reEncodedExponent15.0.sink
  %result.sroa.0.0 = bitcast i32 %or26 to float
  ret float %result.sroa.0.0
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i32 @llvm.ctlz.i32(i32, i1 immarg) #7

; Function Attrs: nounwind willreturn mustprogress
define weak double @halide_float16_bits_to_double(i16 zeroext %bits) local_unnamed_addr #2 {
entry:
  %call = tail call float @halide_float16_bits_to_float(i16 zeroext %bits) #16
  %conv = fpext float %call to double
  ret double %conv
}

; Function Attrs: nounwind
define weak i32 @halide_error_bounds_inference_call_failed(i8* %user_context, i8* %extern_stage_name, i32 %result) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i71 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([41 x i8], [41 x i8]* @.str.111, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i72 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([41 x i8], [41 x i8]* @.str.111, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i71, %entry.split ], [ %call.i72, %if.then6.i ]
  %ref.tmp.sroa.16.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i10 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.16.0, i8* %extern_stage_name) #15
  %call.i13 = tail call i8* @halide_string_to_string(i8* %call.i10, i8* %ref.tmp.sroa.16.0, i8* getelementptr inbounds ([27 x i8], [27 x i8]* @.str.1.112, i64 0, i64 0)) #15
  %conv.i = sext i32 %result to i64
  %call.i16 = tail call i8* @halide_int64_to_string(i8* %call.i13, i8* %ref.tmp.sroa.16.0, i64 %conv.i, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i16 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 %result
}

; Function Attrs: nounwind
define weak i32 @halide_error_extern_stage_failed(i8* %user_context, i8* %extern_stage_name, i32 %result) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i71 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.2.113, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i72 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.2.113, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i71, %entry.split ], [ %call.i72, %if.then6.i ]
  %ref.tmp.sroa.16.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i10 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.16.0, i8* %extern_stage_name) #15
  %call.i13 = tail call i8* @halide_string_to_string(i8* %call.i10, i8* %ref.tmp.sroa.16.0, i8* getelementptr inbounds ([27 x i8], [27 x i8]* @.str.1.112, i64 0, i64 0)) #15
  %conv.i = sext i32 %result to i64
  %call.i16 = tail call i8* @halide_int64_to_string(i8* %call.i13, i8* %ref.tmp.sroa.16.0, i64 %conv.i, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i16 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 %result
}

; Function Attrs: nounwind
define weak i32 @halide_error_explicit_bounds_too_small(i8* %user_context, i8* %func_name, i8* %var_name, i32 %min_bound, i32 %max_bound, i32 %min_required, i32 %max_required) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i151 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([18 x i8], [18 x i8]* @.str.3.114, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i152 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([18 x i8], [18 x i8]* @.str.3.114, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i151, %entry.split ], [ %call.i152, %if.then6.i ]
  %ref.tmp.sroa.34.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i18 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.34.0, i8* %var_name) #15
  %call.i21 = tail call i8* @halide_string_to_string(i8* %call.i18, i8* %ref.tmp.sroa.34.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.4.115, i64 0, i64 0)) #15
  %call.i24 = tail call i8* @halide_string_to_string(i8* %call.i21, i8* %ref.tmp.sroa.34.0, i8* %func_name) #15
  %call.i27 = tail call i8* @halide_string_to_string(i8* %call.i24, i8* %ref.tmp.sroa.34.0, i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str.5.116, i64 0, i64 0)) #15
  %conv.i = sext i32 %min_bound to i64
  %call.i30 = tail call i8* @halide_int64_to_string(i8* %call.i27, i8* %ref.tmp.sroa.34.0, i64 %conv.i, i32 1) #15
  %call.i33 = tail call i8* @halide_string_to_string(i8* %call.i30, i8* %ref.tmp.sroa.34.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.6.117, i64 0, i64 0)) #15
  %conv.i36 = sext i32 %max_bound to i64
  %call.i37 = tail call i8* @halide_int64_to_string(i8* %call.i33, i8* %ref.tmp.sroa.34.0, i64 %conv.i36, i32 1) #15
  %call.i40 = tail call i8* @halide_string_to_string(i8* %call.i37, i8* %ref.tmp.sroa.34.0, i8* getelementptr inbounds ([38 x i8], [38 x i8]* @.str.7.118, i64 0, i64 0)) #15
  %conv.i43 = sext i32 %min_required to i64
  %call.i44 = tail call i8* @halide_int64_to_string(i8* %call.i40, i8* %ref.tmp.sroa.34.0, i64 %conv.i43, i32 1) #15
  %call.i47 = tail call i8* @halide_string_to_string(i8* %call.i44, i8* %ref.tmp.sroa.34.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.6.117, i64 0, i64 0)) #15
  %conv.i50 = sext i32 %max_required to i64
  %call.i51 = tail call i8* @halide_int64_to_string(i8* %call.i47, i8* %ref.tmp.sroa.34.0, i64 %conv.i50, i32 1) #15
  %call.i54 = tail call i8* @halide_string_to_string(i8* %call.i51, i8* %ref.tmp.sroa.34.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.8.119, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i54 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -2
}

; Function Attrs: nounwind
define weak i32 @halide_error_bad_type(i8* %user_context, i8* %func_name, i32 %type_given_bits, i32 %correct_type_bits) local_unnamed_addr #4 {
entry:
  %type_given_bits.addr = alloca i32, align 4
  %correct_type_bits.addr = alloca i32, align 4
  %correct_type = alloca %struct.halide_type_t, align 2
  %type_given = alloca %struct.halide_type_t, align 2
  store i32 %type_given_bits, i32* %type_given_bits.addr, align 4, !tbaa !41
  store i32 %correct_type_bits, i32* %correct_type_bits.addr, align 4, !tbaa !41
  %0 = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %correct_type, i64 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #11
  store i8 0, i8* %0, align 2, !tbaa !304
  %bits.i = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %correct_type, i64 0, i32 1
  store i8 0, i8* %bits.i, align 1, !tbaa !144
  %lanes.i = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %correct_type, i64 0, i32 2
  store i16 0, i16* %lanes.i, align 2, !tbaa !305
  %1 = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %type_given, i64 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #11
  store i8 0, i8* %1, align 2, !tbaa !304
  %bits.i8 = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %type_given, i64 0, i32 1
  store i8 0, i8* %bits.i8, align 1, !tbaa !144
  %lanes.i9 = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %type_given, i64 0, i32 2
  store i16 0, i16* %lanes.i9, align 2, !tbaa !305
  %2 = bitcast i32* %correct_type_bits.addr to i8*
  %call = call i8* @memcpy(i8* nonnull %0, i8* nonnull %2, i64 4) #15
  %3 = bitcast i32* %type_given_bits.addr to i8*
  %call1 = call i8* @memcpy(i8* nonnull %1, i8* nonnull %3, i64 4) #15
  %call.i = call i8* @malloc(i64 1024) #15
  %tobool.not.i12 = icmp eq i8* %call.i, null
  br i1 %tobool.not.i12, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i151 = call i8* @halide_string_to_string(i8* null, i8* null, i8* %func_name) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i152 = call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* %func_name) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i151, %entry.split ], [ %call.i152, %if.then6.i ]
  %ref.tmp.sroa.18.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i18 = call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.18.0, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.9.120, i64 0, i64 0)) #15
  %call.i21 = call i8* @halide_type_to_string(i8* %call.i18, i8* %ref.tmp.sroa.18.0, %struct.halide_type_t* nonnull %correct_type) #15
  %call.i24 = call i8* @halide_string_to_string(i8* %call.i21, i8* %ref.tmp.sroa.18.0, i8* getelementptr inbounds ([38 x i8], [38 x i8]* @.str.10.121, i64 0, i64 0)) #15
  %call.i27 = call i8* @halide_type_to_string(i8* %call.i24, i8* %ref.tmp.sroa.18.0, %struct.halide_type_t* nonnull %type_given) #15
  br i1 %tobool.not.i12, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i27 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  call void @free(i8* %call.i) #15
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #11
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #11
  ret i32 -3
}

; Function Attrs: nounwind
define weak i32 @halide_error_bad_dimensions(i8* %user_context, i8* %func_name, i32 %dimensions_given, i32 %correct_dimensions) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* %func_name) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* %func_name) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([31 x i8], [31 x i8]* @.str.11.122, i64 0, i64 0)) #15
  %conv.i = sext i32 %correct_dimensions to i64
  %call.i14 = tail call i8* @halide_int64_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i64 %conv.i, i32 1) #15
  %call.i17 = tail call i8* @halide_string_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([43 x i8], [43 x i8]* @.str.12.123, i64 0, i64 0)) #15
  %conv.i20 = sext i32 %dimensions_given to i64
  %call.i21 = tail call i8* @halide_int64_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i64 %conv.i20, i32 1) #15
  %call.i24 = tail call i8* @halide_string_to_string(i8* %call.i21, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str.13.124, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i24 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -43
}

; Function Attrs: nounwind
define weak i32 @halide_error_access_out_of_bounds(i8* %user_context, i8* %func_name, i32 %dimension, i32 %min_touched, i32 %max_touched, i32 %min_valid, i32 %max_valid) local_unnamed_addr #4 {
entry:
  %cmp = icmp slt i32 %min_touched, %min_valid
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %if.then.split, label %if.then6.i

if.then.split:                                    ; preds = %if.then
  %call.i271 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* %func_name) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %if.then
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i272 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* %func_name) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %if.then.split, %if.then6.i
  %phi.call = phi i8* [ %call.i271, %if.then.split ], [ %call.i272, %if.then6.i ]
  %ref.tmp.sroa.22.0 = phi i8* [ null, %if.then.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i30 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([17 x i8], [17 x i8]* @.str.14.125, i64 0, i64 0)) #15
  %conv.i = sext i32 %min_touched to i64
  %call.i33 = tail call i8* @halide_int64_to_string(i8* %call.i30, i8* %ref.tmp.sroa.22.0, i64 %conv.i, i32 1) #15
  %call.i36 = tail call i8* @halide_string_to_string(i8* %call.i33, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([28 x i8], [28 x i8]* @.str.15.126, i64 0, i64 0)) #15
  %conv.i39 = sext i32 %min_valid to i64
  %call.i40 = tail call i8* @halide_int64_to_string(i8* %call.i36, i8* %ref.tmp.sroa.22.0, i64 %conv.i39, i32 1) #15
  %call.i43 = tail call i8* @halide_string_to_string(i8* %call.i40, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([16 x i8], [16 x i8]* @.str.16.127, i64 0, i64 0)) #15
  %conv.i46 = sext i32 %dimension to i64
  %call.i47 = tail call i8* @halide_int64_to_string(i8* %call.i43, i8* %ref.tmp.sroa.22.0, i64 %conv.i46, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %if.end17.sink.split

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i47 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %if.end17.sink.split

if.else:                                          ; preds = %entry
  %cmp7 = icmp sgt i32 %max_touched, %max_valid
  br i1 %cmp7, label %if.then8, label %if.end17

if.then8:                                         ; preds = %if.else
  %call.i53 = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i56 = icmp eq i8* %call.i53, null
  br i1 %tobool.not.i56, label %if.then8.split, label %if.then6.i59

if.then8.split:                                   ; preds = %if.then8
  %call.i653 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* %func_name) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit62

if.then6.i59:                                     ; preds = %if.then8
  %add.ptr.i57 = getelementptr inbounds i8, i8* %call.i53, i64 1023
  store i8 0, i8* %add.ptr.i57, align 1, !tbaa !18
  %call.i654 = tail call i8* @halide_string_to_string(i8* nonnull %call.i53, i8* nonnull %add.ptr.i57, i8* %func_name) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit62

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit62: ; preds = %if.then8.split, %if.then6.i59
  %phi.call5 = phi i8* [ %call.i653, %if.then8.split ], [ %call.i654, %if.then6.i59 ]
  %ref.tmp9.sroa.22.0 = phi i8* [ null, %if.then8.split ], [ %add.ptr.i57, %if.then6.i59 ]
  %call.i68 = tail call i8* @halide_string_to_string(i8* %phi.call5, i8* %ref.tmp9.sroa.22.0, i8* getelementptr inbounds ([17 x i8], [17 x i8]* @.str.14.125, i64 0, i64 0)) #15
  %conv.i71 = sext i32 %max_touched to i64
  %call.i72 = tail call i8* @halide_int64_to_string(i8* %call.i68, i8* %ref.tmp9.sroa.22.0, i64 %conv.i71, i32 1) #15
  %call.i75 = tail call i8* @halide_string_to_string(i8* %call.i72, i8* %ref.tmp9.sroa.22.0, i8* getelementptr inbounds ([28 x i8], [28 x i8]* @.str.17.128, i64 0, i64 0)) #15
  %conv.i78 = sext i32 %max_valid to i64
  %call.i79 = tail call i8* @halide_int64_to_string(i8* %call.i75, i8* %ref.tmp9.sroa.22.0, i64 %conv.i78, i32 1) #15
  %call.i82 = tail call i8* @halide_string_to_string(i8* %call.i79, i8* %ref.tmp9.sroa.22.0, i8* getelementptr inbounds ([16 x i8], [16 x i8]* @.str.16.127, i64 0, i64 0)) #15
  %conv.i85 = sext i32 %dimension to i64
  %call.i86 = tail call i8* @halide_int64_to_string(i8* %call.i82, i8* %ref.tmp9.sroa.22.0, i64 %conv.i85, i32 1) #15
  br i1 %tobool.not.i56, label %if.then.i90, label %if.else.i100

if.then.i90:                                      ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit62
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %if.end17.sink.split

if.else.i100:                                     ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit62
  %sub.ptr.lhs.cast.i.i95 = ptrtoint i8* %call.i86 to i64
  %sub.ptr.rhs.cast.i.i96 = ptrtoint i8* %call.i53 to i64
  %sub.ptr.sub.i.i97 = sub i64 1, %sub.ptr.rhs.cast.i.i96
  %add.i.i98 = add i64 %sub.ptr.sub.i.i97, %sub.ptr.lhs.cast.i.i95
  %call.i.i99 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i53, i64 %add.i.i98) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i53) #15
  br label %if.end17.sink.split

if.end17.sink.split:                              ; preds = %if.else.i100, %if.then.i90, %if.else.i, %if.then.i
  %call.i53.sink = phi i8* [ %call.i, %if.else.i ], [ null, %if.then.i ], [ %call.i53, %if.else.i100 ], [ null, %if.then.i90 ]
  tail call void @free(i8* %call.i53.sink) #15
  br label %if.end17

if.end17:                                         ; preds = %if.end17.sink.split, %if.else
  ret i32 -4
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_allocation_too_large(i8* %user_context, i8* %buffer_name, i64 %allocation_size, i64 %max_size) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([29 x i8], [29 x i8]* @.str.18.129, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([29 x i8], [29 x i8]* @.str.18.129, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %buffer_name) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.130, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_uint64_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, i64 %allocation_size, i32 1) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([37 x i8], [37 x i8]* @.str.20.131, i64 0, i64 0)) #15
  %call.i23 = tail call i8* @halide_uint64_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, i64 %max_size, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -5
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_extents_negative(i8* %user_context, i8* %buffer_name, i32 %dimension, i32 %extent) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i91 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.21.132, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i92 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.21.132, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i91, %entry.split ], [ %call.i92, %if.then6.i ]
  %ref.tmp.sroa.22.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i12 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.22.0, i8* %buffer_name) #15
  %call.i15 = tail call i8* @halide_string_to_string(i8* %call.i12, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str.22.133, i64 0, i64 0)) #15
  %conv.i = sext i32 %dimension to i64
  %call.i18 = tail call i8* @halide_int64_to_string(i8* %call.i15, i8* %ref.tmp.sroa.22.0, i64 %conv.i, i32 1) #15
  %call.i21 = tail call i8* @halide_string_to_string(i8* %call.i18, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([15 x i8], [15 x i8]* @.str.23.134, i64 0, i64 0)) #15
  %conv.i24 = sext i32 %extent to i64
  %call.i25 = tail call i8* @halide_int64_to_string(i8* %call.i21, i8* %ref.tmp.sroa.22.0, i64 %conv.i24, i32 1) #15
  %call.i28 = tail call i8* @halide_string_to_string(i8* %call.i25, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.8.119, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i28 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -28
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_extents_too_large(i8* %user_context, i8* %buffer_name, i64 %actual_size, i64 %max_size) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([31 x i8], [31 x i8]* @.str.24.135, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([31 x i8], [31 x i8]* @.str.24.135, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %buffer_name) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.130, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_int64_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, i64 %actual_size, i32 1) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([37 x i8], [37 x i8]* @.str.20.131, i64 0, i64 0)) #15
  %call.i23 = tail call i8* @halide_int64_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, i64 %max_size, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -6
}

; Function Attrs: nounwind
define weak i32 @halide_error_constraints_make_required_region_smaller(i8* %user_context, i8* %buffer_name, i32 %dimension, i32 %constrained_min, i32 %constrained_extent, i32 %required_min, i32 %required_extent) local_unnamed_addr #4 {
entry:
  %add = add i32 %required_min, -1
  %sub = add i32 %add, %required_extent
  %add1 = add i32 %constrained_min, -1
  %sub2 = add i32 %add1, %constrained_extent
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i231 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([29 x i8], [29 x i8]* @.str.25.136, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i232 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([29 x i8], [29 x i8]* @.str.25.136, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i231, %entry.split ], [ %call.i232, %if.then6.i ]
  %ref.tmp.sroa.38.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i26 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.38.0, i8* %buffer_name) #15
  %call.i29 = tail call i8* @halide_string_to_string(i8* %call.i26, i8* %ref.tmp.sroa.38.0, i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.26.137, i64 0, i64 0)) #15
  %conv.i = sext i32 %dimension to i64
  %call.i32 = tail call i8* @halide_int64_to_string(i8* %call.i29, i8* %ref.tmp.sroa.38.0, i64 %conv.i, i32 1) #15
  %call.i35 = tail call i8* @halide_string_to_string(i8* %call.i32, i8* %ref.tmp.sroa.38.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.27.138, i64 0, i64 0)) #15
  %call.i38 = tail call i8* @halide_string_to_string(i8* %call.i35, i8* %ref.tmp.sroa.38.0, i8* getelementptr inbounds ([16 x i8], [16 x i8]* @.str.28.139, i64 0, i64 0)) #15
  %conv.i41 = sext i32 %required_min to i64
  %call.i42 = tail call i8* @halide_int64_to_string(i8* %call.i38, i8* %ref.tmp.sroa.38.0, i64 %conv.i41, i32 1) #15
  %call.i45 = tail call i8* @halide_string_to_string(i8* %call.i42, i8* %ref.tmp.sroa.38.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.6.117, i64 0, i64 0)) #15
  %conv.i48 = sext i32 %sub to i64
  %call.i49 = tail call i8* @halide_int64_to_string(i8* %call.i45, i8* %ref.tmp.sroa.38.0, i64 %conv.i48, i32 1) #15
  %call.i52 = tail call i8* @halide_string_to_string(i8* %call.i49, i8* %ref.tmp.sroa.38.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.27.138, i64 0, i64 0)) #15
  %call.i55 = tail call i8* @halide_string_to_string(i8* %call.i52, i8* %ref.tmp.sroa.38.0, i8* getelementptr inbounds ([19 x i8], [19 x i8]* @.str.29.140, i64 0, i64 0)) #15
  %conv.i58 = sext i32 %constrained_min to i64
  %call.i59 = tail call i8* @halide_int64_to_string(i8* %call.i55, i8* %ref.tmp.sroa.38.0, i64 %conv.i58, i32 1) #15
  %call.i62 = tail call i8* @halide_string_to_string(i8* %call.i59, i8* %ref.tmp.sroa.38.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.6.117, i64 0, i64 0)) #15
  %conv.i65 = sext i32 %sub2 to i64
  %call.i66 = tail call i8* @halide_int64_to_string(i8* %call.i62, i8* %ref.tmp.sroa.38.0, i64 %conv.i65, i32 1) #15
  %call.i69 = tail call i8* @halide_string_to_string(i8* %call.i66, i8* %ref.tmp.sroa.38.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.141, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i69 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -7
}

; Function Attrs: nounwind
define weak i32 @halide_error_constraint_violated(i8* %user_context, i8* %var, i32 %val, i8* %constrained_var, i32 %constrained_val) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i111 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.31.142, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i112 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.31.142, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i111, %entry.split ], [ %call.i112, %if.then6.i ]
  %ref.tmp.sroa.26.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i14 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.26.0, i8* %var) #15
  %call.i17 = tail call i8* @halide_string_to_string(i8* %call.i14, i8* %ref.tmp.sroa.26.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.32.143, i64 0, i64 0)) #15
  %conv.i = sext i32 %val to i64
  %call.i20 = tail call i8* @halide_int64_to_string(i8* %call.i17, i8* %ref.tmp.sroa.26.0, i64 %conv.i, i32 1) #15
  %call.i23 = tail call i8* @halide_string_to_string(i8* %call.i20, i8* %ref.tmp.sroa.26.0, i8* getelementptr inbounds ([6 x i8], [6 x i8]* @.str.33.144, i64 0, i64 0)) #15
  %call.i26 = tail call i8* @halide_string_to_string(i8* %call.i23, i8* %ref.tmp.sroa.26.0, i8* %constrained_var) #15
  %call.i29 = tail call i8* @halide_string_to_string(i8* %call.i26, i8* %ref.tmp.sroa.26.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.32.143, i64 0, i64 0)) #15
  %conv.i32 = sext i32 %constrained_val to i64
  %call.i33 = tail call i8* @halide_int64_to_string(i8* %call.i29, i8* %ref.tmp.sroa.26.0, i64 %conv.i32, i32 1) #15
  %call.i36 = tail call i8* @halide_string_to_string(i8* %call.i33, i8* %ref.tmp.sroa.26.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.8.119, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i36 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -8
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_small_i64(i8* %user_context, i8* %param_name, i64 %val, i64 %min_val) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %param_name) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.130, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_int64_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, i64 %val, i32 1) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.35.146, i64 0, i64 0)) #15
  %call.i23 = tail call i8* @halide_int64_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, i64 %min_val, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -9
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_small_u64(i8* %user_context, i8* %param_name, i64 %val, i64 %min_val) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %param_name) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.130, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_uint64_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, i64 %val, i32 1) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.35.146, i64 0, i64 0)) #15
  %call.i23 = tail call i8* @halide_uint64_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, i64 %min_val, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -9
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_small_f64(i8* %user_context, i8* %param_name, double %val, double %min_val) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %param_name) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.130, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_double_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, double %val, i32 1) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.35.146, i64 0, i64 0)) #15
  %call.i23 = tail call i8* @halide_double_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, double %min_val, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -9
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_large_i64(i8* %user_context, i8* %param_name, i64 %val, i64 %max_val) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %param_name) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.130, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_int64_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, i64 %val, i32 1) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.36, i64 0, i64 0)) #15
  %call.i23 = tail call i8* @halide_int64_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, i64 %max_val, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -10
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_large_u64(i8* %user_context, i8* %param_name, i64 %val, i64 %max_val) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %param_name) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.130, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_uint64_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, i64 %val, i32 1) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.36, i64 0, i64 0)) #15
  %call.i23 = tail call i8* @halide_uint64_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, i64 %max_val, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -10
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_large_f64(i8* %user_context, i8* %param_name, double %val, double %max_val) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.145, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %param_name) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.130, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_double_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, double %val, i32 1) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.36, i64 0, i64 0)) #15
  %call.i23 = tail call i8* @halide_double_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, double %max_val, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -10
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_error_out_of_memory(i8* %user_context) local_unnamed_addr #0 {
entry:
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([47 x i8], [47 x i8]* @.str.37, i64 0, i64 0)) #15
  ret i32 -11
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_argument_is_null(i8* %user_context, i8* %buffer_name) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i51 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([17 x i8], [17 x i8]* @.str.38, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i52 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([17 x i8], [17 x i8]* @.str.38, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i51, %entry.split ], [ %call.i52, %if.then6.i ]
  %ref.tmp.sroa.14.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i8 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.14.0, i8* %buffer_name) #15
  %call.i11 = tail call i8* @halide_string_to_string(i8* %call.i8, i8* %ref.tmp.sroa.14.0, i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str.39, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i11 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -12
}

; Function Attrs: nounwind
define weak i32 @halide_error_debug_to_file_failed(i8* %user_context, i8* %func, i8* %filename, i32 %error_code) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i81 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([25 x i8], [25 x i8]* @.str.40, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i82 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([25 x i8], [25 x i8]* @.str.40, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i81, %entry.split ], [ %call.i82, %if.then6.i ]
  %ref.tmp.sroa.20.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i11 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.20.0, i8* %func) #15
  %call.i14 = tail call i8* @halide_string_to_string(i8* %call.i11, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([10 x i8], [10 x i8]* @.str.41.147, i64 0, i64 0)) #15
  %call.i17 = tail call i8* @halide_string_to_string(i8* %call.i14, i8* %ref.tmp.sroa.20.0, i8* %filename) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.20.0, i8* getelementptr inbounds ([13 x i8], [13 x i8]* @.str.42, i64 0, i64 0)) #15
  %conv.i = sext i32 %error_code to i64
  %call.i23 = tail call i8* @halide_int64_to_string(i8* %call.i20, i8* %ref.tmp.sroa.20.0, i64 %conv.i, i32 1) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i23 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -13
}

; Function Attrs: nounwind
define weak i32 @halide_error_unaligned_host_ptr(i8* %user_context, i8* %func, i32 %alignment) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i71 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([21 x i8], [21 x i8]* @.str.43, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i72 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([21 x i8], [21 x i8]* @.str.43, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i71, %entry.split ], [ %call.i72, %if.then6.i ]
  %ref.tmp.sroa.18.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i10 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.18.0, i8* %func) #15
  %call.i13 = tail call i8* @halide_string_to_string(i8* %call.i10, i8* %ref.tmp.sroa.18.0, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.44, i64 0, i64 0)) #15
  %conv.i = sext i32 %alignment to i64
  %call.i16 = tail call i8* @halide_int64_to_string(i8* %call.i13, i8* %ref.tmp.sroa.18.0, i64 %conv.i, i32 1) #15
  %call.i19 = tail call i8* @halide_string_to_string(i8* %call.i16, i8* %ref.tmp.sroa.18.0, i8* getelementptr inbounds ([17 x i8], [17 x i8]* @.str.45, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i19 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -24
}

; Function Attrs: nounwind
define weak i32 @halide_error_device_dirty_with_no_device_support(i8* %user_context, i8* %func) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i61 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str.46, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i62 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str.46, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i61, %entry.split ], [ %call.i62, %if.then6.i ]
  %ref.tmp.sroa.16.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i9 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.16.0, i8* %func) #15
  %call.i12 = tail call i8* @halide_string_to_string(i8* %call.i9, i8* %ref.tmp.sroa.16.0, i8* getelementptr inbounds ([53 x i8], [53 x i8]* @.str.47, i64 0, i64 0)) #15
  %call.i15 = tail call i8* @halide_string_to_string(i8* %call.i12, i8* %ref.tmp.sroa.16.0, i8* getelementptr inbounds ([43 x i8], [43 x i8]* @.str.48, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i15 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -44
}

; Function Attrs: nounwind
define weak i32 @halide_error_host_is_null(i8* %user_context, i8* %func) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i51 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([21 x i8], [21 x i8]* @.str.43, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i52 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([21 x i8], [21 x i8]* @.str.43, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i51, %entry.split ], [ %call.i52, %if.then6.i ]
  %ref.tmp.sroa.14.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i8 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.14.0, i8* %func) #15
  %call.i11 = tail call i8* @halide_string_to_string(i8* %call.i8, i8* %ref.tmp.sroa.14.0, i8* getelementptr inbounds ([55 x i8], [55 x i8]* @.str.49, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i11 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -34
}

; Function Attrs: nounwind
define weak i32 @halide_error_bad_fold(i8* %user_context, i8* %func_name, i8* %var_name, i8* %loop_name) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i91 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([30 x i8], [30 x i8]* @.str.50.148, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i92 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([30 x i8], [30 x i8]* @.str.50.148, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i91, %entry.split ], [ %call.i92, %if.then6.i ]
  %ref.tmp.sroa.22.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i12 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.22.0, i8* %var_name) #15
  %call.i15 = tail call i8* @halide_string_to_string(i8* %call.i12, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i64 0, i64 0)) #15
  %call.i18 = tail call i8* @halide_string_to_string(i8* %call.i15, i8* %ref.tmp.sroa.22.0, i8* %func_name) #15
  %call.i21 = tail call i8* @halide_string_to_string(i8* %call.i18, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([36 x i8], [36 x i8]* @.str.52, i64 0, i64 0)) #15
  %call.i24 = tail call i8* @halide_string_to_string(i8* %call.i21, i8* %ref.tmp.sroa.22.0, i8* %loop_name) #15
  %call.i27 = tail call i8* @halide_string_to_string(i8* %call.i24, i8* %ref.tmp.sroa.22.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.141, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i27 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -25
}

; Function Attrs: nounwind
define weak i32 @halide_error_bad_extern_fold(i8* %user_context, i8* %func_name, i32 %dim, i32 %min, i32 %extent, i32 %valid_min, i32 %fold_factor) local_unnamed_addr #4 {
entry:
  %cmp = icmp slt i32 %min, %valid_min
  br i1 %cmp, label %if.then, label %lor.lhs.false

lor.lhs.false:                                    ; preds = %entry
  %add = add nsw i32 %extent, %min
  %add1 = add nsw i32 %fold_factor, %valid_min
  %cmp2 = icmp sgt i32 %add, %add1
  br i1 %cmp2, label %if.then, label %if.else

if.then:                                          ; preds = %lor.lhs.false, %entry
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %if.then.split, label %if.then6.i

if.then.split:                                    ; preds = %if.then
  %call.i521 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.53, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %if.then
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i522 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.53, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %if.then.split, %if.then6.i
  %phi.call = phi i8* [ %call.i521, %if.then.split ], [ %call.i522, %if.then6.i ]
  %ref.tmp.sroa.36.0 = phi i8* [ null, %if.then.split ], [ %add.ptr.i, %if.then6.i ]
  %conv.i = sext i32 %dim to i64
  %call.i55 = tail call i8* @halide_int64_to_string(i8* %phi.call, i8* %ref.tmp.sroa.36.0, i64 %conv.i, i32 1) #15
  %call.i58 = tail call i8* @halide_string_to_string(i8* %call.i55, i8* %ref.tmp.sroa.36.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i64 0, i64 0)) #15
  %call.i61 = tail call i8* @halide_string_to_string(i8* %call.i58, i8* %ref.tmp.sroa.36.0, i8* %func_name) #15
  %call.i64 = tail call i8* @halide_string_to_string(i8* %call.i61, i8* %ref.tmp.sroa.36.0, i8* getelementptr inbounds ([36 x i8], [36 x i8]* @.str.54, i64 0, i64 0)) #15
  %conv.i67 = sext i32 %min to i64
  %call.i68 = tail call i8* @halide_int64_to_string(i8* %call.i64, i8* %ref.tmp.sroa.36.0, i64 %conv.i67, i32 1) #15
  %call.i71 = tail call i8* @halide_string_to_string(i8* %call.i68, i8* %ref.tmp.sroa.36.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #15
  %add9 = add nsw i32 %extent, %min
  %sub = add nsw i32 %add9, -1
  %conv.i74 = sext i32 %sub to i64
  %call.i75 = tail call i8* @halide_int64_to_string(i8* %call.i71, i8* %ref.tmp.sroa.36.0, i64 %conv.i74, i32 1) #15
  %call.i78 = tail call i8* @halide_string_to_string(i8* %call.i75, i8* %ref.tmp.sroa.36.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.56, i64 0, i64 0)) #15
  %call.i81 = tail call i8* @halide_string_to_string(i8* %call.i78, i8* %ref.tmp.sroa.36.0, i8* getelementptr inbounds ([47 x i8], [47 x i8]* @.str.57, i64 0, i64 0)) #15
  %conv.i84 = sext i32 %valid_min to i64
  %call.i85 = tail call i8* @halide_int64_to_string(i8* %call.i81, i8* %ref.tmp.sroa.36.0, i64 %conv.i84, i32 1) #15
  %call.i88 = tail call i8* @halide_string_to_string(i8* %call.i85, i8* %ref.tmp.sroa.36.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #15
  %add15 = add nsw i32 %fold_factor, %valid_min
  %sub16 = add nsw i32 %add15, -1
  %conv.i91 = sext i32 %sub16 to i64
  %call.i92 = tail call i8* @halide_int64_to_string(i8* %call.i88, i8* %ref.tmp.sroa.36.0, i64 %conv.i91, i32 1) #15
  %call.i95 = tail call i8* @halide_string_to_string(i8* %call.i92, i8* %ref.tmp.sroa.36.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.58.149, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %if.end

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i95 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %if.end

if.else:                                          ; preds = %lor.lhs.false
  %call.i101 = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i104 = icmp eq i8* %call.i101, null
  br i1 %tobool.not.i104, label %if.else.split, label %if.then6.i107

if.else.split:                                    ; preds = %if.else
  %call.i1133 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.53, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit110

if.then6.i107:                                    ; preds = %if.else
  %add.ptr.i105 = getelementptr inbounds i8, i8* %call.i101, i64 1023
  store i8 0, i8* %add.ptr.i105, align 1, !tbaa !18
  %call.i1134 = tail call i8* @halide_string_to_string(i8* nonnull %call.i101, i8* nonnull %add.ptr.i105, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.53, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit110

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit110: ; preds = %if.else.split, %if.then6.i107
  %phi.call5 = phi i8* [ %call.i1133, %if.else.split ], [ %call.i1134, %if.then6.i107 ]
  %ref.tmp19.sroa.34.0 = phi i8* [ null, %if.else.split ], [ %add.ptr.i105, %if.then6.i107 ]
  %conv.i116 = sext i32 %dim to i64
  %call.i117 = tail call i8* @halide_int64_to_string(i8* %phi.call5, i8* %ref.tmp19.sroa.34.0, i64 %conv.i116, i32 1) #15
  %call.i120 = tail call i8* @halide_string_to_string(i8* %call.i117, i8* %ref.tmp19.sroa.34.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i64 0, i64 0)) #15
  %call.i123 = tail call i8* @halide_string_to_string(i8* %call.i120, i8* %ref.tmp19.sroa.34.0, i8* %func_name) #15
  %call.i126 = tail call i8* @halide_string_to_string(i8* %call.i123, i8* %ref.tmp19.sroa.34.0, i8* getelementptr inbounds ([36 x i8], [36 x i8]* @.str.54, i64 0, i64 0)) #15
  %conv.i129 = sext i32 %min to i64
  %call.i130 = tail call i8* @halide_int64_to_string(i8* %call.i126, i8* %ref.tmp19.sroa.34.0, i64 %conv.i129, i32 1) #15
  %call.i133 = tail call i8* @halide_string_to_string(i8* %call.i130, i8* %ref.tmp19.sroa.34.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i64 0, i64 0)) #15
  %sub28 = add nsw i32 %add, -1
  %conv.i136 = sext i32 %sub28 to i64
  %call.i137 = tail call i8* @halide_int64_to_string(i8* %call.i133, i8* %ref.tmp19.sroa.34.0, i64 %conv.i136, i32 1) #15
  %call.i140 = tail call i8* @halide_string_to_string(i8* %call.i137, i8* %ref.tmp19.sroa.34.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.56, i64 0, i64 0)) #15
  %call.i143 = tail call i8* @halide_string_to_string(i8* %call.i140, i8* %ref.tmp19.sroa.34.0, i8* getelementptr inbounds ([47 x i8], [47 x i8]* @.str.59.150, i64 0, i64 0)) #15
  %call.i146 = tail call i8* @halide_string_to_string(i8* %call.i143, i8* %ref.tmp19.sroa.34.0, i8* getelementptr inbounds ([30 x i8], [30 x i8]* @.str.60.151, i64 0, i64 0)) #15
  %conv.i149 = sext i32 %fold_factor to i64
  %call.i150 = tail call i8* @halide_int64_to_string(i8* %call.i146, i8* %ref.tmp19.sroa.34.0, i64 %conv.i149, i32 1) #15
  %call.i153 = tail call i8* @halide_string_to_string(i8* %call.i150, i8* %ref.tmp19.sroa.34.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.141, i64 0, i64 0)) #15
  br i1 %tobool.not.i104, label %if.then.i157, label %if.else.i167

if.then.i157:                                     ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit110
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %if.end

if.else.i167:                                     ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit110
  %sub.ptr.lhs.cast.i.i162 = ptrtoint i8* %call.i153 to i64
  %sub.ptr.rhs.cast.i.i163 = ptrtoint i8* %call.i101 to i64
  %sub.ptr.sub.i.i164 = sub i64 1, %sub.ptr.rhs.cast.i.i163
  %add.i.i165 = add i64 %sub.ptr.sub.i.i164, %sub.ptr.lhs.cast.i.i162
  %call.i.i166 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i101, i64 %add.i.i165) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i101) #15
  br label %if.end

if.end:                                           ; preds = %if.else.i167, %if.then.i157, %if.else.i, %if.then.i
  %call.i101.sink = phi i8* [ %call.i, %if.else.i ], [ null, %if.then.i ], [ %call.i101, %if.else.i167 ], [ null, %if.then.i157 ]
  tail call void @free(i8* %call.i101.sink) #15
  ret i32 -35
}

; Function Attrs: nounwind
define weak i32 @halide_error_fold_factor_too_small(i8* %user_context, i8* %func_name, i8* %var_name, i32 %fold_factor, i8* %loop_name, i32 %required_extent) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i131 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([18 x i8], [18 x i8]* @.str.61.152, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i132 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([18 x i8], [18 x i8]* @.str.61.152, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i131, %entry.split ], [ %call.i132, %if.then6.i ]
  %ref.tmp.sroa.30.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %conv.i = sext i32 %fold_factor to i64
  %call.i16 = tail call i8* @halide_int64_to_string(i8* %phi.call, i8* %ref.tmp.sroa.30.0, i64 %conv.i, i32 1) #15
  %call.i19 = tail call i8* @halide_string_to_string(i8* %call.i16, i8* %ref.tmp.sroa.30.0, i8* getelementptr inbounds ([16 x i8], [16 x i8]* @.str.62, i64 0, i64 0)) #15
  %call.i22 = tail call i8* @halide_string_to_string(i8* %call.i19, i8* %ref.tmp.sroa.30.0, i8* %var_name) #15
  %call.i25 = tail call i8* @halide_string_to_string(i8* %call.i22, i8* %ref.tmp.sroa.30.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i64 0, i64 0)) #15
  %call.i28 = tail call i8* @halide_string_to_string(i8* %call.i25, i8* %ref.tmp.sroa.30.0, i8* %func_name) #15
  %call.i31 = tail call i8* @halide_string_to_string(i8* %call.i28, i8* %ref.tmp.sroa.30.0, i8* getelementptr inbounds ([61 x i8], [61 x i8]* @.str.63, i64 0, i64 0)) #15
  %call.i34 = tail call i8* @halide_string_to_string(i8* %call.i31, i8* %ref.tmp.sroa.30.0, i8* %loop_name) #15
  %call.i37 = tail call i8* @halide_string_to_string(i8* %call.i34, i8* %ref.tmp.sroa.30.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.32.143, i64 0, i64 0)) #15
  %conv.i40 = sext i32 %required_extent to i64
  %call.i41 = tail call i8* @halide_int64_to_string(i8* %call.i37, i8* %ref.tmp.sroa.30.0, i64 %conv.i40, i32 1) #15
  %call.i44 = tail call i8* @halide_string_to_string(i8* %call.i41, i8* %ref.tmp.sroa.30.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.64.153, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i44 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -26
}

; Function Attrs: nounwind
define weak i32 @halide_error_requirement_failed(i8* %user_context, i8* %condition, i8* %message) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i61 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.65, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i62 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.65, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i61, %entry.split ], [ %call.i62, %if.then6.i ]
  %ref.tmp.sroa.16.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i9 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.16.0, i8* %condition) #15
  %call.i12 = tail call i8* @halide_string_to_string(i8* %call.i9, i8* %ref.tmp.sroa.16.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.66, i64 0, i64 0)) #15
  %call.i15 = tail call i8* @halide_string_to_string(i8* %call.i12, i8* %ref.tmp.sroa.16.0, i8* %message) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i15 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -27
}

; Function Attrs: nounwind
define weak i32 @halide_error_specialize_fail(i8* %user_context, i8* %message) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %call.i41 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([59 x i8], [59 x i8]* @.str.67, i64 0, i64 0)) #15
  %call.i75 = tail call i8* @halide_string_to_string(i8* %call.i41, i8* null, i8* %message) #15
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i42 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([59 x i8], [59 x i8]* @.str.67, i64 0, i64 0)) #15
  %call.i7 = tail call i8* @halide_string_to_string(i8* %call.i42, i8* nonnull %add.ptr.i, i8* %message) #15
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i7 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -31
}

; Function Attrs: nounwind
define weak i32 @halide_error_no_device_interface(i8* %user_context) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %call.i316 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([55 x i8], [55 x i8]* @.str.68, i64 0, i64 0)) #15
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i3 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([55 x i8], [55 x i8]* @.str.68, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i3 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -19
}

; Function Attrs: nounwind
define weak i32 @halide_error_device_interface_no_device(i8* %user_context) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %call.i316 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([57 x i8], [57 x i8]* @.str.69, i64 0, i64 0)) #15
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i3 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([57 x i8], [57 x i8]* @.str.69, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i3 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -36
}

; Function Attrs: nounwind
define weak i32 @halide_error_host_and_device_dirty(i8* %user_context) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %call.i316 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([49 x i8], [49 x i8]* @.str.70, i64 0, i64 0)) #15
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i3 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([49 x i8], [49 x i8]* @.str.70, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i3 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -37
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_is_null(i8* %user_context, i8* %routine) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i51 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([26 x i8], [26 x i8]* @.str.71, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i52 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([26 x i8], [26 x i8]* @.str.71, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i51, %entry.split ], [ %call.i52, %if.then6.i ]
  %ref.tmp.sroa.14.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %call.i8 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %ref.tmp.sroa.14.0, i8* %routine) #15
  %call.i11 = tail call i8* @halide_string_to_string(i8* %call.i8, i8* %ref.tmp.sroa.14.0, i8* getelementptr inbounds ([11 x i8], [11 x i8]* @.str.72, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i11 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -38
}

; Function Attrs: nounwind
define weak i32 @halide_error_storage_bound_too_small(i8* %user_context, i8* %func_name, i8* %var_name, i32 %provided_size, i32 %required_size) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %entry.split, label %if.then6.i

entry.split:                                      ; preds = %entry
  %call.i111 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([32 x i8], [32 x i8]* @.str.73, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i112 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([32 x i8], [32 x i8]* @.str.73, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit: ; preds = %entry.split, %if.then6.i
  %phi.call = phi i8* [ %call.i111, %entry.split ], [ %call.i112, %if.then6.i ]
  %ref.tmp.sroa.26.0 = phi i8* [ null, %entry.split ], [ %add.ptr.i, %if.then6.i ]
  %conv.i = sext i32 %provided_size to i64
  %call.i14 = tail call i8* @halide_int64_to_string(i8* %phi.call, i8* %ref.tmp.sroa.26.0, i64 %conv.i, i32 1) #15
  %call.i17 = tail call i8* @halide_string_to_string(i8* %call.i14, i8* %ref.tmp.sroa.26.0, i8* getelementptr inbounds ([16 x i8], [16 x i8]* @.str.62, i64 0, i64 0)) #15
  %call.i20 = tail call i8* @halide_string_to_string(i8* %call.i17, i8* %ref.tmp.sroa.26.0, i8* %var_name) #15
  %call.i23 = tail call i8* @halide_string_to_string(i8* %call.i20, i8* %ref.tmp.sroa.26.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i64 0, i64 0)) #15
  %call.i26 = tail call i8* @halide_string_to_string(i8* %call.i23, i8* %ref.tmp.sroa.26.0, i8* %func_name) #15
  %call.i29 = tail call i8* @halide_string_to_string(i8* %call.i26, i8* %ref.tmp.sroa.26.0, i8* getelementptr inbounds ([45 x i8], [45 x i8]* @.str.74, i64 0, i64 0)) #15
  %conv.i32 = sext i32 %required_size to i64
  %call.i33 = tail call i8* @halide_int64_to_string(i8* %call.i29, i8* %ref.tmp.sroa.26.0, i64 %conv.i32, i32 1) #15
  %call.i36 = tail call i8* @halide_string_to_string(i8* %call.i33, i8* %ref.tmp.sroa.26.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.64.153, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EEC2EPvPc.exit
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i36 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -45
}

; Function Attrs: nounwind
define weak i32 @halide_error_device_crop_failed(i8* %user_context) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %call.i316 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* getelementptr inbounds ([77 x i8], [77 x i8]* @.str.75, i64 0, i64 0)) #15
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  %call.i3 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* nonnull %add.ptr.i, i8* getelementptr inbounds ([77 x i8], [77 x i8]* @.str.75, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %call.i3 to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  tail call void @halide_error(i8* %user_context, i8* nonnull %call.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE1ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret i32 -41
}

; Function Attrs: nounwind mustprogress
define weak void @halide_profiler_shutdown() #0 {
entry:
  %call = tail call %struct.halide_profiler_state* @halide_profiler_get_state() #16
  %sampling_thread = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 7
  %0 = load %struct.halide_thread*, %struct.halide_thread** %sampling_thread, align 8, !tbaa !337
  %tobool.not = icmp eq %struct.halide_thread* %0, null
  br i1 %tobool.not, label %cleanup, label %if.end

if.end:                                           ; preds = %entry
  %current_func = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 3
  store i32 -2, i32* %current_func, align 8, !tbaa !339
  tail call void @halide_join_thread(%struct.halide_thread* nonnull %0) #15
  store %struct.halide_thread* null, %struct.halide_thread** %sampling_thread, align 8, !tbaa !337
  store i32 -1, i32* %current_func, align 8, !tbaa !339
  tail call void @halide_profiler_report_unlocked(i8* null, %struct.halide_profiler_state* nonnull %call) #16
  tail call void @halide_profiler_reset_unlocked(%struct.halide_profiler_state* nonnull %call) #16
  br label %cleanup

cleanup:                                          ; preds = %if.end, %entry
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak %struct.halide_profiler_state* @halide_profiler_get_state() local_unnamed_addr #2 {
entry:
  ret %struct.halide_profiler_state* @_ZZ25halide_profiler_get_stateE1s
}

; Function Attrs: nounwind
define weak void @halide_profiler_report_unlocked(i8* %user_context, %struct.halide_profiler_state* %s) local_unnamed_addr #4 {
entry:
  %call.i = tail call i8* @malloc(i64 1024) #15
  %tobool.not.i = icmp eq i8* %call.i, null
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EEC2EPvPc.exit, label %if.then6.i

if.then6.i:                                       ; preds = %entry
  %add.ptr.i = getelementptr inbounds i8, i8* %call.i, i64 1023
  store i8 0, i8* %add.ptr.i, align 1, !tbaa !18
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EEC2EPvPc.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EEC2EPvPc.exit: ; preds = %if.then6.i, %entry
  %sstr.sroa.125.0 = phi i8* [ %add.ptr.i, %if.then6.i ], [ null, %entry ]
  %pipelines = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %s, i64 0, i32 5
  %p.0624 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %pipelines, align 8, !tbaa !14
  %tobool.not625 = icmp eq %struct.halide_profiler_pipeline_stats* %p.0624, null
  br i1 %tobool.not625, label %for.cond.cleanup, label %for.body.lr.ph

for.body.lr.ph:                                   ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EEC2EPvPc.exit
  %sub.ptr.rhs.cast.i.i348 = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i349 = sub i64 1, %sub.ptr.rhs.cast.i.i348
  br label %for.body

for.cond.cleanup:                                 ; preds = %cleanup181, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EEC2EPvPc.exit
  %sstr.sroa.19.0.lcssa = phi i8* [ %call.i, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EEC2EPvPc.exit ], [ %sstr.sroa.19.15, %cleanup181 ]
  br i1 %tobool.not.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %for.cond.cleanup
  tail call void @halide_error(i8* %user_context, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0)) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EED2Ev.exit

if.else.i:                                        ; preds = %for.cond.cleanup
  %sub.ptr.lhs.cast.i.i = ptrtoint i8* %sstr.sroa.19.0.lcssa to i64
  %sub.ptr.rhs.cast.i.i = ptrtoint i8* %call.i to i64
  %sub.ptr.sub.i.i = sub i64 1, %sub.ptr.rhs.cast.i.i
  %add.i.i = add i64 %sub.ptr.sub.i.i, %sub.ptr.lhs.cast.i.i
  %call.i.i = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EED2Ev.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EED2Ev.exit: ; preds = %if.else.i, %if.then.i
  tail call void @free(i8* %call.i) #15
  ret void

for.body:                                         ; preds = %cleanup181, %for.body.lr.ph
  %p.0627 = phi %struct.halide_profiler_pipeline_stats* [ %p.0624, %for.body.lr.ph ], [ %p.0, %cleanup181 ]
  %sstr.sroa.19.0626 = phi i8* [ %call.i, %for.body.lr.ph ], [ %sstr.sroa.19.15, %cleanup181 ]
  %time = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 0
  %0 = load i64, i64* %time, align 8, !tbaa !340
  %conv = uitofp i64 %0 to float
  %div = fdiv float %conv, 1.000000e+06
  %runs = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 11
  %1 = load i32, i32* %runs, align 8, !tbaa !342
  %tobool1.not = icmp eq i32 %1, 0
  br i1 %tobool1.not, label %cleanup181, label %if.end

if.end:                                           ; preds = %for.body
  br i1 %tobool.not.i, label %if.end.split, label %if.then.i278

if.end.split:                                     ; preds = %if.end
  %active_threads_numerator1 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 4
  %2 = load i64, i64* %active_threads_numerator1, align 8, !tbaa !343
  %active_threads_denominator2 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 5
  %3 = load i64, i64* %active_threads_denominator2, align 8, !tbaa !344
  %cmp3 = icmp eq i64 %2, %3
  %name4 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 6
  %4 = load i8*, i8** %name4, align 8, !tbaa !345
  %call.i2825 = tail call i8* @halide_string_to_string(i8* null, i8* %sstr.sroa.125.0, i8* %4) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit

if.then.i278:                                     ; preds = %if.end
  store i8 0, i8* %call.i, align 1, !tbaa !18
  %active_threads_numerator6 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 4
  %5 = load i64, i64* %active_threads_numerator6, align 8, !tbaa !343
  %active_threads_denominator7 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 5
  %6 = load i64, i64* %active_threads_denominator7, align 8, !tbaa !344
  %cmp8 = icmp eq i64 %5, %6
  %name9 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 6
  %7 = load i8*, i8** %name9, align 8, !tbaa !345
  %call.i28210 = tail call i8* @halide_string_to_string(i8* nonnull %call.i, i8* %sstr.sroa.125.0, i8* %7) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit: ; preds = %if.end.split, %if.then.i278
  %8 = phi i64 [ %2, %if.end.split ], [ %5, %if.then.i278 ]
  %9 = phi i64 [ %3, %if.end.split ], [ %6, %if.then.i278 ]
  %10 = phi i1 [ %cmp3, %if.end.split ], [ %cmp8, %if.then.i278 ]
  %phi.call = phi i8* [ %call.i2825, %if.end.split ], [ %call.i28210, %if.then.i278 ]
  %call.i285 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.7.164, i64 0, i64 0)) #15
  %call.i288 = tail call i8* @halide_string_to_string(i8* %call.i285, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([14 x i8], [14 x i8]* @.str.8.165, i64 0, i64 0)) #15
  %conv.i = fpext float %div to double
  %call.i291 = tail call i8* @halide_double_to_string(i8* %call.i288, i8* %sstr.sroa.125.0, double %conv.i, i32 0) #15
  %call.i294 = tail call i8* @halide_string_to_string(i8* %call.i291, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.9.166, i64 0, i64 0)) #15
  %call.i297 = tail call i8* @halide_string_to_string(i8* %call.i294, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([12 x i8], [12 x i8]* @.str.10.167, i64 0, i64 0)) #15
  %samples = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 12
  %11 = load i32, i32* %samples, align 4, !tbaa !346
  %conv.i300 = sext i32 %11 to i64
  %call.i301 = tail call i8* @halide_int64_to_string(i8* %call.i297, i8* %sstr.sroa.125.0, i64 %conv.i300, i32 1) #15
  %call.i304 = tail call i8* @halide_string_to_string(i8* %call.i301, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([9 x i8], [9 x i8]* @.str.11.168, i64 0, i64 0)) #15
  %12 = load i32, i32* %runs, align 8, !tbaa !342
  %conv.i307 = sext i32 %12 to i64
  %call.i308 = tail call i8* @halide_int64_to_string(i8* %call.i304, i8* %sstr.sroa.125.0, i64 %conv.i307, i32 1) #15
  %call.i311 = tail call i8* @halide_string_to_string(i8* %call.i308, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([13 x i8], [13 x i8]* @.str.12.169, i64 0, i64 0)) #15
  %13 = load i32, i32* %runs, align 8, !tbaa !342
  %conv19 = sitofp i32 %13 to float
  %div20 = fdiv float %div, %conv19
  %conv.i314 = fpext float %div20 to double
  %call.i315 = tail call i8* @halide_double_to_string(i8* %call.i311, i8* %sstr.sroa.125.0, double %conv.i314, i32 0) #15
  %call.i318 = tail call i8* @halide_string_to_string(i8* %call.i315, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.13.170, i64 0, i64 0)) #15
  br i1 %10, label %if.end28, label %if.then24

if.then24:                                        ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit
  %conv3 = uitofp i64 %8 to double
  %conv5 = uitofp i64 %9 to double
  %add = fadd double %conv5, 1.000000e-10
  %div6 = fdiv double %conv3, %add
  %conv7 = fptrunc double %div6 to float
  %call.i321 = tail call i8* @halide_string_to_string(i8* %call.i318, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.14.171, i64 0, i64 0)) #15
  %conv.i324 = fpext float %conv7 to double
  %call.i325 = tail call i8* @halide_double_to_string(i8* %call.i321, i8* %sstr.sroa.125.0, double %conv.i324, i32 0) #15
  %call.i328 = tail call i8* @halide_string_to_string(i8* %call.i325, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.7.164, i64 0, i64 0)) #15
  br label %if.end28

if.end28:                                         ; preds = %if.then24, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit
  %sstr.sroa.19.1 = phi i8* [ %call.i318, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit ], [ %call.i328, %if.then24 ]
  %call.i331 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.1, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.str.15.172, i64 0, i64 0)) #15
  %num_allocs = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 13
  %14 = load i32, i32* %num_allocs, align 8, !tbaa !347
  %conv.i334 = sext i32 %14 to i64
  %call.i335 = tail call i8* @halide_int64_to_string(i8* %call.i331, i8* %sstr.sroa.125.0, i64 %conv.i334, i32 1) #15
  %call.i338 = tail call i8* @halide_string_to_string(i8* %call.i335, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.str.16.173, i64 0, i64 0)) #15
  %memory_peak = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 2
  %15 = load i64, i64* %memory_peak, align 8, !tbaa !348
  %call.i341 = tail call i8* @halide_uint64_to_string(i8* %call.i338, i8* %sstr.sroa.125.0, i64 %15, i32 1) #15
  %call.i344 = tail call i8* @halide_string_to_string(i8* %call.i341, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str.17.174, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit, label %if.then.i352

if.then.i352:                                     ; preds = %if.end28
  %sub.ptr.lhs.cast.i.i347 = ptrtoint i8* %call.i344 to i64
  %add.i.i350 = add i64 %sub.ptr.sub.i.i349, %sub.ptr.lhs.cast.i.i347
  %call.i.i351 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i350) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit: ; preds = %if.then.i352, %if.end28
  %retval.0.i = phi i8* [ %call.i, %if.then.i352 ], [ getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0), %if.end28 ]
  tail call void @halide_print(i8* %user_context, i8* nonnull %retval.0.i) #15
  %16 = load i64, i64* %time, align 8, !tbaa !340
  %tobool36.not = icmp eq i64 %16, 0
  br i1 %tobool36.not, label %lor.end, label %for.cond53.preheader

lor.end:                                          ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit
  %memory_total = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 3
  %17 = load i64, i64* %memory_total, align 8, !tbaa !349
  %tobool37.not = icmp eq i64 %17, 0
  br i1 %tobool37.not, label %for.cond41.preheader, label %for.cond53.preheader

for.cond41.preheader:                             ; preds = %lor.end
  %num_funcs = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 9
  %18 = load i32, i32* %num_funcs, align 8, !tbaa !350
  %cmp42589 = icmp sgt i32 %18, 0
  br i1 %cmp42589, label %for.body44.lr.ph, label %cleanup181

for.body44.lr.ph:                                 ; preds = %for.cond41.preheader
  %funcs = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 7
  %19 = load %struct.halide_profiler_func_stats*, %struct.halide_profiler_func_stats** %funcs, align 8, !tbaa !351
  %20 = zext i32 %18 to i64
  br label %for.body44

for.cond41:                                       ; preds = %for.body44
  %exitcond.not = icmp eq i64 %indvars.iv.next, %20
  br i1 %exitcond.not, label %cleanup181, label %for.body44, !llvm.loop !352

for.body44:                                       ; preds = %for.cond41, %for.body44.lr.ph
  %indvars.iv = phi i64 [ 0, %for.body44.lr.ph ], [ %indvars.iv.next, %for.cond41 ]
  %stack_peak = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %19, i64 %indvars.iv, i32 4
  %21 = load i64, i64* %stack_peak, align 8, !tbaa !353
  %tobool45.not = icmp eq i64 %21, 0
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  br i1 %tobool45.not, label %for.cond41, label %for.cond53.preheader

for.cond53.preheader:                             ; preds = %for.body44, %lor.end, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit
  %num_funcs54 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 9
  %22 = load i32, i32* %num_funcs54, align 8, !tbaa !350
  %cmp55621 = icmp sgt i32 %22, 0
  br i1 %cmp55621, label %for.body57.lr.ph, label %cleanup181

for.body57.lr.ph:                                 ; preds = %for.cond53.preheader
  %funcs59 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 7
  br label %for.body57

for.body57:                                       ; preds = %cleanup172, %for.body57.lr.ph
  %indvars.iv630 = phi i64 [ 0, %for.body57.lr.ph ], [ %indvars.iv.next631, %cleanup172 ]
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit358, label %if.then.i356

if.then.i356:                                     ; preds = %for.body57
  store i8 0, i8* %call.i, align 1, !tbaa !18
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit358

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit358: ; preds = %if.then.i356, %for.body57
  %23 = load %struct.halide_profiler_func_stats*, %struct.halide_profiler_func_stats** %funcs59, align 8, !tbaa !351
  %add.ptr61 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %23, i64 %indvars.iv630
  %cmp62 = icmp eq i64 %indvars.iv630, 0
  br i1 %cmp62, label %land.lhs.true, label %if.end66

land.lhs.true:                                    ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit358
  %time63 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %add.ptr61, i64 0, i32 0
  %24 = load i64, i64* %time63, align 8, !tbaa !355
  %cmp64 = icmp eq i64 %24, 0
  br i1 %cmp64, label %cleanup172, label %if.end66

if.end66:                                         ; preds = %land.lhs.true, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5clearEv.exit358
  %call.i361 = tail call i8* @halide_string_to_string(i8* %call.i, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.18.175, i64 0, i64 0)) #15
  %name68 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %23, i64 %indvars.iv630, i32 7
  %25 = load i8*, i8** %name68, align 8, !tbaa !356
  %call.i364 = tail call i8* @halide_string_to_string(i8* %call.i361, i8* %sstr.sroa.125.0, i8* %25) #15
  %call.i367 = tail call i8* @halide_string_to_string(i8* %call.i364, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.19.176, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i591 = ptrtoint i8* %call.i367 to i64
  %sub.ptr.sub.i592 = sub i64 %sub.ptr.lhs.cast.i591, %sub.ptr.rhs.cast.i.i348
  %cmp73593 = icmp ult i64 %sub.ptr.sub.i592, 25
  br i1 %cmp73593, label %while.body, label %while.end

while.body:                                       ; preds = %if.end66, %while.body
  %sstr.sroa.19.3594 = phi i8* [ %call.i384, %while.body ], [ %call.i367, %if.end66 ]
  %call.i384 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.3594, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.20.177, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i = ptrtoint i8* %call.i384 to i64
  %sub.ptr.sub.i = sub i64 %sub.ptr.lhs.cast.i, %sub.ptr.rhs.cast.i.i348
  %cmp73 = icmp ult i64 %sub.ptr.sub.i, 25
  br i1 %cmp73, label %while.body, label %while.end, !llvm.loop !357

while.end:                                        ; preds = %while.body, %if.end66
  %sstr.sroa.19.3.lcssa = phi i8* [ %call.i367, %if.end66 ], [ %call.i384, %while.body ]
  %time75 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %add.ptr61, i64 0, i32 0
  %26 = load i64, i64* %time75, align 8, !tbaa !355
  %conv76 = uitofp i64 %26 to float
  %27 = load i32, i32* %runs, align 8, !tbaa !342
  %conv78 = sitofp i32 %27 to float
  %mul = fmul float %conv78, 1.000000e+06
  %div79 = fdiv float %conv76, %mul
  %conv.i387 = fpext float %div79 to double
  %call.i388 = tail call i8* @halide_double_to_string(i8* %sstr.sroa.19.3.lcssa, i8* %sstr.sroa.125.0, double %conv.i387, i32 0) #15
  %tobool.not.i390 = icmp eq i8* %call.i388, null
  br i1 %tobool.not.i390, label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit, label %if.then.i393

if.then.i393:                                     ; preds = %while.end
  %add.ptr.i391 = getelementptr inbounds i8, i8* %call.i388, i64 -3
  %cmp.i = icmp ult i8* %add.ptr.i391, %call.i
  %spec.store.select.i = select i1 %cmp.i, i8* %call.i, i8* %add.ptr.i391
  store i8 0, i8* %spec.store.select.i, align 1, !tbaa !18
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit: ; preds = %if.then.i393, %while.end
  %sstr.sroa.19.4 = phi i8* [ null, %while.end ], [ %spec.store.select.i, %if.then.i393 ]
  %call.i399 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.4, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.21.178, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i400596 = ptrtoint i8* %call.i399 to i64
  %sub.ptr.sub.i402597 = sub i64 %sub.ptr.lhs.cast.i400596, %sub.ptr.rhs.cast.i.i348
  %cmp85598 = icmp ult i64 %sub.ptr.sub.i402597, 35
  br i1 %cmp85598, label %while.body86, label %while.end88

while.body86:                                     ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit, %while.body86
  %sstr.sroa.19.5599 = phi i8* [ %call.i408, %while.body86 ], [ %call.i399, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit ]
  %call.i408 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.5599, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.20.177, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i400 = ptrtoint i8* %call.i408 to i64
  %sub.ptr.sub.i402 = sub i64 %sub.ptr.lhs.cast.i400, %sub.ptr.rhs.cast.i.i348
  %cmp85 = icmp ult i64 %sub.ptr.sub.i402, 35
  br i1 %cmp85, label %while.body86, label %while.end88, !llvm.loop !358

while.end88:                                      ; preds = %while.body86, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit
  %sstr.sroa.19.5.lcssa = phi i8* [ %call.i399, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit ], [ %call.i408, %while.body86 ]
  %28 = load i64, i64* %time, align 8, !tbaa !340
  %cmp90.not = icmp eq i64 %28, 0
  br i1 %cmp90.not, label %if.end97, label %if.then91

if.then91:                                        ; preds = %while.end88
  %29 = load i64, i64* %time75, align 8, !tbaa !355
  %mul93 = mul i64 %29, 100
  %div95 = udiv i64 %mul93, %28
  br label %if.end97

if.end97:                                         ; preds = %if.then91, %while.end88
  %percent.0 = phi i64 [ %div95, %if.then91 ], [ 0, %while.end88 ]
  %call.i411 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.5.lcssa, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.22.179, i64 0, i64 0)) #15
  %sext = shl i64 %percent.0, 32
  %conv.i414 = ashr exact i64 %sext, 32
  %call.i415 = tail call i8* @halide_int64_to_string(i8* %call.i411, i8* %sstr.sroa.125.0, i64 %conv.i414, i32 1) #15
  %call.i418 = tail call i8* @halide_string_to_string(i8* %call.i415, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.23.180, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i419601 = ptrtoint i8* %call.i418 to i64
  %sub.ptr.sub.i421602 = sub i64 %sub.ptr.lhs.cast.i419601, %sub.ptr.rhs.cast.i.i348
  %cmp104603 = icmp ult i64 %sub.ptr.sub.i421602, 43
  br i1 %cmp104603, label %while.body105, label %while.end107

while.body105:                                    ; preds = %if.end97, %while.body105
  %sstr.sroa.19.6604 = phi i8* [ %call.i427, %while.body105 ], [ %call.i418, %if.end97 ]
  %call.i427 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.6604, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.20.177, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i419 = ptrtoint i8* %call.i427 to i64
  %sub.ptr.sub.i421 = sub i64 %sub.ptr.lhs.cast.i419, %sub.ptr.rhs.cast.i.i348
  %cmp104 = icmp ult i64 %sub.ptr.sub.i421, 43
  br i1 %cmp104, label %while.body105, label %while.end107, !llvm.loop !359

while.end107:                                     ; preds = %while.body105, %if.end97
  %sstr.sroa.19.6.lcssa = phi i8* [ %call.i418, %if.end97 ], [ %call.i427, %while.body105 ]
  br i1 %10, label %if.end127, label %if.then109

if.then109:                                       ; preds = %while.end107
  %active_threads_numerator111 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %23, i64 %indvars.iv630, i32 5
  %30 = load i64, i64* %active_threads_numerator111, align 8, !tbaa !360
  %conv112 = uitofp i64 %30 to double
  %active_threads_denominator113 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %23, i64 %indvars.iv630, i32 6
  %31 = load i64, i64* %active_threads_denominator113, align 8, !tbaa !361
  %conv114 = uitofp i64 %31 to double
  %add115 = fadd double %conv114, 1.000000e-10
  %div116 = fdiv double %conv112, %add115
  %conv117 = fptrunc double %div116 to float
  %call.i430 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.6.lcssa, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([10 x i8], [10 x i8]* @.str.24.181, i64 0, i64 0)) #15
  %conv.i433 = fpext float %conv117 to double
  %call.i434 = tail call i8* @halide_double_to_string(i8* %call.i430, i8* %sstr.sroa.125.0, double %conv.i433, i32 0) #15
  %tobool.not.i436 = icmp eq i8* %call.i434, null
  br i1 %tobool.not.i436, label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442, label %if.then.i441

if.then.i441:                                     ; preds = %if.then109
  %add.ptr.i437 = getelementptr inbounds i8, i8* %call.i434, i64 -3
  %cmp.i439 = icmp ult i8* %add.ptr.i437, %call.i
  %spec.store.select.i440 = select i1 %cmp.i439, i8* %call.i, i8* %add.ptr.i437
  store i8 0, i8* %spec.store.select.i440, align 1, !tbaa !18
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442: ; preds = %if.then.i441, %if.then109
  %sstr.sroa.19.7 = phi i8* [ null, %if.then109 ], [ %spec.store.select.i440, %if.then.i441 ]
  %sub.ptr.lhs.cast.i447606 = ptrtoint i8* %sstr.sroa.19.7 to i64
  %sub.ptr.sub.i449607 = sub i64 %sub.ptr.lhs.cast.i447606, %sub.ptr.rhs.cast.i.i348
  %cmp123608 = icmp ult i64 %sub.ptr.sub.i449607, 58
  br i1 %cmp123608, label %while.body124, label %if.end127

while.body124:                                    ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442, %while.body124
  %sstr.sroa.19.8609 = phi i8* [ %call.i455, %while.body124 ], [ %sstr.sroa.19.7, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442 ]
  %call.i455 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.8609, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.20.177, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i447 = ptrtoint i8* %call.i455 to i64
  %sub.ptr.sub.i449 = sub i64 %sub.ptr.lhs.cast.i447, %sub.ptr.rhs.cast.i.i348
  %cmp123 = icmp ult i64 %sub.ptr.sub.i449, 58
  br i1 %cmp123, label %while.body124, label %if.end127, !llvm.loop !362

if.end127:                                        ; preds = %while.body124, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442, %while.end107
  %sstr.sroa.19.9 = phi i8* [ %sstr.sroa.19.6.lcssa, %while.end107 ], [ %sstr.sroa.19.7, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442 ], [ %call.i455, %while.body124 ]
  %cursor.0 = phi i64 [ 58, %while.end107 ], [ 73, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE5eraseEi.exit442 ], [ 73, %while.body124 ]
  %memory_peak128 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %23, i64 %indvars.iv630, i32 2
  %32 = load i64, i64* %memory_peak128, align 8, !tbaa !363
  %tobool129.not = icmp eq i64 %32, 0
  br i1 %tobool129.not, label %if.end162, label %if.then130

if.then130:                                       ; preds = %if.end127
  %call.i458 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.9, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str.25.182, i64 0, i64 0)) #15
  %33 = load i64, i64* %memory_peak128, align 8, !tbaa !363
  %call.i461 = tail call i8* @halide_uint64_to_string(i8* %call.i458, i8* %sstr.sroa.125.0, i64 %33, i32 1) #15
  %sub.ptr.lhs.cast.i462611 = ptrtoint i8* %call.i461 to i64
  %sub.ptr.sub.i464612 = sub i64 %sub.ptr.lhs.cast.i462611, %sub.ptr.rhs.cast.i.i348
  %cmp137613 = icmp ult i64 %sub.ptr.sub.i464612, %cursor.0
  br i1 %cmp137613, label %while.body138, label %while.end140

while.body138:                                    ; preds = %if.then130, %while.body138
  %sstr.sroa.19.10614 = phi i8* [ %call.i470, %while.body138 ], [ %call.i461, %if.then130 ]
  %call.i470 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.10614, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.20.177, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i462 = ptrtoint i8* %call.i470 to i64
  %sub.ptr.sub.i464 = sub i64 %sub.ptr.lhs.cast.i462, %sub.ptr.rhs.cast.i.i348
  %cmp137 = icmp ult i64 %sub.ptr.sub.i464, %cursor.0
  br i1 %cmp137, label %while.body138, label %while.end140, !llvm.loop !364

while.end140:                                     ; preds = %while.body138, %if.then130
  %sstr.sroa.19.10.lcssa = phi i8* [ %call.i461, %if.then130 ], [ %call.i470, %while.body138 ]
  %call.i473 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.10.lcssa, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.26.183, i64 0, i64 0)) #15
  %num_allocs142 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %23, i64 %indvars.iv630, i32 8
  %34 = load i32, i32* %num_allocs142, align 8, !tbaa !365
  %conv.i476 = sext i32 %34 to i64
  %call.i477 = tail call i8* @halide_int64_to_string(i8* %call.i473, i8* %sstr.sroa.125.0, i64 %conv.i476, i32 1) #15
  %add144 = add nuw nsw i64 %cursor.0, 15
  %sub.ptr.lhs.cast.i478616 = ptrtoint i8* %call.i477 to i64
  %sub.ptr.sub.i480617 = sub i64 %sub.ptr.lhs.cast.i478616, %sub.ptr.rhs.cast.i.i348
  %cmp147618 = icmp ult i64 %sub.ptr.sub.i480617, %add144
  br i1 %cmp147618, label %while.body148, label %while.end150

while.body148:                                    ; preds = %while.end140, %while.body148
  %sstr.sroa.19.11619 = phi i8* [ %call.i467, %while.body148 ], [ %call.i477, %while.end140 ]
  %call.i467 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.11619, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.20.177, i64 0, i64 0)) #15
  %sub.ptr.lhs.cast.i478 = ptrtoint i8* %call.i467 to i64
  %sub.ptr.sub.i480 = sub i64 %sub.ptr.lhs.cast.i478, %sub.ptr.rhs.cast.i.i348
  %cmp147 = icmp ult i64 %sub.ptr.sub.i480, %add144
  br i1 %cmp147, label %while.body148, label %while.end150, !llvm.loop !366

while.end150:                                     ; preds = %while.body148, %while.end140
  %sstr.sroa.19.11.lcssa = phi i8* [ %call.i477, %while.end140 ], [ %call.i467, %while.body148 ]
  %35 = load i32, i32* %num_allocs142, align 8, !tbaa !365
  %cmp152.not = icmp eq i32 %35, 0
  br i1 %cmp152.not, label %if.end159, label %if.then153

if.then153:                                       ; preds = %while.end150
  %memory_total154 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %23, i64 %indvars.iv630, i32 3
  %36 = load i64, i64* %memory_total154, align 8, !tbaa !367
  %conv156 = sext i32 %35 to i64
  %div157 = udiv i64 %36, %conv156
  br label %if.end159

if.end159:                                        ; preds = %if.then153, %while.end150
  %alloc_avg.0 = phi i64 [ %div157, %if.then153 ], [ 0, %while.end150 ]
  %call.i452 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.11.lcssa, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.27.184, i64 0, i64 0)) #15
  %sext586 = shl i64 %alloc_avg.0, 32
  %conv.i445 = ashr exact i64 %sext586, 32
  %call.i446 = tail call i8* @halide_int64_to_string(i8* %call.i452, i8* %sstr.sroa.125.0, i64 %conv.i445, i32 1) #15
  br label %if.end162

if.end162:                                        ; preds = %if.end159, %if.end127
  %sstr.sroa.19.12 = phi i8* [ %sstr.sroa.19.9, %if.end127 ], [ %call.i446, %if.end159 ]
  %stack_peak163 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %23, i64 %indvars.iv630, i32 4
  %37 = load i64, i64* %stack_peak163, align 8, !tbaa !353
  %cmp164.not = icmp eq i64 %37, 0
  br i1 %cmp164.not, label %if.end169, label %if.then165

if.then165:                                       ; preds = %if.end162
  %call.i424 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.12, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([9 x i8], [9 x i8]* @.str.28.185, i64 0, i64 0)) #15
  %38 = load i64, i64* %stack_peak163, align 8, !tbaa !353
  %call.i405 = tail call i8* @halide_uint64_to_string(i8* %call.i424, i8* %sstr.sroa.125.0, i64 %38, i32 1) #15
  br label %if.end169

if.end169:                                        ; preds = %if.then165, %if.end162
  %sstr.sroa.19.13 = phi i8* [ %sstr.sroa.19.12, %if.end162 ], [ %call.i405, %if.then165 ]
  %call.i396 = tail call i8* @halide_string_to_string(i8* %sstr.sroa.19.13, i8* %sstr.sroa.125.0, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.7.164, i64 0, i64 0)) #15
  br i1 %tobool.not.i, label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit381, label %if.then.i379

if.then.i379:                                     ; preds = %if.end169
  %sub.ptr.lhs.cast.i.i374 = ptrtoint i8* %call.i396 to i64
  %add.i.i377 = add i64 %sub.ptr.sub.i.i349, %sub.ptr.lhs.cast.i.i374
  %call.i.i378 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* nonnull %call.i, i64 %add.i.i377) #15
  br label %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit381

_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit381: ; preds = %if.then.i379, %if.end169
  %retval.0.i380 = phi i8* [ %call.i, %if.then.i379 ], [ getelementptr inbounds ([35 x i8], [35 x i8]* @.str.29.163, i64 0, i64 0), %if.end169 ]
  tail call void @halide_print(i8* %user_context, i8* nonnull %retval.0.i380) #15
  br label %cleanup172

cleanup172:                                       ; preds = %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit381, %land.lhs.true
  %sstr.sroa.19.14 = phi i8* [ %call.i, %land.lhs.true ], [ %call.i396, %_ZN6Halide7Runtime8Internal12_GLOBAL__N_17PrinterILNS1_11PrinterTypeE2ELy1024EE3strEv.exit381 ]
  %indvars.iv.next631 = add nuw nsw i64 %indvars.iv630, 1
  %39 = load i32, i32* %num_funcs54, align 8, !tbaa !350
  %40 = sext i32 %39 to i64
  %cmp55 = icmp slt i64 %indvars.iv.next631, %40
  br i1 %cmp55, label %for.body57, label %cleanup181, !llvm.loop !368

cleanup181:                                       ; preds = %for.cond41, %cleanup172, %for.cond53.preheader, %for.cond41.preheader, %for.body
  %sstr.sroa.19.15 = phi i8* [ %sstr.sroa.19.0626, %for.body ], [ %call.i344, %for.cond53.preheader ], [ %call.i344, %for.cond41.preheader ], [ %sstr.sroa.19.14, %cleanup172 ], [ %call.i344, %for.cond41 ]
  %next = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0627, i64 0, i32 8
  %41 = bitcast i8** %next to %struct.halide_profiler_pipeline_stats**
  %p.0 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %41, align 8, !tbaa !14
  %tobool.not = icmp eq %struct.halide_profiler_pipeline_stats* %p.0, null
  br i1 %tobool.not, label %for.cond.cleanup, label %for.body, !llvm.loop !369
}

; Function Attrs: nounwind mustprogress
define weak void @halide_profiler_reset_unlocked(%struct.halide_profiler_state* %s) local_unnamed_addr #0 {
entry:
  %pipelines = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %s, i64 0, i32 5
  %0 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %pipelines, align 8, !tbaa !370
  %tobool.not9 = icmp eq %struct.halide_profiler_pipeline_stats* %0, null
  br i1 %tobool.not9, label %while.end, label %while.body

while.body:                                       ; preds = %entry, %while.body
  %1 = phi %struct.halide_profiler_pipeline_stats* [ %7, %while.body ], [ %0, %entry ]
  %next = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %1, i64 0, i32 8
  %2 = bitcast i8** %next to %struct.halide_profiler_pipeline_stats**
  %3 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %2, align 8, !tbaa !371
  store %struct.halide_profiler_pipeline_stats* %3, %struct.halide_profiler_pipeline_stats** %pipelines, align 8, !tbaa !370
  %funcs = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %1, i64 0, i32 7
  %4 = bitcast %struct.halide_profiler_func_stats** %funcs to i8**
  %5 = load i8*, i8** %4, align 8, !tbaa !351
  tail call void @free(i8* %5) #15
  %6 = bitcast %struct.halide_profiler_pipeline_stats* %1 to i8*
  tail call void @free(i8* nonnull %6) #15
  %7 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %pipelines, align 8, !tbaa !370
  %tobool.not = icmp eq %struct.halide_profiler_pipeline_stats* %7, null
  br i1 %tobool.not, label %while.end, label %while.body, !llvm.loop !372

while.end:                                        ; preds = %while.body, %entry
  %first_free_id = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %s, i64 0, i32 2
  store i32 0, i32* %first_free_id, align 4, !tbaa !373
  ret void
}

; Function Attrs: nounwind mustprogress
define linkonce %struct.halide_profiler_pipeline_stats* @_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy(i8* %pipeline_name, i32 %num_funcs, i64* %func_names) local_unnamed_addr #0 {
entry:
  %call = tail call %struct.halide_profiler_state* @halide_profiler_get_state() #16
  %pipelines = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 5
  %p.0121 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %pipelines, align 8, !tbaa !14
  %tobool.not122 = icmp eq %struct.halide_profiler_pipeline_stats* %p.0121, null
  br i1 %tobool.not122, label %for.end, label %for.body

for.body:                                         ; preds = %entry, %for.inc
  %p.0123 = phi %struct.halide_profiler_pipeline_stats* [ %p.0, %for.inc ], [ %p.0121, %entry ]
  %name = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0123, i64 0, i32 6
  %0 = load i8*, i8** %name, align 8, !tbaa !345
  %cmp = icmp eq i8* %0, %pipeline_name
  br i1 %cmp, label %land.lhs.true, label %for.inc

land.lhs.true:                                    ; preds = %for.body
  %num_funcs1 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0123, i64 0, i32 9
  %1 = load i32, i32* %num_funcs1, align 8, !tbaa !350
  %cmp2 = icmp eq i32 %1, %num_funcs
  br i1 %cmp2, label %cleanup62, label %for.inc

for.inc:                                          ; preds = %land.lhs.true, %for.body
  %next = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.0123, i64 0, i32 8
  %2 = bitcast i8** %next to %struct.halide_profiler_pipeline_stats**
  %p.0 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %2, align 8, !tbaa !14
  %tobool.not = icmp eq %struct.halide_profiler_pipeline_stats* %p.0, null
  br i1 %tobool.not, label %for.end, label %for.body, !llvm.loop !374

for.end:                                          ; preds = %for.inc, %entry
  %call4 = tail call i8* @malloc(i64 96) #15
  %3 = bitcast i8* %call4 to %struct.halide_profiler_pipeline_stats*
  %tobool5.not = icmp eq i8* %call4, null
  br i1 %tobool5.not, label %cleanup62, label %if.end7

if.end7:                                          ; preds = %for.end
  %4 = bitcast %struct.halide_profiler_pipeline_stats** %pipelines to i8**
  %5 = load i8*, i8** %4, align 8, !tbaa !370
  %next9 = getelementptr inbounds i8, i8* %call4, i64 64
  %6 = bitcast i8* %next9 to i8**
  store i8* %5, i8** %6, align 8, !tbaa !371
  %name10 = getelementptr inbounds i8, i8* %call4, i64 48
  %7 = bitcast i8* %name10 to i8**
  store i8* %pipeline_name, i8** %7, align 8, !tbaa !345
  %first_free_id = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 2
  %8 = load i32, i32* %first_free_id, align 4, !tbaa !373
  %first_func_id = getelementptr inbounds i8, i8* %call4, i64 76
  %9 = bitcast i8* %first_func_id to i32*
  store i32 %8, i32* %9, align 4, !tbaa !375
  %num_funcs11 = getelementptr inbounds i8, i8* %call4, i64 72
  %10 = bitcast i8* %num_funcs11 to i32*
  store i32 %num_funcs, i32* %10, align 8, !tbaa !350
  %runs = getelementptr inbounds i8, i8* %call4, i64 80
  %11 = bitcast i8* %runs to <2 x i32>*
  store <2 x i32> zeroinitializer, <2 x i32>* %11, align 8, !tbaa !41
  %12 = bitcast i8* %call4 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %12, align 8, !tbaa !22
  %memory_peak = getelementptr inbounds i8, i8* %call4, i64 16
  %13 = bitcast i8* %memory_peak to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %13, align 8, !tbaa !22
  %num_allocs = getelementptr inbounds i8, i8* %call4, i64 88
  %14 = bitcast i8* %num_allocs to i32*
  store i32 0, i32* %14, align 8, !tbaa !347
  %active_threads_numerator = getelementptr inbounds i8, i8* %call4, i64 32
  %15 = bitcast i8* %active_threads_numerator to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %15, align 8, !tbaa !22
  %conv = sext i32 %num_funcs to i64
  %mul = mul nsw i64 %conv, 72
  %call12 = tail call i8* @malloc(i64 %mul) #15
  %funcs = getelementptr inbounds i8, i8* %call4, i64 56
  %16 = bitcast i8* %funcs to i8**
  store i8* %call12, i8** %16, align 8, !tbaa !351
  %tobool14.not = icmp eq i8* %call12, null
  %17 = bitcast i8* %call12 to %struct.halide_profiler_func_stats*
  br i1 %tobool14.not, label %if.then15, label %for.cond17.preheader

for.cond17.preheader:                             ; preds = %if.end7
  %cmp18119 = icmp sgt i32 %num_funcs, 0
  br i1 %cmp18119, label %for.body20.lr.ph, label %for.cond.cleanup19

for.body20.lr.ph:                                 ; preds = %for.cond17.preheader
  %wide.trip.count = zext i32 %num_funcs to i64
  br label %for.body20

if.then15:                                        ; preds = %if.end7
  tail call void @free(i8* nonnull %call4) #15
  br label %cleanup62

for.cond.cleanup19:                               ; preds = %for.body20, %for.cond17.preheader
  %18 = load i32, i32* %first_free_id, align 4, !tbaa !373
  %add = add nsw i32 %18, %num_funcs
  store i32 %add, i32* %first_free_id, align 4, !tbaa !373
  store i8* %call4, i8** %4, align 8, !tbaa !370
  br label %cleanup62

for.body20:                                       ; preds = %for.body20, %for.body20.lr.ph
  %indvars.iv = phi i64 [ 0, %for.body20.lr.ph ], [ %indvars.iv.next, %for.body20 ]
  %time22 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %17, i64 %indvars.iv, i32 0
  store i64 0, i64* %time22, align 8, !tbaa !355
  %arrayidx24 = getelementptr inbounds i64, i64* %func_names, i64 %indvars.iv
  %19 = load i64, i64* %arrayidx24, align 8, !tbaa !22
  %20 = inttoptr i64 %19 to i8*
  %name28 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %17, i64 %indvars.iv, i32 7
  store i8* %20, i8** %name28, align 8, !tbaa !356
  %memory_current32 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %17, i64 %indvars.iv, i32 1
  %21 = bitcast i64* %memory_current32 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %21, align 8, !tbaa !22
  %memory_total40 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %17, i64 %indvars.iv, i32 3
  %num_allocs44 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %17, i64 %indvars.iv, i32 8
  store i32 0, i32* %num_allocs44, align 8, !tbaa !365
  %22 = bitcast i64* %memory_total40 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %22, align 8, !tbaa !22
  %active_threads_numerator51 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %17, i64 %indvars.iv, i32 5
  %23 = bitcast i64* %active_threads_numerator51 to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %23, align 8, !tbaa !22
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %for.cond.cleanup19, label %for.body20, !llvm.loop !376

cleanup62:                                        ; preds = %land.lhs.true, %for.cond.cleanup19, %if.then15, %for.end
  %retval.2 = phi %struct.halide_profiler_pipeline_stats* [ %3, %for.cond.cleanup19 ], [ null, %if.then15 ], [ null, %for.end ], [ %p.0123, %land.lhs.true ]
  ret %struct.halide_profiler_pipeline_stats* %retval.2
}

; Function Attrs: nounwind mustprogress
define linkonce void @_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi(%struct.halide_profiler_state* %s, i32 %func_id, i64 %time, i32 %active_threads) local_unnamed_addr #0 {
entry:
  %pipelines = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %s, i64 0, i32 5
  %p.055 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %pipelines, align 8, !tbaa !14
  %tobool.not56 = icmp eq %struct.halide_profiler_pipeline_stats* %p.055, null
  br i1 %tobool.not56, label %cleanup25, label %for.body

for.body:                                         ; preds = %entry, %if.end23
  %p.058 = phi %struct.halide_profiler_pipeline_stats* [ %p.0, %if.end23 ], [ %p.055, %entry ]
  %p_prev.057 = phi %struct.halide_profiler_pipeline_stats* [ %p.058, %if.end23 ], [ null, %entry ]
  %first_func_id = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 10
  %0 = load i32, i32* %first_func_id, align 4, !tbaa !375
  %cmp.not = icmp sgt i32 %0, %func_id
  br i1 %cmp.not, label %if.end23, label %land.lhs.true

land.lhs.true:                                    ; preds = %for.body
  %num_funcs = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 9
  %1 = load i32, i32* %num_funcs, align 8, !tbaa !350
  %add = add nsw i32 %1, %0
  %cmp2 = icmp sgt i32 %add, %func_id
  br i1 %cmp2, label %if.then, label %if.end23

if.then:                                          ; preds = %land.lhs.true
  %tobool3.not = icmp eq %struct.halide_profiler_pipeline_stats* %p_prev.057, null
  br i1 %tobool3.not, label %if.end, label %if.then4

if.then4:                                         ; preds = %if.then
  %next = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 8
  %2 = load i8*, i8** %next, align 8, !tbaa !371
  %next5 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p_prev.057, i64 0, i32 8
  store i8* %2, i8** %next5, align 8, !tbaa !371
  %3 = bitcast i8** %next to %struct.halide_profiler_pipeline_stats**
  store %struct.halide_profiler_pipeline_stats* %p.055, %struct.halide_profiler_pipeline_stats** %3, align 8, !tbaa !371
  store %struct.halide_profiler_pipeline_stats* %p.058, %struct.halide_profiler_pipeline_stats** %pipelines, align 8, !tbaa !370
  br label %if.end

if.end:                                           ; preds = %if.then4, %if.then
  %funcs = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 7
  %4 = load %struct.halide_profiler_func_stats*, %struct.halide_profiler_func_stats** %funcs, align 8, !tbaa !351
  %idx.ext = sext i32 %func_id to i64
  %add.ptr = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %4, i64 %idx.ext
  %idx.ext10 = sext i32 %0 to i64
  %idx.neg = sub nsw i64 0, %idx.ext10
  %add.ptr11 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %add.ptr, i64 %idx.neg
  %time12 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %add.ptr11, i64 0, i32 0
  %5 = load i64, i64* %time12, align 8, !tbaa !355
  %add13 = add i64 %5, %time
  store i64 %add13, i64* %time12, align 8, !tbaa !355
  %conv = sext i32 %active_threads to i64
  %active_threads_numerator = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %add.ptr11, i64 0, i32 5
  %6 = load i64, i64* %active_threads_numerator, align 8, !tbaa !360
  %add14 = add i64 %6, %conv
  store i64 %add14, i64* %active_threads_numerator, align 8, !tbaa !360
  %active_threads_denominator = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %add.ptr11, i64 0, i32 6
  %7 = load i64, i64* %active_threads_denominator, align 8, !tbaa !361
  %add15 = add i64 %7, 1
  store i64 %add15, i64* %active_threads_denominator, align 8, !tbaa !361
  %time16 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 0
  %8 = load i64, i64* %time16, align 8, !tbaa !340
  %add17 = add i64 %8, %time
  store i64 %add17, i64* %time16, align 8, !tbaa !340
  %samples = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 12
  %9 = load i32, i32* %samples, align 4, !tbaa !346
  %inc = add nsw i32 %9, 1
  store i32 %inc, i32* %samples, align 4, !tbaa !346
  %active_threads_numerator19 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 4
  %10 = load i64, i64* %active_threads_numerator19, align 8, !tbaa !343
  %add20 = add i64 %10, %conv
  store i64 %add20, i64* %active_threads_numerator19, align 8, !tbaa !343
  %active_threads_denominator21 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 5
  %11 = load i64, i64* %active_threads_denominator21, align 8, !tbaa !344
  %add22 = add i64 %11, 1
  store i64 %add22, i64* %active_threads_denominator21, align 8, !tbaa !344
  ret void

if.end23:                                         ; preds = %land.lhs.true, %for.body
  %next24 = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.058, i64 0, i32 8
  %12 = bitcast i8** %next24 to %struct.halide_profiler_pipeline_stats**
  %p.0 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %12, align 8, !tbaa !14
  %tobool.not = icmp eq %struct.halide_profiler_pipeline_stats* %p.0, null
  br i1 %tobool.not, label %cleanup25, label %for.body, !llvm.loop !377

cleanup25:                                        ; preds = %if.end23, %entry
  ret void
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_profiler_sample(%struct.halide_profiler_state* %s, i64* %prev_t) local_unnamed_addr #0 {
entry:
  %func = alloca i32, align 4
  %active_threads = alloca i32, align 4
  %0 = bitcast i32* %func to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #11
  %1 = bitcast i32* %active_threads to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #11
  %get_remote_profiler_state = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %s, i64 0, i32 6
  %2 = load void (i32*, i32*)*, void (i32*, i32*)** %get_remote_profiler_state, align 8, !tbaa !378
  %tobool.not = icmp eq void (i32*, i32*)* %2, null
  br i1 %tobool.not, label %if.else, label %if.then

if.then:                                          ; preds = %entry
  call void %2(i32* nonnull %func, i32* nonnull %active_threads) #15
  br label %if.end

if.else:                                          ; preds = %entry
  %current_func = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %s, i64 0, i32 3
  %3 = load i32, i32* %current_func, align 8, !tbaa !339
  store i32 %3, i32* %func, align 4, !tbaa !41
  %active_threads2 = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %s, i64 0, i32 4
  %4 = load i32, i32* %active_threads2, align 4, !tbaa !379
  store i32 %4, i32* %active_threads, align 4, !tbaa !41
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %call = call i64 @halide_current_time_ns(i8* null) #15
  %5 = load i32, i32* %func, align 4, !tbaa !41
  %cmp = icmp eq i32 %5, -2
  br i1 %cmp, label %cleanup, label %if.else4

if.else4:                                         ; preds = %if.end
  %cmp5 = icmp sgt i32 %5, -1
  br i1 %cmp5, label %if.then6, label %if.end8

if.then6:                                         ; preds = %if.else4
  %6 = load i64, i64* %prev_t, align 8, !tbaa !22
  %sub = sub i64 %call, %6
  %7 = load i32, i32* %active_threads, align 4, !tbaa !41
  call void @_ZN6Halide7Runtime8Internal9bill_funcEP21halide_profiler_stateiyi(%struct.halide_profiler_state* nonnull %s, i32 %5, i64 %sub, i32 %7) #16
  br label %if.end8

if.end8:                                          ; preds = %if.then6, %if.else4
  store i64 %call, i64* %prev_t, align 8, !tbaa !22
  %sleep_time = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %s, i64 0, i32 1
  %8 = load i32, i32* %sleep_time, align 8, !tbaa !380
  br label %cleanup

cleanup:                                          ; preds = %if.end8, %if.end
  %retval.0 = phi i32 [ %8, %if.end8 ], [ -1, %if.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #11
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #11
  ret i32 %retval.0
}

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv(i8* %0) #4 {
entry:
  %t = alloca i64, align 8
  %call = tail call %struct.halide_profiler_state* @halide_profiler_get_state() #16
  %lock = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 0
  tail call void @halide_mutex_lock(%struct.halide_mutex* %lock) #15
  %current_func = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 3
  %1 = load i32, i32* %current_func, align 8, !tbaa !339
  %cmp.not23 = icmp eq i32 %1, -2
  br i1 %cmp.not23, label %while.end8, label %while.body.lr.ph

while.body.lr.ph:                                 ; preds = %entry
  %2 = bitcast i64* %t to i8*
  br label %while.body

while.body:                                       ; preds = %while.end, %while.body.lr.ph
  %call1 = call i64 @halide_current_time_ns(i8* null) #15
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2) #11
  store i64 %call1, i64* %t, align 8, !tbaa !22
  %call420 = call i32 @halide_profiler_sample(%struct.halide_profiler_state* nonnull %call, i64* nonnull %t) #16
  %cmp521 = icmp slt i32 %call420, 0
  br i1 %cmp521, label %while.end, label %if.end

if.end:                                           ; preds = %while.body, %if.end
  %call422 = phi i32 [ %call4, %if.end ], [ %call420, %while.body ]
  call void @halide_mutex_unlock(%struct.halide_mutex* %lock) #15
  call void @halide_sleep_ms(i8* null, i32 %call422) #15
  call void @halide_mutex_lock(%struct.halide_mutex* %lock) #15
  %call4 = call i32 @halide_profiler_sample(%struct.halide_profiler_state* %call, i64* nonnull %t) #16
  %cmp5 = icmp slt i32 %call4, 0
  br i1 %cmp5, label %while.end, label %if.end

while.end:                                        ; preds = %if.end, %while.body
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2) #11
  %3 = load i32, i32* %current_func, align 8, !tbaa !339
  %cmp.not = icmp eq i32 %3, -2
  br i1 %cmp.not, label %while.end8, label %while.body, !llvm.loop !381

while.end8:                                       ; preds = %while.end, %entry
  call void @halide_mutex_unlock(%struct.halide_mutex* nonnull %lock) #15
  ret void
}

; Function Attrs: nounwind
define weak %struct.halide_profiler_pipeline_stats* @halide_profiler_get_pipeline_state(i8* %pipeline_name) local_unnamed_addr #4 {
entry:
  %call = tail call %struct.halide_profiler_state* @halide_profiler_get_state() #16
  %lock.i = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 0
  tail call void @halide_mutex_lock(%struct.halide_mutex* %lock.i) #15
  %pipelines = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 5
  %p.013 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %pipelines, align 8, !tbaa !14
  %tobool.not14 = icmp eq %struct.halide_profiler_pipeline_stats* %p.013, null
  br i1 %tobool.not14, label %cleanup, label %for.body

for.body:                                         ; preds = %entry, %for.inc
  %p.015 = phi %struct.halide_profiler_pipeline_stats* [ %p.0, %for.inc ], [ %p.013, %entry ]
  %name = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.015, i64 0, i32 6
  %0 = load i8*, i8** %name, align 8, !tbaa !345
  %cmp = icmp eq i8* %0, %pipeline_name
  br i1 %cmp, label %cleanup, label %for.inc

for.inc:                                          ; preds = %for.body
  %next = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %p.015, i64 0, i32 8
  %1 = bitcast i8** %next to %struct.halide_profiler_pipeline_stats**
  %p.0 = load %struct.halide_profiler_pipeline_stats*, %struct.halide_profiler_pipeline_stats** %1, align 8, !tbaa !14
  %tobool.not = icmp eq %struct.halide_profiler_pipeline_stats* %p.0, null
  br i1 %tobool.not, label %cleanup, label %for.body, !llvm.loop !382

cleanup:                                          ; preds = %for.inc, %for.body, %entry
  %p.0.lcssa = phi %struct.halide_profiler_pipeline_stats* [ null, %entry ], [ null, %for.inc ], [ %p.015, %for.body ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* %lock.i) #15
  ret %struct.halide_profiler_pipeline_stats* %p.0.lcssa
}

; Function Attrs: nounwind
define weak i32 @halide_profiler_pipeline_start(i8* %user_context, i8* %pipeline_name, i32 %num_funcs, i64* %func_names) local_unnamed_addr #4 {
entry:
  %call = tail call %struct.halide_profiler_state* @halide_profiler_get_state() #16
  %lock.i = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 0
  tail call void @halide_mutex_lock(%struct.halide_mutex* %lock.i) #15
  %sampling_thread = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 7
  %0 = load %struct.halide_thread*, %struct.halide_thread** %sampling_thread, align 8, !tbaa !337
  %tobool.not = icmp eq %struct.halide_thread* %0, null
  br i1 %tobool.not, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  %call1 = tail call i32 @halide_start_clock(i8* %user_context) #15
  %call2 = tail call %struct.halide_thread* @halide_spawn_thread(void (i8*)* nonnull @_ZN6Halide7Runtime8Internal24sampling_profiler_threadEPv, i8* null) #15
  store %struct.halide_thread* %call2, %struct.halide_thread** %sampling_thread, align 8, !tbaa !337
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  %call4 = tail call %struct.halide_profiler_pipeline_stats* @_ZN6Halide7Runtime8Internal23find_or_create_pipelineEPKciPKy(i8* %pipeline_name, i32 %num_funcs, i64* %func_names) #16
  %tobool5.not = icmp eq %struct.halide_profiler_pipeline_stats* %call4, null
  br i1 %tobool5.not, label %if.then6, label %if.end8

if.then6:                                         ; preds = %if.end
  %call7 = tail call i32 @halide_error_out_of_memory(i8* %user_context) #15
  br label %cleanup

if.end8:                                          ; preds = %if.end
  %runs = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %call4, i64 0, i32 11
  %1 = load i32, i32* %runs, align 8, !tbaa !342
  %inc = add nsw i32 %1, 1
  store i32 %inc, i32* %runs, align 8, !tbaa !342
  %first_func_id = getelementptr inbounds %struct.halide_profiler_pipeline_stats, %struct.halide_profiler_pipeline_stats* %call4, i64 0, i32 10
  %2 = load i32, i32* %first_func_id, align 4, !tbaa !375
  br label %cleanup

cleanup:                                          ; preds = %if.end8, %if.then6
  %retval.0 = phi i32 [ %2, %if.end8 ], [ %call7, %if.then6 ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull %lock.i) #15
  ret i32 %retval.0
}

; Function Attrs: nounwind mustprogress
define weak void @halide_profiler_stack_peak_update(i8* %user_context, i8* %pipeline_state, i64* %f_values) local_unnamed_addr #0 {
entry:
  %cmp.not = icmp eq i8* %pipeline_state, null
  br i1 %cmp.not, label %if.then, label %do.end

if.then:                                          ; preds = %entry
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.186, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end

do.end:                                           ; preds = %if.then, %entry
  %num_funcs = getelementptr inbounds i8, i8* %pipeline_state, i64 72
  %0 = bitcast i8* %num_funcs to i32*
  %1 = load i32, i32* %0, align 8, !tbaa !350
  %cmp118 = icmp sgt i32 %1, 0
  br i1 %cmp118, label %for.body.lr.ph, label %for.cond.cleanup

for.body.lr.ph:                                   ; preds = %do.end
  %funcs = getelementptr inbounds i8, i8* %pipeline_state, i64 56
  %2 = bitcast i8* %funcs to %struct.halide_profiler_func_stats**
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.inc, %do.end
  ret void

for.body:                                         ; preds = %for.inc, %for.body.lr.ph
  %3 = phi i32 [ %1, %for.body.lr.ph ], [ %9, %for.inc ]
  %indvars.iv = phi i64 [ 0, %for.body.lr.ph ], [ %indvars.iv.next, %for.inc ]
  %arrayidx = getelementptr inbounds i64, i64* %f_values, i64 %indvars.iv
  %4 = load i64, i64* %arrayidx, align 8, !tbaa !22
  %cmp2.not = icmp eq i64 %4, 0
  br i1 %cmp2.not, label %for.inc, label %if.then3

if.then3:                                         ; preds = %for.body
  %5 = load %struct.halide_profiler_func_stats*, %struct.halide_profiler_func_stats** %2, align 8, !tbaa !351
  %stack_peak = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %5, i64 %indvars.iv, i32 4
  %6 = load i64, i64* %stack_peak, align 8, !tbaa !22
  br label %while.cond.i

while.cond.i:                                     ; preds = %while.body.i, %if.then3
  %old_val.0.i = phi i64 [ %6, %if.then3 ], [ %8, %while.body.i ]
  %cmp.i = icmp ult i64 %old_val.0.i, %4
  br i1 %cmp.i, label %while.body.i, label %for.inc.loopexit

while.body.i:                                     ; preds = %while.cond.i
  %7 = cmpxchg i64* %stack_peak, i64 %old_val.0.i, i64 %4 seq_cst seq_cst
  %8 = extractvalue { i64, i1 } %7, 0
  %cmp1.i = icmp eq i64 %old_val.0.i, %8
  br i1 %cmp1.i, label %for.inc.loopexit, label %while.cond.i, !llvm.loop !383

for.inc.loopexit:                                 ; preds = %while.body.i, %while.cond.i
  %.pre = load i32, i32* %0, align 8, !tbaa !350
  br label %for.inc

for.inc:                                          ; preds = %for.inc.loopexit, %for.body
  %9 = phi i32 [ %.pre, %for.inc.loopexit ], [ %3, %for.body ]
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %10 = sext i32 %9 to i64
  %cmp1 = icmp slt i64 %indvars.iv.next, %10
  br i1 %cmp1, label %for.body, label %for.cond.cleanup, !llvm.loop !384
}

; Function Attrs: nounwind mustprogress
define weak void @halide_profiler_memory_allocate(i8* %user_context, i8* %pipeline_state, i32 %func_id, i64 %incr) local_unnamed_addr #0 {
entry:
  %cmp = icmp eq i64 %incr, 0
  br i1 %cmp, label %return, label %if.end

if.end:                                           ; preds = %entry
  %cmp1.not = icmp eq i8* %pipeline_state, null
  br i1 %cmp1.not, label %if.then2, label %do.body4

if.then2:                                         ; preds = %if.end
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.1.187, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.body4

do.body4:                                         ; preds = %if.then2, %if.end
  %cmp5 = icmp sgt i32 %func_id, -1
  br i1 %cmp5, label %do.body10, label %if.then6

if.then6:                                         ; preds = %do.body4
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([122 x i8], [122 x i8]* @.str.2.188, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.body10

do.body10:                                        ; preds = %if.then6, %do.body4
  %num_funcs = getelementptr inbounds i8, i8* %pipeline_state, i64 72
  %0 = bitcast i8* %num_funcs to i32*
  %1 = load i32, i32* %0, align 8, !tbaa !350
  %cmp11 = icmp sgt i32 %1, %func_id
  br i1 %cmp11, label %do.end15, label %if.then12

if.then12:                                        ; preds = %do.body10
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([138 x i8], [138 x i8]* @.str.3.189, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end15

do.end15:                                         ; preds = %if.then12, %do.body10
  %funcs = getelementptr inbounds i8, i8* %pipeline_state, i64 56
  %2 = bitcast i8* %funcs to %struct.halide_profiler_func_stats**
  %3 = load %struct.halide_profiler_func_stats*, %struct.halide_profiler_func_stats** %2, align 8, !tbaa !351
  %idxprom = sext i32 %func_id to i64
  %num_allocs = getelementptr inbounds i8, i8* %pipeline_state, i64 88
  %4 = bitcast i8* %num_allocs to i32*
  %5 = atomicrmw add i32* %4, i32 1 seq_cst
  %memory_total = getelementptr inbounds i8, i8* %pipeline_state, i64 24
  %6 = bitcast i8* %memory_total to i64*
  %7 = atomicrmw add i64* %6, i64 %incr seq_cst
  %memory_current = getelementptr inbounds i8, i8* %pipeline_state, i64 8
  %8 = bitcast i8* %memory_current to i64*
  %9 = atomicrmw add i64* %8, i64 %incr seq_cst
  %10 = add i64 %9, %incr
  %memory_peak = getelementptr inbounds i8, i8* %pipeline_state, i64 16
  %11 = bitcast i8* %memory_peak to i64*
  %12 = load i64, i64* %11, align 8, !tbaa !22
  br label %while.cond.i

while.cond.i:                                     ; preds = %while.body.i, %do.end15
  %old_val.0.i = phi i64 [ %12, %do.end15 ], [ %14, %while.body.i ]
  %cmp.i = icmp ult i64 %old_val.0.i, %10
  br i1 %cmp.i, label %while.body.i, label %_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit

while.body.i:                                     ; preds = %while.cond.i
  %13 = cmpxchg i64* %11, i64 %old_val.0.i, i64 %10 seq_cst seq_cst
  %14 = extractvalue { i64, i1 } %13, 0
  %cmp1.i = icmp eq i64 %old_val.0.i, %14
  br i1 %cmp1.i, label %_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit, label %while.cond.i, !llvm.loop !383

_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit: ; preds = %while.body.i, %while.cond.i
  %num_allocs16 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %3, i64 %idxprom, i32 8
  %15 = atomicrmw add i32* %num_allocs16, i32 1 seq_cst
  %memory_total17 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %3, i64 %idxprom, i32 3
  %16 = atomicrmw add i64* %memory_total17, i64 %incr seq_cst
  %memory_current18 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %3, i64 %idxprom, i32 1
  %17 = atomicrmw add i64* %memory_current18, i64 %incr seq_cst
  %18 = add i64 %17, %incr
  %memory_peak19 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %3, i64 %idxprom, i32 2
  %19 = load i64, i64* %memory_peak19, align 8, !tbaa !22
  br label %while.cond.i43

while.cond.i43:                                   ; preds = %while.body.i45, %_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit
  %old_val.0.i41 = phi i64 [ %19, %_ZN12_GLOBAL__N_125sync_compare_max_and_swapIyEEvPT_S1_.exit ], [ %21, %while.body.i45 ]
  %cmp.i42 = icmp ult i64 %old_val.0.i41, %18
  br i1 %cmp.i42, label %while.body.i45, label %return

while.body.i45:                                   ; preds = %while.cond.i43
  %20 = cmpxchg i64* %memory_peak19, i64 %old_val.0.i41, i64 %18 seq_cst seq_cst
  %21 = extractvalue { i64, i1 } %20, 0
  %cmp1.i44 = icmp eq i64 %old_val.0.i41, %21
  br i1 %cmp1.i44, label %return, label %while.cond.i43, !llvm.loop !383

return:                                           ; preds = %while.body.i45, %while.cond.i43, %entry
  ret void
}

; Function Attrs: nounwind mustprogress
define weak void @halide_profiler_memory_free(i8* %user_context, i8* %pipeline_state, i32 %func_id, i64 %decr) local_unnamed_addr #0 {
entry:
  %cmp = icmp eq i64 %decr, 0
  br i1 %cmp, label %return, label %if.end

if.end:                                           ; preds = %entry
  %cmp1.not = icmp eq i8* %pipeline_state, null
  br i1 %cmp1.not, label %if.then2, label %do.body4

if.then2:                                         ; preds = %if.end
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.4.190, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.body4

do.body4:                                         ; preds = %if.then2, %if.end
  %cmp5 = icmp sgt i32 %func_id, -1
  br i1 %cmp5, label %do.body10, label %if.then6

if.then6:                                         ; preds = %do.body4
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([122 x i8], [122 x i8]* @.str.5.191, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.body10

do.body10:                                        ; preds = %if.then6, %do.body4
  %num_funcs = getelementptr inbounds i8, i8* %pipeline_state, i64 72
  %0 = bitcast i8* %num_funcs to i32*
  %1 = load i32, i32* %0, align 8, !tbaa !350
  %cmp11 = icmp sgt i32 %1, %func_id
  br i1 %cmp11, label %do.end15, label %if.then12

if.then12:                                        ; preds = %do.body10
  tail call void @halide_print(i8* %user_context, i8* getelementptr inbounds ([138 x i8], [138 x i8]* @.str.6.192, i64 0, i64 0)) #15
  tail call void @abort() #15
  br label %do.end15

do.end15:                                         ; preds = %if.then12, %do.body10
  %funcs = getelementptr inbounds i8, i8* %pipeline_state, i64 56
  %2 = bitcast i8* %funcs to %struct.halide_profiler_func_stats**
  %3 = load %struct.halide_profiler_func_stats*, %struct.halide_profiler_func_stats** %2, align 8, !tbaa !351
  %idxprom = sext i32 %func_id to i64
  %memory_current = getelementptr inbounds i8, i8* %pipeline_state, i64 8
  %4 = bitcast i8* %memory_current to i64*
  %5 = atomicrmw sub i64* %4, i64 %decr seq_cst
  %memory_current16 = getelementptr inbounds %struct.halide_profiler_func_stats, %struct.halide_profiler_func_stats* %3, i64 %idxprom, i32 1
  %6 = atomicrmw sub i64* %memory_current16, i64 %decr seq_cst
  br label %return

return:                                           ; preds = %do.end15, %entry
  ret void
}

; Function Attrs: nounwind
define weak void @halide_profiler_report(i8* %user_context) local_unnamed_addr #4 {
entry:
  %call = tail call %struct.halide_profiler_state* @halide_profiler_get_state() #16
  %lock.i = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 0
  tail call void @halide_mutex_lock(%struct.halide_mutex* %lock.i) #15
  tail call void @halide_profiler_report_unlocked(i8* %user_context, %struct.halide_profiler_state* %call) #16
  tail call void @halide_mutex_unlock(%struct.halide_mutex* %lock.i) #15
  ret void
}

; Function Attrs: nounwind
define weak void @halide_profiler_reset() local_unnamed_addr #4 {
entry:
  %call = tail call %struct.halide_profiler_state* @halide_profiler_get_state() #16
  %lock.i = getelementptr inbounds %struct.halide_profiler_state, %struct.halide_profiler_state* %call, i64 0, i32 0
  tail call void @halide_mutex_lock(%struct.halide_mutex* %lock.i) #15
  tail call void @halide_profiler_reset_unlocked(%struct.halide_profiler_state* %call) #16
  tail call void @halide_mutex_unlock(%struct.halide_mutex* %lock.i) #15
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak void @halide_profiler_pipeline_end(i8* %user_context, i8* %state) local_unnamed_addr #2 {
entry:
  %current_func = getelementptr inbounds i8, i8* %state, i64 16
  %0 = bitcast i8* %current_func to i32*
  store i32 -1, i32* %0, align 8, !tbaa !339
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 @halide_msan_annotate_memory_is_initialized(i8* %user_context, i8* %ptr, i64 %len) local_unnamed_addr #2 {
entry:
  ret i32 0
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 @halide_msan_check_memory_is_initialized(i8* %user_context, i8* %ptr, i64 %len, i8* %name) local_unnamed_addr #2 {
entry:
  ret i32 0
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 @halide_msan_check_buffer_is_initialized(i8* %user_context, %struct.halide_buffer_t* %b, i8* %buf_name) local_unnamed_addr #2 {
entry:
  ret i32 0
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 @halide_msan_annotate_buffer_is_initialized(i8* %user_context, %struct.halide_buffer_t* %b) local_unnamed_addr #2 {
entry:
  ret i32 0
}

; Function Attrs: nounwind willreturn mustprogress
define weak void @halide_msan_annotate_buffer_is_initialized_as_destructor(i8* %user_context, i8* %b) local_unnamed_addr #2 {
entry:
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_default_can_use_target_features(i32 %count, i64* %features) #4 {
entry:
  %tmp = alloca %"struct.Halide::Runtime::Internal::CpuFeatures", align 8
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE) #15
  %0 = load i8, i8* @_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE, align 1, !tbaa !19, !range !21
  %tobool.not = icmp eq i8 %0, 0
  br i1 %tobool.not, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  %1 = bitcast %"struct.Halide::Runtime::Internal::CpuFeatures"* %tmp to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %1) #11
  call void @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv(%"struct.Halide::Runtime::Internal::CpuFeatures"* nonnull sret(%"struct.Halide::Runtime::Internal::CpuFeatures") align 8 %tmp) #15
  %call = call i8* @memcpy(i8* bitcast ([4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE to i8*), i8* nonnull %1, i64 32) #15
  store i8 1, i8* @_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE, align 1, !tbaa !19
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %1) #11
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE) #15
  %cmp.not = icmp eq i32 %count, 2
  br i1 %cmp.not, label %if.end2, label %if.then1

if.then1:                                         ; preds = %if.end
  call void @halide_error(i8* null, i8* getelementptr inbounds ([81 x i8], [81 x i8]* @.str.197, i64 0, i64 0)) #15
  br label %if.end2

if.end2:                                          ; preds = %if.then1, %if.end
  %2 = load i64, i64* %features, align 8, !tbaa !22
  %3 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, i64 0, i64 0), align 8, !tbaa !22
  %and = and i64 %3, %2
  %cmp6.not = icmp eq i64 %and, 0
  br i1 %cmp6.not, label %for.inc.critedge, label %if.then7

if.then7:                                         ; preds = %if.end2
  %4 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, i64 0, i64 2), align 8, !tbaa !22
  %and10 = and i64 %4, %and
  %cmp11.not = icmp eq i64 %and10, %and
  br i1 %cmp11.not, label %for.inc.critedge, label %cleanup15

for.inc.critedge:                                 ; preds = %if.then7, %if.end2
  %arrayidx.1 = getelementptr inbounds i64, i64* %features, i64 1
  %5 = load i64, i64* %arrayidx.1, align 8, !tbaa !22
  %6 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, i64 0, i64 1), align 8, !tbaa !22
  %and.1 = and i64 %6, %5
  %cmp6.not.1 = icmp eq i64 %and.1, 0
  br i1 %cmp6.not.1, label %for.inc.critedge.1, label %if.then7.1

cleanup15:                                        ; preds = %for.inc.critedge.1, %if.then7.1, %if.then7
  %cmp3.lcssa = phi i32 [ 0, %if.then7 ], [ 0, %if.then7.1 ], [ 1, %for.inc.critedge.1 ]
  ret i32 %cmp3.lcssa

if.then7.1:                                       ; preds = %for.inc.critedge
  %7 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, i64 0, i64 3), align 8, !tbaa !22
  %and10.1 = and i64 %7, %and.1
  %cmp11.not.1 = icmp eq i64 %and10.1, %and.1
  br i1 %cmp11.not.1, label %for.inc.critedge.1, label %cleanup15

for.inc.critedge.1:                               ; preds = %if.then7.1, %for.inc.critedge
  br label %cleanup15
}

; Function Attrs: nounwind willreturn mustprogress
define weak i32 (i32, i64*)* @halide_set_custom_can_use_target_features(i32 (i32, i64*)* %fn) local_unnamed_addr #2 {
entry:
  %0 = load i32 (i32, i64*)*, i32 (i32, i64*)** @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, align 8, !tbaa !14
  store i32 (i32, i64*)* %fn, i32 (i32, i64*)** @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, align 8, !tbaa !14
  ret i32 (i32, i64*)* %0
}

; Function Attrs: nounwind mustprogress
define weak i32 @halide_can_use_target_features(i32 %count, i64* %features) local_unnamed_addr #0 {
entry:
  %0 = load i32 (i32, i64*)*, i32 (i32, i64*)** @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, align 8, !tbaa !14
  %call = tail call i32 %0(i32 %count, i64* %features) #15
  ret i32 %call
}

; Function Attrs: nounwind willreturn
define linkonce void @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv(%"struct.Halide::Runtime::Internal::CpuFeatures"* noalias sret(%"struct.Halide::Runtime::Internal::CpuFeatures") align 8 %agg.result) local_unnamed_addr #8 {
entry:
  %arrayidx3.i = getelementptr inbounds %"struct.Halide::Runtime::Internal::CpuFeatures", %"struct.Halide::Runtime::Internal::CpuFeatures"* %agg.result, i64 0, i32 1, i64 0
  %0 = bitcast %"struct.Halide::Runtime::Internal::CpuFeatures"* %agg.result to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %0, align 8, !tbaa !22
  %1 = bitcast i64* %arrayidx3.i to <2 x i64>*
  store <2 x i64> zeroinitializer, <2 x i64>* %1, align 8, !tbaa !22
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak void @halide_use_jit_module() local_unnamed_addr #2 {
entry:
  ret void
}

; Function Attrs: nounwind willreturn mustprogress
define weak void @halide_release_jit_module() local_unnamed_addr #2 {
entry:
  ret void
}

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16>, <4 x i16>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16>, <8 x i16>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32>, <4 x i32>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8>, <16 x i8>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8>, <16 x i8>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32>, <4 x i32>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32>, <4 x i32>) #9

; Function Attrs: nofree nosync nounwind readnone willreturn
declare <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32>, <4 x i32>) #9

; Function Attrs: nounwind
define i32 @depthwise_conv(%struct.halide_buffer_t* noalias nocapture readonly %input.buffer, i8 %input_zero, %struct.halide_buffer_t* noalias nocapture readonly %filter.buffer, i8 %filter_zero, %struct.halide_buffer_t* noalias nocapture readonly %bias.buffer, i32 %depth_multiplier, i32 %stride_x, i32 %stride_y, i32 %a614, i32 %a613, i32 %output_multiplier, i32 %output_shift, i8 %output_zero, i8 %output_min, i8 %output_max, %struct.halide_buffer_t* noalias readonly %output.buffer) local_unnamed_addr #10 {
entry:
  %sum_filter945 = alloca [64 x i32], align 16
  %resampled_input.pseudostack_slot = alloca %struct.halide_pseudostack_slot_t, align 8
  %.fca.0.gep874 = getelementptr inbounds %struct.halide_pseudostack_slot_t, %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot, i64 0, i32 0
  %.fca.1.gep875 = getelementptr inbounds %struct.halide_pseudostack_slot_t, %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot, i64 0, i32 1
  %.fca.2.gep876 = getelementptr inbounds %struct.halide_pseudostack_slot_t, %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot, i64 0, i32 2
  %offset_c636946 = alloca [16 x i32], align 16
  %offset_c947 = alloca [16 x i32], align 16
  %filter_zeroed.pseudostack_slot = alloca %struct.halide_pseudostack_slot_t, align 8
  %.fca.0.gep = getelementptr inbounds %struct.halide_pseudostack_slot_t, %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot, i64 0, i32 0
  %0 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 dereferenceable(24) %0, i8 0, i64 24, i1 false)
  %.fca.1.gep = getelementptr inbounds %struct.halide_pseudostack_slot_t, %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot, i64 0, i32 1
  %.fca.2.gep = getelementptr inbounds %struct.halide_pseudostack_slot_t, %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot, i64 0, i32 2
  %host.i = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %bias.buffer, i64 0, i32 2
  %1 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 dereferenceable(24) %1, i8 0, i64 24, i1 false)
  %2 = load i8*, i8** %host.i, align 8, !tbaa !180
  %host.i2937 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %filter.buffer, i64 0, i32 2
  %3 = load i8*, i8** %host.i2937, align 8, !tbaa !180
  %dim.i2938 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %filter.buffer, i64 0, i32 6
  %4 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i2938, align 8, !tbaa !184
  %extent.i = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %4, i64 0, i32 1
  %5 = load i32, i32* %extent.i, align 4, !tbaa !189
  %extent.i2946 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %4, i64 1, i32 1
  %6 = load i32, i32* %extent.i2946, align 4, !tbaa !189
  %stride.i2948 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %4, i64 1, i32 2
  %7 = load i32, i32* %stride.i2948, align 4, !tbaa !185
  %extent.i2952 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %4, i64 2, i32 1
  %8 = load i32, i32* %extent.i2952, align 4, !tbaa !189
  %stride.i2954 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %4, i64 2, i32 2
  %9 = load i32, i32* %stride.i2954, align 4, !tbaa !185
  %host.i2955 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %input.buffer, i64 0, i32 2
  %10 = load i8*, i8** %host.i2955, align 8, !tbaa !180
  %dim.i2956 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %input.buffer, i64 0, i32 6
  %11 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i2956, align 8, !tbaa !184
  %min.i2961 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %11, i64 1, i32 0
  %12 = load i32, i32* %min.i2961, align 4, !tbaa !221
  %stride.i2963 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %11, i64 1, i32 2
  %13 = load i32, i32* %stride.i2963, align 4, !tbaa !185
  %min.i2965 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %11, i64 2, i32 0
  %14 = load i32, i32* %min.i2965, align 4, !tbaa !221
  %stride.i2967 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %11, i64 2, i32 2
  %15 = load i32, i32* %stride.i2967, align 4, !tbaa !185
  %min.i2969 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %11, i64 3, i32 0
  %16 = load i32, i32* %min.i2969, align 4, !tbaa !221
  %extent.i2971 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %11, i64 3, i32 1
  %17 = load i32, i32* %extent.i2971, align 4, !tbaa !189
  %stride.i2973 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %11, i64 3, i32 2
  %18 = load i32, i32* %stride.i2973, align 4, !tbaa !185
  %host.i2974 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %output.buffer, i64 0, i32 2
  %19 = load i8*, i8** %host.i2974, align 8, !tbaa !180
  %dim.i2975 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %output.buffer, i64 0, i32 6
  %20 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i2975, align 8, !tbaa !184
  %min.i2982 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %20, i64 1, i32 0
  %21 = load i32, i32* %min.i2982, align 4, !tbaa !221
  %extent.i2984 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %20, i64 1, i32 1
  %22 = load i32, i32* %extent.i2984, align 4, !tbaa !189
  %stride.i2986 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %20, i64 1, i32 2
  %23 = load i32, i32* %stride.i2986, align 4, !tbaa !185
  %min.i2988 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %20, i64 2, i32 0
  %24 = load i32, i32* %min.i2988, align 4, !tbaa !221
  %extent.i2990 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %20, i64 2, i32 1
  %25 = load i32, i32* %extent.i2990, align 4, !tbaa !189
  %stride.i2992 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %20, i64 2, i32 2
  %26 = load i32, i32* %stride.i2992, align 4, !tbaa !185
  %stride.i2998 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %20, i64 3, i32 2
  %27 = load i32, i32* %stride.i2998, align 4, !tbaa !185
  %28 = icmp sgt i32 %22, 7
  %29 = and i32 %22, 1
  %30 = icmp eq i32 %29, 0
  %31 = or i1 %28, %30
  %32 = icmp sgt i32 %25, 7
  %33 = and i32 %25, 1
  %34 = icmp eq i32 %33, 0
  %35 = or i1 %32, %34
  %36 = and i1 %31, %35
  %37 = icmp sgt i32 %5, 15
  %38 = and i1 %37, %36
  br i1 %38, label %then_bb, label %next_bb

after_bb.loopexit:                                ; preds = %"end for output.s0.b.rebased"
  %39 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8*
  br label %after_bb

after_bb.loopexit5216:                            ; preds = %"end for output.s0.b.rebased48"
  %40 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8*
  br label %after_bb

after_bb.loopexit5218:                            ; preds = %"end for output.s0.y.yo187"
  %41 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8*
  br label %after_bb

after_bb:                                         ; preds = %after_bb.loopexit5218, %after_bb.loopexit5216, %after_bb.loopexit, %"consume sum_filter114", %next_bb19
  %.05065 = phi i8* [ null, %"consume sum_filter114" ], [ null, %next_bb19 ], [ %.2, %after_bb.loopexit ], [ %.4, %after_bb.loopexit5216 ], [ %41, %after_bb.loopexit5218 ]
  %.0 = phi i8* [ %5357, %"consume sum_filter114" ], [ null, %next_bb19 ], [ %39, %after_bb.loopexit ], [ %40, %after_bb.loopexit5216 ], [ %5357, %after_bb.loopexit5218 ]
  %42 = load i8*, i8** %host.i, align 8, !tbaa !180
  %43 = load i8*, i8** %host.i2937, align 8, !tbaa !180
  %44 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i2938, align 8, !tbaa !184
  %extent.i3008 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %44, i64 0, i32 1
  %45 = load i32, i32* %extent.i3008, align 4, !tbaa !189
  %extent.i3014 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %44, i64 1, i32 1
  %46 = load i32, i32* %extent.i3014, align 4, !tbaa !189
  %stride.i3016 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %44, i64 1, i32 2
  %47 = load i32, i32* %stride.i3016, align 4, !tbaa !185
  %extent.i3020 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %44, i64 2, i32 1
  %48 = load i32, i32* %extent.i3020, align 4, !tbaa !189
  %stride.i3022 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %44, i64 2, i32 2
  %49 = load i32, i32* %stride.i3022, align 4, !tbaa !185
  %50 = load i8*, i8** %host.i2955, align 8, !tbaa !180
  %51 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i2956, align 8, !tbaa !184
  %min.i3029 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %51, i64 1, i32 0
  %52 = load i32, i32* %min.i3029, align 4, !tbaa !221
  %stride.i3031 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %51, i64 1, i32 2
  %53 = load i32, i32* %stride.i3031, align 4, !tbaa !185
  %min.i3033 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %51, i64 2, i32 0
  %54 = load i32, i32* %min.i3033, align 4, !tbaa !221
  %stride.i3035 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %51, i64 2, i32 2
  %55 = load i32, i32* %stride.i3035, align 4, !tbaa !185
  %min.i3037 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %51, i64 3, i32 0
  %56 = load i32, i32* %min.i3037, align 4, !tbaa !221
  %extent.i3039 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %51, i64 3, i32 1
  %57 = load i32, i32* %extent.i3039, align 4, !tbaa !189
  %stride.i3041 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %51, i64 3, i32 2
  %58 = load i32, i32* %stride.i3041, align 4, !tbaa !185
  %59 = load i8*, i8** %host.i2974, align 8, !tbaa !180
  %60 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %dim.i2975, align 8, !tbaa !184
  %min.i3050 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %60, i64 1, i32 0
  %61 = load i32, i32* %min.i3050, align 4, !tbaa !221
  %extent.i3052 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %60, i64 1, i32 1
  %62 = load i32, i32* %extent.i3052, align 4, !tbaa !189
  %stride.i3054 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %60, i64 1, i32 2
  %63 = load i32, i32* %stride.i3054, align 4, !tbaa !185
  %min.i3056 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %60, i64 2, i32 0
  %64 = load i32, i32* %min.i3056, align 4, !tbaa !221
  %extent.i3058 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %60, i64 2, i32 1
  %65 = load i32, i32* %extent.i3058, align 4, !tbaa !189
  %stride.i3060 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %60, i64 2, i32 2
  %66 = load i32, i32* %stride.i3060, align 4, !tbaa !185
  %stride.i3066 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %60, i64 3, i32 2
  %67 = load i32, i32* %stride.i3066, align 4, !tbaa !185
  %68 = icmp sgt i32 %62, 7
  %69 = and i32 %62, 1
  %70 = icmp eq i32 %69, 0
  %71 = or i1 %68, %70
  %72 = icmp sgt i32 %65, 7
  %73 = and i32 %65, 1
  %74 = icmp eq i32 %73, 0
  %75 = or i1 %72, %74
  %76 = and i1 %71, %75
  %77 = icmp sgt i32 %45, 15
  %78 = and i1 %77, %76
  br i1 %78, label %then_bb237, label %next_bb238

then_bb:                                          ; preds = %entry
  %t2201 = icmp slt i32 %a614, 0
  %79 = add nsw i32 %6, -1
  %t2202 = mul nsw i32 %79, %a614
  %t2203 = icmp slt i32 %stride_x, 0
  %80 = icmp slt i32 %22, 2
  %t2204 = select i1 %80, i32 %22, i32 2
  %t2205 = add nsw i32 %t2204, -2
  %t2206 = add nsw i32 %22, -1
  %t2207 = select i1 %t2201, i32 %t2202, i32 0
  %81 = select i1 %t2203, i32 %t2206, i32 %t2205
  %82 = add nsw i32 %81, %21
  %t2208 = mul nsw i32 %82, %stride_x
  %t2209 = icmp slt i32 %a613, 0
  %83 = add nsw i32 %8, -1
  %t2210 = mul nsw i32 %83, %a613
  %t2211 = icmp slt i32 %stride_y, 0
  %84 = icmp slt i32 %25, 2
  %t2212 = select i1 %84, i32 %25, i32 2
  %t2213 = add nsw i32 %t2212, -2
  %t2214 = add nsw i32 %25, -1
  %t2215 = select i1 %t2209, i32 %t2210, i32 0
  %85 = select i1 %t2211, i32 %t2214, i32 %t2213
  %86 = add nsw i32 %85, %24
  %t2216 = mul nsw i32 %86, %stride_y
  %87 = icmp eq i32 %6, 3
  %88 = icmp eq i32 %8, 3
  %t2217 = and i1 %87, %88
  %a614.op2914 = shl i32 %a614, 1
  %t2218 = select i1 %t2201, i32 %a614.op2914, i32 0
  %a613.op2915 = shl i32 %a613, 1
  %t2219 = select i1 %t2209, i32 %a613.op2915, i32 0
  %t2194 = icmp eq i32 %depth_multiplier, 1
  %89 = add nuw nsw i32 %25, 1
  %t2196 = ashr i32 %89, 1
  %90 = add nuw nsw i32 %22, 1
  %t2197 = ashr i32 %90, 1
  %91 = add nuw nsw i32 %5, 15
  %t2172 = ashr i32 %91, 4
  %92 = icmp sgt i32 %a613, 0
  %93 = select i1 %92, i32 %a613, i32 0
  %t2185 = shl nuw nsw i32 %93, 1
  %94 = icmp sgt i32 %a614, 0
  %95 = select i1 %94, i32 %a614, i32 0
  %t2192 = shl nuw nsw i32 %95, 1
  %96 = select i1 %t2217, i32 %t2219, i32 %t2215
  %b16 = add nsw i32 %t2216, %96
  %97 = select i1 %t2217, i32 %t2218, i32 %t2207
  %b15 = add nsw i32 %t2208, %97
  %b20 = add nsw i32 %t2216, %t2215
  %98 = select i1 %t2209, i32 0, i32 %t2210
  %99 = select i1 %t2211, i32 %t2213, i32 %t2214
  %100 = add nsw i32 %99, %24
  %101 = mul nsw i32 %100, %stride_y
  %b19 = add nsw i32 %101, %98
  %b18 = add nsw i32 %t2208, %t2207
  %102 = select i1 %t2201, i32 0, i32 %t2202
  %103 = select i1 %t2203, i32 %t2205, i32 %t2206
  %104 = add nsw i32 %103, %21
  %105 = mul nsw i32 %104, %stride_x
  %b17 = add nsw i32 %105, %102
  %106 = mul nsw i32 %15, %14
  %107 = mul nsw i32 %18, %16
  %108 = mul nsw i32 %13, %12
  %109 = add i32 %106, %108
  %t2195 = add i32 %109, %107
  %b10 = add nsw i32 %5, -16
  %110 = icmp sgt i32 %6, 0
  %111 = select i1 %110, i32 %6, i32 0
  %t2652 = zext i32 %111 to i64
  %112 = icmp sgt i32 %8, 0
  %113 = select i1 %112, i32 %8, i32 0
  %t2653 = zext i32 %113 to i64
  %114 = shl nuw nsw i64 %t2652, 5
  %115 = mul i64 %114, %t2653
  %116 = or i64 %115, 6
  %117 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8***
  %118 = sext i32 %7 to i64
  %119 = insertelement <8 x i8> undef, i8 %filter_zero, i32 0
  %120 = shufflevector <8 x i8> %119, <8 x i8> undef, <8 x i32> zeroinitializer
  %121 = zext <8 x i8> %120 to <8 x i16>
  %122 = bitcast [16 x i32]* %offset_c636946 to <4 x i32>*
  %123 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 4
  %124 = bitcast i32* %123 to <4 x i32>*
  %125 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 8
  %126 = bitcast i32* %125 to <4 x i32>*
  %127 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 12
  %128 = bitcast i32* %127 to <4 x i32>*
  %129 = bitcast i8* %2 to i32*
  %130 = zext i8 %input_zero to i32
  %131 = insertelement <16 x i32> undef, i32 %130, i32 0
  %132 = shufflevector <16 x i32> %131, <16 x i32> undef, <16 x i32> zeroinitializer
  %133 = bitcast [16 x i32]* %offset_c947 to <4 x i32>*
  %134 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 4
  %135 = bitcast i32* %134 to <4 x i32>*
  %136 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 8
  %137 = bitcast i32* %136 to <4 x i32>*
  %138 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 12
  %139 = bitcast i32* %138 to <4 x i32>*
  %140 = icmp slt i32 %t2207, %97
  %t2225 = select i1 %140, i32 %b18, i32 %b15
  %141 = icmp slt i32 %t2215, %96
  %t2223 = select i1 %141, i32 %b20, i32 %b16
  %142 = select i1 %t2217, i32 %t2192, i32 %102
  %a15 = add nsw i32 %105, %142
  %143 = icmp sgt i32 %142, %102
  %144 = select i1 %143, i32 %a15, i32 %b17
  %t2226 = sub nsw i32 %144, %t2225
  %145 = select i1 %t2217, i32 %t2185, i32 %98
  %a17 = add nsw i32 %101, %145
  %146 = icmp sgt i32 %145, %98
  %147 = select i1 %146, i32 %a17, i32 %b19
  %t2224 = sub nsw i32 %147, %t2223
  %t2228 = sub nsw i32 %b17, %b18
  %t2227 = sub nsw i32 %b19, %b20
  %.neg5159 = mul i32 %26, %24
  %.neg5160 = mul i32 %23, %21
  %.neg5161 = mul i32 %27, %16
  %148 = icmp sgt i32 %17, 0
  %a22 = add nsw i32 %t2224, 1
  %a21 = add nsw i32 %t2226, 1
  %.inv2918 = icmp slt i32 %t2226, 0
  %149 = select i1 %.inv2918, i32 0, i32 %a21
  %t2654 = zext i32 %149 to i64
  %.inv2919 = icmp slt i32 %t2224, 0
  %150 = select i1 %.inv2919, i32 0, i32 %a22
  %t2655 = zext i32 %150 to i64
  %t2656 = shl nuw nsw i64 %t2654, 4
  %151 = mul i64 %t2656, %t2655
  %152 = or i64 %151, 3
  %153 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8***
  %t2245 = sub i32 %b18, %t2225
  %t2246 = sub i32 %b20, %t2223
  %154 = icmp eq i32 %depth_multiplier, 0
  %t2659 = select i1 %154, <16 x i32> <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>, <16 x i32> zeroinitializer
  %depth_multiplier.lobit2928 = ashr i32 %depth_multiplier, 31
  %155 = insertelement <16 x i32> undef, i32 %depth_multiplier, i32 0
  %156 = shufflevector <16 x i32> %155, <16 x i32> undef, <16 x i32> zeroinitializer
  %157 = sub nsw <16 x i32> %156, %t2659
  %158 = xor i32 %depth_multiplier.lobit2928, -1
  %159 = sub nsw i32 %158, %depth_multiplier.lobit2928
  %160 = insertelement <16 x i32> undef, i32 %159, i32 0
  %161 = shufflevector <16 x i32> %160, <16 x i32> undef, <16 x i32> zeroinitializer
  %162 = xor <16 x i32> %t2659, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  %163 = sext i32 %b18 to i64
  %164 = sext i32 %13 to i64
  %t2254 = sub nsw i32 %a613.op2915, %t2223
  %t2250 = sub nsw i32 %a614.op2914, %t2225
  %reass.add5163 = add i32 %.neg5159, %.neg5160
  %reass.add5164 = add i32 %reass.add5163, %.neg5161
  %165 = icmp sgt i32 %25, 0
  %b25 = add nsw i32 %25, -2
  %t2284 = sub nsw i32 %a614, %t2225
  %t2285 = sub nsw i32 %a613, %t2223
  %166 = icmp sgt i32 %22, 0
  %b26 = add nsw i32 %22, -2
  %167 = bitcast [64 x i32]* %sum_filter945 to <4 x i32>*
  %168 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 4
  %169 = bitcast i32* %168 to <4 x i32>*
  %170 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 8
  %171 = bitcast i32* %170 to <4 x i32>*
  %172 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 12
  %173 = bitcast i32* %172 to <4 x i32>*
  %174 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 16
  %175 = bitcast i32* %174 to <4 x i32>*
  %176 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 20
  %177 = bitcast i32* %176 to <4 x i32>*
  %178 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 24
  %179 = bitcast i32* %178 to <4 x i32>*
  %180 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 28
  %181 = bitcast i32* %180 to <4 x i32>*
  %182 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 32
  %183 = bitcast i32* %182 to <4 x i32>*
  %184 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 36
  %185 = bitcast i32* %184 to <4 x i32>*
  %186 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 40
  %187 = bitcast i32* %186 to <4 x i32>*
  %188 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 44
  %189 = bitcast i32* %188 to <4 x i32>*
  %190 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 48
  %191 = bitcast i32* %190 to <4 x i32>*
  %192 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 52
  %193 = bitcast i32* %192 to <4 x i32>*
  %194 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 56
  %195 = bitcast i32* %194 to <4 x i32>*
  %196 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 60
  %197 = bitcast i32* %196 to <4 x i32>*
  %198 = sext i32 %a614 to i64
  %199 = sext i32 %21 to i64
  %200 = sext i32 %stride_x to i64
  %201 = insertelement <16 x i32> undef, i32 %output_multiplier, i32 0
  %202 = shufflevector <16 x i32> %201, <16 x i32> undef, <4 x i32> zeroinitializer
  %203 = sub nsw i32 0, %output_shift
  %204 = insertelement <16 x i32> undef, i32 %203, i32 0
  %205 = shufflevector <16 x i32> %204, <16 x i32> undef, <4 x i32> zeroinitializer
  %206 = zext i8 %output_zero to i16
  %207 = insertelement <16 x i16> undef, i16 %206, i32 0
  %208 = shufflevector <16 x i16> %207, <16 x i16> undef, <8 x i32> zeroinitializer
  %209 = insertelement <16 x i8> undef, i8 %output_max, i32 0
  %210 = shufflevector <16 x i8> %209, <16 x i8> undef, <16 x i32> zeroinitializer
  %211 = insertelement <16 x i8> undef, i8 %output_min, i32 0
  %212 = shufflevector <16 x i8> %211, <16 x i8> undef, <16 x i32> zeroinitializer
  %213 = sext i32 %23 to i64
  %214 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8*
  %215 = zext i32 %6 to i64
  %216 = sext i32 %6 to i64
  %217 = sext i32 %9 to i64
  %218 = zext i32 %8 to i64
  %219 = zext i32 %t2228 to i64
  %220 = sext i32 %t2246 to i64
  %221 = sext i32 %a21 to i64
  %222 = sext i32 %t2245 to i64
  %223 = sext i32 %b20 to i64
  %224 = sext i32 %15 to i64
  %225 = zext i32 %t2227 to i64
  %226 = sext i32 %16 to i64
  %227 = sext i32 %18 to i64
  %228 = bitcast [16 x i32]* %offset_c636946 to i8*
  %229 = bitcast [16 x i32]* %offset_c636946 to i8*
  %230 = bitcast [16 x i32]* %offset_c636946 to i8*
  %zext6391 = zext i32 %17 to i64
  %231 = or i32 %t2227, %t2228
  %232 = icmp slt i32 %231, 0
  %233 = or i32 %t2227, %t2228
  %.not6880 = icmp sgt i32 %233, -1
  br label %"for output.s0.c.co"

next_bb:                                          ; preds = %entry
  br i1 %37, label %then_bb18, label %next_bb19

"for output.s0.c.co":                             ; preds = %then_bb, %"end for output.s0.b.rebased"
  %.15066 = phi i8* [ null, %then_bb ], [ %.2, %"end for output.s0.b.rebased" ]
  %output.s0.c.co = phi i32 [ 0, %then_bb ], [ %364, %"end for output.s0.b.rebased" ]
  %a8 = shl nsw i32 %output.s0.c.co, 4
  %234 = icmp slt i32 %a8, %b10
  %output.s0.c.c.base = select i1 %234, i32 %a8, i32 %b10
  %235 = load i64, i64* %.fca.1.gep, align 8, !tbaa !385
  %cmp.i = icmp ult i64 %235, %116
  %236 = load i8*, i8** %.fca.0.gep, align 8, !tbaa !387
  br i1 %cmp.i, label %if.then.i, label %pseudostack_alloc.exit, !prof !388

if.then.i:                                        ; preds = %"for output.s0.c.co"
  %tobool1.not.i = icmp ne i8* %236, null
  %237 = load i64, i64* %.fca.2.gep, align 8
  %cmp2.i = icmp ugt i64 %237, 16384
  %or.cond = and i1 %tobool1.not.i, %cmp2.i
  br i1 %or.cond, label %if.then3.i, label %if.end.i

if.then3.i:                                       ; preds = %if.then.i
  call void @halide_free(i8* null, i8* nonnull %236) #15
  %.pre6505 = load i64, i64* %.fca.2.gep, align 8, !tbaa !389
  br label %if.end.i

if.end.i:                                         ; preds = %if.then3.i, %if.then.i
  %238 = phi i64 [ %.pre6505, %if.then3.i ], [ %237, %if.then.i ]
  %add.i = add i64 %238, %116
  store i64 %add.i, i64* %.fca.2.gep, align 8, !tbaa !389
  %cmp7.i = icmp ugt i64 %add.i, 16384
  br i1 %cmp7.i, label %if.then8.i, label %if.end11.i

if.then8.i:                                       ; preds = %if.end.i
  %call.i = call i8* @halide_malloc(i8* null, i64 %116) #15
  br label %if.end11.i

if.end11.i:                                       ; preds = %if.then8.i, %if.end.i
  %storemerge.i = phi i8* [ %call.i, %if.then8.i ], [ null, %if.end.i ]
  store i8* %storemerge.i, i8** %.fca.0.gep, align 8, !tbaa !387
  store i64 %116, i64* %.fca.1.gep, align 8, !tbaa !385
  br label %pseudostack_alloc.exit

pseudostack_alloc.exit:                           ; preds = %"for output.s0.c.co", %if.end11.i
  %239 = phi i8* [ %storemerge.i, %if.end11.i ], [ %236, %"for output.s0.c.co" ]
  %240 = bitcast i8* %239 to i16*
  %.not2916 = icmp eq i8* %239, null
  br i1 %.not2916, label %then_bb2, label %"produce filter_zeroed", !prof !390

then_bb2:                                         ; preds = %pseudostack_alloc.exit
  %241 = alloca i8*, i64 %116, align 16
  %242 = bitcast i8** %241 to i16*
  store i8** %241, i8*** %117, align 8
  br label %"produce filter_zeroed"

"produce filter_zeroed":                          ; preds = %pseudostack_alloc.exit, %then_bb2
  %filter_zeroed = phi i16* [ %242, %then_bb2 ], [ %240, %pseudostack_alloc.exit ]
  br i1 %112, label %"for filter_zeroed.s0.y.preheader", label %"consume sum_filter.critedge", !prof !391

"for filter_zeroed.s0.y.preheader":               ; preds = %"produce filter_zeroed"
  br i1 %110, label %"for filter_zeroed.s0.y.us.preheader", label %"for sum_filter.s1.r19$y.preheader.thread", !prof !391

"for sum_filter.s1.r19$y.preheader.thread":       ; preds = %"for filter_zeroed.s0.y.preheader"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %229, i8 0, i64 64, i1 false)
  br label %"consume sum_filter"

"for filter_zeroed.s0.y.us.preheader":            ; preds = %"for filter_zeroed.s0.y.preheader"
  %243 = sext i32 %output.s0.c.c.base to i64
  br label %"for filter_zeroed.s0.y.us"

"for filter_zeroed.s0.y.us":                      ; preds = %"for filter_zeroed.s0.y.us.preheader", %"end for filter_zeroed.s0.x.loopexit.us"
  %indvars.iv6361 = phi i64 [ 0, %"for filter_zeroed.s0.y.us.preheader" ], [ %indvars.iv.next6362, %"end for filter_zeroed.s0.x.loopexit.us" ]
  %244 = mul nsw i64 %indvars.iv6361, %216
  %245 = mul nsw i64 %indvars.iv6361, %217
  %246 = add nsw i64 %245, %243
  br label %"for filter_zeroed.s0.x.us"

"for filter_zeroed.s0.x.us":                      ; preds = %"for filter_zeroed.s0.y.us", %"for filter_zeroed.s0.x.us"
  %indvars.iv6359 = phi i64 [ 0, %"for filter_zeroed.s0.y.us" ], [ %indvars.iv.next6360, %"for filter_zeroed.s0.x.us" ]
  %247 = mul nsw i64 %indvars.iv6359, %118
  %248 = add nsw i64 %247, %246
  %249 = getelementptr inbounds i8, i8* %3, i64 %248
  %250 = bitcast i8* %249 to <8 x i8>*
  %251 = load <8 x i8>, <8 x i8>* %250, align 1, !tbaa !392
  %252 = zext <8 x i8> %251 to <8 x i16>
  %253 = sub nsw <8 x i16> %252, %121
  %254 = add nsw i64 %indvars.iv6359, %244
  %255 = shl nsw i64 %254, 4
  %256 = getelementptr inbounds i16, i16* %filter_zeroed, i64 %255
  %257 = bitcast i16* %256 to <8 x i16>*
  store <8 x i16> %253, <8 x i16>* %257, align 16, !tbaa !395
  %258 = getelementptr inbounds i8, i8* %249, i64 8
  %259 = bitcast i8* %258 to <8 x i8>*
  %260 = load <8 x i8>, <8 x i8>* %259, align 1, !tbaa !392
  %261 = zext <8 x i8> %260 to <8 x i16>
  %262 = sub nsw <8 x i16> %261, %121
  %263 = getelementptr inbounds i16, i16* %256, i64 8
  %264 = bitcast i16* %263 to <8 x i16>*
  store <8 x i16> %262, <8 x i16>* %264, align 16, !tbaa !395
  %indvars.iv.next6360 = add nuw nsw i64 %indvars.iv6359, 1
  %.not2935.us = icmp eq i64 %indvars.iv.next6360, %215
  br i1 %.not2935.us, label %"end for filter_zeroed.s0.x.loopexit.us", label %"for filter_zeroed.s0.x.us"

"end for filter_zeroed.s0.x.loopexit.us":         ; preds = %"for filter_zeroed.s0.x.us"
  %indvars.iv.next6362 = add nuw nsw i64 %indvars.iv6361, 1
  %.not2934.us = icmp eq i64 %indvars.iv.next6362, %218
  br i1 %.not2934.us, label %"for sum_filter.s1.r19$y.preheader", label %"for filter_zeroed.s0.y.us"

"for sum_filter.s1.r19$y.preheader":              ; preds = %"end for filter_zeroed.s0.x.loopexit.us"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %230, i8 0, i64 64, i1 false)
  br i1 %110, label %"for sum_filter.s1.r19$y.us", label %"consume sum_filter", !prof !391

"for sum_filter.s1.r19$y.us":                     ; preds = %"for sum_filter.s1.r19$y.preheader", %"end for sum_filter.s1.r19$x.loopexit.us"
  %indvars.iv6367 = phi i64 [ %indvars.iv.next6368, %"end for sum_filter.s1.r19$x.loopexit.us" ], [ 0, %"for sum_filter.s1.r19$y.preheader" ]
  %.lcssa5658.us5666 = phi <4 x i32> [ %287, %"end for sum_filter.s1.r19$x.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader" ]
  %.lcssa5656.us5664 = phi <4 x i32> [ %286, %"end for sum_filter.s1.r19$x.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader" ]
  %.lcssa5654.us5662 = phi <4 x i32> [ %285, %"end for sum_filter.s1.r19$x.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader" ]
  %.lcssa5652.us5660 = phi <4 x i32> [ %284, %"end for sum_filter.s1.r19$x.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader" ]
  %265 = mul nsw i64 %indvars.iv6367, %216
  br label %"for sum_filter.s1.r19$x.us"

"for sum_filter.s1.r19$x.us":                     ; preds = %"for sum_filter.s1.r19$y.us", %"for sum_filter.s1.r19$x.us"
  %indvars.iv6365 = phi i64 [ 0, %"for sum_filter.s1.r19$y.us" ], [ %indvars.iv.next6366, %"for sum_filter.s1.r19$x.us" ]
  %266 = phi <4 x i32> [ %.lcssa5658.us5666, %"for sum_filter.s1.r19$y.us" ], [ %287, %"for sum_filter.s1.r19$x.us" ]
  %267 = phi <4 x i32> [ %.lcssa5656.us5664, %"for sum_filter.s1.r19$y.us" ], [ %286, %"for sum_filter.s1.r19$x.us" ]
  %268 = phi <4 x i32> [ %.lcssa5654.us5662, %"for sum_filter.s1.r19$y.us" ], [ %285, %"for sum_filter.s1.r19$x.us" ]
  %269 = phi <4 x i32> [ %.lcssa5652.us5660, %"for sum_filter.s1.r19$y.us" ], [ %284, %"for sum_filter.s1.r19$x.us" ]
  %270 = shufflevector <4 x i32> %269, <4 x i32> %268, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %271 = shufflevector <4 x i32> %267, <4 x i32> %266, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %272 = shufflevector <8 x i32> %270, <8 x i32> %271, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %273 = add nsw i64 %indvars.iv6365, %265
  %274 = shl nsw i64 %273, 4
  %275 = getelementptr inbounds i16, i16* %filter_zeroed, i64 %274
  %276 = bitcast i16* %275 to <8 x i16>*
  %277 = load <8 x i16>, <8 x i16>* %276, align 16, !tbaa !395
  %278 = getelementptr inbounds i16, i16* %275, i64 8
  %279 = bitcast i16* %278 to <8 x i16>*
  %280 = load <8 x i16>, <8 x i16>* %279, align 16, !tbaa !395
  %281 = shufflevector <8 x i16> %277, <8 x i16> %280, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %282 = sext <16 x i16> %281 to <16 x i32>
  %283 = add nsw <16 x i32> %272, %282
  %284 = shufflevector <16 x i32> %283, <16 x i32> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %285 = shufflevector <16 x i32> %283, <16 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %286 = shufflevector <16 x i32> %283, <16 x i32> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %287 = shufflevector <16 x i32> %283, <16 x i32> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %indvars.iv.next6366 = add nuw nsw i64 %indvars.iv6365, 1
  %.not2933.us = icmp eq i64 %indvars.iv.next6366, %215
  br i1 %.not2933.us, label %"end for sum_filter.s1.r19$x.loopexit.us", label %"for sum_filter.s1.r19$x.us"

"end for sum_filter.s1.r19$x.loopexit.us":        ; preds = %"for sum_filter.s1.r19$x.us"
  %indvars.iv.next6368 = add nuw nsw i64 %indvars.iv6367, 1
  %.not2932.us = icmp eq i64 %indvars.iv.next6368, %218
  br i1 %.not2932.us, label %"consume sum_filter.loopexit.split.us", label %"for sum_filter.s1.r19$y.us"

"consume sum_filter.loopexit.split.us":           ; preds = %"end for sum_filter.s1.r19$x.loopexit.us"
  store <4 x i32> %284, <4 x i32>* %122, align 16, !tbaa !397
  store <4 x i32> %285, <4 x i32>* %124, align 16, !tbaa !408
  store <4 x i32> %286, <4 x i32>* %126, align 16, !tbaa !410
  store <4 x i32> %287, <4 x i32>* %128, align 16, !tbaa !413
  br label %"consume sum_filter"

"consume sum_filter.critedge":                    ; preds = %"produce filter_zeroed"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %228, i8 0, i64 64, i1 false)
  br label %"consume sum_filter"

"consume sum_filter":                             ; preds = %"for sum_filter.s1.r19$y.preheader", %"for sum_filter.s1.r19$y.preheader.thread", %"consume sum_filter.loopexit.split.us", %"consume sum_filter.critedge"
  %288 = phi <4 x i32> [ %287, %"consume sum_filter.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader" ]
  %289 = phi <4 x i32> [ %286, %"consume sum_filter.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader" ]
  %290 = phi <4 x i32> [ %285, %"consume sum_filter.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader" ]
  %291 = phi <4 x i32> [ %284, %"consume sum_filter.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y.preheader" ]
  %292 = sext i32 %output.s0.c.c.base to i64
  %293 = getelementptr inbounds i32, i32* %129, i64 %292
  %294 = bitcast i32* %293 to <4 x i32>*
  %295 = load <4 x i32>, <4 x i32>* %294, align 4, !tbaa !415
  %296 = getelementptr inbounds i32, i32* %293, i64 4
  %297 = bitcast i32* %296 to <4 x i32>*
  %298 = load <4 x i32>, <4 x i32>* %297, align 4, !tbaa !415
  %299 = getelementptr inbounds i32, i32* %293, i64 8
  %300 = bitcast i32* %299 to <4 x i32>*
  %301 = load <4 x i32>, <4 x i32>* %300, align 4, !tbaa !415
  %302 = getelementptr inbounds i32, i32* %293, i64 12
  %303 = bitcast i32* %302 to <4 x i32>*
  %304 = load <4 x i32>, <4 x i32>* %303, align 4, !tbaa !415
  %305 = shufflevector <4 x i32> %295, <4 x i32> %298, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %306 = shufflevector <4 x i32> %301, <4 x i32> %304, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %307 = shufflevector <8 x i32> %305, <8 x i32> %306, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %308 = shufflevector <4 x i32> %291, <4 x i32> %290, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %309 = shufflevector <4 x i32> %289, <4 x i32> %288, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %310 = shufflevector <8 x i32> %308, <8 x i32> %309, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %311 = mul nsw <16 x i32> %310, %132
  %312 = sub nsw <16 x i32> %307, %311
  %313 = shufflevector <16 x i32> %312, <16 x i32> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %313, <4 x i32>* %133, align 16, !tbaa !417
  %314 = shufflevector <16 x i32> %312, <16 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i32> %314, <4 x i32>* %135, align 16, !tbaa !428
  %315 = shufflevector <16 x i32> %312, <16 x i32> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  store <4 x i32> %315, <4 x i32>* %137, align 16, !tbaa !430
  %316 = shufflevector <16 x i32> %312, <16 x i32> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  store <4 x i32> %316, <4 x i32>* %139, align 16, !tbaa !433
  br i1 %148, label %"for output.s0.b.rebased.preheader", label %"end for output.s0.b.rebased", !prof !391

"for output.s0.b.rebased.preheader":              ; preds = %"consume sum_filter"
  %317 = insertelement <16 x i32> undef, i32 %output.s0.c.c.base, i32 0
  %318 = shufflevector <16 x i32> %317, <16 x i32> undef, <16 x i32> zeroinitializer
  %319 = add nsw <16 x i32> %318, <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %.lobit2927 = ashr <16 x i32> %319, <i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31>
  %320 = sub nsw <16 x i32> %319, %.lobit2927
  %321 = and <16 x i32> %.lobit2927, %161
  %322 = sub i32 %output.s0.c.c.base, %t2195
  %323 = bitcast i16* %filter_zeroed to <8 x i16>*
  %324 = getelementptr inbounds i16, i16* %filter_zeroed, i64 8
  %325 = bitcast i16* %324 to <8 x i16>*
  %326 = getelementptr inbounds i16, i16* %filter_zeroed, i64 16
  %327 = bitcast i16* %326 to <8 x i16>*
  %328 = getelementptr inbounds i16, i16* %filter_zeroed, i64 24
  %329 = bitcast i16* %328 to <8 x i16>*
  %330 = getelementptr inbounds i16, i16* %filter_zeroed, i64 32
  %331 = bitcast i16* %330 to <8 x i16>*
  %332 = getelementptr inbounds i16, i16* %filter_zeroed, i64 40
  %333 = bitcast i16* %332 to <8 x i16>*
  %334 = getelementptr inbounds i16, i16* %filter_zeroed, i64 48
  %335 = bitcast i16* %334 to <8 x i16>*
  %336 = getelementptr inbounds i16, i16* %filter_zeroed, i64 56
  %337 = bitcast i16* %336 to <8 x i16>*
  %338 = getelementptr inbounds i16, i16* %filter_zeroed, i64 64
  %339 = bitcast i16* %338 to <8 x i16>*
  %340 = getelementptr inbounds i16, i16* %filter_zeroed, i64 72
  %341 = bitcast i16* %340 to <8 x i16>*
  %342 = getelementptr inbounds i16, i16* %filter_zeroed, i64 80
  %343 = bitcast i16* %342 to <8 x i16>*
  %344 = getelementptr inbounds i16, i16* %filter_zeroed, i64 88
  %345 = bitcast i16* %344 to <8 x i16>*
  %346 = getelementptr inbounds i16, i16* %filter_zeroed, i64 96
  %347 = bitcast i16* %346 to <8 x i16>*
  %348 = getelementptr inbounds i16, i16* %filter_zeroed, i64 104
  %349 = bitcast i16* %348 to <8 x i16>*
  %350 = getelementptr inbounds i16, i16* %filter_zeroed, i64 112
  %351 = bitcast i16* %350 to <8 x i16>*
  %352 = getelementptr inbounds i16, i16* %filter_zeroed, i64 120
  %353 = bitcast i16* %352 to <8 x i16>*
  %354 = getelementptr inbounds i16, i16* %filter_zeroed, i64 128
  %355 = bitcast i16* %354 to <8 x i16>*
  %356 = getelementptr inbounds i16, i16* %filter_zeroed, i64 136
  %357 = bitcast i16* %356 to <8 x i16>*
  %358 = sext i32 %322 to i64
  br label %"for output.s0.b.rebased"

"for output.s0.b.rebased":                        ; preds = %"for output.s0.b.rebased.preheader", %"end for output.s0.y.yo"
  %indvars.iv6388 = phi i64 [ 0, %"for output.s0.b.rebased.preheader" ], [ %indvars.iv.next6389, %"end for output.s0.y.yo" ]
  %359 = load i64, i64* %.fca.1.gep875, align 8, !tbaa !385
  %cmp.i3068 = icmp ult i64 %359, %152
  %360 = load i8*, i8** %.fca.0.gep874, align 8, !tbaa !387
  br i1 %cmp.i3068, label %if.then.i3071, label %pseudostack_alloc.exit3084, !prof !388

if.then.i3071:                                    ; preds = %"for output.s0.b.rebased"
  %tobool1.not.i3070 = icmp ne i8* %360, null
  %361 = load i64, i64* %.fca.2.gep876, align 8
  %cmp2.i3073 = icmp ugt i64 %361, 16384
  %or.cond5126 = and i1 %tobool1.not.i3070, %cmp2.i3073
  br i1 %or.cond5126, label %if.then3.i3075, label %if.end.i3079

if.then3.i3075:                                   ; preds = %if.then.i3071
  call void @halide_free(i8* null, i8* nonnull %360) #15
  %.pre6506 = load i64, i64* %.fca.2.gep876, align 8, !tbaa !389
  br label %if.end.i3079

if.end.i3079:                                     ; preds = %if.then3.i3075, %if.then.i3071
  %362 = phi i64 [ %.pre6506, %if.then3.i3075 ], [ %361, %if.then.i3071 ]
  %add.i3077 = add i64 %362, %152
  store i64 %add.i3077, i64* %.fca.2.gep876, align 8, !tbaa !389
  %cmp7.i3078 = icmp ugt i64 %add.i3077, 16384
  br i1 %cmp7.i3078, label %if.then8.i3081, label %if.end11.i3083

if.then8.i3081:                                   ; preds = %if.end.i3079
  %call.i3080 = call i8* @halide_malloc(i8* null, i64 %152) #15
  br label %if.end11.i3083

if.end11.i3083:                                   ; preds = %if.then8.i3081, %if.end.i3079
  %storemerge.i3082 = phi i8* [ %call.i3080, %if.then8.i3081 ], [ null, %if.end.i3079 ]
  store i8* %storemerge.i3082, i8** %.fca.0.gep874, align 8, !tbaa !387
  store i64 %152, i64* %.fca.1.gep875, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3084

pseudostack_alloc.exit3084:                       ; preds = %"for output.s0.b.rebased", %if.end11.i3083
  %363 = phi i8* [ %storemerge.i3082, %if.end11.i3083 ], [ %360, %"for output.s0.b.rebased" ]
  %.not2920 = icmp eq i8* %363, null
  br i1 %.not2920, label %then_bb5, label %"produce resampled_input", !prof !390

"end for output.s0.b.rebased":                    ; preds = %"end for output.s0.y.yo", %"consume sum_filter"
  %.2 = phi i8* [ %.15066, %"consume sum_filter" ], [ %214, %"end for output.s0.y.yo" ]
  %364 = add nuw nsw i32 %output.s0.c.co, 1
  %.not2917 = icmp eq i32 %364, %t2172
  br i1 %.not2917, label %after_bb.loopexit, label %"for output.s0.c.co"

then_bb5:                                         ; preds = %pseudostack_alloc.exit3084
  %365 = alloca i8*, i64 %152, align 16
  %366 = bitcast i8** %365 to i8*
  store i8** %365, i8*** %153, align 8
  br label %"produce resampled_input"

"produce resampled_input":                        ; preds = %pseudostack_alloc.exit3084, %then_bb5
  %resampled_input = phi i8* [ %366, %then_bb5 ], [ %363, %pseudostack_alloc.exit3084 ]
  %367 = add nsw i64 %indvars.iv6388, %226
  br i1 %t2194, label %then_bb7, label %next_bb8

then_bb7:                                         ; preds = %"produce resampled_input"
  %368 = mul nsw i64 %367, %227
  %369 = add nsw i64 %368, %358
  br i1 %.not6880, label %"for resampled_input.s0.y.rebased.us", label %"consume resampled_input", !prof !435

"for resampled_input.s0.y.rebased.us":            ; preds = %then_bb7, %"end for resampled_input.s0.x.rebased.loopexit.us"
  %indvars.iv6378 = phi i64 [ %indvars.iv.next6379, %"end for resampled_input.s0.x.rebased.loopexit.us" ], [ 0, %then_bb7 ]
  %370 = add nsw i64 %indvars.iv6378, %220
  %371 = mul nsw i64 %370, %221
  %372 = add nsw i64 %371, %222
  %373 = add nsw i64 %indvars.iv6378, %223
  %374 = mul nsw i64 %373, %224
  %375 = add nsw i64 %369, %374
  br label %"for resampled_input.s0.x.rebased.us"

"for resampled_input.s0.x.rebased.us":            ; preds = %"for resampled_input.s0.y.rebased.us", %"for resampled_input.s0.x.rebased.us"
  %indvars.iv6376 = phi i64 [ 0, %"for resampled_input.s0.y.rebased.us" ], [ %indvars.iv.next6377, %"for resampled_input.s0.x.rebased.us" ]
  %376 = add nsw i64 %indvars.iv6376, %163
  %377 = mul nsw i64 %376, %164
  %378 = add nsw i64 %377, %375
  %379 = getelementptr inbounds i8, i8* %10, i64 %378
  %380 = bitcast i8* %379 to <16 x i8>*
  %381 = load <16 x i8>, <16 x i8>* %380, align 1, !tbaa !436
  %382 = add nsw i64 %indvars.iv6376, %372
  %383 = shl nsw i64 %382, 4
  %384 = getelementptr inbounds i8, i8* %resampled_input, i64 %383
  %385 = bitcast i8* %384 to <16 x i8>*
  store <16 x i8> %381, <16 x i8>* %385, align 16, !tbaa !438
  %indvars.iv.next6377 = add nuw nsw i64 %indvars.iv6376, 1
  %.not2931.us = icmp eq i64 %indvars.iv6376, %219
  br i1 %.not2931.us, label %"end for resampled_input.s0.x.rebased.loopexit.us", label %"for resampled_input.s0.x.rebased.us"

"end for resampled_input.s0.x.rebased.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased.us"
  %indvars.iv.next6379 = add nuw nsw i64 %indvars.iv6378, 1
  %.not2930.us = icmp eq i64 %indvars.iv6378, %225
  br i1 %.not2930.us, label %"consume resampled_input", label %"for resampled_input.s0.y.rebased.us"

next_bb8:                                         ; preds = %"produce resampled_input"
  %386 = trunc i64 %367 to i32
  %387 = mul i32 %18, %386
  %t2244 = sub i32 %387, %t2195
  br i1 %232, label %"consume resampled_input", label %"for resampled_input.s0.y.rebased9.preheader.split.us", !prof !440

"for resampled_input.s0.y.rebased9.preheader.split.us": ; preds = %next_bb8
  %388 = sdiv <16 x i32> %320, %157
  %389 = add nsw <16 x i32> %388, %321
  %390 = and <16 x i32> %389, %162
  br label %"for resampled_input.s0.y.rebased9.us"

"for resampled_input.s0.y.rebased9.us":           ; preds = %"end for resampled_input.s0.x.rebased13.loopexit.us", %"for resampled_input.s0.y.rebased9.preheader.split.us"
  %indvars.iv6372 = phi i64 [ %indvars.iv.next6373, %"end for resampled_input.s0.x.rebased13.loopexit.us" ], [ 0, %"for resampled_input.s0.y.rebased9.preheader.split.us" ]
  %391 = add nsw i64 %indvars.iv6372, %220
  %392 = mul nsw i64 %391, %221
  %393 = add nsw i64 %392, %222
  %394 = trunc i64 %indvars.iv6372 to i32
  %395 = add i32 %b20, %394
  %396 = mul i32 %395, %15
  %397 = add i32 %396, %t2244
  br label %"for resampled_input.s0.x.rebased12.us"

"for resampled_input.s0.x.rebased12.us":          ; preds = %"for resampled_input.s0.y.rebased9.us", %"for resampled_input.s0.x.rebased12.us"
  %indvars.iv6370 = phi i64 [ 0, %"for resampled_input.s0.y.rebased9.us" ], [ %indvars.iv.next6371, %"for resampled_input.s0.x.rebased12.us" ]
  %398 = trunc i64 %indvars.iv6370 to i32
  %399 = add nsw i32 %b18, %398
  %400 = mul nsw i32 %399, %13
  %401 = add nsw i32 %397, %400
  %402 = insertelement <16 x i32> undef, i32 %401, i32 0
  %403 = shufflevector <16 x i32> %402, <16 x i32> undef, <16 x i32> zeroinitializer
  %404 = add nsw <16 x i32> %403, %390
  %405 = extractelement <16 x i32> %404, i32 0
  %406 = sext i32 %405 to i64
  %407 = getelementptr inbounds i8, i8* %10, i64 %406
  %408 = load i8, i8* %407, align 1, !tbaa !436
  %409 = insertelement <16 x i8> undef, i8 %408, i32 0
  %410 = extractelement <16 x i32> %404, i32 1
  %411 = sext i32 %410 to i64
  %412 = getelementptr inbounds i8, i8* %10, i64 %411
  %413 = load i8, i8* %412, align 1, !tbaa !436
  %414 = insertelement <16 x i8> %409, i8 %413, i32 1
  %415 = extractelement <16 x i32> %404, i32 2
  %416 = sext i32 %415 to i64
  %417 = getelementptr inbounds i8, i8* %10, i64 %416
  %418 = load i8, i8* %417, align 1, !tbaa !436
  %419 = insertelement <16 x i8> %414, i8 %418, i32 2
  %420 = extractelement <16 x i32> %404, i32 3
  %421 = sext i32 %420 to i64
  %422 = getelementptr inbounds i8, i8* %10, i64 %421
  %423 = load i8, i8* %422, align 1, !tbaa !436
  %424 = insertelement <16 x i8> %419, i8 %423, i32 3
  %425 = extractelement <16 x i32> %404, i32 4
  %426 = sext i32 %425 to i64
  %427 = getelementptr inbounds i8, i8* %10, i64 %426
  %428 = load i8, i8* %427, align 1, !tbaa !436
  %429 = insertelement <16 x i8> %424, i8 %428, i32 4
  %430 = extractelement <16 x i32> %404, i32 5
  %431 = sext i32 %430 to i64
  %432 = getelementptr inbounds i8, i8* %10, i64 %431
  %433 = load i8, i8* %432, align 1, !tbaa !436
  %434 = insertelement <16 x i8> %429, i8 %433, i32 5
  %435 = extractelement <16 x i32> %404, i32 6
  %436 = sext i32 %435 to i64
  %437 = getelementptr inbounds i8, i8* %10, i64 %436
  %438 = load i8, i8* %437, align 1, !tbaa !436
  %439 = insertelement <16 x i8> %434, i8 %438, i32 6
  %440 = extractelement <16 x i32> %404, i32 7
  %441 = sext i32 %440 to i64
  %442 = getelementptr inbounds i8, i8* %10, i64 %441
  %443 = load i8, i8* %442, align 1, !tbaa !436
  %444 = insertelement <16 x i8> %439, i8 %443, i32 7
  %445 = extractelement <16 x i32> %404, i32 8
  %446 = sext i32 %445 to i64
  %447 = getelementptr inbounds i8, i8* %10, i64 %446
  %448 = load i8, i8* %447, align 1, !tbaa !436
  %449 = insertelement <16 x i8> %444, i8 %448, i32 8
  %450 = extractelement <16 x i32> %404, i32 9
  %451 = sext i32 %450 to i64
  %452 = getelementptr inbounds i8, i8* %10, i64 %451
  %453 = load i8, i8* %452, align 1, !tbaa !436
  %454 = insertelement <16 x i8> %449, i8 %453, i32 9
  %455 = extractelement <16 x i32> %404, i32 10
  %456 = sext i32 %455 to i64
  %457 = getelementptr inbounds i8, i8* %10, i64 %456
  %458 = load i8, i8* %457, align 1, !tbaa !436
  %459 = insertelement <16 x i8> %454, i8 %458, i32 10
  %460 = extractelement <16 x i32> %404, i32 11
  %461 = sext i32 %460 to i64
  %462 = getelementptr inbounds i8, i8* %10, i64 %461
  %463 = load i8, i8* %462, align 1, !tbaa !436
  %464 = insertelement <16 x i8> %459, i8 %463, i32 11
  %465 = extractelement <16 x i32> %404, i32 12
  %466 = sext i32 %465 to i64
  %467 = getelementptr inbounds i8, i8* %10, i64 %466
  %468 = load i8, i8* %467, align 1, !tbaa !436
  %469 = insertelement <16 x i8> %464, i8 %468, i32 12
  %470 = extractelement <16 x i32> %404, i32 13
  %471 = sext i32 %470 to i64
  %472 = getelementptr inbounds i8, i8* %10, i64 %471
  %473 = load i8, i8* %472, align 1, !tbaa !436
  %474 = insertelement <16 x i8> %469, i8 %473, i32 13
  %475 = extractelement <16 x i32> %404, i32 14
  %476 = sext i32 %475 to i64
  %477 = getelementptr inbounds i8, i8* %10, i64 %476
  %478 = load i8, i8* %477, align 1, !tbaa !436
  %479 = insertelement <16 x i8> %474, i8 %478, i32 14
  %480 = extractelement <16 x i32> %404, i32 15
  %481 = sext i32 %480 to i64
  %482 = getelementptr inbounds i8, i8* %10, i64 %481
  %483 = load i8, i8* %482, align 1, !tbaa !436
  %484 = insertelement <16 x i8> %479, i8 %483, i32 15
  %485 = add nsw i64 %indvars.iv6370, %393
  %486 = shl nsw i64 %485, 4
  %487 = getelementptr inbounds i8, i8* %resampled_input, i64 %486
  %488 = bitcast i8* %487 to <16 x i8>*
  store <16 x i8> %484, <16 x i8>* %488, align 16, !tbaa !438
  %indvars.iv.next6371 = add nuw nsw i64 %indvars.iv6370, 1
  %.not2929.us = icmp eq i64 %indvars.iv6370, %219
  br i1 %.not2929.us, label %"end for resampled_input.s0.x.rebased13.loopexit.us", label %"for resampled_input.s0.x.rebased12.us"

"end for resampled_input.s0.x.rebased13.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased12.us"
  %indvars.iv.next6373 = add nuw nsw i64 %indvars.iv6372, 1
  %.not2926.us = icmp eq i64 %indvars.iv6372, %225
  br i1 %.not2926.us, label %"consume resampled_input", label %"for resampled_input.s0.y.rebased9.us"

"consume resampled_input":                        ; preds = %"end for resampled_input.s0.x.rebased13.loopexit.us", %"end for resampled_input.s0.x.rebased.loopexit.us", %next_bb8, %then_bb7
  %489 = trunc i64 %367 to i32
  %490 = mul i32 %27, %489
  %t2234 = add i32 %490, %output.s0.c.c.base
  %t2255 = sub i32 %t2234, %reass.add5164
  br i1 %165, label %"for output.s0.y.yo.preheader", label %"end for output.s0.y.yo", !prof !391

"for output.s0.y.yo.preheader":                   ; preds = %"consume resampled_input"
  %491 = load <4 x i32>, <4 x i32>* %133, align 16
  %492 = load <4 x i32>, <4 x i32>* %135, align 16
  %493 = load <4 x i32>, <4 x i32>* %137, align 16
  %494 = load <4 x i32>, <4 x i32>* %139, align 16
  br label %"for output.s0.y.yo"

"for output.s0.y.yo":                             ; preds = %"for output.s0.y.yo.preheader", %"end for output.s0.x.xo"
  %output.s0.y.yo = phi i32 [ %524, %"end for output.s0.x.xo" ], [ 0, %"for output.s0.y.yo.preheader" ]
  %a23 = shl nuw nsw i32 %output.s0.y.yo, 1
  %495 = icmp slt i32 %a23, %b25
  %output.s0.y.y.base.s = select i1 %495, i32 %a23, i32 %b25
  %t2280 = add nsw i32 %output.s0.y.y.base.s, %24
  %t2281 = add nsw i32 %t2280, 1
  %t2282 = mul nsw i32 %t2280, %stride_y
  %496 = add nsw i32 %t2282, %t2254
  %t2283 = mul nsw i32 %496, %a21
  %497 = add nsw i32 %t2282, %t2285
  %t2286 = mul nsw i32 %497, %a21
  %t2287 = mul nsw i32 %t2281, %stride_y
  %498 = add nsw i32 %t2287, %t2254
  %t2288 = mul nsw i32 %498, %a21
  %499 = add nsw i32 %t2287, %t2285
  %t2289 = mul nsw i32 %499, %a21
  %t2290 = sub nsw i32 %t2282, %t2223
  %t2291 = mul nsw i32 %t2290, %a21
  %t2292 = sub nsw i32 %t2287, %t2223
  %t2293 = mul nsw i32 %t2292, %a21
  br i1 %166, label %"for output.s0.x.xo.preheader", label %"end for output.s0.x.xo", !prof !391

"for output.s0.x.xo.preheader":                   ; preds = %"for output.s0.y.yo"
  %500 = mul nsw i32 %t2280, %26
  %t2278 = add nsw i32 %500, %t2255
  %501 = mul nsw i32 %t2281, %26
  %t2279 = add nsw i32 %501, %t2255
  %t2274 = add nsw i32 %t2283, %t2250
  %t2272 = add nsw i32 %t2283, %t2284
  %t2267 = add nsw i32 %t2286, %t2250
  %t2265 = add nsw i32 %t2286, %t2284
  %t2275 = add nsw i32 %t2288, %t2250
  %t2273 = add nsw i32 %t2288, %t2284
  %t2268 = add nsw i32 %t2289, %t2250
  %t2266 = add nsw i32 %t2289, %t2284
  %t2260 = add nsw i32 %t2291, %t2250
  %t2258 = add nsw i32 %t2291, %t2284
  %t2261 = add nsw i32 %t2293, %t2250
  %t2259 = add nsw i32 %t2293, %t2284
  %t2270 = sub nsw i32 %t2283, %t2225
  %t2263 = sub nsw i32 %t2286, %t2225
  %t2271 = sub nsw i32 %t2288, %t2225
  %t2264 = sub nsw i32 %t2289, %t2225
  %t2256 = sub nsw i32 %t2291, %t2225
  %t2257 = sub nsw i32 %t2293, %t2225
  %502 = sext i32 %t2256 to i64
  %503 = sext i32 %t2257 to i64
  %504 = sext i32 %t2258 to i64
  %505 = sext i32 %t2259 to i64
  %506 = sext i32 %t2260 to i64
  %507 = sext i32 %t2261 to i64
  %508 = sext i32 %t2263 to i64
  %509 = sext i32 %t2264 to i64
  %510 = sext i32 %t2265 to i64
  %511 = sext i32 %t2266 to i64
  %512 = sext i32 %t2267 to i64
  %513 = sext i32 %t2268 to i64
  %514 = sext i32 %t2270 to i64
  %515 = sext i32 %t2271 to i64
  %516 = sext i32 %t2272 to i64
  %517 = sext i32 %t2273 to i64
  %518 = sext i32 %t2274 to i64
  %519 = sext i32 %t2275 to i64
  %520 = sext i32 %t2278 to i64
  %521 = sext i32 %t2279 to i64
  br label %"for output.s0.x.xo"

"end for output.s0.y.yo":                         ; preds = %"end for output.s0.x.xo", %"consume resampled_input"
  %indvars.iv.next6389 = add nuw nsw i64 %indvars.iv6388, 1
  %522 = icmp eq i64 %indvars.iv.next6389, %zext6391
  br i1 %522, label %"end for output.s0.b.rebased", label %"for output.s0.b.rebased"

"for output.s0.x.xo":                             ; preds = %"for output.s0.x.xo.preheader", %"consume convolved"
  %output.s0.x.xo = phi i32 [ %1475, %"consume convolved" ], [ 0, %"for output.s0.x.xo.preheader" ]
  %a24 = shl nuw nsw i32 %output.s0.x.xo, 1
  %523 = icmp slt i32 %a24, %b26
  %output.s0.x.x.base.s = select i1 %523, i32 %a24, i32 %b26
  br i1 %t2217, label %then_bb16, label %next_bb17

"end for output.s0.x.xo.loopexit":                ; preds = %"consume convolved"
  store <4 x i32> %1369, <4 x i32>* %167, align 16, !tbaa !441
  store <4 x i32> %1368, <4 x i32>* %169, align 16, !tbaa !452
  store <4 x i32> %1367, <4 x i32>* %171, align 16, !tbaa !454
  store <4 x i32> %1366, <4 x i32>* %173, align 16, !tbaa !457
  store <4 x i32> %1365, <4 x i32>* %175, align 16, !tbaa !459
  store <4 x i32> %1364, <4 x i32>* %177, align 16, !tbaa !463
  store <4 x i32> %1363, <4 x i32>* %179, align 16, !tbaa !465
  store <4 x i32> %1362, <4 x i32>* %181, align 16, !tbaa !468
  store <4 x i32> %1361, <4 x i32>* %183, align 16, !tbaa !470
  store <4 x i32> %1360, <4 x i32>* %185, align 16, !tbaa !475
  store <4 x i32> %1359, <4 x i32>* %187, align 16, !tbaa !477
  store <4 x i32> %1358, <4 x i32>* %189, align 16, !tbaa !480
  store <4 x i32> %1357, <4 x i32>* %191, align 16, !tbaa !482
  store <4 x i32> %1356, <4 x i32>* %193, align 16, !tbaa !486
  store <4 x i32> %1355, <4 x i32>* %195, align 16, !tbaa !488
  store <4 x i32> %1354, <4 x i32>* %197, align 16, !tbaa !491
  br label %"end for output.s0.x.xo"

"end for output.s0.x.xo":                         ; preds = %"end for output.s0.x.xo.loopexit", %"for output.s0.y.yo"
  %524 = add nuw nsw i32 %output.s0.y.yo, 1
  %.not2922 = icmp eq i32 %524, %t2196
  br i1 %.not2922, label %"end for output.s0.y.yo", label %"for output.s0.y.yo"

then_bb16:                                        ; preds = %"for output.s0.x.xo"
  %525 = load <8 x i16>, <8 x i16>* %323, align 16, !tbaa !493
  %526 = load <8 x i16>, <8 x i16>* %325, align 16, !tbaa !502
  %527 = shufflevector <8 x i16> %526, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %528 = sext i32 %output.s0.x.x.base.s to i64
  %529 = add nsw i64 %528, %199
  %530 = mul nsw i64 %529, %200
  %531 = add nsw i64 %530, %502
  %532 = shl nsw i64 %531, 4
  %533 = getelementptr inbounds i8, i8* %resampled_input, i64 %532
  %534 = bitcast i8* %533 to <16 x i8>*
  %535 = load <16 x i8>, <16 x i8>* %534, align 16, !tbaa !438
  %536 = zext <16 x i8> %535 to <16 x i16>
  %537 = shufflevector <8 x i16> %525, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %538 = shufflevector <16 x i16> %536, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %539 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %537, <4 x i16> %538)
  %540 = shufflevector <8 x i16> %525, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %541 = shufflevector <16 x i16> %536, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %542 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %540, <4 x i16> %541)
  %543 = shufflevector <8 x i16> %526, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %544 = shufflevector <16 x i16> %536, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %545 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %543, <4 x i16> %544)
  %546 = shufflevector <16 x i16> %527, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %547 = shufflevector <16 x i16> %536, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %548 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %546, <4 x i16> %547)
  %549 = add nsw <4 x i32> %539, %491
  %550 = add nsw <4 x i32> %542, %492
  %551 = add nsw <4 x i32> %545, %493
  %552 = add nsw <4 x i32> %548, %494
  %553 = add nsw i64 %529, 1
  %554 = mul nsw i64 %553, %200
  %555 = add nsw i64 %554, %502
  %556 = shl nsw i64 %555, 4
  %557 = getelementptr inbounds i8, i8* %resampled_input, i64 %556
  %558 = bitcast i8* %557 to <16 x i8>*
  %559 = load <16 x i8>, <16 x i8>* %558, align 16, !tbaa !438
  %560 = zext <16 x i8> %559 to <16 x i16>
  %561 = shufflevector <16 x i16> %560, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %562 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %537, <4 x i16> %561)
  %563 = shufflevector <16 x i16> %560, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %564 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %540, <4 x i16> %563)
  %565 = shufflevector <16 x i16> %560, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %566 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %543, <4 x i16> %565)
  %567 = shufflevector <16 x i16> %560, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %568 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %546, <4 x i16> %567)
  %569 = add nsw <4 x i32> %562, %491
  %570 = add nsw <4 x i32> %564, %492
  %571 = add nsw <4 x i32> %566, %493
  %572 = add nsw <4 x i32> %568, %494
  %573 = add nsw i64 %530, %503
  %574 = shl nsw i64 %573, 4
  %575 = getelementptr inbounds i8, i8* %resampled_input, i64 %574
  %576 = bitcast i8* %575 to <16 x i8>*
  %577 = load <16 x i8>, <16 x i8>* %576, align 16, !tbaa !438
  %578 = zext <16 x i8> %577 to <16 x i16>
  %579 = shufflevector <16 x i16> %578, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %580 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %537, <4 x i16> %579)
  %581 = shufflevector <16 x i16> %578, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %582 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %540, <4 x i16> %581)
  %583 = shufflevector <16 x i16> %578, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %584 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %543, <4 x i16> %583)
  %585 = shufflevector <16 x i16> %578, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %586 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %546, <4 x i16> %585)
  %587 = add nsw <4 x i32> %580, %491
  %588 = add nsw <4 x i32> %582, %492
  %589 = add nsw <4 x i32> %584, %493
  %590 = add nsw <4 x i32> %586, %494
  %591 = add nsw i64 %554, %503
  %592 = shl nsw i64 %591, 4
  %593 = getelementptr inbounds i8, i8* %resampled_input, i64 %592
  %594 = bitcast i8* %593 to <16 x i8>*
  %595 = load <16 x i8>, <16 x i8>* %594, align 16, !tbaa !438
  %596 = zext <16 x i8> %595 to <16 x i16>
  %597 = shufflevector <16 x i16> %596, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %598 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %537, <4 x i16> %597)
  %599 = shufflevector <16 x i16> %596, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %600 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %540, <4 x i16> %599)
  %601 = shufflevector <16 x i16> %596, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %602 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %543, <4 x i16> %601)
  %603 = shufflevector <16 x i16> %596, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %604 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %546, <4 x i16> %603)
  %605 = add nsw <4 x i32> %598, %491
  %606 = add nsw <4 x i32> %600, %492
  %607 = add nsw <4 x i32> %602, %493
  %608 = add nsw <4 x i32> %604, %494
  %609 = load <8 x i16>, <8 x i16>* %327, align 16, !tbaa !504
  %610 = load <8 x i16>, <8 x i16>* %329, align 16, !tbaa !507
  %611 = shufflevector <8 x i16> %610, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %612 = add nsw i64 %530, %504
  %613 = shl nsw i64 %612, 4
  %614 = getelementptr inbounds i8, i8* %resampled_input, i64 %613
  %615 = bitcast i8* %614 to <16 x i8>*
  %616 = load <16 x i8>, <16 x i8>* %615, align 16, !tbaa !438
  %617 = zext <16 x i8> %616 to <16 x i16>
  %618 = shufflevector <8 x i16> %609, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %619 = shufflevector <16 x i16> %617, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %620 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %618, <4 x i16> %619)
  %621 = shufflevector <8 x i16> %609, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %622 = shufflevector <16 x i16> %617, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %623 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %621, <4 x i16> %622)
  %624 = shufflevector <8 x i16> %610, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %625 = shufflevector <16 x i16> %617, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %626 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %624, <4 x i16> %625)
  %627 = shufflevector <16 x i16> %611, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %628 = shufflevector <16 x i16> %617, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %629 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %627, <4 x i16> %628)
  %630 = add nsw <4 x i32> %620, %549
  %631 = add nsw <4 x i32> %623, %550
  %632 = add nsw <4 x i32> %626, %551
  %633 = add nsw <4 x i32> %629, %552
  %634 = add nsw i64 %554, %504
  %635 = shl nsw i64 %634, 4
  %636 = getelementptr inbounds i8, i8* %resampled_input, i64 %635
  %637 = bitcast i8* %636 to <16 x i8>*
  %638 = load <16 x i8>, <16 x i8>* %637, align 16, !tbaa !438
  %639 = zext <16 x i8> %638 to <16 x i16>
  %640 = shufflevector <16 x i16> %639, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %641 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %618, <4 x i16> %640)
  %642 = shufflevector <16 x i16> %639, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %643 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %621, <4 x i16> %642)
  %644 = shufflevector <16 x i16> %639, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %645 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %624, <4 x i16> %644)
  %646 = shufflevector <16 x i16> %639, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %647 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %627, <4 x i16> %646)
  %648 = add nsw <4 x i32> %641, %569
  %649 = add nsw <4 x i32> %643, %570
  %650 = add nsw <4 x i32> %645, %571
  %651 = add nsw <4 x i32> %647, %572
  %652 = add nsw i64 %530, %505
  %653 = shl nsw i64 %652, 4
  %654 = getelementptr inbounds i8, i8* %resampled_input, i64 %653
  %655 = bitcast i8* %654 to <16 x i8>*
  %656 = load <16 x i8>, <16 x i8>* %655, align 16, !tbaa !438
  %657 = zext <16 x i8> %656 to <16 x i16>
  %658 = shufflevector <16 x i16> %657, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %659 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %618, <4 x i16> %658)
  %660 = shufflevector <16 x i16> %657, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %661 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %621, <4 x i16> %660)
  %662 = shufflevector <16 x i16> %657, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %663 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %624, <4 x i16> %662)
  %664 = shufflevector <16 x i16> %657, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %665 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %627, <4 x i16> %664)
  %666 = add nsw <4 x i32> %659, %587
  %667 = add nsw <4 x i32> %661, %588
  %668 = add nsw <4 x i32> %663, %589
  %669 = add nsw <4 x i32> %665, %590
  %670 = add nsw i64 %554, %505
  %671 = shl nsw i64 %670, 4
  %672 = getelementptr inbounds i8, i8* %resampled_input, i64 %671
  %673 = bitcast i8* %672 to <16 x i8>*
  %674 = load <16 x i8>, <16 x i8>* %673, align 16, !tbaa !438
  %675 = zext <16 x i8> %674 to <16 x i16>
  %676 = shufflevector <16 x i16> %675, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %677 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %618, <4 x i16> %676)
  %678 = shufflevector <16 x i16> %675, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %679 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %621, <4 x i16> %678)
  %680 = shufflevector <16 x i16> %675, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %681 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %624, <4 x i16> %680)
  %682 = shufflevector <16 x i16> %675, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %683 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %627, <4 x i16> %682)
  %684 = add nsw <4 x i32> %677, %605
  %685 = add nsw <4 x i32> %679, %606
  %686 = add nsw <4 x i32> %681, %607
  %687 = add nsw <4 x i32> %683, %608
  %688 = load <8 x i16>, <8 x i16>* %331, align 16, !tbaa !509
  %689 = load <8 x i16>, <8 x i16>* %333, align 16, !tbaa !513
  %690 = shufflevector <8 x i16> %689, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %691 = add nsw i64 %530, %506
  %692 = shl nsw i64 %691, 4
  %693 = getelementptr inbounds i8, i8* %resampled_input, i64 %692
  %694 = bitcast i8* %693 to <16 x i8>*
  %695 = load <16 x i8>, <16 x i8>* %694, align 16, !tbaa !438
  %696 = zext <16 x i8> %695 to <16 x i16>
  %697 = shufflevector <8 x i16> %688, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %698 = shufflevector <16 x i16> %696, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %699 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %697, <4 x i16> %698)
  %700 = shufflevector <8 x i16> %688, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %701 = shufflevector <16 x i16> %696, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %702 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %700, <4 x i16> %701)
  %703 = shufflevector <8 x i16> %689, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %704 = shufflevector <16 x i16> %696, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %705 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %703, <4 x i16> %704)
  %706 = shufflevector <16 x i16> %690, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %707 = shufflevector <16 x i16> %696, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %708 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %706, <4 x i16> %707)
  %709 = add nsw <4 x i32> %699, %630
  %710 = add nsw <4 x i32> %702, %631
  %711 = add nsw <4 x i32> %705, %632
  %712 = add nsw <4 x i32> %708, %633
  %713 = add nsw i64 %554, %506
  %714 = shl nsw i64 %713, 4
  %715 = getelementptr inbounds i8, i8* %resampled_input, i64 %714
  %716 = bitcast i8* %715 to <16 x i8>*
  %717 = load <16 x i8>, <16 x i8>* %716, align 16, !tbaa !438
  %718 = zext <16 x i8> %717 to <16 x i16>
  %719 = shufflevector <16 x i16> %718, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %720 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %697, <4 x i16> %719)
  %721 = shufflevector <16 x i16> %718, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %722 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %700, <4 x i16> %721)
  %723 = shufflevector <16 x i16> %718, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %724 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %703, <4 x i16> %723)
  %725 = shufflevector <16 x i16> %718, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %726 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %706, <4 x i16> %725)
  %727 = add nsw <4 x i32> %720, %648
  %728 = add nsw <4 x i32> %722, %649
  %729 = add nsw <4 x i32> %724, %650
  %730 = add nsw <4 x i32> %726, %651
  %731 = add nsw i64 %530, %507
  %732 = shl nsw i64 %731, 4
  %733 = getelementptr inbounds i8, i8* %resampled_input, i64 %732
  %734 = bitcast i8* %733 to <16 x i8>*
  %735 = load <16 x i8>, <16 x i8>* %734, align 16, !tbaa !438
  %736 = zext <16 x i8> %735 to <16 x i16>
  %737 = shufflevector <16 x i16> %736, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %738 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %697, <4 x i16> %737)
  %739 = shufflevector <16 x i16> %736, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %740 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %700, <4 x i16> %739)
  %741 = shufflevector <16 x i16> %736, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %742 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %703, <4 x i16> %741)
  %743 = shufflevector <16 x i16> %736, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %744 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %706, <4 x i16> %743)
  %745 = add nsw <4 x i32> %738, %666
  %746 = add nsw <4 x i32> %740, %667
  %747 = add nsw <4 x i32> %742, %668
  %748 = add nsw <4 x i32> %744, %669
  %749 = add nsw i64 %554, %507
  %750 = shl nsw i64 %749, 4
  %751 = getelementptr inbounds i8, i8* %resampled_input, i64 %750
  %752 = bitcast i8* %751 to <16 x i8>*
  %753 = load <16 x i8>, <16 x i8>* %752, align 16, !tbaa !438
  %754 = zext <16 x i8> %753 to <16 x i16>
  %755 = shufflevector <16 x i16> %754, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %756 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %697, <4 x i16> %755)
  %757 = shufflevector <16 x i16> %754, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %758 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %700, <4 x i16> %757)
  %759 = shufflevector <16 x i16> %754, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %760 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %703, <4 x i16> %759)
  %761 = shufflevector <16 x i16> %754, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %762 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %706, <4 x i16> %761)
  %763 = add nsw <4 x i32> %756, %684
  %764 = add nsw <4 x i32> %758, %685
  %765 = add nsw <4 x i32> %760, %686
  %766 = add nsw <4 x i32> %762, %687
  %767 = load <8 x i16>, <8 x i16>* %335, align 16, !tbaa !395
  %768 = load <8 x i16>, <8 x i16>* %337, align 16, !tbaa !395
  %769 = shufflevector <8 x i16> %768, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %770 = add nsw i64 %530, %508
  %771 = shl nsw i64 %770, 4
  %772 = getelementptr inbounds i8, i8* %resampled_input, i64 %771
  %773 = bitcast i8* %772 to <16 x i8>*
  %774 = load <16 x i8>, <16 x i8>* %773, align 16, !tbaa !438
  %775 = zext <16 x i8> %774 to <16 x i16>
  %776 = shufflevector <8 x i16> %767, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %777 = shufflevector <16 x i16> %775, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %778 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %776, <4 x i16> %777)
  %779 = shufflevector <8 x i16> %767, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %780 = shufflevector <16 x i16> %775, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %781 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %779, <4 x i16> %780)
  %782 = shufflevector <8 x i16> %768, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %783 = shufflevector <16 x i16> %775, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %784 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %782, <4 x i16> %783)
  %785 = shufflevector <16 x i16> %769, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %786 = shufflevector <16 x i16> %775, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %787 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %785, <4 x i16> %786)
  %788 = add nsw <4 x i32> %778, %709
  %789 = add nsw <4 x i32> %781, %710
  %790 = add nsw <4 x i32> %784, %711
  %791 = add nsw <4 x i32> %787, %712
  %792 = add nsw i64 %554, %508
  %793 = shl nsw i64 %792, 4
  %794 = getelementptr inbounds i8, i8* %resampled_input, i64 %793
  %795 = bitcast i8* %794 to <16 x i8>*
  %796 = load <16 x i8>, <16 x i8>* %795, align 16, !tbaa !438
  %797 = zext <16 x i8> %796 to <16 x i16>
  %798 = shufflevector <16 x i16> %797, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %799 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %776, <4 x i16> %798)
  %800 = shufflevector <16 x i16> %797, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %801 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %779, <4 x i16> %800)
  %802 = shufflevector <16 x i16> %797, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %803 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %782, <4 x i16> %802)
  %804 = shufflevector <16 x i16> %797, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %805 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %785, <4 x i16> %804)
  %806 = add nsw <4 x i32> %799, %727
  %807 = add nsw <4 x i32> %801, %728
  %808 = add nsw <4 x i32> %803, %729
  %809 = add nsw <4 x i32> %805, %730
  %810 = add nsw i64 %530, %509
  %811 = shl nsw i64 %810, 4
  %812 = getelementptr inbounds i8, i8* %resampled_input, i64 %811
  %813 = bitcast i8* %812 to <16 x i8>*
  %814 = load <16 x i8>, <16 x i8>* %813, align 16, !tbaa !438
  %815 = zext <16 x i8> %814 to <16 x i16>
  %816 = shufflevector <16 x i16> %815, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %817 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %776, <4 x i16> %816)
  %818 = shufflevector <16 x i16> %815, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %819 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %779, <4 x i16> %818)
  %820 = shufflevector <16 x i16> %815, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %821 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %782, <4 x i16> %820)
  %822 = shufflevector <16 x i16> %815, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %823 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %785, <4 x i16> %822)
  %824 = add nsw <4 x i32> %817, %745
  %825 = add nsw <4 x i32> %819, %746
  %826 = add nsw <4 x i32> %821, %747
  %827 = add nsw <4 x i32> %823, %748
  %828 = add nsw i64 %554, %509
  %829 = shl nsw i64 %828, 4
  %830 = getelementptr inbounds i8, i8* %resampled_input, i64 %829
  %831 = bitcast i8* %830 to <16 x i8>*
  %832 = load <16 x i8>, <16 x i8>* %831, align 16, !tbaa !438
  %833 = zext <16 x i8> %832 to <16 x i16>
  %834 = shufflevector <16 x i16> %833, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %835 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %776, <4 x i16> %834)
  %836 = shufflevector <16 x i16> %833, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %837 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %779, <4 x i16> %836)
  %838 = shufflevector <16 x i16> %833, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %839 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %782, <4 x i16> %838)
  %840 = shufflevector <16 x i16> %833, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %841 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %785, <4 x i16> %840)
  %842 = add nsw <4 x i32> %835, %763
  %843 = add nsw <4 x i32> %837, %764
  %844 = add nsw <4 x i32> %839, %765
  %845 = add nsw <4 x i32> %841, %766
  %846 = load <8 x i16>, <8 x i16>* %339, align 16, !tbaa !395
  %847 = load <8 x i16>, <8 x i16>* %341, align 16, !tbaa !395
  %848 = shufflevector <8 x i16> %847, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %849 = add nsw i64 %530, %510
  %850 = shl nsw i64 %849, 4
  %851 = getelementptr inbounds i8, i8* %resampled_input, i64 %850
  %852 = bitcast i8* %851 to <16 x i8>*
  %853 = load <16 x i8>, <16 x i8>* %852, align 16, !tbaa !438
  %854 = zext <16 x i8> %853 to <16 x i16>
  %855 = shufflevector <8 x i16> %846, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %856 = shufflevector <16 x i16> %854, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %857 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %855, <4 x i16> %856)
  %858 = shufflevector <8 x i16> %846, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %859 = shufflevector <16 x i16> %854, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %860 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %858, <4 x i16> %859)
  %861 = shufflevector <8 x i16> %847, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %862 = shufflevector <16 x i16> %854, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %863 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %861, <4 x i16> %862)
  %864 = shufflevector <16 x i16> %848, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %865 = shufflevector <16 x i16> %854, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %866 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %864, <4 x i16> %865)
  %867 = add nsw <4 x i32> %857, %788
  %868 = add nsw <4 x i32> %860, %789
  %869 = add nsw <4 x i32> %863, %790
  %870 = add nsw <4 x i32> %866, %791
  %871 = add nsw i64 %554, %510
  %872 = shl nsw i64 %871, 4
  %873 = getelementptr inbounds i8, i8* %resampled_input, i64 %872
  %874 = bitcast i8* %873 to <16 x i8>*
  %875 = load <16 x i8>, <16 x i8>* %874, align 16, !tbaa !438
  %876 = zext <16 x i8> %875 to <16 x i16>
  %877 = shufflevector <16 x i16> %876, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %878 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %855, <4 x i16> %877)
  %879 = shufflevector <16 x i16> %876, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %880 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %858, <4 x i16> %879)
  %881 = shufflevector <16 x i16> %876, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %882 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %861, <4 x i16> %881)
  %883 = shufflevector <16 x i16> %876, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %884 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %864, <4 x i16> %883)
  %885 = add nsw <4 x i32> %878, %806
  %886 = add nsw <4 x i32> %880, %807
  %887 = add nsw <4 x i32> %882, %808
  %888 = add nsw <4 x i32> %884, %809
  %889 = add nsw i64 %530, %511
  %890 = shl nsw i64 %889, 4
  %891 = getelementptr inbounds i8, i8* %resampled_input, i64 %890
  %892 = bitcast i8* %891 to <16 x i8>*
  %893 = load <16 x i8>, <16 x i8>* %892, align 16, !tbaa !438
  %894 = zext <16 x i8> %893 to <16 x i16>
  %895 = shufflevector <16 x i16> %894, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %896 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %855, <4 x i16> %895)
  %897 = shufflevector <16 x i16> %894, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %898 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %858, <4 x i16> %897)
  %899 = shufflevector <16 x i16> %894, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %900 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %861, <4 x i16> %899)
  %901 = shufflevector <16 x i16> %894, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %902 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %864, <4 x i16> %901)
  %903 = add nsw <4 x i32> %896, %824
  %904 = add nsw <4 x i32> %898, %825
  %905 = add nsw <4 x i32> %900, %826
  %906 = add nsw <4 x i32> %902, %827
  %907 = add nsw i64 %554, %511
  %908 = shl nsw i64 %907, 4
  %909 = getelementptr inbounds i8, i8* %resampled_input, i64 %908
  %910 = bitcast i8* %909 to <16 x i8>*
  %911 = load <16 x i8>, <16 x i8>* %910, align 16, !tbaa !438
  %912 = zext <16 x i8> %911 to <16 x i16>
  %913 = shufflevector <16 x i16> %912, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %914 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %855, <4 x i16> %913)
  %915 = shufflevector <16 x i16> %912, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %916 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %858, <4 x i16> %915)
  %917 = shufflevector <16 x i16> %912, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %918 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %861, <4 x i16> %917)
  %919 = shufflevector <16 x i16> %912, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %920 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %864, <4 x i16> %919)
  %921 = add nsw <4 x i32> %914, %842
  %922 = add nsw <4 x i32> %916, %843
  %923 = add nsw <4 x i32> %918, %844
  %924 = add nsw <4 x i32> %920, %845
  %925 = load <8 x i16>, <8 x i16>* %343, align 16, !tbaa !395
  %926 = load <8 x i16>, <8 x i16>* %345, align 16, !tbaa !395
  %927 = shufflevector <8 x i16> %926, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %928 = add nsw i64 %530, %512
  %929 = shl nsw i64 %928, 4
  %930 = getelementptr inbounds i8, i8* %resampled_input, i64 %929
  %931 = bitcast i8* %930 to <16 x i8>*
  %932 = load <16 x i8>, <16 x i8>* %931, align 16, !tbaa !438
  %933 = zext <16 x i8> %932 to <16 x i16>
  %934 = shufflevector <8 x i16> %925, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %935 = shufflevector <16 x i16> %933, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %936 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %934, <4 x i16> %935)
  %937 = shufflevector <8 x i16> %925, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %938 = shufflevector <16 x i16> %933, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %939 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %937, <4 x i16> %938)
  %940 = shufflevector <8 x i16> %926, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %941 = shufflevector <16 x i16> %933, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %942 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %940, <4 x i16> %941)
  %943 = shufflevector <16 x i16> %927, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %944 = shufflevector <16 x i16> %933, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %945 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %943, <4 x i16> %944)
  %946 = add nsw <4 x i32> %936, %867
  %947 = add nsw <4 x i32> %939, %868
  %948 = add nsw <4 x i32> %942, %869
  %949 = add nsw <4 x i32> %945, %870
  %950 = add nsw i64 %554, %512
  %951 = shl nsw i64 %950, 4
  %952 = getelementptr inbounds i8, i8* %resampled_input, i64 %951
  %953 = bitcast i8* %952 to <16 x i8>*
  %954 = load <16 x i8>, <16 x i8>* %953, align 16, !tbaa !438
  %955 = zext <16 x i8> %954 to <16 x i16>
  %956 = shufflevector <16 x i16> %955, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %957 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %934, <4 x i16> %956)
  %958 = shufflevector <16 x i16> %955, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %959 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %937, <4 x i16> %958)
  %960 = shufflevector <16 x i16> %955, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %961 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %940, <4 x i16> %960)
  %962 = shufflevector <16 x i16> %955, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %963 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %943, <4 x i16> %962)
  %964 = add nsw <4 x i32> %957, %885
  %965 = add nsw <4 x i32> %959, %886
  %966 = add nsw <4 x i32> %961, %887
  %967 = add nsw <4 x i32> %963, %888
  %968 = add nsw i64 %530, %513
  %969 = shl nsw i64 %968, 4
  %970 = getelementptr inbounds i8, i8* %resampled_input, i64 %969
  %971 = bitcast i8* %970 to <16 x i8>*
  %972 = load <16 x i8>, <16 x i8>* %971, align 16, !tbaa !438
  %973 = zext <16 x i8> %972 to <16 x i16>
  %974 = shufflevector <16 x i16> %973, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %975 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %934, <4 x i16> %974)
  %976 = shufflevector <16 x i16> %973, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %977 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %937, <4 x i16> %976)
  %978 = shufflevector <16 x i16> %973, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %979 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %940, <4 x i16> %978)
  %980 = shufflevector <16 x i16> %973, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %981 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %943, <4 x i16> %980)
  %982 = add nsw <4 x i32> %975, %903
  %983 = add nsw <4 x i32> %977, %904
  %984 = add nsw <4 x i32> %979, %905
  %985 = add nsw <4 x i32> %981, %906
  %986 = add nsw i64 %554, %513
  %987 = shl nsw i64 %986, 4
  %988 = getelementptr inbounds i8, i8* %resampled_input, i64 %987
  %989 = bitcast i8* %988 to <16 x i8>*
  %990 = load <16 x i8>, <16 x i8>* %989, align 16, !tbaa !438
  %991 = zext <16 x i8> %990 to <16 x i16>
  %992 = shufflevector <16 x i16> %991, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %993 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %934, <4 x i16> %992)
  %994 = shufflevector <16 x i16> %991, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %995 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %937, <4 x i16> %994)
  %996 = shufflevector <16 x i16> %991, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %997 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %940, <4 x i16> %996)
  %998 = shufflevector <16 x i16> %991, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %999 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %943, <4 x i16> %998)
  %1000 = add nsw <4 x i32> %993, %921
  %1001 = add nsw <4 x i32> %995, %922
  %1002 = add nsw <4 x i32> %997, %923
  %1003 = add nsw <4 x i32> %999, %924
  %1004 = load <8 x i16>, <8 x i16>* %347, align 16, !tbaa !395
  %1005 = load <8 x i16>, <8 x i16>* %349, align 16, !tbaa !395
  %1006 = shufflevector <8 x i16> %1005, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %1007 = add nsw i64 %530, %514
  %1008 = shl nsw i64 %1007, 4
  %1009 = getelementptr inbounds i8, i8* %resampled_input, i64 %1008
  %1010 = bitcast i8* %1009 to <16 x i8>*
  %1011 = load <16 x i8>, <16 x i8>* %1010, align 16, !tbaa !438
  %1012 = zext <16 x i8> %1011 to <16 x i16>
  %1013 = shufflevector <8 x i16> %1004, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1014 = shufflevector <16 x i16> %1012, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1015 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1013, <4 x i16> %1014)
  %1016 = shufflevector <8 x i16> %1004, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1017 = shufflevector <16 x i16> %1012, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1018 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1016, <4 x i16> %1017)
  %1019 = shufflevector <8 x i16> %1005, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1020 = shufflevector <16 x i16> %1012, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1021 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1019, <4 x i16> %1020)
  %1022 = shufflevector <16 x i16> %1006, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1023 = shufflevector <16 x i16> %1012, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1024 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1022, <4 x i16> %1023)
  %1025 = add nsw <4 x i32> %1015, %946
  %1026 = add nsw <4 x i32> %1018, %947
  %1027 = add nsw <4 x i32> %1021, %948
  %1028 = add nsw <4 x i32> %1024, %949
  %1029 = add nsw i64 %554, %514
  %1030 = shl nsw i64 %1029, 4
  %1031 = getelementptr inbounds i8, i8* %resampled_input, i64 %1030
  %1032 = bitcast i8* %1031 to <16 x i8>*
  %1033 = load <16 x i8>, <16 x i8>* %1032, align 16, !tbaa !438
  %1034 = zext <16 x i8> %1033 to <16 x i16>
  %1035 = shufflevector <16 x i16> %1034, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1036 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1013, <4 x i16> %1035)
  %1037 = shufflevector <16 x i16> %1034, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1038 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1016, <4 x i16> %1037)
  %1039 = shufflevector <16 x i16> %1034, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1040 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1019, <4 x i16> %1039)
  %1041 = shufflevector <16 x i16> %1034, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1042 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1022, <4 x i16> %1041)
  %1043 = add nsw <4 x i32> %1036, %964
  %1044 = add nsw <4 x i32> %1038, %965
  %1045 = add nsw <4 x i32> %1040, %966
  %1046 = add nsw <4 x i32> %1042, %967
  %1047 = add nsw i64 %530, %515
  %1048 = shl nsw i64 %1047, 4
  %1049 = getelementptr inbounds i8, i8* %resampled_input, i64 %1048
  %1050 = bitcast i8* %1049 to <16 x i8>*
  %1051 = load <16 x i8>, <16 x i8>* %1050, align 16, !tbaa !438
  %1052 = zext <16 x i8> %1051 to <16 x i16>
  %1053 = shufflevector <16 x i16> %1052, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1054 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1013, <4 x i16> %1053)
  %1055 = shufflevector <16 x i16> %1052, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1056 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1016, <4 x i16> %1055)
  %1057 = shufflevector <16 x i16> %1052, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1058 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1019, <4 x i16> %1057)
  %1059 = shufflevector <16 x i16> %1052, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1060 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1022, <4 x i16> %1059)
  %1061 = add nsw <4 x i32> %1054, %982
  %1062 = add nsw <4 x i32> %1056, %983
  %1063 = add nsw <4 x i32> %1058, %984
  %1064 = add nsw <4 x i32> %1060, %985
  %1065 = add nsw i64 %554, %515
  %1066 = shl nsw i64 %1065, 4
  %1067 = getelementptr inbounds i8, i8* %resampled_input, i64 %1066
  %1068 = bitcast i8* %1067 to <16 x i8>*
  %1069 = load <16 x i8>, <16 x i8>* %1068, align 16, !tbaa !438
  %1070 = zext <16 x i8> %1069 to <16 x i16>
  %1071 = shufflevector <16 x i16> %1070, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1072 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1013, <4 x i16> %1071)
  %1073 = shufflevector <16 x i16> %1070, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1074 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1016, <4 x i16> %1073)
  %1075 = shufflevector <16 x i16> %1070, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1076 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1019, <4 x i16> %1075)
  %1077 = shufflevector <16 x i16> %1070, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1078 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1022, <4 x i16> %1077)
  %1079 = add nsw <4 x i32> %1072, %1000
  %1080 = add nsw <4 x i32> %1074, %1001
  %1081 = add nsw <4 x i32> %1076, %1002
  %1082 = add nsw <4 x i32> %1078, %1003
  %1083 = load <8 x i16>, <8 x i16>* %351, align 16, !tbaa !395
  %1084 = load <8 x i16>, <8 x i16>* %353, align 16, !tbaa !395
  %1085 = shufflevector <8 x i16> %1084, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %1086 = add nsw i64 %530, %516
  %1087 = shl nsw i64 %1086, 4
  %1088 = getelementptr inbounds i8, i8* %resampled_input, i64 %1087
  %1089 = bitcast i8* %1088 to <16 x i8>*
  %1090 = load <16 x i8>, <16 x i8>* %1089, align 16, !tbaa !438
  %1091 = zext <16 x i8> %1090 to <16 x i16>
  %1092 = shufflevector <8 x i16> %1083, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1093 = shufflevector <16 x i16> %1091, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1094 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1092, <4 x i16> %1093)
  %1095 = shufflevector <8 x i16> %1083, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1096 = shufflevector <16 x i16> %1091, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1097 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1095, <4 x i16> %1096)
  %1098 = shufflevector <8 x i16> %1084, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1099 = shufflevector <16 x i16> %1091, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1100 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1098, <4 x i16> %1099)
  %1101 = shufflevector <16 x i16> %1085, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1102 = shufflevector <16 x i16> %1091, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1103 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1101, <4 x i16> %1102)
  %1104 = add nsw <4 x i32> %1094, %1025
  %1105 = add nsw <4 x i32> %1097, %1026
  %1106 = add nsw <4 x i32> %1100, %1027
  %1107 = add nsw <4 x i32> %1103, %1028
  %1108 = add nsw i64 %554, %516
  %1109 = shl nsw i64 %1108, 4
  %1110 = getelementptr inbounds i8, i8* %resampled_input, i64 %1109
  %1111 = bitcast i8* %1110 to <16 x i8>*
  %1112 = load <16 x i8>, <16 x i8>* %1111, align 16, !tbaa !438
  %1113 = zext <16 x i8> %1112 to <16 x i16>
  %1114 = shufflevector <16 x i16> %1113, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1115 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1092, <4 x i16> %1114)
  %1116 = shufflevector <16 x i16> %1113, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1117 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1095, <4 x i16> %1116)
  %1118 = shufflevector <16 x i16> %1113, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1119 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1098, <4 x i16> %1118)
  %1120 = shufflevector <16 x i16> %1113, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1121 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1101, <4 x i16> %1120)
  %1122 = add nsw <4 x i32> %1115, %1043
  %1123 = add nsw <4 x i32> %1117, %1044
  %1124 = add nsw <4 x i32> %1119, %1045
  %1125 = add nsw <4 x i32> %1121, %1046
  %1126 = add nsw i64 %530, %517
  %1127 = shl nsw i64 %1126, 4
  %1128 = getelementptr inbounds i8, i8* %resampled_input, i64 %1127
  %1129 = bitcast i8* %1128 to <16 x i8>*
  %1130 = load <16 x i8>, <16 x i8>* %1129, align 16, !tbaa !438
  %1131 = zext <16 x i8> %1130 to <16 x i16>
  %1132 = shufflevector <16 x i16> %1131, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1133 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1092, <4 x i16> %1132)
  %1134 = shufflevector <16 x i16> %1131, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1135 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1095, <4 x i16> %1134)
  %1136 = shufflevector <16 x i16> %1131, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1137 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1098, <4 x i16> %1136)
  %1138 = shufflevector <16 x i16> %1131, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1139 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1101, <4 x i16> %1138)
  %1140 = add nsw <4 x i32> %1133, %1061
  %1141 = add nsw <4 x i32> %1135, %1062
  %1142 = add nsw <4 x i32> %1137, %1063
  %1143 = add nsw <4 x i32> %1139, %1064
  %1144 = add nsw i64 %554, %517
  %1145 = shl nsw i64 %1144, 4
  %1146 = getelementptr inbounds i8, i8* %resampled_input, i64 %1145
  %1147 = bitcast i8* %1146 to <16 x i8>*
  %1148 = load <16 x i8>, <16 x i8>* %1147, align 16, !tbaa !438
  %1149 = zext <16 x i8> %1148 to <16 x i16>
  %1150 = shufflevector <16 x i16> %1149, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1151 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1092, <4 x i16> %1150)
  %1152 = shufflevector <16 x i16> %1149, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1153 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1095, <4 x i16> %1152)
  %1154 = shufflevector <16 x i16> %1149, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1155 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1098, <4 x i16> %1154)
  %1156 = shufflevector <16 x i16> %1149, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1157 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1101, <4 x i16> %1156)
  %1158 = add nsw <4 x i32> %1151, %1079
  %1159 = add nsw <4 x i32> %1153, %1080
  %1160 = add nsw <4 x i32> %1155, %1081
  %1161 = add nsw <4 x i32> %1157, %1082
  %1162 = load <8 x i16>, <8 x i16>* %355, align 16, !tbaa !395
  %1163 = load <8 x i16>, <8 x i16>* %357, align 16, !tbaa !395
  %1164 = shufflevector <8 x i16> %1163, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %1165 = add nsw i64 %530, %518
  %1166 = shl nsw i64 %1165, 4
  %1167 = getelementptr inbounds i8, i8* %resampled_input, i64 %1166
  %1168 = bitcast i8* %1167 to <16 x i8>*
  %1169 = load <16 x i8>, <16 x i8>* %1168, align 16, !tbaa !438
  %1170 = zext <16 x i8> %1169 to <16 x i16>
  %1171 = shufflevector <8 x i16> %1162, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1172 = shufflevector <16 x i16> %1170, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1173 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1171, <4 x i16> %1172)
  %1174 = shufflevector <8 x i16> %1162, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1175 = shufflevector <16 x i16> %1170, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1176 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1174, <4 x i16> %1175)
  %1177 = shufflevector <8 x i16> %1163, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1178 = shufflevector <16 x i16> %1170, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1179 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1177, <4 x i16> %1178)
  %1180 = shufflevector <16 x i16> %1164, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1181 = shufflevector <16 x i16> %1170, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1182 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1180, <4 x i16> %1181)
  %1183 = add nsw <4 x i32> %1173, %1104
  %1184 = add nsw <4 x i32> %1176, %1105
  %1185 = add nsw <4 x i32> %1179, %1106
  %1186 = add nsw <4 x i32> %1182, %1107
  %1187 = add nsw i64 %554, %518
  %1188 = shl nsw i64 %1187, 4
  %1189 = getelementptr inbounds i8, i8* %resampled_input, i64 %1188
  %1190 = bitcast i8* %1189 to <16 x i8>*
  %1191 = load <16 x i8>, <16 x i8>* %1190, align 16, !tbaa !438
  %1192 = zext <16 x i8> %1191 to <16 x i16>
  %1193 = shufflevector <16 x i16> %1192, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1194 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1171, <4 x i16> %1193)
  %1195 = shufflevector <16 x i16> %1192, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1196 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1174, <4 x i16> %1195)
  %1197 = shufflevector <16 x i16> %1192, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1198 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1177, <4 x i16> %1197)
  %1199 = shufflevector <16 x i16> %1192, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1200 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1180, <4 x i16> %1199)
  %1201 = add nsw <4 x i32> %1194, %1122
  %1202 = add nsw <4 x i32> %1196, %1123
  %1203 = add nsw <4 x i32> %1198, %1124
  %1204 = add nsw <4 x i32> %1200, %1125
  %1205 = add nsw i64 %530, %519
  %1206 = shl nsw i64 %1205, 4
  %1207 = getelementptr inbounds i8, i8* %resampled_input, i64 %1206
  %1208 = bitcast i8* %1207 to <16 x i8>*
  %1209 = load <16 x i8>, <16 x i8>* %1208, align 16, !tbaa !438
  %1210 = zext <16 x i8> %1209 to <16 x i16>
  %1211 = shufflevector <16 x i16> %1210, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1212 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1171, <4 x i16> %1211)
  %1213 = shufflevector <16 x i16> %1210, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1214 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1174, <4 x i16> %1213)
  %1215 = shufflevector <16 x i16> %1210, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1216 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1177, <4 x i16> %1215)
  %1217 = shufflevector <16 x i16> %1210, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1218 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1180, <4 x i16> %1217)
  %1219 = add nsw <4 x i32> %1212, %1140
  %1220 = add nsw <4 x i32> %1214, %1141
  %1221 = add nsw <4 x i32> %1216, %1142
  %1222 = add nsw <4 x i32> %1218, %1143
  %1223 = add nsw i64 %554, %519
  %1224 = shl nsw i64 %1223, 4
  %1225 = getelementptr inbounds i8, i8* %resampled_input, i64 %1224
  %1226 = bitcast i8* %1225 to <16 x i8>*
  %1227 = load <16 x i8>, <16 x i8>* %1226, align 16, !tbaa !438
  %1228 = zext <16 x i8> %1227 to <16 x i16>
  %1229 = shufflevector <16 x i16> %1228, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1230 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1171, <4 x i16> %1229)
  %1231 = shufflevector <16 x i16> %1228, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1232 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1174, <4 x i16> %1231)
  %1233 = shufflevector <16 x i16> %1228, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1234 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1177, <4 x i16> %1233)
  %1235 = shufflevector <16 x i16> %1228, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1236 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1180, <4 x i16> %1235)
  %1237 = add nsw <4 x i32> %1230, %1158
  %1238 = add nsw <4 x i32> %1232, %1159
  %1239 = add nsw <4 x i32> %1234, %1160
  %1240 = add nsw <4 x i32> %1236, %1161
  br label %"consume convolved"

next_bb17:                                        ; preds = %"for output.s0.x.xo"
  %t2296 = add nsw i32 %output.s0.x.x.base.s, %21
  %1241 = add nsw i32 %t2296, 1
  %1242 = mul nsw i32 %1241, %stride_x
  %t2295 = sub nsw i32 %1242, %t2225
  %1243 = mul nsw i32 %t2296, %stride_x
  %t2294 = sub nsw i32 %1243, %t2225
  br i1 %112, label %"for convolved.s1.r19$y.preheader", label %"consume convolved", !prof !391

"for convolved.s1.r19$y.preheader":               ; preds = %next_bb17
  br i1 %110, label %"for convolved.s1.r19$y.us", label %"consume convolved", !prof !391

"for convolved.s1.r19$y.us":                      ; preds = %"for convolved.s1.r19$y.preheader", %"end for convolved.s1.r19$x.loopexit.us"
  %indvars.iv6384 = phi i64 [ %indvars.iv.next6385, %"end for convolved.s1.r19$x.loopexit.us" ], [ 0, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5698.us5730 = phi <4 x i32> [ %1352, %"end for convolved.s1.r19$x.loopexit.us" ], [ %494, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5696.us5728 = phi <4 x i32> [ %1351, %"end for convolved.s1.r19$x.loopexit.us" ], [ %493, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5694.us5726 = phi <4 x i32> [ %1350, %"end for convolved.s1.r19$x.loopexit.us" ], [ %492, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5692.us5724 = phi <4 x i32> [ %1349, %"end for convolved.s1.r19$x.loopexit.us" ], [ %491, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5690.us5722 = phi <4 x i32> [ %1334, %"end for convolved.s1.r19$x.loopexit.us" ], [ %494, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5688.us5720 = phi <4 x i32> [ %1333, %"end for convolved.s1.r19$x.loopexit.us" ], [ %493, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5686.us5718 = phi <4 x i32> [ %1332, %"end for convolved.s1.r19$x.loopexit.us" ], [ %492, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5684.us5716 = phi <4 x i32> [ %1331, %"end for convolved.s1.r19$x.loopexit.us" ], [ %491, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5682.us5714 = phi <4 x i32> [ %1316, %"end for convolved.s1.r19$x.loopexit.us" ], [ %494, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5680.us5712 = phi <4 x i32> [ %1315, %"end for convolved.s1.r19$x.loopexit.us" ], [ %493, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5678.us5710 = phi <4 x i32> [ %1314, %"end for convolved.s1.r19$x.loopexit.us" ], [ %492, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5676.us5708 = phi <4 x i32> [ %1313, %"end for convolved.s1.r19$x.loopexit.us" ], [ %491, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5674.us5706 = phi <4 x i32> [ %1298, %"end for convolved.s1.r19$x.loopexit.us" ], [ %494, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5672.us5704 = phi <4 x i32> [ %1297, %"end for convolved.s1.r19$x.loopexit.us" ], [ %493, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5670.us5702 = phi <4 x i32> [ %1296, %"end for convolved.s1.r19$x.loopexit.us" ], [ %492, %"for convolved.s1.r19$y.preheader" ]
  %.lcssa5668.us5700 = phi <4 x i32> [ %1295, %"end for convolved.s1.r19$x.loopexit.us" ], [ %491, %"for convolved.s1.r19$y.preheader" ]
  %"convolved.s1.r19$y.us" = phi i32 [ %1353, %"end for convolved.s1.r19$x.loopexit.us" ], [ 0, %"for convolved.s1.r19$y.preheader" ]
  %t2302.us = mul nsw i32 %"convolved.s1.r19$y.us", %a613
  %1244 = add nsw i32 %t2302.us, %t2290
  %t2303.us = mul nsw i32 %1244, %a21
  %1245 = add nsw i32 %t2302.us, %t2292
  %t2304.us = mul nsw i32 %1245, %a21
  %1246 = mul nsw i64 %indvars.iv6384, %216
  %t2301.us = add nsw i32 %t2304.us, %t2295
  %t2300.us = add nsw i32 %t2304.us, %t2294
  %t2299.us = add nsw i32 %t2303.us, %t2295
  %t2298.us = add nsw i32 %t2303.us, %t2294
  %1247 = sext i32 %t2298.us to i64
  %1248 = sext i32 %t2299.us to i64
  %1249 = sext i32 %t2300.us to i64
  %1250 = sext i32 %t2301.us to i64
  br label %"for convolved.s1.r19$x.us"

"for convolved.s1.r19$x.us":                      ; preds = %"for convolved.s1.r19$y.us", %"for convolved.s1.r19$x.us"
  %indvars.iv6382 = phi i64 [ 0, %"for convolved.s1.r19$y.us" ], [ %indvars.iv.next6383, %"for convolved.s1.r19$x.us" ]
  %1251 = phi <4 x i32> [ %.lcssa5698.us5730, %"for convolved.s1.r19$y.us" ], [ %1352, %"for convolved.s1.r19$x.us" ]
  %1252 = phi <4 x i32> [ %.lcssa5696.us5728, %"for convolved.s1.r19$y.us" ], [ %1351, %"for convolved.s1.r19$x.us" ]
  %1253 = phi <4 x i32> [ %.lcssa5694.us5726, %"for convolved.s1.r19$y.us" ], [ %1350, %"for convolved.s1.r19$x.us" ]
  %1254 = phi <4 x i32> [ %.lcssa5692.us5724, %"for convolved.s1.r19$y.us" ], [ %1349, %"for convolved.s1.r19$x.us" ]
  %1255 = phi <4 x i32> [ %.lcssa5690.us5722, %"for convolved.s1.r19$y.us" ], [ %1334, %"for convolved.s1.r19$x.us" ]
  %1256 = phi <4 x i32> [ %.lcssa5688.us5720, %"for convolved.s1.r19$y.us" ], [ %1333, %"for convolved.s1.r19$x.us" ]
  %1257 = phi <4 x i32> [ %.lcssa5686.us5718, %"for convolved.s1.r19$y.us" ], [ %1332, %"for convolved.s1.r19$x.us" ]
  %1258 = phi <4 x i32> [ %.lcssa5684.us5716, %"for convolved.s1.r19$y.us" ], [ %1331, %"for convolved.s1.r19$x.us" ]
  %1259 = phi <4 x i32> [ %.lcssa5682.us5714, %"for convolved.s1.r19$y.us" ], [ %1316, %"for convolved.s1.r19$x.us" ]
  %1260 = phi <4 x i32> [ %.lcssa5680.us5712, %"for convolved.s1.r19$y.us" ], [ %1315, %"for convolved.s1.r19$x.us" ]
  %1261 = phi <4 x i32> [ %.lcssa5678.us5710, %"for convolved.s1.r19$y.us" ], [ %1314, %"for convolved.s1.r19$x.us" ]
  %1262 = phi <4 x i32> [ %.lcssa5676.us5708, %"for convolved.s1.r19$y.us" ], [ %1313, %"for convolved.s1.r19$x.us" ]
  %1263 = phi <4 x i32> [ %.lcssa5674.us5706, %"for convolved.s1.r19$y.us" ], [ %1298, %"for convolved.s1.r19$x.us" ]
  %1264 = phi <4 x i32> [ %.lcssa5672.us5704, %"for convolved.s1.r19$y.us" ], [ %1297, %"for convolved.s1.r19$x.us" ]
  %1265 = phi <4 x i32> [ %.lcssa5670.us5702, %"for convolved.s1.r19$y.us" ], [ %1296, %"for convolved.s1.r19$x.us" ]
  %1266 = phi <4 x i32> [ %.lcssa5668.us5700, %"for convolved.s1.r19$y.us" ], [ %1295, %"for convolved.s1.r19$x.us" ]
  %1267 = add nsw i64 %indvars.iv6382, %1246
  %1268 = shl nsw i64 %1267, 4
  %1269 = getelementptr inbounds i16, i16* %filter_zeroed, i64 %1268
  %1270 = bitcast i16* %1269 to <8 x i16>*
  %1271 = load <8 x i16>, <8 x i16>* %1270, align 16, !tbaa !395
  %1272 = getelementptr inbounds i16, i16* %1269, i64 8
  %1273 = bitcast i16* %1272 to <8 x i16>*
  %1274 = load <8 x i16>, <8 x i16>* %1273, align 16, !tbaa !395
  %1275 = shufflevector <8 x i16> %1274, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %1276 = mul nsw i64 %indvars.iv6382, %198
  %1277 = add nsw i64 %1276, %1247
  %1278 = shl nsw i64 %1277, 4
  %1279 = getelementptr inbounds i8, i8* %resampled_input, i64 %1278
  %1280 = bitcast i8* %1279 to <16 x i8>*
  %1281 = load <16 x i8>, <16 x i8>* %1280, align 16, !tbaa !438
  %1282 = zext <16 x i8> %1281 to <16 x i16>
  %1283 = shufflevector <8 x i16> %1271, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1284 = shufflevector <16 x i16> %1282, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1285 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1283, <4 x i16> %1284)
  %1286 = shufflevector <8 x i16> %1271, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1287 = shufflevector <16 x i16> %1282, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1288 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1286, <4 x i16> %1287)
  %1289 = shufflevector <8 x i16> %1274, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1290 = shufflevector <16 x i16> %1282, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1291 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1289, <4 x i16> %1290)
  %1292 = shufflevector <16 x i16> %1275, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1293 = shufflevector <16 x i16> %1282, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1294 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1292, <4 x i16> %1293)
  %1295 = add nsw <4 x i32> %1285, %1266
  %1296 = add nsw <4 x i32> %1288, %1265
  %1297 = add nsw <4 x i32> %1291, %1264
  %1298 = add nsw <4 x i32> %1294, %1263
  %1299 = add nsw i64 %1276, %1248
  %1300 = shl nsw i64 %1299, 4
  %1301 = getelementptr inbounds i8, i8* %resampled_input, i64 %1300
  %1302 = bitcast i8* %1301 to <16 x i8>*
  %1303 = load <16 x i8>, <16 x i8>* %1302, align 16, !tbaa !438
  %1304 = zext <16 x i8> %1303 to <16 x i16>
  %1305 = shufflevector <16 x i16> %1304, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1306 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1283, <4 x i16> %1305)
  %1307 = shufflevector <16 x i16> %1304, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1308 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1286, <4 x i16> %1307)
  %1309 = shufflevector <16 x i16> %1304, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1310 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1289, <4 x i16> %1309)
  %1311 = shufflevector <16 x i16> %1304, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1312 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1292, <4 x i16> %1311)
  %1313 = add nsw <4 x i32> %1306, %1262
  %1314 = add nsw <4 x i32> %1308, %1261
  %1315 = add nsw <4 x i32> %1310, %1260
  %1316 = add nsw <4 x i32> %1312, %1259
  %1317 = add nsw i64 %1276, %1249
  %1318 = shl nsw i64 %1317, 4
  %1319 = getelementptr inbounds i8, i8* %resampled_input, i64 %1318
  %1320 = bitcast i8* %1319 to <16 x i8>*
  %1321 = load <16 x i8>, <16 x i8>* %1320, align 16, !tbaa !438
  %1322 = zext <16 x i8> %1321 to <16 x i16>
  %1323 = shufflevector <16 x i16> %1322, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1324 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1283, <4 x i16> %1323)
  %1325 = shufflevector <16 x i16> %1322, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1326 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1286, <4 x i16> %1325)
  %1327 = shufflevector <16 x i16> %1322, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1328 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1289, <4 x i16> %1327)
  %1329 = shufflevector <16 x i16> %1322, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1330 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1292, <4 x i16> %1329)
  %1331 = add nsw <4 x i32> %1324, %1258
  %1332 = add nsw <4 x i32> %1326, %1257
  %1333 = add nsw <4 x i32> %1328, %1256
  %1334 = add nsw <4 x i32> %1330, %1255
  %1335 = add nsw i64 %1276, %1250
  %1336 = shl nsw i64 %1335, 4
  %1337 = getelementptr inbounds i8, i8* %resampled_input, i64 %1336
  %1338 = bitcast i8* %1337 to <16 x i8>*
  %1339 = load <16 x i8>, <16 x i8>* %1338, align 16, !tbaa !438
  %1340 = zext <16 x i8> %1339 to <16 x i16>
  %1341 = shufflevector <16 x i16> %1340, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1342 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1283, <4 x i16> %1341)
  %1343 = shufflevector <16 x i16> %1340, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1344 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1286, <4 x i16> %1343)
  %1345 = shufflevector <16 x i16> %1340, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1346 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1289, <4 x i16> %1345)
  %1347 = shufflevector <16 x i16> %1340, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1348 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1292, <4 x i16> %1347)
  %1349 = add nsw <4 x i32> %1342, %1254
  %1350 = add nsw <4 x i32> %1344, %1253
  %1351 = add nsw <4 x i32> %1346, %1252
  %1352 = add nsw <4 x i32> %1348, %1251
  %indvars.iv.next6383 = add nuw nsw i64 %indvars.iv6382, 1
  %.not2925.us = icmp eq i64 %indvars.iv.next6383, %215
  br i1 %.not2925.us, label %"end for convolved.s1.r19$x.loopexit.us", label %"for convolved.s1.r19$x.us"

"end for convolved.s1.r19$x.loopexit.us":         ; preds = %"for convolved.s1.r19$x.us"
  %indvars.iv.next6385 = add nuw nsw i64 %indvars.iv6384, 1
  %1353 = add nuw nsw i32 %"convolved.s1.r19$y.us", 1
  %.not2924.us = icmp eq i64 %indvars.iv.next6385, %218
  br i1 %.not2924.us, label %"consume convolved", label %"for convolved.s1.r19$y.us"

"consume convolved":                              ; preds = %"end for convolved.s1.r19$x.loopexit.us", %"for convolved.s1.r19$y.preheader", %next_bb17, %then_bb16
  %1354 = phi <4 x i32> [ %494, %next_bb17 ], [ %1240, %then_bb16 ], [ %494, %"for convolved.s1.r19$y.preheader" ], [ %1352, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1355 = phi <4 x i32> [ %493, %next_bb17 ], [ %1239, %then_bb16 ], [ %493, %"for convolved.s1.r19$y.preheader" ], [ %1351, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1356 = phi <4 x i32> [ %492, %next_bb17 ], [ %1238, %then_bb16 ], [ %492, %"for convolved.s1.r19$y.preheader" ], [ %1350, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1357 = phi <4 x i32> [ %491, %next_bb17 ], [ %1237, %then_bb16 ], [ %491, %"for convolved.s1.r19$y.preheader" ], [ %1349, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1358 = phi <4 x i32> [ %494, %next_bb17 ], [ %1222, %then_bb16 ], [ %494, %"for convolved.s1.r19$y.preheader" ], [ %1334, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1359 = phi <4 x i32> [ %493, %next_bb17 ], [ %1221, %then_bb16 ], [ %493, %"for convolved.s1.r19$y.preheader" ], [ %1333, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1360 = phi <4 x i32> [ %492, %next_bb17 ], [ %1220, %then_bb16 ], [ %492, %"for convolved.s1.r19$y.preheader" ], [ %1332, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1361 = phi <4 x i32> [ %491, %next_bb17 ], [ %1219, %then_bb16 ], [ %491, %"for convolved.s1.r19$y.preheader" ], [ %1331, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1362 = phi <4 x i32> [ %494, %next_bb17 ], [ %1204, %then_bb16 ], [ %494, %"for convolved.s1.r19$y.preheader" ], [ %1316, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1363 = phi <4 x i32> [ %493, %next_bb17 ], [ %1203, %then_bb16 ], [ %493, %"for convolved.s1.r19$y.preheader" ], [ %1315, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1364 = phi <4 x i32> [ %492, %next_bb17 ], [ %1202, %then_bb16 ], [ %492, %"for convolved.s1.r19$y.preheader" ], [ %1314, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1365 = phi <4 x i32> [ %491, %next_bb17 ], [ %1201, %then_bb16 ], [ %491, %"for convolved.s1.r19$y.preheader" ], [ %1313, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1366 = phi <4 x i32> [ %494, %next_bb17 ], [ %1186, %then_bb16 ], [ %494, %"for convolved.s1.r19$y.preheader" ], [ %1298, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1367 = phi <4 x i32> [ %493, %next_bb17 ], [ %1185, %then_bb16 ], [ %493, %"for convolved.s1.r19$y.preheader" ], [ %1297, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1368 = phi <4 x i32> [ %492, %next_bb17 ], [ %1184, %then_bb16 ], [ %492, %"for convolved.s1.r19$y.preheader" ], [ %1296, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1369 = phi <4 x i32> [ %491, %next_bb17 ], [ %1183, %then_bb16 ], [ %491, %"for convolved.s1.r19$y.preheader" ], [ %1295, %"end for convolved.s1.r19$x.loopexit.us" ]
  %1370 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1369, <4 x i32> %202)
  %1371 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1368, <4 x i32> %202)
  %1372 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1367, <4 x i32> %202)
  %1373 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1366, <4 x i32> %202)
  %1374 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1370, <4 x i32> %205)
  %1375 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1371, <4 x i32> %205)
  %1376 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1372, <4 x i32> %205)
  %1377 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1373, <4 x i32> %205)
  %1378 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1374)
  %1379 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1375)
  %1380 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1376)
  %1381 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1377)
  %1382 = shufflevector <4 x i16> %1378, <4 x i16> %1379, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1383 = shufflevector <4 x i16> %1380, <4 x i16> %1381, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1384 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %1382, <8 x i16> %208)
  %1385 = shufflevector <16 x i16> %1383, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1386 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %1385, <8 x i16> %208)
  %1387 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1384)
  %1388 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1386)
  %1389 = shufflevector <8 x i8> %1387, <8 x i8> %1388, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1390 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %1389, <16 x i8> %210)
  %1391 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %1390, <16 x i8> %212)
  %1392 = sext i32 %output.s0.x.x.base.s to i64
  %1393 = add nsw i64 %1392, %199
  %1394 = mul nsw i64 %1393, %213
  %1395 = add nsw i64 %1394, %520
  %1396 = getelementptr inbounds i8, i8* %19, i64 %1395
  %1397 = bitcast i8* %1396 to <16 x i8>*
  store <16 x i8> %1391, <16 x i8>* %1397, align 1, !tbaa !515
  %1398 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1365, <4 x i32> %202)
  %1399 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1364, <4 x i32> %202)
  %1400 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1363, <4 x i32> %202)
  %1401 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1362, <4 x i32> %202)
  %1402 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1398, <4 x i32> %205)
  %1403 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1399, <4 x i32> %205)
  %1404 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1400, <4 x i32> %205)
  %1405 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1401, <4 x i32> %205)
  %1406 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1402)
  %1407 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1403)
  %1408 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1404)
  %1409 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1405)
  %1410 = shufflevector <4 x i16> %1406, <4 x i16> %1407, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1411 = shufflevector <4 x i16> %1408, <4 x i16> %1409, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1412 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %1410, <8 x i16> %208)
  %1413 = shufflevector <16 x i16> %1411, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1414 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %1413, <8 x i16> %208)
  %1415 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1412)
  %1416 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1414)
  %1417 = shufflevector <8 x i8> %1415, <8 x i8> %1416, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1418 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %1417, <16 x i8> %210)
  %1419 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %1418, <16 x i8> %212)
  %1420 = add nsw i64 %1393, 1
  %1421 = mul nsw i64 %1420, %213
  %1422 = add nsw i64 %1421, %520
  %1423 = getelementptr inbounds i8, i8* %19, i64 %1422
  %1424 = bitcast i8* %1423 to <16 x i8>*
  store <16 x i8> %1419, <16 x i8>* %1424, align 1, !tbaa !515
  %1425 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1361, <4 x i32> %202)
  %1426 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1360, <4 x i32> %202)
  %1427 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1359, <4 x i32> %202)
  %1428 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1358, <4 x i32> %202)
  %1429 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1425, <4 x i32> %205)
  %1430 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1426, <4 x i32> %205)
  %1431 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1427, <4 x i32> %205)
  %1432 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1428, <4 x i32> %205)
  %1433 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1429)
  %1434 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1430)
  %1435 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1431)
  %1436 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1432)
  %1437 = shufflevector <4 x i16> %1433, <4 x i16> %1434, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1438 = shufflevector <4 x i16> %1435, <4 x i16> %1436, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1439 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %1437, <8 x i16> %208)
  %1440 = shufflevector <16 x i16> %1438, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1441 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %1440, <8 x i16> %208)
  %1442 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1439)
  %1443 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1441)
  %1444 = shufflevector <8 x i8> %1442, <8 x i8> %1443, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1445 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %1444, <16 x i8> %210)
  %1446 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %1445, <16 x i8> %212)
  %1447 = add nsw i64 %1394, %521
  %1448 = getelementptr inbounds i8, i8* %19, i64 %1447
  %1449 = bitcast i8* %1448 to <16 x i8>*
  store <16 x i8> %1446, <16 x i8>* %1449, align 1, !tbaa !515
  %1450 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1357, <4 x i32> %202)
  %1451 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1356, <4 x i32> %202)
  %1452 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1355, <4 x i32> %202)
  %1453 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %1354, <4 x i32> %202)
  %1454 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1450, <4 x i32> %205)
  %1455 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1451, <4 x i32> %205)
  %1456 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1452, <4 x i32> %205)
  %1457 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %1453, <4 x i32> %205)
  %1458 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1454)
  %1459 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1455)
  %1460 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1456)
  %1461 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %1457)
  %1462 = shufflevector <4 x i16> %1458, <4 x i16> %1459, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1463 = shufflevector <4 x i16> %1460, <4 x i16> %1461, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1464 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %1462, <8 x i16> %208)
  %1465 = shufflevector <16 x i16> %1463, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1466 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %1465, <8 x i16> %208)
  %1467 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1464)
  %1468 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %1466)
  %1469 = shufflevector <8 x i8> %1467, <8 x i8> %1468, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1470 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %1469, <16 x i8> %210)
  %1471 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %1470, <16 x i8> %212)
  %1472 = add nsw i64 %1421, %521
  %1473 = getelementptr inbounds i8, i8* %19, i64 %1472
  %1474 = bitcast i8* %1473 to <16 x i8>*
  store <16 x i8> %1471, <16 x i8>* %1474, align 1, !tbaa !515
  %1475 = add nuw nsw i32 %output.s0.x.xo, 1
  %.not2923 = icmp eq i32 %1475, %t2197
  br i1 %.not2923, label %"end for output.s0.x.xo.loopexit", label %"for output.s0.x.xo"

then_bb18:                                        ; preds = %next_bb
  %t2334 = icmp slt i32 %a614, 0
  %1476 = add nsw i32 %6, -1
  %t2335 = mul nsw i32 %1476, %a614
  %t2336 = icmp slt i32 %stride_x, 0
  %1477 = add nsw i32 %22, 3
  %t2337 = ashr i32 %1477, 2
  %t2338 = and i32 %1477, -4
  %t2339 = select i1 %t2334, i32 %t2335, i32 0
  %t2340 = icmp slt i32 %a613, 0
  %1478 = add nsw i32 %8, -1
  %t2341 = mul nsw i32 %1478, %a613
  %t2342 = icmp slt i32 %stride_y, 0
  %1479 = add nsw i32 %25, 3
  %t2343 = ashr i32 %1479, 2
  %t2344 = and i32 %1479, -4
  %t2345 = select i1 %t2340, i32 %t2341, i32 0
  %1480 = icmp eq i32 %6, 3
  %1481 = icmp eq i32 %8, 3
  %t2346 = and i1 %1480, %1481
  %a614.op2892 = shl i32 %a614, 1
  %t2347 = select i1 %t2334, i32 %a614.op2892, i32 0
  %1482 = add nsw i32 %22, -1
  %t2348 = and i32 %1482, -4
  %a613.op2893 = shl i32 %a613, 1
  %t2349 = select i1 %t2340, i32 %a613.op2893, i32 0
  %1483 = add nsw i32 %25, -1
  %t2350 = and i32 %1483, -4
  %t2327 = icmp eq i32 %depth_multiplier, 1
  %1484 = add nuw nsw i32 %5, 15
  %t2305 = ashr i32 %1484, 4
  %1485 = icmp sgt i32 %a613, 0
  %1486 = select i1 %1485, i32 %a613, i32 0
  %t2317 = shl nuw nsw i32 %1486, 1
  %1487 = icmp sgt i32 %a614, 0
  %1488 = select i1 %1487, i32 %a614, i32 0
  %t2324 = shl nuw nsw i32 %1488, 1
  %1489 = select i1 %t2346, i32 %t2349, i32 %t2345
  %1490 = select i1 %t2342, i32 %t2350, i32 -3
  %1491 = add i32 %24, 3
  %1492 = add i32 %1491, %1490
  %1493 = mul nsw i32 %1492, %stride_y
  %b39 = add nsw i32 %1493, %1489
  %1494 = select i1 %t2346, i32 %t2347, i32 %t2339
  %1495 = select i1 %t2336, i32 %t2348, i32 -3
  %1496 = add i32 %21, 3
  %1497 = add i32 %1496, %1495
  %1498 = mul nsw i32 %1497, %stride_x
  %b38 = add nsw i32 %1498, %1494
  %1499 = select i1 %t2342, i32 %t2344, i32 1
  %1500 = add i32 %24, -1
  %1501 = add i32 %1500, %1499
  %1502 = mul nsw i32 %1501, %stride_y
  %b43 = add nsw i32 %1502, %t2345
  %1503 = select i1 %t2340, i32 0, i32 %t2341
  %1504 = add nsw i32 %t2344, -1
  %1505 = select i1 %t2342, i32 0, i32 %1504
  %1506 = add nsw i32 %1505, %24
  %1507 = mul nsw i32 %1506, %stride_y
  %b42 = add nsw i32 %1507, %1503
  %1508 = select i1 %t2336, i32 %t2338, i32 1
  %1509 = add i32 %21, -1
  %1510 = add i32 %1509, %1508
  %1511 = mul nsw i32 %1510, %stride_x
  %b41 = add nsw i32 %1511, %t2339
  %1512 = select i1 %t2334, i32 0, i32 %t2335
  %1513 = add nsw i32 %t2338, -1
  %1514 = select i1 %t2336, i32 0, i32 %1513
  %1515 = add nsw i32 %1514, %21
  %1516 = mul nsw i32 %1515, %stride_x
  %b40 = add nsw i32 %1516, %1512
  %1517 = mul nsw i32 %15, %14
  %1518 = mul nsw i32 %18, %16
  %1519 = mul nsw i32 %13, %12
  %1520 = add i32 %1517, %1519
  %t2328 = add i32 %1520, %1518
  %b33 = add nsw i32 %5, -16
  %1521 = icmp sgt i32 %6, 0
  %1522 = select i1 %1521, i32 %6, i32 0
  %t2661 = zext i32 %1522 to i64
  %1523 = icmp sgt i32 %8, 0
  %1524 = select i1 %1523, i32 %8, i32 0
  %t2662 = zext i32 %1524 to i64
  %1525 = shl nuw nsw i64 %t2661, 5
  %1526 = mul i64 %1525, %t2662
  %1527 = or i64 %1526, 6
  %1528 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8***
  %1529 = sext i32 %7 to i64
  %1530 = insertelement <8 x i8> undef, i8 %filter_zero, i32 0
  %1531 = shufflevector <8 x i8> %1530, <8 x i8> undef, <8 x i32> zeroinitializer
  %1532 = zext <8 x i8> %1531 to <8 x i16>
  %1533 = bitcast [64 x i32]* %sum_filter945 to <4 x i32>*
  %1534 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 4
  %1535 = bitcast i32* %1534 to <4 x i32>*
  %1536 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 8
  %1537 = bitcast i32* %1536 to <4 x i32>*
  %1538 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 12
  %1539 = bitcast i32* %1538 to <4 x i32>*
  %1540 = bitcast i8* %2 to i32*
  %1541 = zext i8 %input_zero to i32
  %1542 = insertelement <16 x i32> undef, i32 %1541, i32 0
  %1543 = shufflevector <16 x i32> %1542, <16 x i32> undef, <16 x i32> zeroinitializer
  %1544 = bitcast [16 x i32]* %offset_c636946 to <4 x i32>*
  %1545 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 4
  %1546 = bitcast i32* %1545 to <4 x i32>*
  %1547 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 8
  %1548 = bitcast i32* %1547 to <4 x i32>*
  %1549 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 12
  %1550 = bitcast i32* %1549 to <4 x i32>*
  %1551 = icmp slt i32 %b41, %b38
  %t2356 = select i1 %1551, i32 %b41, i32 %b38
  %1552 = icmp slt i32 %b43, %b39
  %t2354 = select i1 %1552, i32 %b43, i32 %b39
  %1553 = select i1 %t2346, i32 %t2324, i32 %1512
  %1554 = or i32 %1482, 3
  %1555 = select i1 %t2336, i32 0, i32 %1554
  %1556 = add nsw i32 %1555, %21
  %1557 = mul nsw i32 %1556, %stride_x
  %a38 = add nsw i32 %1557, %1553
  %1558 = icmp sgt i32 %a38, %b40
  %1559 = select i1 %1558, i32 %a38, i32 %b40
  %t2357 = sub nsw i32 %1559, %t2356
  %1560 = select i1 %t2346, i32 %t2317, i32 %1503
  %1561 = or i32 %1483, 3
  %1562 = select i1 %t2342, i32 0, i32 %1561
  %1563 = add nsw i32 %1562, %24
  %1564 = mul nsw i32 %1563, %stride_y
  %a40 = add nsw i32 %1564, %1560
  %1565 = icmp sgt i32 %a40, %b42
  %1566 = select i1 %1565, i32 %a40, i32 %b42
  %t2355 = sub nsw i32 %1566, %t2354
  %t2359 = sub nsw i32 %b40, %b41
  %t2358 = sub nsw i32 %b42, %b43
  %.neg5152 = mul i32 %26, %24
  %.neg5153 = mul i32 %23, %21
  %.neg5154 = mul i32 %27, %16
  %1567 = icmp sgt i32 %17, 0
  %a45 = add nsw i32 %t2355, 1
  %a44 = add nsw i32 %t2357, 1
  %.inv2896 = icmp slt i32 %t2357, 0
  %1568 = select i1 %.inv2896, i32 0, i32 %a44
  %t2663 = zext i32 %1568 to i64
  %.inv2897 = icmp slt i32 %t2355, 0
  %1569 = select i1 %.inv2897, i32 0, i32 %a45
  %t2664 = zext i32 %1569 to i64
  %t2665 = shl nuw nsw i64 %t2663, 4
  %1570 = mul i64 %t2665, %t2664
  %1571 = or i64 %1570, 3
  %1572 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8***
  %t2372 = sub i32 %b41, %t2356
  %t2373 = sub i32 %b43, %t2354
  %1573 = icmp eq i32 %depth_multiplier, 0
  %t2668 = select i1 %1573, <16 x i32> <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>, <16 x i32> zeroinitializer
  %depth_multiplier.lobit2906 = ashr i32 %depth_multiplier, 31
  %1574 = insertelement <16 x i32> undef, i32 %depth_multiplier, i32 0
  %1575 = shufflevector <16 x i32> %1574, <16 x i32> undef, <16 x i32> zeroinitializer
  %1576 = sub nsw <16 x i32> %1575, %t2668
  %1577 = xor i32 %depth_multiplier.lobit2906, -1
  %1578 = sub nsw i32 %1577, %depth_multiplier.lobit2906
  %1579 = insertelement <16 x i32> undef, i32 %1578, i32 0
  %1580 = shufflevector <16 x i32> %1579, <16 x i32> undef, <16 x i32> zeroinitializer
  %1581 = xor <16 x i32> %t2668, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  %1582 = sext i32 %b41 to i64
  %1583 = sext i32 %13 to i64
  %t2381 = sub nsw i32 %a613.op2893, %t2354
  %t2377 = sub nsw i32 %a614.op2892, %t2356
  %reass.add5156 = add i32 %.neg5152, %.neg5153
  %reass.add5157 = add i32 %reass.add5156, %.neg5154
  %1584 = icmp sgt i32 %25, 0
  %t2435 = sub nsw i32 %a614, %t2356
  %t2436 = sub nsw i32 %a613, %t2354
  %1585 = icmp sgt i32 %22, 0
  %1586 = sext i32 %a614 to i64
  %1587 = sext i32 %21 to i64
  %1588 = sext i32 %stride_x to i64
  %1589 = insertelement <16 x i32> undef, i32 %output_multiplier, i32 0
  %1590 = shufflevector <16 x i32> %1589, <16 x i32> undef, <4 x i32> zeroinitializer
  %1591 = sub nsw i32 0, %output_shift
  %1592 = insertelement <16 x i32> undef, i32 %1591, i32 0
  %1593 = shufflevector <16 x i32> %1592, <16 x i32> undef, <4 x i32> zeroinitializer
  %1594 = zext i8 %output_zero to i16
  %1595 = insertelement <16 x i16> undef, i16 %1594, i32 0
  %1596 = shufflevector <16 x i16> %1595, <16 x i16> undef, <8 x i32> zeroinitializer
  %1597 = insertelement <16 x i8> undef, i8 %output_max, i32 0
  %1598 = shufflevector <16 x i8> %1597, <16 x i8> undef, <16 x i32> zeroinitializer
  %1599 = insertelement <16 x i8> undef, i8 %output_min, i32 0
  %1600 = shufflevector <16 x i8> %1599, <16 x i8> undef, <16 x i32> zeroinitializer
  %1601 = sext i32 %23 to i64
  %1602 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8*
  %1603 = zext i32 %6 to i64
  %1604 = sext i32 %6 to i64
  %1605 = sext i32 %9 to i64
  %1606 = zext i32 %8 to i64
  %1607 = zext i32 %t2359 to i64
  %1608 = sext i32 %t2373 to i64
  %1609 = sext i32 %a44 to i64
  %1610 = sext i32 %t2372 to i64
  %1611 = sext i32 %b43 to i64
  %1612 = sext i32 %15 to i64
  %1613 = zext i32 %t2358 to i64
  %1614 = zext i32 %t2337 to i64
  %1615 = zext i32 %t2343 to i64
  %1616 = sext i32 %16 to i64
  %1617 = sext i32 %18 to i64
  %1618 = bitcast [64 x i32]* %sum_filter945 to i8*
  %1619 = bitcast [64 x i32]* %sum_filter945 to i8*
  %1620 = bitcast [64 x i32]* %sum_filter945 to i8*
  %zext6358 = zext i32 %17 to i64
  %1621 = or i32 %t2358, %t2359
  %1622 = icmp slt i32 %1621, 0
  %1623 = or i32 %t2358, %t2359
  %.not6879 = icmp sgt i32 %1623, -1
  br label %"for output.s0.c.co20"

next_bb19:                                        ; preds = %next_bb
  %1624 = icmp sgt i32 %5, 0
  br i1 %1624, label %if.end.i3133, label %after_bb

"for output.s0.c.co20":                           ; preds = %then_bb18, %"end for output.s0.b.rebased48"
  %.3 = phi i8* [ null, %then_bb18 ], [ %.4, %"end for output.s0.b.rebased48" ]
  %output.s0.c.co22 = phi i32 [ 0, %then_bb18 ], [ %1755, %"end for output.s0.b.rebased48" ]
  %a31 = shl nsw i32 %output.s0.c.co22, 4
  %1625 = icmp slt i32 %a31, %b33
  %output.s0.c.c.base23 = select i1 %1625, i32 %a31, i32 %b33
  %1626 = load i64, i64* %.fca.1.gep, align 8, !tbaa !385
  %cmp.i3086 = icmp ult i64 %1626, %1527
  %1627 = load i8*, i8** %.fca.0.gep, align 8, !tbaa !387
  br i1 %cmp.i3086, label %if.then.i3089, label %pseudostack_alloc.exit3102, !prof !388

if.then.i3089:                                    ; preds = %"for output.s0.c.co20"
  %tobool1.not.i3088 = icmp ne i8* %1627, null
  %1628 = load i64, i64* %.fca.2.gep, align 8
  %cmp2.i3091 = icmp ugt i64 %1628, 16384
  %or.cond5127 = and i1 %tobool1.not.i3088, %cmp2.i3091
  br i1 %or.cond5127, label %if.then3.i3093, label %if.end.i3097

if.then3.i3093:                                   ; preds = %if.then.i3089
  call void @halide_free(i8* null, i8* nonnull %1627) #15
  %.pre6503 = load i64, i64* %.fca.2.gep, align 8, !tbaa !389
  br label %if.end.i3097

if.end.i3097:                                     ; preds = %if.then3.i3093, %if.then.i3089
  %1629 = phi i64 [ %.pre6503, %if.then3.i3093 ], [ %1628, %if.then.i3089 ]
  %add.i3095 = add i64 %1629, %1527
  store i64 %add.i3095, i64* %.fca.2.gep, align 8, !tbaa !389
  %cmp7.i3096 = icmp ugt i64 %add.i3095, 16384
  br i1 %cmp7.i3096, label %if.then8.i3099, label %if.end11.i3101

if.then8.i3099:                                   ; preds = %if.end.i3097
  %call.i3098 = call i8* @halide_malloc(i8* null, i64 %1527) #15
  br label %if.end11.i3101

if.end11.i3101:                                   ; preds = %if.then8.i3099, %if.end.i3097
  %storemerge.i3100 = phi i8* [ %call.i3098, %if.then8.i3099 ], [ null, %if.end.i3097 ]
  store i8* %storemerge.i3100, i8** %.fca.0.gep, align 8, !tbaa !387
  store i64 %1527, i64* %.fca.1.gep, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3102

pseudostack_alloc.exit3102:                       ; preds = %"for output.s0.c.co20", %if.end11.i3101
  %1630 = phi i8* [ %storemerge.i3100, %if.end11.i3101 ], [ %1627, %"for output.s0.c.co20" ]
  %1631 = bitcast i8* %1630 to i16*
  %.not2894 = icmp eq i8* %1630, null
  br i1 %.not2894, label %then_bb25, label %"produce filter_zeroed27", !prof !390

then_bb25:                                        ; preds = %pseudostack_alloc.exit3102
  %1632 = alloca i8*, i64 %1527, align 16
  %1633 = bitcast i8** %1632 to i16*
  store i8** %1632, i8*** %1528, align 8
  br label %"produce filter_zeroed27"

"produce filter_zeroed27":                        ; preds = %pseudostack_alloc.exit3102, %then_bb25
  %filter_zeroed26 = phi i16* [ %1633, %then_bb25 ], [ %1631, %pseudostack_alloc.exit3102 ]
  br i1 %1523, label %"for filter_zeroed.s0.y28.preheader", label %"consume sum_filter44.critedge", !prof !391

"for filter_zeroed.s0.y28.preheader":             ; preds = %"produce filter_zeroed27"
  br i1 %1521, label %"for filter_zeroed.s0.y28.us.preheader", label %"for sum_filter.s1.r19$y38.preheader.thread", !prof !391

"for sum_filter.s1.r19$y38.preheader.thread":     ; preds = %"for filter_zeroed.s0.y28.preheader"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %1619, i8 0, i64 64, i1 false)
  br label %"consume sum_filter44"

"for filter_zeroed.s0.y28.us.preheader":          ; preds = %"for filter_zeroed.s0.y28.preheader"
  %1634 = sext i32 %output.s0.c.c.base23 to i64
  br label %"for filter_zeroed.s0.y28.us"

"for filter_zeroed.s0.y28.us":                    ; preds = %"for filter_zeroed.s0.y28.us.preheader", %"end for filter_zeroed.s0.x32.loopexit.us"
  %indvars.iv6324 = phi i64 [ 0, %"for filter_zeroed.s0.y28.us.preheader" ], [ %indvars.iv.next6325, %"end for filter_zeroed.s0.x32.loopexit.us" ]
  %1635 = mul nsw i64 %indvars.iv6324, %1604
  %1636 = mul nsw i64 %indvars.iv6324, %1605
  %1637 = add nsw i64 %1636, %1634
  br label %"for filter_zeroed.s0.x31.us"

"for filter_zeroed.s0.x31.us":                    ; preds = %"for filter_zeroed.s0.y28.us", %"for filter_zeroed.s0.x31.us"
  %indvars.iv6322 = phi i64 [ 0, %"for filter_zeroed.s0.y28.us" ], [ %indvars.iv.next6323, %"for filter_zeroed.s0.x31.us" ]
  %1638 = mul nsw i64 %indvars.iv6322, %1529
  %1639 = add nsw i64 %1638, %1637
  %1640 = getelementptr inbounds i8, i8* %3, i64 %1639
  %1641 = bitcast i8* %1640 to <8 x i8>*
  %1642 = load <8 x i8>, <8 x i8>* %1641, align 1, !tbaa !392
  %1643 = zext <8 x i8> %1642 to <8 x i16>
  %1644 = sub nsw <8 x i16> %1643, %1532
  %1645 = add nsw i64 %indvars.iv6322, %1635
  %1646 = shl nsw i64 %1645, 4
  %1647 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 %1646
  %1648 = bitcast i16* %1647 to <8 x i16>*
  store <8 x i16> %1644, <8 x i16>* %1648, align 16, !tbaa !395
  %1649 = getelementptr inbounds i8, i8* %1640, i64 8
  %1650 = bitcast i8* %1649 to <8 x i8>*
  %1651 = load <8 x i8>, <8 x i8>* %1650, align 1, !tbaa !392
  %1652 = zext <8 x i8> %1651 to <8 x i16>
  %1653 = sub nsw <8 x i16> %1652, %1532
  %1654 = getelementptr inbounds i16, i16* %1647, i64 8
  %1655 = bitcast i16* %1654 to <8 x i16>*
  store <8 x i16> %1653, <8 x i16>* %1655, align 16, !tbaa !395
  %indvars.iv.next6323 = add nuw nsw i64 %indvars.iv6322, 1
  %.not2913.us = icmp eq i64 %indvars.iv.next6323, %1603
  br i1 %.not2913.us, label %"end for filter_zeroed.s0.x32.loopexit.us", label %"for filter_zeroed.s0.x31.us"

"end for filter_zeroed.s0.x32.loopexit.us":       ; preds = %"for filter_zeroed.s0.x31.us"
  %indvars.iv.next6325 = add nuw nsw i64 %indvars.iv6324, 1
  %.not2912.us = icmp eq i64 %indvars.iv.next6325, %1606
  br i1 %.not2912.us, label %"for sum_filter.s1.r19$y38.preheader", label %"for filter_zeroed.s0.y28.us"

"for sum_filter.s1.r19$y38.preheader":            ; preds = %"end for filter_zeroed.s0.x32.loopexit.us"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %1620, i8 0, i64 64, i1 false)
  br i1 %1521, label %"for sum_filter.s1.r19$y38.us", label %"consume sum_filter44", !prof !391

"for sum_filter.s1.r19$y38.us":                   ; preds = %"for sum_filter.s1.r19$y38.preheader", %"end for sum_filter.s1.r19$x42.loopexit.us"
  %indvars.iv6330 = phi i64 [ %indvars.iv.next6331, %"end for sum_filter.s1.r19$x42.loopexit.us" ], [ 0, %"for sum_filter.s1.r19$y38.preheader" ]
  %.lcssa5579.us5586 = phi <4 x i32> [ %1678, %"end for sum_filter.s1.r19$x42.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader" ]
  %.lcssa5577.us5584 = phi <4 x i32> [ %1677, %"end for sum_filter.s1.r19$x42.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader" ]
  %.lcssa5575.us5582 = phi <4 x i32> [ %1676, %"end for sum_filter.s1.r19$x42.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader" ]
  %.lcssa5573.us5580 = phi <4 x i32> [ %1675, %"end for sum_filter.s1.r19$x42.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader" ]
  %1656 = mul nsw i64 %indvars.iv6330, %1604
  br label %"for sum_filter.s1.r19$x41.us"

"for sum_filter.s1.r19$x41.us":                   ; preds = %"for sum_filter.s1.r19$y38.us", %"for sum_filter.s1.r19$x41.us"
  %indvars.iv6328 = phi i64 [ 0, %"for sum_filter.s1.r19$y38.us" ], [ %indvars.iv.next6329, %"for sum_filter.s1.r19$x41.us" ]
  %1657 = phi <4 x i32> [ %.lcssa5579.us5586, %"for sum_filter.s1.r19$y38.us" ], [ %1678, %"for sum_filter.s1.r19$x41.us" ]
  %1658 = phi <4 x i32> [ %.lcssa5577.us5584, %"for sum_filter.s1.r19$y38.us" ], [ %1677, %"for sum_filter.s1.r19$x41.us" ]
  %1659 = phi <4 x i32> [ %.lcssa5575.us5582, %"for sum_filter.s1.r19$y38.us" ], [ %1676, %"for sum_filter.s1.r19$x41.us" ]
  %1660 = phi <4 x i32> [ %.lcssa5573.us5580, %"for sum_filter.s1.r19$y38.us" ], [ %1675, %"for sum_filter.s1.r19$x41.us" ]
  %1661 = shufflevector <4 x i32> %1660, <4 x i32> %1659, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1662 = shufflevector <4 x i32> %1658, <4 x i32> %1657, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1663 = shufflevector <8 x i32> %1661, <8 x i32> %1662, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1664 = add nsw i64 %indvars.iv6328, %1656
  %1665 = shl nsw i64 %1664, 4
  %1666 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 %1665
  %1667 = bitcast i16* %1666 to <8 x i16>*
  %1668 = load <8 x i16>, <8 x i16>* %1667, align 16, !tbaa !395
  %1669 = getelementptr inbounds i16, i16* %1666, i64 8
  %1670 = bitcast i16* %1669 to <8 x i16>*
  %1671 = load <8 x i16>, <8 x i16>* %1670, align 16, !tbaa !395
  %1672 = shufflevector <8 x i16> %1668, <8 x i16> %1671, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1673 = sext <16 x i16> %1672 to <16 x i32>
  %1674 = add nsw <16 x i32> %1663, %1673
  %1675 = shufflevector <16 x i32> %1674, <16 x i32> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1676 = shufflevector <16 x i32> %1674, <16 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1677 = shufflevector <16 x i32> %1674, <16 x i32> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1678 = shufflevector <16 x i32> %1674, <16 x i32> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %indvars.iv.next6329 = add nuw nsw i64 %indvars.iv6328, 1
  %.not2911.us = icmp eq i64 %indvars.iv.next6329, %1603
  br i1 %.not2911.us, label %"end for sum_filter.s1.r19$x42.loopexit.us", label %"for sum_filter.s1.r19$x41.us"

"end for sum_filter.s1.r19$x42.loopexit.us":      ; preds = %"for sum_filter.s1.r19$x41.us"
  %indvars.iv.next6331 = add nuw nsw i64 %indvars.iv6330, 1
  %.not2910.us = icmp eq i64 %indvars.iv.next6331, %1606
  br i1 %.not2910.us, label %"consume sum_filter44.loopexit.split.us", label %"for sum_filter.s1.r19$y38.us"

"consume sum_filter44.loopexit.split.us":         ; preds = %"end for sum_filter.s1.r19$x42.loopexit.us"
  store <4 x i32> %1675, <4 x i32>* %1533, align 16, !tbaa !441
  store <4 x i32> %1676, <4 x i32>* %1535, align 16, !tbaa !452
  store <4 x i32> %1677, <4 x i32>* %1537, align 16, !tbaa !454
  store <4 x i32> %1678, <4 x i32>* %1539, align 16, !tbaa !457
  br label %"consume sum_filter44"

"consume sum_filter44.critedge":                  ; preds = %"produce filter_zeroed27"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %1618, i8 0, i64 64, i1 false)
  br label %"consume sum_filter44"

"consume sum_filter44":                           ; preds = %"for sum_filter.s1.r19$y38.preheader", %"for sum_filter.s1.r19$y38.preheader.thread", %"consume sum_filter44.loopexit.split.us", %"consume sum_filter44.critedge"
  %1679 = phi <4 x i32> [ %1678, %"consume sum_filter44.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter44.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader" ]
  %1680 = phi <4 x i32> [ %1677, %"consume sum_filter44.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter44.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader" ]
  %1681 = phi <4 x i32> [ %1676, %"consume sum_filter44.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter44.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader" ]
  %1682 = phi <4 x i32> [ %1675, %"consume sum_filter44.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter44.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y38.preheader" ]
  %1683 = sext i32 %output.s0.c.c.base23 to i64
  %1684 = getelementptr inbounds i32, i32* %1540, i64 %1683
  %1685 = bitcast i32* %1684 to <4 x i32>*
  %1686 = load <4 x i32>, <4 x i32>* %1685, align 4, !tbaa !415
  %1687 = getelementptr inbounds i32, i32* %1684, i64 4
  %1688 = bitcast i32* %1687 to <4 x i32>*
  %1689 = load <4 x i32>, <4 x i32>* %1688, align 4, !tbaa !415
  %1690 = getelementptr inbounds i32, i32* %1684, i64 8
  %1691 = bitcast i32* %1690 to <4 x i32>*
  %1692 = load <4 x i32>, <4 x i32>* %1691, align 4, !tbaa !415
  %1693 = getelementptr inbounds i32, i32* %1684, i64 12
  %1694 = bitcast i32* %1693 to <4 x i32>*
  %1695 = load <4 x i32>, <4 x i32>* %1694, align 4, !tbaa !415
  %1696 = shufflevector <4 x i32> %1686, <4 x i32> %1689, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1697 = shufflevector <4 x i32> %1692, <4 x i32> %1695, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1698 = shufflevector <8 x i32> %1696, <8 x i32> %1697, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1699 = shufflevector <4 x i32> %1682, <4 x i32> %1681, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1700 = shufflevector <4 x i32> %1680, <4 x i32> %1679, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1701 = shufflevector <8 x i32> %1699, <8 x i32> %1700, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1702 = mul nsw <16 x i32> %1701, %1543
  %1703 = sub nsw <16 x i32> %1698, %1702
  %1704 = shufflevector <16 x i32> %1703, <16 x i32> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  store <4 x i32> %1704, <4 x i32>* %1544, align 16, !tbaa !397
  %1705 = shufflevector <16 x i32> %1703, <16 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  store <4 x i32> %1705, <4 x i32>* %1546, align 16, !tbaa !408
  %1706 = shufflevector <16 x i32> %1703, <16 x i32> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  store <4 x i32> %1706, <4 x i32>* %1548, align 16, !tbaa !410
  %1707 = shufflevector <16 x i32> %1703, <16 x i32> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  store <4 x i32> %1707, <4 x i32>* %1550, align 16, !tbaa !413
  br i1 %1567, label %"for output.s0.b.rebased47.preheader", label %"end for output.s0.b.rebased48", !prof !391

"for output.s0.b.rebased47.preheader":            ; preds = %"consume sum_filter44"
  %1708 = insertelement <16 x i32> undef, i32 %output.s0.c.c.base23, i32 0
  %1709 = shufflevector <16 x i32> %1708, <16 x i32> undef, <16 x i32> zeroinitializer
  %1710 = add nsw <16 x i32> %1709, <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %.lobit2905 = ashr <16 x i32> %1710, <i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31>
  %1711 = sub nsw <16 x i32> %1710, %.lobit2905
  %1712 = and <16 x i32> %.lobit2905, %1580
  %1713 = sub i32 %output.s0.c.c.base23, %t2328
  %1714 = bitcast i16* %filter_zeroed26 to <8 x i16>*
  %1715 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 8
  %1716 = bitcast i16* %1715 to <8 x i16>*
  %1717 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 16
  %1718 = bitcast i16* %1717 to <8 x i16>*
  %1719 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 24
  %1720 = bitcast i16* %1719 to <8 x i16>*
  %1721 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 32
  %1722 = bitcast i16* %1721 to <8 x i16>*
  %1723 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 40
  %1724 = bitcast i16* %1723 to <8 x i16>*
  %1725 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 48
  %1726 = bitcast i16* %1725 to <8 x i16>*
  %1727 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 56
  %1728 = bitcast i16* %1727 to <8 x i16>*
  %1729 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 64
  %1730 = bitcast i16* %1729 to <8 x i16>*
  %1731 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 72
  %1732 = bitcast i16* %1731 to <8 x i16>*
  %1733 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 80
  %1734 = bitcast i16* %1733 to <8 x i16>*
  %1735 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 88
  %1736 = bitcast i16* %1735 to <8 x i16>*
  %1737 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 96
  %1738 = bitcast i16* %1737 to <8 x i16>*
  %1739 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 104
  %1740 = bitcast i16* %1739 to <8 x i16>*
  %1741 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 112
  %1742 = bitcast i16* %1741 to <8 x i16>*
  %1743 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 120
  %1744 = bitcast i16* %1743 to <8 x i16>*
  %1745 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 128
  %1746 = bitcast i16* %1745 to <8 x i16>*
  %1747 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 136
  %1748 = bitcast i16* %1747 to <8 x i16>*
  %1749 = sext i32 %1713 to i64
  br label %"for output.s0.b.rebased47"

"for output.s0.b.rebased47":                      ; preds = %"for output.s0.b.rebased47.preheader", %"end for output.s0.y.yo71"
  %indvars.iv6355 = phi i64 [ 0, %"for output.s0.b.rebased47.preheader" ], [ %indvars.iv.next6356, %"end for output.s0.y.yo71" ]
  %1750 = load i64, i64* %.fca.1.gep875, align 8, !tbaa !385
  %cmp.i3104 = icmp ult i64 %1750, %1571
  %1751 = load i8*, i8** %.fca.0.gep874, align 8, !tbaa !387
  br i1 %cmp.i3104, label %if.then.i3107, label %pseudostack_alloc.exit3120, !prof !388

if.then.i3107:                                    ; preds = %"for output.s0.b.rebased47"
  %tobool1.not.i3106 = icmp ne i8* %1751, null
  %1752 = load i64, i64* %.fca.2.gep876, align 8
  %cmp2.i3109 = icmp ugt i64 %1752, 16384
  %or.cond5128 = and i1 %tobool1.not.i3106, %cmp2.i3109
  br i1 %or.cond5128, label %if.then3.i3111, label %if.end.i3115

if.then3.i3111:                                   ; preds = %if.then.i3107
  call void @halide_free(i8* null, i8* nonnull %1751) #15
  %.pre6504 = load i64, i64* %.fca.2.gep876, align 8, !tbaa !389
  br label %if.end.i3115

if.end.i3115:                                     ; preds = %if.then3.i3111, %if.then.i3107
  %1753 = phi i64 [ %.pre6504, %if.then3.i3111 ], [ %1752, %if.then.i3107 ]
  %add.i3113 = add i64 %1753, %1571
  store i64 %add.i3113, i64* %.fca.2.gep876, align 8, !tbaa !389
  %cmp7.i3114 = icmp ugt i64 %add.i3113, 16384
  br i1 %cmp7.i3114, label %if.then8.i3117, label %if.end11.i3119

if.then8.i3117:                                   ; preds = %if.end.i3115
  %call.i3116 = call i8* @halide_malloc(i8* null, i64 %1571) #15
  br label %if.end11.i3119

if.end11.i3119:                                   ; preds = %if.then8.i3117, %if.end.i3115
  %storemerge.i3118 = phi i8* [ %call.i3116, %if.then8.i3117 ], [ null, %if.end.i3115 ]
  store i8* %storemerge.i3118, i8** %.fca.0.gep874, align 8, !tbaa !387
  store i64 %1571, i64* %.fca.1.gep875, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3120

pseudostack_alloc.exit3120:                       ; preds = %"for output.s0.b.rebased47", %if.end11.i3119
  %1754 = phi i8* [ %storemerge.i3118, %if.end11.i3119 ], [ %1751, %"for output.s0.b.rebased47" ]
  %.not2898 = icmp eq i8* %1754, null
  br i1 %.not2898, label %then_bb51, label %"produce resampled_input53", !prof !390

"end for output.s0.b.rebased48":                  ; preds = %"end for output.s0.y.yo71", %"consume sum_filter44"
  %.4 = phi i8* [ %.3, %"consume sum_filter44" ], [ %1602, %"end for output.s0.y.yo71" ]
  %1755 = add nuw nsw i32 %output.s0.c.co22, 1
  %.not2895 = icmp eq i32 %1755, %t2305
  br i1 %.not2895, label %after_bb.loopexit5216, label %"for output.s0.c.co20"

then_bb51:                                        ; preds = %pseudostack_alloc.exit3120
  %1756 = alloca i8*, i64 %1571, align 16
  %1757 = bitcast i8** %1756 to i8*
  store i8** %1756, i8*** %1572, align 8
  br label %"produce resampled_input53"

"produce resampled_input53":                      ; preds = %pseudostack_alloc.exit3120, %then_bb51
  %resampled_input52 = phi i8* [ %1757, %then_bb51 ], [ %1754, %pseudostack_alloc.exit3120 ]
  %1758 = add nsw i64 %indvars.iv6355, %1616
  br i1 %t2327, label %then_bb55, label %next_bb56

then_bb55:                                        ; preds = %"produce resampled_input53"
  %1759 = mul nsw i64 %1758, %1617
  %1760 = add nsw i64 %1759, %1749
  br i1 %.not6879, label %"for resampled_input.s0.y.rebased57.us", label %"consume resampled_input69", !prof !435

"for resampled_input.s0.y.rebased57.us":          ; preds = %then_bb55, %"end for resampled_input.s0.x.rebased61.loopexit.us"
  %indvars.iv6341 = phi i64 [ %indvars.iv.next6342, %"end for resampled_input.s0.x.rebased61.loopexit.us" ], [ 0, %then_bb55 ]
  %1761 = add nsw i64 %indvars.iv6341, %1608
  %1762 = mul nsw i64 %1761, %1609
  %1763 = add nsw i64 %1762, %1610
  %1764 = add nsw i64 %indvars.iv6341, %1611
  %1765 = mul nsw i64 %1764, %1612
  %1766 = add nsw i64 %1760, %1765
  br label %"for resampled_input.s0.x.rebased60.us"

"for resampled_input.s0.x.rebased60.us":          ; preds = %"for resampled_input.s0.y.rebased57.us", %"for resampled_input.s0.x.rebased60.us"
  %indvars.iv6339 = phi i64 [ 0, %"for resampled_input.s0.y.rebased57.us" ], [ %indvars.iv.next6340, %"for resampled_input.s0.x.rebased60.us" ]
  %1767 = add nsw i64 %indvars.iv6339, %1582
  %1768 = mul nsw i64 %1767, %1583
  %1769 = add nsw i64 %1768, %1766
  %1770 = getelementptr inbounds i8, i8* %10, i64 %1769
  %1771 = bitcast i8* %1770 to <16 x i8>*
  %1772 = load <16 x i8>, <16 x i8>* %1771, align 1, !tbaa !436
  %1773 = add nsw i64 %indvars.iv6339, %1763
  %1774 = shl nsw i64 %1773, 4
  %1775 = getelementptr inbounds i8, i8* %resampled_input52, i64 %1774
  %1776 = bitcast i8* %1775 to <16 x i8>*
  store <16 x i8> %1772, <16 x i8>* %1776, align 16, !tbaa !438
  %indvars.iv.next6340 = add nuw nsw i64 %indvars.iv6339, 1
  %.not2909.us = icmp eq i64 %indvars.iv6339, %1607
  br i1 %.not2909.us, label %"end for resampled_input.s0.x.rebased61.loopexit.us", label %"for resampled_input.s0.x.rebased60.us"

"end for resampled_input.s0.x.rebased61.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased60.us"
  %indvars.iv.next6342 = add nuw nsw i64 %indvars.iv6341, 1
  %.not2908.us = icmp eq i64 %indvars.iv6341, %1613
  br i1 %.not2908.us, label %"consume resampled_input69", label %"for resampled_input.s0.y.rebased57.us"

next_bb56:                                        ; preds = %"produce resampled_input53"
  %1777 = trunc i64 %1758 to i32
  %1778 = mul i32 %18, %1777
  %t2371 = sub i32 %1778, %t2328
  br i1 %1622, label %"consume resampled_input69", label %"for resampled_input.s0.y.rebased63.preheader.split.us", !prof !440

"for resampled_input.s0.y.rebased63.preheader.split.us": ; preds = %next_bb56
  %1779 = sdiv <16 x i32> %1711, %1576
  %1780 = add nsw <16 x i32> %1779, %1712
  %1781 = and <16 x i32> %1780, %1581
  br label %"for resampled_input.s0.y.rebased63.us"

"for resampled_input.s0.y.rebased63.us":          ; preds = %"end for resampled_input.s0.x.rebased67.loopexit.us", %"for resampled_input.s0.y.rebased63.preheader.split.us"
  %indvars.iv6335 = phi i64 [ %indvars.iv.next6336, %"end for resampled_input.s0.x.rebased67.loopexit.us" ], [ 0, %"for resampled_input.s0.y.rebased63.preheader.split.us" ]
  %1782 = add nsw i64 %indvars.iv6335, %1608
  %1783 = mul nsw i64 %1782, %1609
  %1784 = add nsw i64 %1783, %1610
  %1785 = trunc i64 %indvars.iv6335 to i32
  %1786 = add i32 %b43, %1785
  %1787 = mul i32 %1786, %15
  %1788 = add i32 %1787, %t2371
  br label %"for resampled_input.s0.x.rebased66.us"

"for resampled_input.s0.x.rebased66.us":          ; preds = %"for resampled_input.s0.y.rebased63.us", %"for resampled_input.s0.x.rebased66.us"
  %indvars.iv6333 = phi i64 [ 0, %"for resampled_input.s0.y.rebased63.us" ], [ %indvars.iv.next6334, %"for resampled_input.s0.x.rebased66.us" ]
  %1789 = trunc i64 %indvars.iv6333 to i32
  %1790 = add nsw i32 %b41, %1789
  %1791 = mul nsw i32 %1790, %13
  %1792 = add nsw i32 %1788, %1791
  %1793 = insertelement <16 x i32> undef, i32 %1792, i32 0
  %1794 = shufflevector <16 x i32> %1793, <16 x i32> undef, <16 x i32> zeroinitializer
  %1795 = add nsw <16 x i32> %1794, %1781
  %1796 = extractelement <16 x i32> %1795, i32 0
  %1797 = sext i32 %1796 to i64
  %1798 = getelementptr inbounds i8, i8* %10, i64 %1797
  %1799 = load i8, i8* %1798, align 1, !tbaa !436
  %1800 = insertelement <16 x i8> undef, i8 %1799, i32 0
  %1801 = extractelement <16 x i32> %1795, i32 1
  %1802 = sext i32 %1801 to i64
  %1803 = getelementptr inbounds i8, i8* %10, i64 %1802
  %1804 = load i8, i8* %1803, align 1, !tbaa !436
  %1805 = insertelement <16 x i8> %1800, i8 %1804, i32 1
  %1806 = extractelement <16 x i32> %1795, i32 2
  %1807 = sext i32 %1806 to i64
  %1808 = getelementptr inbounds i8, i8* %10, i64 %1807
  %1809 = load i8, i8* %1808, align 1, !tbaa !436
  %1810 = insertelement <16 x i8> %1805, i8 %1809, i32 2
  %1811 = extractelement <16 x i32> %1795, i32 3
  %1812 = sext i32 %1811 to i64
  %1813 = getelementptr inbounds i8, i8* %10, i64 %1812
  %1814 = load i8, i8* %1813, align 1, !tbaa !436
  %1815 = insertelement <16 x i8> %1810, i8 %1814, i32 3
  %1816 = extractelement <16 x i32> %1795, i32 4
  %1817 = sext i32 %1816 to i64
  %1818 = getelementptr inbounds i8, i8* %10, i64 %1817
  %1819 = load i8, i8* %1818, align 1, !tbaa !436
  %1820 = insertelement <16 x i8> %1815, i8 %1819, i32 4
  %1821 = extractelement <16 x i32> %1795, i32 5
  %1822 = sext i32 %1821 to i64
  %1823 = getelementptr inbounds i8, i8* %10, i64 %1822
  %1824 = load i8, i8* %1823, align 1, !tbaa !436
  %1825 = insertelement <16 x i8> %1820, i8 %1824, i32 5
  %1826 = extractelement <16 x i32> %1795, i32 6
  %1827 = sext i32 %1826 to i64
  %1828 = getelementptr inbounds i8, i8* %10, i64 %1827
  %1829 = load i8, i8* %1828, align 1, !tbaa !436
  %1830 = insertelement <16 x i8> %1825, i8 %1829, i32 6
  %1831 = extractelement <16 x i32> %1795, i32 7
  %1832 = sext i32 %1831 to i64
  %1833 = getelementptr inbounds i8, i8* %10, i64 %1832
  %1834 = load i8, i8* %1833, align 1, !tbaa !436
  %1835 = insertelement <16 x i8> %1830, i8 %1834, i32 7
  %1836 = extractelement <16 x i32> %1795, i32 8
  %1837 = sext i32 %1836 to i64
  %1838 = getelementptr inbounds i8, i8* %10, i64 %1837
  %1839 = load i8, i8* %1838, align 1, !tbaa !436
  %1840 = insertelement <16 x i8> %1835, i8 %1839, i32 8
  %1841 = extractelement <16 x i32> %1795, i32 9
  %1842 = sext i32 %1841 to i64
  %1843 = getelementptr inbounds i8, i8* %10, i64 %1842
  %1844 = load i8, i8* %1843, align 1, !tbaa !436
  %1845 = insertelement <16 x i8> %1840, i8 %1844, i32 9
  %1846 = extractelement <16 x i32> %1795, i32 10
  %1847 = sext i32 %1846 to i64
  %1848 = getelementptr inbounds i8, i8* %10, i64 %1847
  %1849 = load i8, i8* %1848, align 1, !tbaa !436
  %1850 = insertelement <16 x i8> %1845, i8 %1849, i32 10
  %1851 = extractelement <16 x i32> %1795, i32 11
  %1852 = sext i32 %1851 to i64
  %1853 = getelementptr inbounds i8, i8* %10, i64 %1852
  %1854 = load i8, i8* %1853, align 1, !tbaa !436
  %1855 = insertelement <16 x i8> %1850, i8 %1854, i32 11
  %1856 = extractelement <16 x i32> %1795, i32 12
  %1857 = sext i32 %1856 to i64
  %1858 = getelementptr inbounds i8, i8* %10, i64 %1857
  %1859 = load i8, i8* %1858, align 1, !tbaa !436
  %1860 = insertelement <16 x i8> %1855, i8 %1859, i32 12
  %1861 = extractelement <16 x i32> %1795, i32 13
  %1862 = sext i32 %1861 to i64
  %1863 = getelementptr inbounds i8, i8* %10, i64 %1862
  %1864 = load i8, i8* %1863, align 1, !tbaa !436
  %1865 = insertelement <16 x i8> %1860, i8 %1864, i32 13
  %1866 = extractelement <16 x i32> %1795, i32 14
  %1867 = sext i32 %1866 to i64
  %1868 = getelementptr inbounds i8, i8* %10, i64 %1867
  %1869 = load i8, i8* %1868, align 1, !tbaa !436
  %1870 = insertelement <16 x i8> %1865, i8 %1869, i32 14
  %1871 = extractelement <16 x i32> %1795, i32 15
  %1872 = sext i32 %1871 to i64
  %1873 = getelementptr inbounds i8, i8* %10, i64 %1872
  %1874 = load i8, i8* %1873, align 1, !tbaa !436
  %1875 = insertelement <16 x i8> %1870, i8 %1874, i32 15
  %1876 = add nsw i64 %indvars.iv6333, %1784
  %1877 = shl nsw i64 %1876, 4
  %1878 = getelementptr inbounds i8, i8* %resampled_input52, i64 %1877
  %1879 = bitcast i8* %1878 to <16 x i8>*
  store <16 x i8> %1875, <16 x i8>* %1879, align 16, !tbaa !438
  %indvars.iv.next6334 = add nuw nsw i64 %indvars.iv6333, 1
  %.not2907.us = icmp eq i64 %indvars.iv6333, %1607
  br i1 %.not2907.us, label %"end for resampled_input.s0.x.rebased67.loopexit.us", label %"for resampled_input.s0.x.rebased66.us"

"end for resampled_input.s0.x.rebased67.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased66.us"
  %indvars.iv.next6336 = add nuw nsw i64 %indvars.iv6335, 1
  %.not2904.us = icmp eq i64 %indvars.iv6335, %1613
  br i1 %.not2904.us, label %"consume resampled_input69", label %"for resampled_input.s0.y.rebased63.us"

"consume resampled_input69":                      ; preds = %"end for resampled_input.s0.x.rebased67.loopexit.us", %"end for resampled_input.s0.x.rebased61.loopexit.us", %next_bb56, %then_bb55
  %1880 = trunc i64 %1758 to i32
  %1881 = mul i32 %27, %1880
  %t2365 = add i32 %1881, %output.s0.c.c.base23
  %t2382 = sub i32 %t2365, %reass.add5157
  br i1 %1584, label %"for output.s0.y.yo70.preheader", label %"end for output.s0.y.yo71", !prof !391

"for output.s0.y.yo70.preheader":                 ; preds = %"consume resampled_input69"
  %1882 = load <4 x i32>, <4 x i32>* %1544, align 16
  %1883 = load <4 x i32>, <4 x i32>* %1546, align 16
  %1884 = load <4 x i32>, <4 x i32>* %1548, align 16
  %1885 = load <4 x i32>, <4 x i32>* %1550, align 16
  br label %"for output.s0.y.yo70"

"for output.s0.y.yo70":                           ; preds = %"for output.s0.y.yo70.preheader", %"end for output.s0.x.xo74"
  %indvars.iv6353 = phi i64 [ 0, %"for output.s0.y.yo70.preheader" ], [ %indvars.iv.next6354, %"end for output.s0.x.xo74" ]
  %1886 = trunc i64 %indvars.iv6353 to i32
  %1887 = shl nsw i32 %1886, 2
  %t2429 = add nsw i32 %1887, %24
  %t2430 = add nsw i32 %t2429, 1
  %t2431 = add nsw i32 %t2429, 2
  %t2432 = add nsw i32 %t2429, 3
  %t2433 = mul nsw i32 %t2430, %stride_y
  %1888 = add nsw i32 %t2433, %t2381
  %t2434 = mul nsw i32 %1888, %a44
  %1889 = add nsw i32 %t2433, %t2436
  %t2437 = mul nsw i32 %1889, %a44
  %t2438 = mul nsw i32 %t2431, %stride_y
  %1890 = add nsw i32 %t2438, %t2381
  %t2439 = mul nsw i32 %1890, %a44
  %1891 = add nsw i32 %t2438, %t2436
  %t2440 = mul nsw i32 %1891, %a44
  %t2441 = mul nsw i32 %t2432, %stride_y
  %1892 = add nsw i32 %t2441, %t2381
  %t2442 = mul nsw i32 %1892, %a44
  %1893 = add nsw i32 %t2441, %t2436
  %t2443 = mul nsw i32 %1893, %a44
  %t2444 = mul nsw i32 %t2429, %stride_y
  %1894 = add nsw i32 %t2444, %t2381
  %t2445 = mul nsw i32 %1894, %a44
  %1895 = add nsw i32 %t2444, %t2436
  %t2446 = mul nsw i32 %1895, %a44
  %t2447 = sub nsw i32 %t2433, %t2354
  %t2448 = mul nsw i32 %t2447, %a44
  %t2449 = sub nsw i32 %t2438, %t2354
  %t2450 = mul nsw i32 %t2449, %a44
  %t2451 = sub nsw i32 %t2441, %t2354
  %t2452 = mul nsw i32 %t2451, %a44
  %t2453 = sub nsw i32 %t2444, %t2354
  %t2454 = mul nsw i32 %t2453, %a44
  br i1 %1585, label %"for output.s0.x.xo73.preheader", label %"end for output.s0.x.xo74", !prof !391

"for output.s0.x.xo73.preheader":                 ; preds = %"for output.s0.y.yo70"
  %1896 = mul nsw i32 %t2430, %26
  %t2426 = add nsw i32 %1896, %t2382
  %1897 = mul nsw i32 %t2431, %26
  %t2427 = add nsw i32 %1897, %t2382
  %1898 = mul nsw i32 %t2432, %26
  %t2428 = add nsw i32 %1898, %t2382
  %1899 = mul nsw i32 %t2429, %26
  %t2425 = add nsw i32 %1899, %t2382
  %t2418 = add nsw i32 %t2434, %t2377
  %t2414 = add nsw i32 %t2434, %t2435
  %t2405 = add nsw i32 %t2437, %t2377
  %t2401 = add nsw i32 %t2437, %t2435
  %t2419 = add nsw i32 %t2439, %t2377
  %t2415 = add nsw i32 %t2439, %t2435
  %t2406 = add nsw i32 %t2440, %t2377
  %t2402 = add nsw i32 %t2440, %t2435
  %t2420 = add nsw i32 %t2442, %t2377
  %t2416 = add nsw i32 %t2442, %t2435
  %t2407 = add nsw i32 %t2443, %t2377
  %t2403 = add nsw i32 %t2443, %t2435
  %t2417 = add nsw i32 %t2445, %t2377
  %t2413 = add nsw i32 %t2445, %t2435
  %t2404 = add nsw i32 %t2446, %t2377
  %t2400 = add nsw i32 %t2446, %t2435
  %t2392 = add nsw i32 %t2448, %t2377
  %t2388 = add nsw i32 %t2448, %t2435
  %t2393 = add nsw i32 %t2450, %t2377
  %t2389 = add nsw i32 %t2450, %t2435
  %t2394 = add nsw i32 %t2452, %t2377
  %t2390 = add nsw i32 %t2452, %t2435
  %t2391 = add nsw i32 %t2454, %t2377
  %t2387 = add nsw i32 %t2454, %t2435
  %t2410 = sub nsw i32 %t2434, %t2356
  %t2397 = sub nsw i32 %t2437, %t2356
  %t2411 = sub nsw i32 %t2439, %t2356
  %t2398 = sub nsw i32 %t2440, %t2356
  %t2412 = sub nsw i32 %t2442, %t2356
  %t2399 = sub nsw i32 %t2443, %t2356
  %t2409 = sub nsw i32 %t2445, %t2356
  %t2396 = sub nsw i32 %t2446, %t2356
  %t2384 = sub nsw i32 %t2448, %t2356
  %t2385 = sub nsw i32 %t2450, %t2356
  %t2386 = sub nsw i32 %t2452, %t2356
  %t2383 = sub nsw i32 %t2454, %t2356
  %1900 = sext i32 %t2383 to i64
  %1901 = sext i32 %t2384 to i64
  %1902 = sext i32 %t2385 to i64
  %1903 = sext i32 %t2386 to i64
  %1904 = sext i32 %t2387 to i64
  %1905 = sext i32 %t2388 to i64
  %1906 = sext i32 %t2389 to i64
  %1907 = sext i32 %t2390 to i64
  %1908 = sext i32 %t2391 to i64
  %1909 = sext i32 %t2392 to i64
  %1910 = sext i32 %t2393 to i64
  %1911 = sext i32 %t2394 to i64
  %1912 = sext i32 %t2396 to i64
  %1913 = sext i32 %t2397 to i64
  %1914 = sext i32 %t2398 to i64
  %1915 = sext i32 %t2399 to i64
  %1916 = sext i32 %t2400 to i64
  %1917 = sext i32 %t2401 to i64
  %1918 = sext i32 %t2402 to i64
  %1919 = sext i32 %t2403 to i64
  %1920 = sext i32 %t2404 to i64
  %1921 = sext i32 %t2405 to i64
  %1922 = sext i32 %t2406 to i64
  %1923 = sext i32 %t2407 to i64
  %1924 = sext i32 %t2409 to i64
  %1925 = sext i32 %t2410 to i64
  %1926 = sext i32 %t2411 to i64
  %1927 = sext i32 %t2412 to i64
  %1928 = sext i32 %t2413 to i64
  %1929 = sext i32 %t2414 to i64
  %1930 = sext i32 %t2415 to i64
  %1931 = sext i32 %t2416 to i64
  %1932 = sext i32 %t2417 to i64
  %1933 = sext i32 %t2418 to i64
  %1934 = sext i32 %t2419 to i64
  %1935 = sext i32 %t2420 to i64
  %1936 = sext i32 %t2425 to i64
  %1937 = sext i32 %t2426 to i64
  %1938 = sext i32 %t2427 to i64
  %1939 = sext i32 %t2428 to i64
  br label %"for output.s0.x.xo73"

"end for output.s0.y.yo71":                       ; preds = %"end for output.s0.x.xo74", %"consume resampled_input69"
  %indvars.iv.next6356 = add nuw nsw i64 %indvars.iv6355, 1
  %1940 = icmp eq i64 %indvars.iv.next6356, %zext6358
  br i1 %1940, label %"end for output.s0.b.rebased48", label %"for output.s0.b.rebased47"

"for output.s0.x.xo73":                           ; preds = %"for output.s0.x.xo73.preheader", %"consume convolved86"
  %indvars.iv6351 = phi i64 [ 0, %"for output.s0.x.xo73.preheader" ], [ %indvars.iv.next6352, %"consume convolved86" ]
  br i1 %t2346, label %then_bb78, label %next_bb79

"end for output.s0.x.xo74":                       ; preds = %"consume convolved86", %"for output.s0.y.yo70"
  %indvars.iv.next6354 = add nuw nsw i64 %indvars.iv6353, 1
  %.not2900 = icmp eq i64 %indvars.iv.next6354, %1615
  br i1 %.not2900, label %"end for output.s0.y.yo71", label %"for output.s0.y.yo70"

then_bb78:                                        ; preds = %"for output.s0.x.xo73"
  %1941 = load <8 x i16>, <8 x i16>* %1714, align 16, !tbaa !493
  %1942 = load <8 x i16>, <8 x i16>* %1716, align 16, !tbaa !502
  %1943 = shufflevector <8 x i16> %1942, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %1944 = shl nuw nsw i64 %indvars.iv6351, 2
  %1945 = add nsw i64 %1944, %1587
  %1946 = mul nsw i64 %1945, %1588
  %1947 = add nsw i64 %1946, %1900
  %1948 = shl nsw i64 %1947, 4
  %1949 = getelementptr inbounds i8, i8* %resampled_input52, i64 %1948
  %1950 = bitcast i8* %1949 to <16 x i8>*
  %1951 = load <16 x i8>, <16 x i8>* %1950, align 16, !tbaa !438
  %1952 = zext <16 x i8> %1951 to <16 x i16>
  %1953 = shufflevector <8 x i16> %1941, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1954 = shufflevector <16 x i16> %1952, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1955 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %1954)
  %1956 = shufflevector <8 x i16> %1941, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1957 = shufflevector <16 x i16> %1952, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1958 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %1957)
  %1959 = shufflevector <8 x i16> %1942, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1960 = shufflevector <16 x i16> %1952, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1961 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %1960)
  %1962 = shufflevector <16 x i16> %1943, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1963 = shufflevector <16 x i16> %1952, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1964 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %1963)
  %1965 = add nsw <4 x i32> %1955, %1882
  %1966 = add nsw <4 x i32> %1958, %1883
  %1967 = add nsw <4 x i32> %1961, %1884
  %1968 = add nsw <4 x i32> %1964, %1885
  %1969 = add nsw i64 %1945, 1
  %1970 = mul nsw i64 %1969, %1588
  %1971 = add nsw i64 %1970, %1900
  %1972 = shl nsw i64 %1971, 4
  %1973 = getelementptr inbounds i8, i8* %resampled_input52, i64 %1972
  %1974 = bitcast i8* %1973 to <16 x i8>*
  %1975 = load <16 x i8>, <16 x i8>* %1974, align 16, !tbaa !438
  %1976 = zext <16 x i8> %1975 to <16 x i16>
  %1977 = shufflevector <16 x i16> %1976, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1978 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %1977)
  %1979 = shufflevector <16 x i16> %1976, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1980 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %1979)
  %1981 = shufflevector <16 x i16> %1976, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %1982 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %1981)
  %1983 = shufflevector <16 x i16> %1976, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %1984 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %1983)
  %1985 = add nsw <4 x i32> %1978, %1882
  %1986 = add nsw <4 x i32> %1980, %1883
  %1987 = add nsw <4 x i32> %1982, %1884
  %1988 = add nsw <4 x i32> %1984, %1885
  %1989 = add nsw i64 %1945, 2
  %1990 = mul nsw i64 %1989, %1588
  %1991 = add nsw i64 %1990, %1900
  %1992 = shl nsw i64 %1991, 4
  %1993 = getelementptr inbounds i8, i8* %resampled_input52, i64 %1992
  %1994 = bitcast i8* %1993 to <16 x i8>*
  %1995 = load <16 x i8>, <16 x i8>* %1994, align 16, !tbaa !438
  %1996 = zext <16 x i8> %1995 to <16 x i16>
  %1997 = shufflevector <16 x i16> %1996, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1998 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %1997)
  %1999 = shufflevector <16 x i16> %1996, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2000 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %1999)
  %2001 = shufflevector <16 x i16> %1996, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2002 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2001)
  %2003 = shufflevector <16 x i16> %1996, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2004 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2003)
  %2005 = add nsw <4 x i32> %1998, %1882
  %2006 = add nsw <4 x i32> %2000, %1883
  %2007 = add nsw <4 x i32> %2002, %1884
  %2008 = add nsw <4 x i32> %2004, %1885
  %2009 = add nsw i64 %1945, 3
  %2010 = mul nsw i64 %2009, %1588
  %2011 = add nsw i64 %2010, %1900
  %2012 = shl nsw i64 %2011, 4
  %2013 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2012
  %2014 = bitcast i8* %2013 to <16 x i8>*
  %2015 = load <16 x i8>, <16 x i8>* %2014, align 16, !tbaa !438
  %2016 = zext <16 x i8> %2015 to <16 x i16>
  %2017 = shufflevector <16 x i16> %2016, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2018 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2017)
  %2019 = shufflevector <16 x i16> %2016, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2020 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2019)
  %2021 = shufflevector <16 x i16> %2016, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2022 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2021)
  %2023 = shufflevector <16 x i16> %2016, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2024 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2023)
  %2025 = add nsw <4 x i32> %2018, %1882
  %2026 = add nsw <4 x i32> %2020, %1883
  %2027 = add nsw <4 x i32> %2022, %1884
  %2028 = add nsw <4 x i32> %2024, %1885
  %2029 = add nsw i64 %1946, %1901
  %2030 = shl nsw i64 %2029, 4
  %2031 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2030
  %2032 = bitcast i8* %2031 to <16 x i8>*
  %2033 = load <16 x i8>, <16 x i8>* %2032, align 16, !tbaa !438
  %2034 = zext <16 x i8> %2033 to <16 x i16>
  %2035 = shufflevector <16 x i16> %2034, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2036 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2035)
  %2037 = shufflevector <16 x i16> %2034, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2038 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2037)
  %2039 = shufflevector <16 x i16> %2034, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2040 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2039)
  %2041 = shufflevector <16 x i16> %2034, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2042 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2041)
  %2043 = add nsw <4 x i32> %2036, %1882
  %2044 = add nsw <4 x i32> %2038, %1883
  %2045 = add nsw <4 x i32> %2040, %1884
  %2046 = add nsw <4 x i32> %2042, %1885
  %2047 = add nsw i64 %1970, %1901
  %2048 = shl nsw i64 %2047, 4
  %2049 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2048
  %2050 = bitcast i8* %2049 to <16 x i8>*
  %2051 = load <16 x i8>, <16 x i8>* %2050, align 16, !tbaa !438
  %2052 = zext <16 x i8> %2051 to <16 x i16>
  %2053 = shufflevector <16 x i16> %2052, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2054 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2053)
  %2055 = shufflevector <16 x i16> %2052, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2056 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2055)
  %2057 = shufflevector <16 x i16> %2052, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2058 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2057)
  %2059 = shufflevector <16 x i16> %2052, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2060 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2059)
  %2061 = add nsw <4 x i32> %2054, %1882
  %2062 = add nsw <4 x i32> %2056, %1883
  %2063 = add nsw <4 x i32> %2058, %1884
  %2064 = add nsw <4 x i32> %2060, %1885
  %2065 = add nsw i64 %1990, %1901
  %2066 = shl nsw i64 %2065, 4
  %2067 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2066
  %2068 = bitcast i8* %2067 to <16 x i8>*
  %2069 = load <16 x i8>, <16 x i8>* %2068, align 16, !tbaa !438
  %2070 = zext <16 x i8> %2069 to <16 x i16>
  %2071 = shufflevector <16 x i16> %2070, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2072 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2071)
  %2073 = shufflevector <16 x i16> %2070, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2074 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2073)
  %2075 = shufflevector <16 x i16> %2070, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2076 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2075)
  %2077 = shufflevector <16 x i16> %2070, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2078 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2077)
  %2079 = add nsw <4 x i32> %2072, %1882
  %2080 = add nsw <4 x i32> %2074, %1883
  %2081 = add nsw <4 x i32> %2076, %1884
  %2082 = add nsw <4 x i32> %2078, %1885
  %2083 = add nsw i64 %2010, %1901
  %2084 = shl nsw i64 %2083, 4
  %2085 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2084
  %2086 = bitcast i8* %2085 to <16 x i8>*
  %2087 = load <16 x i8>, <16 x i8>* %2086, align 16, !tbaa !438
  %2088 = zext <16 x i8> %2087 to <16 x i16>
  %2089 = shufflevector <16 x i16> %2088, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2090 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2089)
  %2091 = shufflevector <16 x i16> %2088, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2092 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2091)
  %2093 = shufflevector <16 x i16> %2088, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2094 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2093)
  %2095 = shufflevector <16 x i16> %2088, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2096 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2095)
  %2097 = add nsw <4 x i32> %2090, %1882
  %2098 = add nsw <4 x i32> %2092, %1883
  %2099 = add nsw <4 x i32> %2094, %1884
  %2100 = add nsw <4 x i32> %2096, %1885
  %2101 = add nsw i64 %1946, %1902
  %2102 = shl nsw i64 %2101, 4
  %2103 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2102
  %2104 = bitcast i8* %2103 to <16 x i8>*
  %2105 = load <16 x i8>, <16 x i8>* %2104, align 16, !tbaa !438
  %2106 = zext <16 x i8> %2105 to <16 x i16>
  %2107 = shufflevector <16 x i16> %2106, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2108 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2107)
  %2109 = shufflevector <16 x i16> %2106, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2110 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2109)
  %2111 = shufflevector <16 x i16> %2106, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2112 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2111)
  %2113 = shufflevector <16 x i16> %2106, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2114 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2113)
  %2115 = add nsw <4 x i32> %2108, %1882
  %2116 = add nsw <4 x i32> %2110, %1883
  %2117 = add nsw <4 x i32> %2112, %1884
  %2118 = add nsw <4 x i32> %2114, %1885
  %2119 = add nsw i64 %1970, %1902
  %2120 = shl nsw i64 %2119, 4
  %2121 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2120
  %2122 = bitcast i8* %2121 to <16 x i8>*
  %2123 = load <16 x i8>, <16 x i8>* %2122, align 16, !tbaa !438
  %2124 = zext <16 x i8> %2123 to <16 x i16>
  %2125 = shufflevector <16 x i16> %2124, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2126 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2125)
  %2127 = shufflevector <16 x i16> %2124, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2128 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2127)
  %2129 = shufflevector <16 x i16> %2124, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2130 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2129)
  %2131 = shufflevector <16 x i16> %2124, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2132 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2131)
  %2133 = add nsw <4 x i32> %2126, %1882
  %2134 = add nsw <4 x i32> %2128, %1883
  %2135 = add nsw <4 x i32> %2130, %1884
  %2136 = add nsw <4 x i32> %2132, %1885
  %2137 = add nsw i64 %1990, %1902
  %2138 = shl nsw i64 %2137, 4
  %2139 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2138
  %2140 = bitcast i8* %2139 to <16 x i8>*
  %2141 = load <16 x i8>, <16 x i8>* %2140, align 16, !tbaa !438
  %2142 = zext <16 x i8> %2141 to <16 x i16>
  %2143 = shufflevector <16 x i16> %2142, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2144 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2143)
  %2145 = shufflevector <16 x i16> %2142, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2146 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2145)
  %2147 = shufflevector <16 x i16> %2142, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2148 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2147)
  %2149 = shufflevector <16 x i16> %2142, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2150 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2149)
  %2151 = add nsw <4 x i32> %2144, %1882
  %2152 = add nsw <4 x i32> %2146, %1883
  %2153 = add nsw <4 x i32> %2148, %1884
  %2154 = add nsw <4 x i32> %2150, %1885
  %2155 = add nsw i64 %2010, %1902
  %2156 = shl nsw i64 %2155, 4
  %2157 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2156
  %2158 = bitcast i8* %2157 to <16 x i8>*
  %2159 = load <16 x i8>, <16 x i8>* %2158, align 16, !tbaa !438
  %2160 = zext <16 x i8> %2159 to <16 x i16>
  %2161 = shufflevector <16 x i16> %2160, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2162 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2161)
  %2163 = shufflevector <16 x i16> %2160, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2164 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2163)
  %2165 = shufflevector <16 x i16> %2160, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2166 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2165)
  %2167 = shufflevector <16 x i16> %2160, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2168 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2167)
  %2169 = add nsw <4 x i32> %2162, %1882
  %2170 = add nsw <4 x i32> %2164, %1883
  %2171 = add nsw <4 x i32> %2166, %1884
  %2172 = add nsw <4 x i32> %2168, %1885
  %2173 = add nsw i64 %1946, %1903
  %2174 = shl nsw i64 %2173, 4
  %2175 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2174
  %2176 = bitcast i8* %2175 to <16 x i8>*
  %2177 = load <16 x i8>, <16 x i8>* %2176, align 16, !tbaa !438
  %2178 = zext <16 x i8> %2177 to <16 x i16>
  %2179 = shufflevector <16 x i16> %2178, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2180 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2179)
  %2181 = shufflevector <16 x i16> %2178, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2182 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2181)
  %2183 = shufflevector <16 x i16> %2178, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2184 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2183)
  %2185 = shufflevector <16 x i16> %2178, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2186 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2185)
  %2187 = add nsw <4 x i32> %2180, %1882
  %2188 = add nsw <4 x i32> %2182, %1883
  %2189 = add nsw <4 x i32> %2184, %1884
  %2190 = add nsw <4 x i32> %2186, %1885
  %2191 = add nsw i64 %1970, %1903
  %2192 = shl nsw i64 %2191, 4
  %2193 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2192
  %2194 = bitcast i8* %2193 to <16 x i8>*
  %2195 = load <16 x i8>, <16 x i8>* %2194, align 16, !tbaa !438
  %2196 = zext <16 x i8> %2195 to <16 x i16>
  %2197 = shufflevector <16 x i16> %2196, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2198 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2197)
  %2199 = shufflevector <16 x i16> %2196, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2200 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2199)
  %2201 = shufflevector <16 x i16> %2196, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2202 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2201)
  %2203 = shufflevector <16 x i16> %2196, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2204 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2203)
  %2205 = add nsw <4 x i32> %2198, %1882
  %2206 = add nsw <4 x i32> %2200, %1883
  %2207 = add nsw <4 x i32> %2202, %1884
  %2208 = add nsw <4 x i32> %2204, %1885
  %2209 = add nsw i64 %1990, %1903
  %2210 = shl nsw i64 %2209, 4
  %2211 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2210
  %2212 = bitcast i8* %2211 to <16 x i8>*
  %2213 = load <16 x i8>, <16 x i8>* %2212, align 16, !tbaa !438
  %2214 = zext <16 x i8> %2213 to <16 x i16>
  %2215 = shufflevector <16 x i16> %2214, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2216 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2215)
  %2217 = shufflevector <16 x i16> %2214, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2218 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2217)
  %2219 = shufflevector <16 x i16> %2214, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2220 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2219)
  %2221 = shufflevector <16 x i16> %2214, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2222 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2221)
  %2223 = add nsw <4 x i32> %2216, %1882
  %2224 = add nsw <4 x i32> %2218, %1883
  %2225 = add nsw <4 x i32> %2220, %1884
  %2226 = add nsw <4 x i32> %2222, %1885
  %2227 = add nsw i64 %2010, %1903
  %2228 = shl nsw i64 %2227, 4
  %2229 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2228
  %2230 = bitcast i8* %2229 to <16 x i8>*
  %2231 = load <16 x i8>, <16 x i8>* %2230, align 16, !tbaa !438
  %2232 = zext <16 x i8> %2231 to <16 x i16>
  %2233 = shufflevector <16 x i16> %2232, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2234 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1953, <4 x i16> %2233)
  %2235 = shufflevector <16 x i16> %2232, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2236 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1956, <4 x i16> %2235)
  %2237 = shufflevector <16 x i16> %2232, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2238 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1959, <4 x i16> %2237)
  %2239 = shufflevector <16 x i16> %2232, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2240 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %1962, <4 x i16> %2239)
  %2241 = add nsw <4 x i32> %2234, %1882
  %2242 = add nsw <4 x i32> %2236, %1883
  %2243 = add nsw <4 x i32> %2238, %1884
  %2244 = add nsw <4 x i32> %2240, %1885
  %2245 = load <8 x i16>, <8 x i16>* %1718, align 16, !tbaa !504
  %2246 = load <8 x i16>, <8 x i16>* %1720, align 16, !tbaa !507
  %2247 = shufflevector <8 x i16> %2246, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %2248 = add nsw i64 %1946, %1904
  %2249 = shl nsw i64 %2248, 4
  %2250 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2249
  %2251 = bitcast i8* %2250 to <16 x i8>*
  %2252 = load <16 x i8>, <16 x i8>* %2251, align 16, !tbaa !438
  %2253 = zext <16 x i8> %2252 to <16 x i16>
  %2254 = shufflevector <8 x i16> %2245, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2255 = shufflevector <16 x i16> %2253, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2256 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2255)
  %2257 = shufflevector <8 x i16> %2245, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2258 = shufflevector <16 x i16> %2253, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2259 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2258)
  %2260 = shufflevector <8 x i16> %2246, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2261 = shufflevector <16 x i16> %2253, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2262 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2261)
  %2263 = shufflevector <16 x i16> %2247, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2264 = shufflevector <16 x i16> %2253, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2265 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2264)
  %2266 = add nsw <4 x i32> %1965, %2256
  %2267 = add nsw <4 x i32> %1966, %2259
  %2268 = add nsw <4 x i32> %1967, %2262
  %2269 = add nsw <4 x i32> %1968, %2265
  %2270 = add nsw i64 %1970, %1904
  %2271 = shl nsw i64 %2270, 4
  %2272 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2271
  %2273 = bitcast i8* %2272 to <16 x i8>*
  %2274 = load <16 x i8>, <16 x i8>* %2273, align 16, !tbaa !438
  %2275 = zext <16 x i8> %2274 to <16 x i16>
  %2276 = shufflevector <16 x i16> %2275, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2277 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2276)
  %2278 = shufflevector <16 x i16> %2275, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2279 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2278)
  %2280 = shufflevector <16 x i16> %2275, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2281 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2280)
  %2282 = shufflevector <16 x i16> %2275, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2283 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2282)
  %2284 = add nsw <4 x i32> %1985, %2277
  %2285 = add nsw <4 x i32> %1986, %2279
  %2286 = add nsw <4 x i32> %1987, %2281
  %2287 = add nsw <4 x i32> %1988, %2283
  %2288 = add nsw i64 %1990, %1904
  %2289 = shl nsw i64 %2288, 4
  %2290 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2289
  %2291 = bitcast i8* %2290 to <16 x i8>*
  %2292 = load <16 x i8>, <16 x i8>* %2291, align 16, !tbaa !438
  %2293 = zext <16 x i8> %2292 to <16 x i16>
  %2294 = shufflevector <16 x i16> %2293, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2295 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2294)
  %2296 = shufflevector <16 x i16> %2293, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2297 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2296)
  %2298 = shufflevector <16 x i16> %2293, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2299 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2298)
  %2300 = shufflevector <16 x i16> %2293, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2301 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2300)
  %2302 = add nsw <4 x i32> %2005, %2295
  %2303 = add nsw <4 x i32> %2006, %2297
  %2304 = add nsw <4 x i32> %2007, %2299
  %2305 = add nsw <4 x i32> %2008, %2301
  %2306 = add nsw i64 %2010, %1904
  %2307 = shl nsw i64 %2306, 4
  %2308 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2307
  %2309 = bitcast i8* %2308 to <16 x i8>*
  %2310 = load <16 x i8>, <16 x i8>* %2309, align 16, !tbaa !438
  %2311 = zext <16 x i8> %2310 to <16 x i16>
  %2312 = shufflevector <16 x i16> %2311, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2313 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2312)
  %2314 = shufflevector <16 x i16> %2311, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2315 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2314)
  %2316 = shufflevector <16 x i16> %2311, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2317 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2316)
  %2318 = shufflevector <16 x i16> %2311, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2319 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2318)
  %2320 = add nsw <4 x i32> %2025, %2313
  %2321 = add nsw <4 x i32> %2026, %2315
  %2322 = add nsw <4 x i32> %2027, %2317
  %2323 = add nsw <4 x i32> %2028, %2319
  %2324 = add nsw i64 %1946, %1905
  %2325 = shl nsw i64 %2324, 4
  %2326 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2325
  %2327 = bitcast i8* %2326 to <16 x i8>*
  %2328 = load <16 x i8>, <16 x i8>* %2327, align 16, !tbaa !438
  %2329 = zext <16 x i8> %2328 to <16 x i16>
  %2330 = shufflevector <16 x i16> %2329, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2331 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2330)
  %2332 = shufflevector <16 x i16> %2329, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2333 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2332)
  %2334 = shufflevector <16 x i16> %2329, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2335 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2334)
  %2336 = shufflevector <16 x i16> %2329, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2337 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2336)
  %2338 = add nsw <4 x i32> %2043, %2331
  %2339 = add nsw <4 x i32> %2044, %2333
  %2340 = add nsw <4 x i32> %2045, %2335
  %2341 = add nsw <4 x i32> %2046, %2337
  %2342 = add nsw i64 %1970, %1905
  %2343 = shl nsw i64 %2342, 4
  %2344 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2343
  %2345 = bitcast i8* %2344 to <16 x i8>*
  %2346 = load <16 x i8>, <16 x i8>* %2345, align 16, !tbaa !438
  %2347 = zext <16 x i8> %2346 to <16 x i16>
  %2348 = shufflevector <16 x i16> %2347, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2349 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2348)
  %2350 = shufflevector <16 x i16> %2347, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2351 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2350)
  %2352 = shufflevector <16 x i16> %2347, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2353 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2352)
  %2354 = shufflevector <16 x i16> %2347, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2355 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2354)
  %2356 = add nsw <4 x i32> %2061, %2349
  %2357 = add nsw <4 x i32> %2062, %2351
  %2358 = add nsw <4 x i32> %2063, %2353
  %2359 = add nsw <4 x i32> %2064, %2355
  %2360 = add nsw i64 %1990, %1905
  %2361 = shl nsw i64 %2360, 4
  %2362 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2361
  %2363 = bitcast i8* %2362 to <16 x i8>*
  %2364 = load <16 x i8>, <16 x i8>* %2363, align 16, !tbaa !438
  %2365 = zext <16 x i8> %2364 to <16 x i16>
  %2366 = shufflevector <16 x i16> %2365, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2367 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2366)
  %2368 = shufflevector <16 x i16> %2365, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2369 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2368)
  %2370 = shufflevector <16 x i16> %2365, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2371 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2370)
  %2372 = shufflevector <16 x i16> %2365, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2373 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2372)
  %2374 = add nsw <4 x i32> %2079, %2367
  %2375 = add nsw <4 x i32> %2080, %2369
  %2376 = add nsw <4 x i32> %2081, %2371
  %2377 = add nsw <4 x i32> %2082, %2373
  %2378 = add nsw i64 %2010, %1905
  %2379 = shl nsw i64 %2378, 4
  %2380 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2379
  %2381 = bitcast i8* %2380 to <16 x i8>*
  %2382 = load <16 x i8>, <16 x i8>* %2381, align 16, !tbaa !438
  %2383 = zext <16 x i8> %2382 to <16 x i16>
  %2384 = shufflevector <16 x i16> %2383, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2385 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2384)
  %2386 = shufflevector <16 x i16> %2383, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2387 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2386)
  %2388 = shufflevector <16 x i16> %2383, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2389 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2388)
  %2390 = shufflevector <16 x i16> %2383, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2391 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2390)
  %2392 = add nsw <4 x i32> %2097, %2385
  %2393 = add nsw <4 x i32> %2098, %2387
  %2394 = add nsw <4 x i32> %2099, %2389
  %2395 = add nsw <4 x i32> %2100, %2391
  %2396 = add nsw i64 %1946, %1906
  %2397 = shl nsw i64 %2396, 4
  %2398 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2397
  %2399 = bitcast i8* %2398 to <16 x i8>*
  %2400 = load <16 x i8>, <16 x i8>* %2399, align 16, !tbaa !438
  %2401 = zext <16 x i8> %2400 to <16 x i16>
  %2402 = shufflevector <16 x i16> %2401, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2403 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2402)
  %2404 = shufflevector <16 x i16> %2401, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2405 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2404)
  %2406 = shufflevector <16 x i16> %2401, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2407 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2406)
  %2408 = shufflevector <16 x i16> %2401, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2409 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2408)
  %2410 = add nsw <4 x i32> %2115, %2403
  %2411 = add nsw <4 x i32> %2116, %2405
  %2412 = add nsw <4 x i32> %2117, %2407
  %2413 = add nsw <4 x i32> %2118, %2409
  %2414 = add nsw i64 %1970, %1906
  %2415 = shl nsw i64 %2414, 4
  %2416 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2415
  %2417 = bitcast i8* %2416 to <16 x i8>*
  %2418 = load <16 x i8>, <16 x i8>* %2417, align 16, !tbaa !438
  %2419 = zext <16 x i8> %2418 to <16 x i16>
  %2420 = shufflevector <16 x i16> %2419, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2421 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2420)
  %2422 = shufflevector <16 x i16> %2419, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2423 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2422)
  %2424 = shufflevector <16 x i16> %2419, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2425 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2424)
  %2426 = shufflevector <16 x i16> %2419, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2427 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2426)
  %2428 = add nsw <4 x i32> %2133, %2421
  %2429 = add nsw <4 x i32> %2134, %2423
  %2430 = add nsw <4 x i32> %2135, %2425
  %2431 = add nsw <4 x i32> %2136, %2427
  %2432 = add nsw i64 %1990, %1906
  %2433 = shl nsw i64 %2432, 4
  %2434 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2433
  %2435 = bitcast i8* %2434 to <16 x i8>*
  %2436 = load <16 x i8>, <16 x i8>* %2435, align 16, !tbaa !438
  %2437 = zext <16 x i8> %2436 to <16 x i16>
  %2438 = shufflevector <16 x i16> %2437, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2439 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2438)
  %2440 = shufflevector <16 x i16> %2437, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2441 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2440)
  %2442 = shufflevector <16 x i16> %2437, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2443 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2442)
  %2444 = shufflevector <16 x i16> %2437, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2445 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2444)
  %2446 = add nsw <4 x i32> %2151, %2439
  %2447 = add nsw <4 x i32> %2152, %2441
  %2448 = add nsw <4 x i32> %2153, %2443
  %2449 = add nsw <4 x i32> %2154, %2445
  %2450 = add nsw i64 %2010, %1906
  %2451 = shl nsw i64 %2450, 4
  %2452 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2451
  %2453 = bitcast i8* %2452 to <16 x i8>*
  %2454 = load <16 x i8>, <16 x i8>* %2453, align 16, !tbaa !438
  %2455 = zext <16 x i8> %2454 to <16 x i16>
  %2456 = shufflevector <16 x i16> %2455, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2457 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2456)
  %2458 = shufflevector <16 x i16> %2455, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2459 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2458)
  %2460 = shufflevector <16 x i16> %2455, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2461 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2460)
  %2462 = shufflevector <16 x i16> %2455, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2463 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2462)
  %2464 = add nsw <4 x i32> %2169, %2457
  %2465 = add nsw <4 x i32> %2170, %2459
  %2466 = add nsw <4 x i32> %2171, %2461
  %2467 = add nsw <4 x i32> %2172, %2463
  %2468 = add nsw i64 %1946, %1907
  %2469 = shl nsw i64 %2468, 4
  %2470 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2469
  %2471 = bitcast i8* %2470 to <16 x i8>*
  %2472 = load <16 x i8>, <16 x i8>* %2471, align 16, !tbaa !438
  %2473 = zext <16 x i8> %2472 to <16 x i16>
  %2474 = shufflevector <16 x i16> %2473, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2475 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2474)
  %2476 = shufflevector <16 x i16> %2473, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2477 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2476)
  %2478 = shufflevector <16 x i16> %2473, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2479 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2478)
  %2480 = shufflevector <16 x i16> %2473, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2481 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2480)
  %2482 = add nsw <4 x i32> %2187, %2475
  %2483 = add nsw <4 x i32> %2188, %2477
  %2484 = add nsw <4 x i32> %2189, %2479
  %2485 = add nsw <4 x i32> %2190, %2481
  %2486 = add nsw i64 %1970, %1907
  %2487 = shl nsw i64 %2486, 4
  %2488 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2487
  %2489 = bitcast i8* %2488 to <16 x i8>*
  %2490 = load <16 x i8>, <16 x i8>* %2489, align 16, !tbaa !438
  %2491 = zext <16 x i8> %2490 to <16 x i16>
  %2492 = shufflevector <16 x i16> %2491, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2493 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2492)
  %2494 = shufflevector <16 x i16> %2491, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2495 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2494)
  %2496 = shufflevector <16 x i16> %2491, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2497 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2496)
  %2498 = shufflevector <16 x i16> %2491, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2499 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2498)
  %2500 = add nsw <4 x i32> %2205, %2493
  %2501 = add nsw <4 x i32> %2206, %2495
  %2502 = add nsw <4 x i32> %2207, %2497
  %2503 = add nsw <4 x i32> %2208, %2499
  %2504 = add nsw i64 %1990, %1907
  %2505 = shl nsw i64 %2504, 4
  %2506 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2505
  %2507 = bitcast i8* %2506 to <16 x i8>*
  %2508 = load <16 x i8>, <16 x i8>* %2507, align 16, !tbaa !438
  %2509 = zext <16 x i8> %2508 to <16 x i16>
  %2510 = shufflevector <16 x i16> %2509, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2511 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2510)
  %2512 = shufflevector <16 x i16> %2509, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2513 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2512)
  %2514 = shufflevector <16 x i16> %2509, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2515 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2514)
  %2516 = shufflevector <16 x i16> %2509, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2517 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2516)
  %2518 = add nsw <4 x i32> %2223, %2511
  %2519 = add nsw <4 x i32> %2224, %2513
  %2520 = add nsw <4 x i32> %2225, %2515
  %2521 = add nsw <4 x i32> %2226, %2517
  %2522 = add nsw i64 %2010, %1907
  %2523 = shl nsw i64 %2522, 4
  %2524 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2523
  %2525 = bitcast i8* %2524 to <16 x i8>*
  %2526 = load <16 x i8>, <16 x i8>* %2525, align 16, !tbaa !438
  %2527 = zext <16 x i8> %2526 to <16 x i16>
  %2528 = shufflevector <16 x i16> %2527, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2529 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2254, <4 x i16> %2528)
  %2530 = shufflevector <16 x i16> %2527, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2531 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2257, <4 x i16> %2530)
  %2532 = shufflevector <16 x i16> %2527, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2533 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2260, <4 x i16> %2532)
  %2534 = shufflevector <16 x i16> %2527, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2535 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2263, <4 x i16> %2534)
  %2536 = add nsw <4 x i32> %2241, %2529
  %2537 = add nsw <4 x i32> %2242, %2531
  %2538 = add nsw <4 x i32> %2243, %2533
  %2539 = add nsw <4 x i32> %2244, %2535
  %2540 = load <8 x i16>, <8 x i16>* %1722, align 16, !tbaa !509
  %2541 = load <8 x i16>, <8 x i16>* %1724, align 16, !tbaa !513
  %2542 = shufflevector <8 x i16> %2541, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %2543 = add nsw i64 %1946, %1908
  %2544 = shl nsw i64 %2543, 4
  %2545 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2544
  %2546 = bitcast i8* %2545 to <16 x i8>*
  %2547 = load <16 x i8>, <16 x i8>* %2546, align 16, !tbaa !438
  %2548 = zext <16 x i8> %2547 to <16 x i16>
  %2549 = shufflevector <8 x i16> %2540, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2550 = shufflevector <16 x i16> %2548, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2551 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2550)
  %2552 = shufflevector <8 x i16> %2540, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2553 = shufflevector <16 x i16> %2548, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2554 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2553)
  %2555 = shufflevector <8 x i16> %2541, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2556 = shufflevector <16 x i16> %2548, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2557 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2556)
  %2558 = shufflevector <16 x i16> %2542, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2559 = shufflevector <16 x i16> %2548, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2560 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2559)
  %2561 = add nsw <4 x i32> %2266, %2551
  %2562 = add nsw <4 x i32> %2267, %2554
  %2563 = add nsw <4 x i32> %2268, %2557
  %2564 = add nsw <4 x i32> %2269, %2560
  %2565 = add nsw i64 %1970, %1908
  %2566 = shl nsw i64 %2565, 4
  %2567 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2566
  %2568 = bitcast i8* %2567 to <16 x i8>*
  %2569 = load <16 x i8>, <16 x i8>* %2568, align 16, !tbaa !438
  %2570 = zext <16 x i8> %2569 to <16 x i16>
  %2571 = shufflevector <16 x i16> %2570, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2572 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2571)
  %2573 = shufflevector <16 x i16> %2570, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2574 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2573)
  %2575 = shufflevector <16 x i16> %2570, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2576 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2575)
  %2577 = shufflevector <16 x i16> %2570, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2578 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2577)
  %2579 = add nsw <4 x i32> %2284, %2572
  %2580 = add nsw <4 x i32> %2285, %2574
  %2581 = add nsw <4 x i32> %2286, %2576
  %2582 = add nsw <4 x i32> %2287, %2578
  %2583 = add nsw i64 %1990, %1908
  %2584 = shl nsw i64 %2583, 4
  %2585 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2584
  %2586 = bitcast i8* %2585 to <16 x i8>*
  %2587 = load <16 x i8>, <16 x i8>* %2586, align 16, !tbaa !438
  %2588 = zext <16 x i8> %2587 to <16 x i16>
  %2589 = shufflevector <16 x i16> %2588, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2590 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2589)
  %2591 = shufflevector <16 x i16> %2588, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2592 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2591)
  %2593 = shufflevector <16 x i16> %2588, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2594 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2593)
  %2595 = shufflevector <16 x i16> %2588, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2596 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2595)
  %2597 = add nsw <4 x i32> %2302, %2590
  %2598 = add nsw <4 x i32> %2303, %2592
  %2599 = add nsw <4 x i32> %2304, %2594
  %2600 = add nsw <4 x i32> %2305, %2596
  %2601 = add nsw i64 %2010, %1908
  %2602 = shl nsw i64 %2601, 4
  %2603 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2602
  %2604 = bitcast i8* %2603 to <16 x i8>*
  %2605 = load <16 x i8>, <16 x i8>* %2604, align 16, !tbaa !438
  %2606 = zext <16 x i8> %2605 to <16 x i16>
  %2607 = shufflevector <16 x i16> %2606, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2608 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2607)
  %2609 = shufflevector <16 x i16> %2606, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2610 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2609)
  %2611 = shufflevector <16 x i16> %2606, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2612 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2611)
  %2613 = shufflevector <16 x i16> %2606, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2614 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2613)
  %2615 = add nsw <4 x i32> %2320, %2608
  %2616 = add nsw <4 x i32> %2321, %2610
  %2617 = add nsw <4 x i32> %2322, %2612
  %2618 = add nsw <4 x i32> %2323, %2614
  %2619 = add nsw i64 %1946, %1909
  %2620 = shl nsw i64 %2619, 4
  %2621 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2620
  %2622 = bitcast i8* %2621 to <16 x i8>*
  %2623 = load <16 x i8>, <16 x i8>* %2622, align 16, !tbaa !438
  %2624 = zext <16 x i8> %2623 to <16 x i16>
  %2625 = shufflevector <16 x i16> %2624, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2626 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2625)
  %2627 = shufflevector <16 x i16> %2624, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2628 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2627)
  %2629 = shufflevector <16 x i16> %2624, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2630 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2629)
  %2631 = shufflevector <16 x i16> %2624, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2632 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2631)
  %2633 = add nsw <4 x i32> %2338, %2626
  %2634 = add nsw <4 x i32> %2339, %2628
  %2635 = add nsw <4 x i32> %2340, %2630
  %2636 = add nsw <4 x i32> %2341, %2632
  %2637 = add nsw i64 %1970, %1909
  %2638 = shl nsw i64 %2637, 4
  %2639 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2638
  %2640 = bitcast i8* %2639 to <16 x i8>*
  %2641 = load <16 x i8>, <16 x i8>* %2640, align 16, !tbaa !438
  %2642 = zext <16 x i8> %2641 to <16 x i16>
  %2643 = shufflevector <16 x i16> %2642, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2644 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2643)
  %2645 = shufflevector <16 x i16> %2642, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2646 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2645)
  %2647 = shufflevector <16 x i16> %2642, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2648 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2647)
  %2649 = shufflevector <16 x i16> %2642, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2650 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2649)
  %2651 = add nsw <4 x i32> %2356, %2644
  %2652 = add nsw <4 x i32> %2357, %2646
  %2653 = add nsw <4 x i32> %2358, %2648
  %2654 = add nsw <4 x i32> %2359, %2650
  %2655 = add nsw i64 %1990, %1909
  %2656 = shl nsw i64 %2655, 4
  %2657 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2656
  %2658 = bitcast i8* %2657 to <16 x i8>*
  %2659 = load <16 x i8>, <16 x i8>* %2658, align 16, !tbaa !438
  %2660 = zext <16 x i8> %2659 to <16 x i16>
  %2661 = shufflevector <16 x i16> %2660, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2662 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2661)
  %2663 = shufflevector <16 x i16> %2660, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2664 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2663)
  %2665 = shufflevector <16 x i16> %2660, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2666 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2665)
  %2667 = shufflevector <16 x i16> %2660, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2668 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2667)
  %2669 = add nsw <4 x i32> %2374, %2662
  %2670 = add nsw <4 x i32> %2375, %2664
  %2671 = add nsw <4 x i32> %2376, %2666
  %2672 = add nsw <4 x i32> %2377, %2668
  %2673 = add nsw i64 %2010, %1909
  %2674 = shl nsw i64 %2673, 4
  %2675 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2674
  %2676 = bitcast i8* %2675 to <16 x i8>*
  %2677 = load <16 x i8>, <16 x i8>* %2676, align 16, !tbaa !438
  %2678 = zext <16 x i8> %2677 to <16 x i16>
  %2679 = shufflevector <16 x i16> %2678, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2680 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2679)
  %2681 = shufflevector <16 x i16> %2678, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2682 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2681)
  %2683 = shufflevector <16 x i16> %2678, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2684 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2683)
  %2685 = shufflevector <16 x i16> %2678, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2686 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2685)
  %2687 = add nsw <4 x i32> %2392, %2680
  %2688 = add nsw <4 x i32> %2393, %2682
  %2689 = add nsw <4 x i32> %2394, %2684
  %2690 = add nsw <4 x i32> %2395, %2686
  %2691 = add nsw i64 %1946, %1910
  %2692 = shl nsw i64 %2691, 4
  %2693 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2692
  %2694 = bitcast i8* %2693 to <16 x i8>*
  %2695 = load <16 x i8>, <16 x i8>* %2694, align 16, !tbaa !438
  %2696 = zext <16 x i8> %2695 to <16 x i16>
  %2697 = shufflevector <16 x i16> %2696, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2698 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2697)
  %2699 = shufflevector <16 x i16> %2696, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2700 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2699)
  %2701 = shufflevector <16 x i16> %2696, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2702 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2701)
  %2703 = shufflevector <16 x i16> %2696, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2704 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2703)
  %2705 = add nsw <4 x i32> %2410, %2698
  %2706 = add nsw <4 x i32> %2411, %2700
  %2707 = add nsw <4 x i32> %2412, %2702
  %2708 = add nsw <4 x i32> %2413, %2704
  %2709 = add nsw i64 %1970, %1910
  %2710 = shl nsw i64 %2709, 4
  %2711 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2710
  %2712 = bitcast i8* %2711 to <16 x i8>*
  %2713 = load <16 x i8>, <16 x i8>* %2712, align 16, !tbaa !438
  %2714 = zext <16 x i8> %2713 to <16 x i16>
  %2715 = shufflevector <16 x i16> %2714, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2716 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2715)
  %2717 = shufflevector <16 x i16> %2714, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2718 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2717)
  %2719 = shufflevector <16 x i16> %2714, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2720 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2719)
  %2721 = shufflevector <16 x i16> %2714, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2722 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2721)
  %2723 = add nsw <4 x i32> %2428, %2716
  %2724 = add nsw <4 x i32> %2429, %2718
  %2725 = add nsw <4 x i32> %2430, %2720
  %2726 = add nsw <4 x i32> %2431, %2722
  %2727 = add nsw i64 %1990, %1910
  %2728 = shl nsw i64 %2727, 4
  %2729 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2728
  %2730 = bitcast i8* %2729 to <16 x i8>*
  %2731 = load <16 x i8>, <16 x i8>* %2730, align 16, !tbaa !438
  %2732 = zext <16 x i8> %2731 to <16 x i16>
  %2733 = shufflevector <16 x i16> %2732, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2734 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2733)
  %2735 = shufflevector <16 x i16> %2732, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2736 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2735)
  %2737 = shufflevector <16 x i16> %2732, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2738 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2737)
  %2739 = shufflevector <16 x i16> %2732, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2740 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2739)
  %2741 = add nsw <4 x i32> %2446, %2734
  %2742 = add nsw <4 x i32> %2447, %2736
  %2743 = add nsw <4 x i32> %2448, %2738
  %2744 = add nsw <4 x i32> %2449, %2740
  %2745 = add nsw i64 %2010, %1910
  %2746 = shl nsw i64 %2745, 4
  %2747 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2746
  %2748 = bitcast i8* %2747 to <16 x i8>*
  %2749 = load <16 x i8>, <16 x i8>* %2748, align 16, !tbaa !438
  %2750 = zext <16 x i8> %2749 to <16 x i16>
  %2751 = shufflevector <16 x i16> %2750, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2752 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2751)
  %2753 = shufflevector <16 x i16> %2750, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2754 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2753)
  %2755 = shufflevector <16 x i16> %2750, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2756 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2755)
  %2757 = shufflevector <16 x i16> %2750, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2758 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2757)
  %2759 = add nsw <4 x i32> %2464, %2752
  %2760 = add nsw <4 x i32> %2465, %2754
  %2761 = add nsw <4 x i32> %2466, %2756
  %2762 = add nsw <4 x i32> %2467, %2758
  %2763 = add nsw i64 %1946, %1911
  %2764 = shl nsw i64 %2763, 4
  %2765 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2764
  %2766 = bitcast i8* %2765 to <16 x i8>*
  %2767 = load <16 x i8>, <16 x i8>* %2766, align 16, !tbaa !438
  %2768 = zext <16 x i8> %2767 to <16 x i16>
  %2769 = shufflevector <16 x i16> %2768, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2770 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2769)
  %2771 = shufflevector <16 x i16> %2768, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2772 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2771)
  %2773 = shufflevector <16 x i16> %2768, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2774 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2773)
  %2775 = shufflevector <16 x i16> %2768, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2776 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2775)
  %2777 = add nsw <4 x i32> %2482, %2770
  %2778 = add nsw <4 x i32> %2483, %2772
  %2779 = add nsw <4 x i32> %2484, %2774
  %2780 = add nsw <4 x i32> %2485, %2776
  %2781 = add nsw i64 %1970, %1911
  %2782 = shl nsw i64 %2781, 4
  %2783 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2782
  %2784 = bitcast i8* %2783 to <16 x i8>*
  %2785 = load <16 x i8>, <16 x i8>* %2784, align 16, !tbaa !438
  %2786 = zext <16 x i8> %2785 to <16 x i16>
  %2787 = shufflevector <16 x i16> %2786, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2788 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2787)
  %2789 = shufflevector <16 x i16> %2786, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2790 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2789)
  %2791 = shufflevector <16 x i16> %2786, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2792 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2791)
  %2793 = shufflevector <16 x i16> %2786, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2794 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2793)
  %2795 = add nsw <4 x i32> %2500, %2788
  %2796 = add nsw <4 x i32> %2501, %2790
  %2797 = add nsw <4 x i32> %2502, %2792
  %2798 = add nsw <4 x i32> %2503, %2794
  %2799 = add nsw i64 %1990, %1911
  %2800 = shl nsw i64 %2799, 4
  %2801 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2800
  %2802 = bitcast i8* %2801 to <16 x i8>*
  %2803 = load <16 x i8>, <16 x i8>* %2802, align 16, !tbaa !438
  %2804 = zext <16 x i8> %2803 to <16 x i16>
  %2805 = shufflevector <16 x i16> %2804, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2806 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2805)
  %2807 = shufflevector <16 x i16> %2804, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2808 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2807)
  %2809 = shufflevector <16 x i16> %2804, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2810 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2809)
  %2811 = shufflevector <16 x i16> %2804, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2812 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2811)
  %2813 = add nsw <4 x i32> %2518, %2806
  %2814 = add nsw <4 x i32> %2519, %2808
  %2815 = add nsw <4 x i32> %2520, %2810
  %2816 = add nsw <4 x i32> %2521, %2812
  %2817 = add nsw i64 %2010, %1911
  %2818 = shl nsw i64 %2817, 4
  %2819 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2818
  %2820 = bitcast i8* %2819 to <16 x i8>*
  %2821 = load <16 x i8>, <16 x i8>* %2820, align 16, !tbaa !438
  %2822 = zext <16 x i8> %2821 to <16 x i16>
  %2823 = shufflevector <16 x i16> %2822, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2824 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2549, <4 x i16> %2823)
  %2825 = shufflevector <16 x i16> %2822, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2826 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2552, <4 x i16> %2825)
  %2827 = shufflevector <16 x i16> %2822, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2828 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2555, <4 x i16> %2827)
  %2829 = shufflevector <16 x i16> %2822, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2830 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2558, <4 x i16> %2829)
  %2831 = add nsw <4 x i32> %2536, %2824
  %2832 = add nsw <4 x i32> %2537, %2826
  %2833 = add nsw <4 x i32> %2538, %2828
  %2834 = add nsw <4 x i32> %2539, %2830
  %2835 = load <8 x i16>, <8 x i16>* %1726, align 16, !tbaa !395
  %2836 = load <8 x i16>, <8 x i16>* %1728, align 16, !tbaa !395
  %2837 = shufflevector <8 x i16> %2836, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %2838 = add nsw i64 %1946, %1912
  %2839 = shl nsw i64 %2838, 4
  %2840 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2839
  %2841 = bitcast i8* %2840 to <16 x i8>*
  %2842 = load <16 x i8>, <16 x i8>* %2841, align 16, !tbaa !438
  %2843 = zext <16 x i8> %2842 to <16 x i16>
  %2844 = shufflevector <8 x i16> %2835, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2845 = shufflevector <16 x i16> %2843, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2846 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2845)
  %2847 = shufflevector <8 x i16> %2835, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2848 = shufflevector <16 x i16> %2843, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2849 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2848)
  %2850 = shufflevector <8 x i16> %2836, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2851 = shufflevector <16 x i16> %2843, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2852 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2851)
  %2853 = shufflevector <16 x i16> %2837, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2854 = shufflevector <16 x i16> %2843, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2855 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2854)
  %2856 = add nsw <4 x i32> %2561, %2846
  %2857 = add nsw <4 x i32> %2562, %2849
  %2858 = add nsw <4 x i32> %2563, %2852
  %2859 = add nsw <4 x i32> %2564, %2855
  %2860 = add nsw i64 %1970, %1912
  %2861 = shl nsw i64 %2860, 4
  %2862 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2861
  %2863 = bitcast i8* %2862 to <16 x i8>*
  %2864 = load <16 x i8>, <16 x i8>* %2863, align 16, !tbaa !438
  %2865 = zext <16 x i8> %2864 to <16 x i16>
  %2866 = shufflevector <16 x i16> %2865, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2867 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2866)
  %2868 = shufflevector <16 x i16> %2865, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2869 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2868)
  %2870 = shufflevector <16 x i16> %2865, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2871 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2870)
  %2872 = shufflevector <16 x i16> %2865, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2873 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2872)
  %2874 = add nsw <4 x i32> %2579, %2867
  %2875 = add nsw <4 x i32> %2580, %2869
  %2876 = add nsw <4 x i32> %2581, %2871
  %2877 = add nsw <4 x i32> %2582, %2873
  %2878 = add nsw i64 %1990, %1912
  %2879 = shl nsw i64 %2878, 4
  %2880 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2879
  %2881 = bitcast i8* %2880 to <16 x i8>*
  %2882 = load <16 x i8>, <16 x i8>* %2881, align 16, !tbaa !438
  %2883 = zext <16 x i8> %2882 to <16 x i16>
  %2884 = shufflevector <16 x i16> %2883, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2885 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2884)
  %2886 = shufflevector <16 x i16> %2883, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2887 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2886)
  %2888 = shufflevector <16 x i16> %2883, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2889 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2888)
  %2890 = shufflevector <16 x i16> %2883, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2891 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2890)
  %2892 = add nsw <4 x i32> %2597, %2885
  %2893 = add nsw <4 x i32> %2598, %2887
  %2894 = add nsw <4 x i32> %2599, %2889
  %2895 = add nsw <4 x i32> %2600, %2891
  %2896 = add nsw i64 %2010, %1912
  %2897 = shl nsw i64 %2896, 4
  %2898 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2897
  %2899 = bitcast i8* %2898 to <16 x i8>*
  %2900 = load <16 x i8>, <16 x i8>* %2899, align 16, !tbaa !438
  %2901 = zext <16 x i8> %2900 to <16 x i16>
  %2902 = shufflevector <16 x i16> %2901, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2903 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2902)
  %2904 = shufflevector <16 x i16> %2901, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2905 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2904)
  %2906 = shufflevector <16 x i16> %2901, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2907 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2906)
  %2908 = shufflevector <16 x i16> %2901, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2909 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2908)
  %2910 = add nsw <4 x i32> %2615, %2903
  %2911 = add nsw <4 x i32> %2616, %2905
  %2912 = add nsw <4 x i32> %2617, %2907
  %2913 = add nsw <4 x i32> %2618, %2909
  %2914 = add nsw i64 %1946, %1913
  %2915 = shl nsw i64 %2914, 4
  %2916 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2915
  %2917 = bitcast i8* %2916 to <16 x i8>*
  %2918 = load <16 x i8>, <16 x i8>* %2917, align 16, !tbaa !438
  %2919 = zext <16 x i8> %2918 to <16 x i16>
  %2920 = shufflevector <16 x i16> %2919, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2921 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2920)
  %2922 = shufflevector <16 x i16> %2919, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2923 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2922)
  %2924 = shufflevector <16 x i16> %2919, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2925 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2924)
  %2926 = shufflevector <16 x i16> %2919, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2927 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2926)
  %2928 = add nsw <4 x i32> %2633, %2921
  %2929 = add nsw <4 x i32> %2634, %2923
  %2930 = add nsw <4 x i32> %2635, %2925
  %2931 = add nsw <4 x i32> %2636, %2927
  %2932 = add nsw i64 %1970, %1913
  %2933 = shl nsw i64 %2932, 4
  %2934 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2933
  %2935 = bitcast i8* %2934 to <16 x i8>*
  %2936 = load <16 x i8>, <16 x i8>* %2935, align 16, !tbaa !438
  %2937 = zext <16 x i8> %2936 to <16 x i16>
  %2938 = shufflevector <16 x i16> %2937, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2939 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2938)
  %2940 = shufflevector <16 x i16> %2937, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2941 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2940)
  %2942 = shufflevector <16 x i16> %2937, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2943 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2942)
  %2944 = shufflevector <16 x i16> %2937, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2945 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2944)
  %2946 = add nsw <4 x i32> %2651, %2939
  %2947 = add nsw <4 x i32> %2652, %2941
  %2948 = add nsw <4 x i32> %2653, %2943
  %2949 = add nsw <4 x i32> %2654, %2945
  %2950 = add nsw i64 %1990, %1913
  %2951 = shl nsw i64 %2950, 4
  %2952 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2951
  %2953 = bitcast i8* %2952 to <16 x i8>*
  %2954 = load <16 x i8>, <16 x i8>* %2953, align 16, !tbaa !438
  %2955 = zext <16 x i8> %2954 to <16 x i16>
  %2956 = shufflevector <16 x i16> %2955, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2957 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2956)
  %2958 = shufflevector <16 x i16> %2955, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2959 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2958)
  %2960 = shufflevector <16 x i16> %2955, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2961 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2960)
  %2962 = shufflevector <16 x i16> %2955, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2963 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2962)
  %2964 = add nsw <4 x i32> %2669, %2957
  %2965 = add nsw <4 x i32> %2670, %2959
  %2966 = add nsw <4 x i32> %2671, %2961
  %2967 = add nsw <4 x i32> %2672, %2963
  %2968 = add nsw i64 %2010, %1913
  %2969 = shl nsw i64 %2968, 4
  %2970 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2969
  %2971 = bitcast i8* %2970 to <16 x i8>*
  %2972 = load <16 x i8>, <16 x i8>* %2971, align 16, !tbaa !438
  %2973 = zext <16 x i8> %2972 to <16 x i16>
  %2974 = shufflevector <16 x i16> %2973, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2975 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2974)
  %2976 = shufflevector <16 x i16> %2973, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2977 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2976)
  %2978 = shufflevector <16 x i16> %2973, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2979 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2978)
  %2980 = shufflevector <16 x i16> %2973, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2981 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2980)
  %2982 = add nsw <4 x i32> %2687, %2975
  %2983 = add nsw <4 x i32> %2688, %2977
  %2984 = add nsw <4 x i32> %2689, %2979
  %2985 = add nsw <4 x i32> %2690, %2981
  %2986 = add nsw i64 %1946, %1914
  %2987 = shl nsw i64 %2986, 4
  %2988 = getelementptr inbounds i8, i8* %resampled_input52, i64 %2987
  %2989 = bitcast i8* %2988 to <16 x i8>*
  %2990 = load <16 x i8>, <16 x i8>* %2989, align 16, !tbaa !438
  %2991 = zext <16 x i8> %2990 to <16 x i16>
  %2992 = shufflevector <16 x i16> %2991, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2993 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %2992)
  %2994 = shufflevector <16 x i16> %2991, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2995 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %2994)
  %2996 = shufflevector <16 x i16> %2991, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %2997 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %2996)
  %2998 = shufflevector <16 x i16> %2991, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %2999 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %2998)
  %3000 = add nsw <4 x i32> %2705, %2993
  %3001 = add nsw <4 x i32> %2706, %2995
  %3002 = add nsw <4 x i32> %2707, %2997
  %3003 = add nsw <4 x i32> %2708, %2999
  %3004 = add nsw i64 %1970, %1914
  %3005 = shl nsw i64 %3004, 4
  %3006 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3005
  %3007 = bitcast i8* %3006 to <16 x i8>*
  %3008 = load <16 x i8>, <16 x i8>* %3007, align 16, !tbaa !438
  %3009 = zext <16 x i8> %3008 to <16 x i16>
  %3010 = shufflevector <16 x i16> %3009, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3011 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %3010)
  %3012 = shufflevector <16 x i16> %3009, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3013 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %3012)
  %3014 = shufflevector <16 x i16> %3009, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3015 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %3014)
  %3016 = shufflevector <16 x i16> %3009, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3017 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %3016)
  %3018 = add nsw <4 x i32> %2723, %3011
  %3019 = add nsw <4 x i32> %2724, %3013
  %3020 = add nsw <4 x i32> %2725, %3015
  %3021 = add nsw <4 x i32> %2726, %3017
  %3022 = add nsw i64 %1990, %1914
  %3023 = shl nsw i64 %3022, 4
  %3024 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3023
  %3025 = bitcast i8* %3024 to <16 x i8>*
  %3026 = load <16 x i8>, <16 x i8>* %3025, align 16, !tbaa !438
  %3027 = zext <16 x i8> %3026 to <16 x i16>
  %3028 = shufflevector <16 x i16> %3027, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3029 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %3028)
  %3030 = shufflevector <16 x i16> %3027, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3031 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %3030)
  %3032 = shufflevector <16 x i16> %3027, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3033 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %3032)
  %3034 = shufflevector <16 x i16> %3027, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3035 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %3034)
  %3036 = add nsw <4 x i32> %2741, %3029
  %3037 = add nsw <4 x i32> %2742, %3031
  %3038 = add nsw <4 x i32> %2743, %3033
  %3039 = add nsw <4 x i32> %2744, %3035
  %3040 = add nsw i64 %2010, %1914
  %3041 = shl nsw i64 %3040, 4
  %3042 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3041
  %3043 = bitcast i8* %3042 to <16 x i8>*
  %3044 = load <16 x i8>, <16 x i8>* %3043, align 16, !tbaa !438
  %3045 = zext <16 x i8> %3044 to <16 x i16>
  %3046 = shufflevector <16 x i16> %3045, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3047 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %3046)
  %3048 = shufflevector <16 x i16> %3045, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3049 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %3048)
  %3050 = shufflevector <16 x i16> %3045, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3051 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %3050)
  %3052 = shufflevector <16 x i16> %3045, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3053 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %3052)
  %3054 = add nsw <4 x i32> %2759, %3047
  %3055 = add nsw <4 x i32> %2760, %3049
  %3056 = add nsw <4 x i32> %2761, %3051
  %3057 = add nsw <4 x i32> %2762, %3053
  %3058 = add nsw i64 %1946, %1915
  %3059 = shl nsw i64 %3058, 4
  %3060 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3059
  %3061 = bitcast i8* %3060 to <16 x i8>*
  %3062 = load <16 x i8>, <16 x i8>* %3061, align 16, !tbaa !438
  %3063 = zext <16 x i8> %3062 to <16 x i16>
  %3064 = shufflevector <16 x i16> %3063, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3065 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %3064)
  %3066 = shufflevector <16 x i16> %3063, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3067 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %3066)
  %3068 = shufflevector <16 x i16> %3063, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3069 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %3068)
  %3070 = shufflevector <16 x i16> %3063, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3071 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %3070)
  %3072 = add nsw <4 x i32> %2777, %3065
  %3073 = add nsw <4 x i32> %2778, %3067
  %3074 = add nsw <4 x i32> %2779, %3069
  %3075 = add nsw <4 x i32> %2780, %3071
  %3076 = add nsw i64 %1970, %1915
  %3077 = shl nsw i64 %3076, 4
  %3078 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3077
  %3079 = bitcast i8* %3078 to <16 x i8>*
  %3080 = load <16 x i8>, <16 x i8>* %3079, align 16, !tbaa !438
  %3081 = zext <16 x i8> %3080 to <16 x i16>
  %3082 = shufflevector <16 x i16> %3081, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3083 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %3082)
  %3084 = shufflevector <16 x i16> %3081, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3085 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %3084)
  %3086 = shufflevector <16 x i16> %3081, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3087 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %3086)
  %3088 = shufflevector <16 x i16> %3081, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3089 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %3088)
  %3090 = add nsw <4 x i32> %2795, %3083
  %3091 = add nsw <4 x i32> %2796, %3085
  %3092 = add nsw <4 x i32> %2797, %3087
  %3093 = add nsw <4 x i32> %2798, %3089
  %3094 = add nsw i64 %1990, %1915
  %3095 = shl nsw i64 %3094, 4
  %3096 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3095
  %3097 = bitcast i8* %3096 to <16 x i8>*
  %3098 = load <16 x i8>, <16 x i8>* %3097, align 16, !tbaa !438
  %3099 = zext <16 x i8> %3098 to <16 x i16>
  %3100 = shufflevector <16 x i16> %3099, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3101 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %3100)
  %3102 = shufflevector <16 x i16> %3099, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3103 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %3102)
  %3104 = shufflevector <16 x i16> %3099, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3105 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %3104)
  %3106 = shufflevector <16 x i16> %3099, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3107 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %3106)
  %3108 = add nsw <4 x i32> %2813, %3101
  %3109 = add nsw <4 x i32> %2814, %3103
  %3110 = add nsw <4 x i32> %2815, %3105
  %3111 = add nsw <4 x i32> %2816, %3107
  %3112 = add nsw i64 %2010, %1915
  %3113 = shl nsw i64 %3112, 4
  %3114 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3113
  %3115 = bitcast i8* %3114 to <16 x i8>*
  %3116 = load <16 x i8>, <16 x i8>* %3115, align 16, !tbaa !438
  %3117 = zext <16 x i8> %3116 to <16 x i16>
  %3118 = shufflevector <16 x i16> %3117, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3119 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2844, <4 x i16> %3118)
  %3120 = shufflevector <16 x i16> %3117, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3121 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2847, <4 x i16> %3120)
  %3122 = shufflevector <16 x i16> %3117, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3123 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2850, <4 x i16> %3122)
  %3124 = shufflevector <16 x i16> %3117, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3125 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %2853, <4 x i16> %3124)
  %3126 = add nsw <4 x i32> %2831, %3119
  %3127 = add nsw <4 x i32> %2832, %3121
  %3128 = add nsw <4 x i32> %2833, %3123
  %3129 = add nsw <4 x i32> %2834, %3125
  %3130 = load <8 x i16>, <8 x i16>* %1730, align 16, !tbaa !395
  %3131 = load <8 x i16>, <8 x i16>* %1732, align 16, !tbaa !395
  %3132 = shufflevector <8 x i16> %3131, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %3133 = add nsw i64 %1946, %1916
  %3134 = shl nsw i64 %3133, 4
  %3135 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3134
  %3136 = bitcast i8* %3135 to <16 x i8>*
  %3137 = load <16 x i8>, <16 x i8>* %3136, align 16, !tbaa !438
  %3138 = zext <16 x i8> %3137 to <16 x i16>
  %3139 = shufflevector <8 x i16> %3130, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3140 = shufflevector <16 x i16> %3138, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3141 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3140)
  %3142 = shufflevector <8 x i16> %3130, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3143 = shufflevector <16 x i16> %3138, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3144 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3143)
  %3145 = shufflevector <8 x i16> %3131, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3146 = shufflevector <16 x i16> %3138, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3147 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3146)
  %3148 = shufflevector <16 x i16> %3132, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3149 = shufflevector <16 x i16> %3138, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3150 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3149)
  %3151 = add nsw <4 x i32> %2856, %3141
  %3152 = add nsw <4 x i32> %2857, %3144
  %3153 = add nsw <4 x i32> %2858, %3147
  %3154 = add nsw <4 x i32> %2859, %3150
  %3155 = add nsw i64 %1970, %1916
  %3156 = shl nsw i64 %3155, 4
  %3157 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3156
  %3158 = bitcast i8* %3157 to <16 x i8>*
  %3159 = load <16 x i8>, <16 x i8>* %3158, align 16, !tbaa !438
  %3160 = zext <16 x i8> %3159 to <16 x i16>
  %3161 = shufflevector <16 x i16> %3160, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3162 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3161)
  %3163 = shufflevector <16 x i16> %3160, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3164 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3163)
  %3165 = shufflevector <16 x i16> %3160, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3166 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3165)
  %3167 = shufflevector <16 x i16> %3160, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3168 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3167)
  %3169 = add nsw <4 x i32> %2874, %3162
  %3170 = add nsw <4 x i32> %2875, %3164
  %3171 = add nsw <4 x i32> %2876, %3166
  %3172 = add nsw <4 x i32> %2877, %3168
  %3173 = add nsw i64 %1990, %1916
  %3174 = shl nsw i64 %3173, 4
  %3175 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3174
  %3176 = bitcast i8* %3175 to <16 x i8>*
  %3177 = load <16 x i8>, <16 x i8>* %3176, align 16, !tbaa !438
  %3178 = zext <16 x i8> %3177 to <16 x i16>
  %3179 = shufflevector <16 x i16> %3178, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3180 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3179)
  %3181 = shufflevector <16 x i16> %3178, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3182 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3181)
  %3183 = shufflevector <16 x i16> %3178, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3184 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3183)
  %3185 = shufflevector <16 x i16> %3178, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3186 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3185)
  %3187 = add nsw <4 x i32> %2892, %3180
  %3188 = add nsw <4 x i32> %2893, %3182
  %3189 = add nsw <4 x i32> %2894, %3184
  %3190 = add nsw <4 x i32> %2895, %3186
  %3191 = add nsw i64 %2010, %1916
  %3192 = shl nsw i64 %3191, 4
  %3193 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3192
  %3194 = bitcast i8* %3193 to <16 x i8>*
  %3195 = load <16 x i8>, <16 x i8>* %3194, align 16, !tbaa !438
  %3196 = zext <16 x i8> %3195 to <16 x i16>
  %3197 = shufflevector <16 x i16> %3196, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3198 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3197)
  %3199 = shufflevector <16 x i16> %3196, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3200 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3199)
  %3201 = shufflevector <16 x i16> %3196, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3202 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3201)
  %3203 = shufflevector <16 x i16> %3196, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3204 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3203)
  %3205 = add nsw <4 x i32> %2910, %3198
  %3206 = add nsw <4 x i32> %2911, %3200
  %3207 = add nsw <4 x i32> %2912, %3202
  %3208 = add nsw <4 x i32> %2913, %3204
  %3209 = add nsw i64 %1946, %1917
  %3210 = shl nsw i64 %3209, 4
  %3211 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3210
  %3212 = bitcast i8* %3211 to <16 x i8>*
  %3213 = load <16 x i8>, <16 x i8>* %3212, align 16, !tbaa !438
  %3214 = zext <16 x i8> %3213 to <16 x i16>
  %3215 = shufflevector <16 x i16> %3214, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3216 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3215)
  %3217 = shufflevector <16 x i16> %3214, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3218 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3217)
  %3219 = shufflevector <16 x i16> %3214, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3220 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3219)
  %3221 = shufflevector <16 x i16> %3214, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3222 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3221)
  %3223 = add nsw <4 x i32> %2928, %3216
  %3224 = add nsw <4 x i32> %2929, %3218
  %3225 = add nsw <4 x i32> %2930, %3220
  %3226 = add nsw <4 x i32> %2931, %3222
  %3227 = add nsw i64 %1970, %1917
  %3228 = shl nsw i64 %3227, 4
  %3229 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3228
  %3230 = bitcast i8* %3229 to <16 x i8>*
  %3231 = load <16 x i8>, <16 x i8>* %3230, align 16, !tbaa !438
  %3232 = zext <16 x i8> %3231 to <16 x i16>
  %3233 = shufflevector <16 x i16> %3232, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3234 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3233)
  %3235 = shufflevector <16 x i16> %3232, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3236 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3235)
  %3237 = shufflevector <16 x i16> %3232, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3238 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3237)
  %3239 = shufflevector <16 x i16> %3232, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3240 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3239)
  %3241 = add nsw <4 x i32> %2946, %3234
  %3242 = add nsw <4 x i32> %2947, %3236
  %3243 = add nsw <4 x i32> %2948, %3238
  %3244 = add nsw <4 x i32> %2949, %3240
  %3245 = add nsw i64 %1990, %1917
  %3246 = shl nsw i64 %3245, 4
  %3247 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3246
  %3248 = bitcast i8* %3247 to <16 x i8>*
  %3249 = load <16 x i8>, <16 x i8>* %3248, align 16, !tbaa !438
  %3250 = zext <16 x i8> %3249 to <16 x i16>
  %3251 = shufflevector <16 x i16> %3250, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3252 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3251)
  %3253 = shufflevector <16 x i16> %3250, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3254 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3253)
  %3255 = shufflevector <16 x i16> %3250, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3256 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3255)
  %3257 = shufflevector <16 x i16> %3250, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3258 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3257)
  %3259 = add nsw <4 x i32> %2964, %3252
  %3260 = add nsw <4 x i32> %2965, %3254
  %3261 = add nsw <4 x i32> %2966, %3256
  %3262 = add nsw <4 x i32> %2967, %3258
  %3263 = add nsw i64 %2010, %1917
  %3264 = shl nsw i64 %3263, 4
  %3265 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3264
  %3266 = bitcast i8* %3265 to <16 x i8>*
  %3267 = load <16 x i8>, <16 x i8>* %3266, align 16, !tbaa !438
  %3268 = zext <16 x i8> %3267 to <16 x i16>
  %3269 = shufflevector <16 x i16> %3268, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3270 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3269)
  %3271 = shufflevector <16 x i16> %3268, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3272 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3271)
  %3273 = shufflevector <16 x i16> %3268, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3274 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3273)
  %3275 = shufflevector <16 x i16> %3268, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3276 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3275)
  %3277 = add nsw <4 x i32> %2982, %3270
  %3278 = add nsw <4 x i32> %2983, %3272
  %3279 = add nsw <4 x i32> %2984, %3274
  %3280 = add nsw <4 x i32> %2985, %3276
  %3281 = add nsw i64 %1946, %1918
  %3282 = shl nsw i64 %3281, 4
  %3283 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3282
  %3284 = bitcast i8* %3283 to <16 x i8>*
  %3285 = load <16 x i8>, <16 x i8>* %3284, align 16, !tbaa !438
  %3286 = zext <16 x i8> %3285 to <16 x i16>
  %3287 = shufflevector <16 x i16> %3286, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3288 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3287)
  %3289 = shufflevector <16 x i16> %3286, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3290 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3289)
  %3291 = shufflevector <16 x i16> %3286, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3292 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3291)
  %3293 = shufflevector <16 x i16> %3286, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3294 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3293)
  %3295 = add nsw <4 x i32> %3000, %3288
  %3296 = add nsw <4 x i32> %3001, %3290
  %3297 = add nsw <4 x i32> %3002, %3292
  %3298 = add nsw <4 x i32> %3003, %3294
  %3299 = add nsw i64 %1970, %1918
  %3300 = shl nsw i64 %3299, 4
  %3301 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3300
  %3302 = bitcast i8* %3301 to <16 x i8>*
  %3303 = load <16 x i8>, <16 x i8>* %3302, align 16, !tbaa !438
  %3304 = zext <16 x i8> %3303 to <16 x i16>
  %3305 = shufflevector <16 x i16> %3304, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3306 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3305)
  %3307 = shufflevector <16 x i16> %3304, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3308 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3307)
  %3309 = shufflevector <16 x i16> %3304, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3310 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3309)
  %3311 = shufflevector <16 x i16> %3304, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3312 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3311)
  %3313 = add nsw <4 x i32> %3018, %3306
  %3314 = add nsw <4 x i32> %3019, %3308
  %3315 = add nsw <4 x i32> %3020, %3310
  %3316 = add nsw <4 x i32> %3021, %3312
  %3317 = add nsw i64 %1990, %1918
  %3318 = shl nsw i64 %3317, 4
  %3319 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3318
  %3320 = bitcast i8* %3319 to <16 x i8>*
  %3321 = load <16 x i8>, <16 x i8>* %3320, align 16, !tbaa !438
  %3322 = zext <16 x i8> %3321 to <16 x i16>
  %3323 = shufflevector <16 x i16> %3322, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3324 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3323)
  %3325 = shufflevector <16 x i16> %3322, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3326 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3325)
  %3327 = shufflevector <16 x i16> %3322, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3328 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3327)
  %3329 = shufflevector <16 x i16> %3322, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3330 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3329)
  %3331 = add nsw <4 x i32> %3036, %3324
  %3332 = add nsw <4 x i32> %3037, %3326
  %3333 = add nsw <4 x i32> %3038, %3328
  %3334 = add nsw <4 x i32> %3039, %3330
  %3335 = add nsw i64 %2010, %1918
  %3336 = shl nsw i64 %3335, 4
  %3337 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3336
  %3338 = bitcast i8* %3337 to <16 x i8>*
  %3339 = load <16 x i8>, <16 x i8>* %3338, align 16, !tbaa !438
  %3340 = zext <16 x i8> %3339 to <16 x i16>
  %3341 = shufflevector <16 x i16> %3340, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3342 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3341)
  %3343 = shufflevector <16 x i16> %3340, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3344 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3343)
  %3345 = shufflevector <16 x i16> %3340, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3346 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3345)
  %3347 = shufflevector <16 x i16> %3340, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3348 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3347)
  %3349 = add nsw <4 x i32> %3054, %3342
  %3350 = add nsw <4 x i32> %3055, %3344
  %3351 = add nsw <4 x i32> %3056, %3346
  %3352 = add nsw <4 x i32> %3057, %3348
  %3353 = add nsw i64 %1946, %1919
  %3354 = shl nsw i64 %3353, 4
  %3355 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3354
  %3356 = bitcast i8* %3355 to <16 x i8>*
  %3357 = load <16 x i8>, <16 x i8>* %3356, align 16, !tbaa !438
  %3358 = zext <16 x i8> %3357 to <16 x i16>
  %3359 = shufflevector <16 x i16> %3358, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3360 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3359)
  %3361 = shufflevector <16 x i16> %3358, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3362 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3361)
  %3363 = shufflevector <16 x i16> %3358, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3364 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3363)
  %3365 = shufflevector <16 x i16> %3358, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3366 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3365)
  %3367 = add nsw <4 x i32> %3072, %3360
  %3368 = add nsw <4 x i32> %3073, %3362
  %3369 = add nsw <4 x i32> %3074, %3364
  %3370 = add nsw <4 x i32> %3075, %3366
  %3371 = add nsw i64 %1970, %1919
  %3372 = shl nsw i64 %3371, 4
  %3373 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3372
  %3374 = bitcast i8* %3373 to <16 x i8>*
  %3375 = load <16 x i8>, <16 x i8>* %3374, align 16, !tbaa !438
  %3376 = zext <16 x i8> %3375 to <16 x i16>
  %3377 = shufflevector <16 x i16> %3376, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3378 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3377)
  %3379 = shufflevector <16 x i16> %3376, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3380 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3379)
  %3381 = shufflevector <16 x i16> %3376, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3382 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3381)
  %3383 = shufflevector <16 x i16> %3376, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3384 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3383)
  %3385 = add nsw <4 x i32> %3090, %3378
  %3386 = add nsw <4 x i32> %3091, %3380
  %3387 = add nsw <4 x i32> %3092, %3382
  %3388 = add nsw <4 x i32> %3093, %3384
  %3389 = add nsw i64 %1990, %1919
  %3390 = shl nsw i64 %3389, 4
  %3391 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3390
  %3392 = bitcast i8* %3391 to <16 x i8>*
  %3393 = load <16 x i8>, <16 x i8>* %3392, align 16, !tbaa !438
  %3394 = zext <16 x i8> %3393 to <16 x i16>
  %3395 = shufflevector <16 x i16> %3394, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3396 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3395)
  %3397 = shufflevector <16 x i16> %3394, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3398 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3397)
  %3399 = shufflevector <16 x i16> %3394, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3400 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3399)
  %3401 = shufflevector <16 x i16> %3394, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3402 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3401)
  %3403 = add nsw <4 x i32> %3108, %3396
  %3404 = add nsw <4 x i32> %3109, %3398
  %3405 = add nsw <4 x i32> %3110, %3400
  %3406 = add nsw <4 x i32> %3111, %3402
  %3407 = add nsw i64 %2010, %1919
  %3408 = shl nsw i64 %3407, 4
  %3409 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3408
  %3410 = bitcast i8* %3409 to <16 x i8>*
  %3411 = load <16 x i8>, <16 x i8>* %3410, align 16, !tbaa !438
  %3412 = zext <16 x i8> %3411 to <16 x i16>
  %3413 = shufflevector <16 x i16> %3412, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3414 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3139, <4 x i16> %3413)
  %3415 = shufflevector <16 x i16> %3412, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3416 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3142, <4 x i16> %3415)
  %3417 = shufflevector <16 x i16> %3412, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3418 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3145, <4 x i16> %3417)
  %3419 = shufflevector <16 x i16> %3412, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3420 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3148, <4 x i16> %3419)
  %3421 = add nsw <4 x i32> %3126, %3414
  %3422 = add nsw <4 x i32> %3127, %3416
  %3423 = add nsw <4 x i32> %3128, %3418
  %3424 = add nsw <4 x i32> %3129, %3420
  %3425 = load <8 x i16>, <8 x i16>* %1734, align 16, !tbaa !395
  %3426 = load <8 x i16>, <8 x i16>* %1736, align 16, !tbaa !395
  %3427 = shufflevector <8 x i16> %3426, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %3428 = add nsw i64 %1946, %1920
  %3429 = shl nsw i64 %3428, 4
  %3430 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3429
  %3431 = bitcast i8* %3430 to <16 x i8>*
  %3432 = load <16 x i8>, <16 x i8>* %3431, align 16, !tbaa !438
  %3433 = zext <16 x i8> %3432 to <16 x i16>
  %3434 = shufflevector <8 x i16> %3425, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3435 = shufflevector <16 x i16> %3433, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3436 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3435)
  %3437 = shufflevector <8 x i16> %3425, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3438 = shufflevector <16 x i16> %3433, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3439 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3438)
  %3440 = shufflevector <8 x i16> %3426, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3441 = shufflevector <16 x i16> %3433, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3442 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3441)
  %3443 = shufflevector <16 x i16> %3427, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3444 = shufflevector <16 x i16> %3433, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3445 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3444)
  %3446 = add nsw <4 x i32> %3151, %3436
  %3447 = add nsw <4 x i32> %3152, %3439
  %3448 = add nsw <4 x i32> %3153, %3442
  %3449 = add nsw <4 x i32> %3154, %3445
  %3450 = add nsw i64 %1970, %1920
  %3451 = shl nsw i64 %3450, 4
  %3452 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3451
  %3453 = bitcast i8* %3452 to <16 x i8>*
  %3454 = load <16 x i8>, <16 x i8>* %3453, align 16, !tbaa !438
  %3455 = zext <16 x i8> %3454 to <16 x i16>
  %3456 = shufflevector <16 x i16> %3455, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3457 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3456)
  %3458 = shufflevector <16 x i16> %3455, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3459 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3458)
  %3460 = shufflevector <16 x i16> %3455, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3461 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3460)
  %3462 = shufflevector <16 x i16> %3455, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3463 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3462)
  %3464 = add nsw <4 x i32> %3169, %3457
  %3465 = add nsw <4 x i32> %3170, %3459
  %3466 = add nsw <4 x i32> %3171, %3461
  %3467 = add nsw <4 x i32> %3172, %3463
  %3468 = add nsw i64 %1990, %1920
  %3469 = shl nsw i64 %3468, 4
  %3470 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3469
  %3471 = bitcast i8* %3470 to <16 x i8>*
  %3472 = load <16 x i8>, <16 x i8>* %3471, align 16, !tbaa !438
  %3473 = zext <16 x i8> %3472 to <16 x i16>
  %3474 = shufflevector <16 x i16> %3473, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3475 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3474)
  %3476 = shufflevector <16 x i16> %3473, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3477 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3476)
  %3478 = shufflevector <16 x i16> %3473, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3479 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3478)
  %3480 = shufflevector <16 x i16> %3473, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3481 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3480)
  %3482 = add nsw <4 x i32> %3187, %3475
  %3483 = add nsw <4 x i32> %3188, %3477
  %3484 = add nsw <4 x i32> %3189, %3479
  %3485 = add nsw <4 x i32> %3190, %3481
  %3486 = add nsw i64 %2010, %1920
  %3487 = shl nsw i64 %3486, 4
  %3488 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3487
  %3489 = bitcast i8* %3488 to <16 x i8>*
  %3490 = load <16 x i8>, <16 x i8>* %3489, align 16, !tbaa !438
  %3491 = zext <16 x i8> %3490 to <16 x i16>
  %3492 = shufflevector <16 x i16> %3491, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3493 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3492)
  %3494 = shufflevector <16 x i16> %3491, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3495 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3494)
  %3496 = shufflevector <16 x i16> %3491, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3497 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3496)
  %3498 = shufflevector <16 x i16> %3491, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3499 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3498)
  %3500 = add nsw <4 x i32> %3205, %3493
  %3501 = add nsw <4 x i32> %3206, %3495
  %3502 = add nsw <4 x i32> %3207, %3497
  %3503 = add nsw <4 x i32> %3208, %3499
  %3504 = add nsw i64 %1946, %1921
  %3505 = shl nsw i64 %3504, 4
  %3506 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3505
  %3507 = bitcast i8* %3506 to <16 x i8>*
  %3508 = load <16 x i8>, <16 x i8>* %3507, align 16, !tbaa !438
  %3509 = zext <16 x i8> %3508 to <16 x i16>
  %3510 = shufflevector <16 x i16> %3509, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3511 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3510)
  %3512 = shufflevector <16 x i16> %3509, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3513 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3512)
  %3514 = shufflevector <16 x i16> %3509, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3515 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3514)
  %3516 = shufflevector <16 x i16> %3509, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3517 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3516)
  %3518 = add nsw <4 x i32> %3223, %3511
  %3519 = add nsw <4 x i32> %3224, %3513
  %3520 = add nsw <4 x i32> %3225, %3515
  %3521 = add nsw <4 x i32> %3226, %3517
  %3522 = add nsw i64 %1970, %1921
  %3523 = shl nsw i64 %3522, 4
  %3524 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3523
  %3525 = bitcast i8* %3524 to <16 x i8>*
  %3526 = load <16 x i8>, <16 x i8>* %3525, align 16, !tbaa !438
  %3527 = zext <16 x i8> %3526 to <16 x i16>
  %3528 = shufflevector <16 x i16> %3527, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3529 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3528)
  %3530 = shufflevector <16 x i16> %3527, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3531 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3530)
  %3532 = shufflevector <16 x i16> %3527, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3533 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3532)
  %3534 = shufflevector <16 x i16> %3527, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3535 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3534)
  %3536 = add nsw <4 x i32> %3241, %3529
  %3537 = add nsw <4 x i32> %3242, %3531
  %3538 = add nsw <4 x i32> %3243, %3533
  %3539 = add nsw <4 x i32> %3244, %3535
  %3540 = add nsw i64 %1990, %1921
  %3541 = shl nsw i64 %3540, 4
  %3542 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3541
  %3543 = bitcast i8* %3542 to <16 x i8>*
  %3544 = load <16 x i8>, <16 x i8>* %3543, align 16, !tbaa !438
  %3545 = zext <16 x i8> %3544 to <16 x i16>
  %3546 = shufflevector <16 x i16> %3545, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3547 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3546)
  %3548 = shufflevector <16 x i16> %3545, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3549 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3548)
  %3550 = shufflevector <16 x i16> %3545, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3551 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3550)
  %3552 = shufflevector <16 x i16> %3545, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3553 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3552)
  %3554 = add nsw <4 x i32> %3259, %3547
  %3555 = add nsw <4 x i32> %3260, %3549
  %3556 = add nsw <4 x i32> %3261, %3551
  %3557 = add nsw <4 x i32> %3262, %3553
  %3558 = add nsw i64 %2010, %1921
  %3559 = shl nsw i64 %3558, 4
  %3560 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3559
  %3561 = bitcast i8* %3560 to <16 x i8>*
  %3562 = load <16 x i8>, <16 x i8>* %3561, align 16, !tbaa !438
  %3563 = zext <16 x i8> %3562 to <16 x i16>
  %3564 = shufflevector <16 x i16> %3563, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3565 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3564)
  %3566 = shufflevector <16 x i16> %3563, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3567 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3566)
  %3568 = shufflevector <16 x i16> %3563, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3569 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3568)
  %3570 = shufflevector <16 x i16> %3563, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3571 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3570)
  %3572 = add nsw <4 x i32> %3277, %3565
  %3573 = add nsw <4 x i32> %3278, %3567
  %3574 = add nsw <4 x i32> %3279, %3569
  %3575 = add nsw <4 x i32> %3280, %3571
  %3576 = add nsw i64 %1946, %1922
  %3577 = shl nsw i64 %3576, 4
  %3578 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3577
  %3579 = bitcast i8* %3578 to <16 x i8>*
  %3580 = load <16 x i8>, <16 x i8>* %3579, align 16, !tbaa !438
  %3581 = zext <16 x i8> %3580 to <16 x i16>
  %3582 = shufflevector <16 x i16> %3581, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3583 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3582)
  %3584 = shufflevector <16 x i16> %3581, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3585 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3584)
  %3586 = shufflevector <16 x i16> %3581, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3587 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3586)
  %3588 = shufflevector <16 x i16> %3581, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3589 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3588)
  %3590 = add nsw <4 x i32> %3295, %3583
  %3591 = add nsw <4 x i32> %3296, %3585
  %3592 = add nsw <4 x i32> %3297, %3587
  %3593 = add nsw <4 x i32> %3298, %3589
  %3594 = add nsw i64 %1970, %1922
  %3595 = shl nsw i64 %3594, 4
  %3596 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3595
  %3597 = bitcast i8* %3596 to <16 x i8>*
  %3598 = load <16 x i8>, <16 x i8>* %3597, align 16, !tbaa !438
  %3599 = zext <16 x i8> %3598 to <16 x i16>
  %3600 = shufflevector <16 x i16> %3599, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3601 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3600)
  %3602 = shufflevector <16 x i16> %3599, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3603 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3602)
  %3604 = shufflevector <16 x i16> %3599, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3605 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3604)
  %3606 = shufflevector <16 x i16> %3599, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3607 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3606)
  %3608 = add nsw <4 x i32> %3313, %3601
  %3609 = add nsw <4 x i32> %3314, %3603
  %3610 = add nsw <4 x i32> %3315, %3605
  %3611 = add nsw <4 x i32> %3316, %3607
  %3612 = add nsw i64 %1990, %1922
  %3613 = shl nsw i64 %3612, 4
  %3614 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3613
  %3615 = bitcast i8* %3614 to <16 x i8>*
  %3616 = load <16 x i8>, <16 x i8>* %3615, align 16, !tbaa !438
  %3617 = zext <16 x i8> %3616 to <16 x i16>
  %3618 = shufflevector <16 x i16> %3617, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3619 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3618)
  %3620 = shufflevector <16 x i16> %3617, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3621 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3620)
  %3622 = shufflevector <16 x i16> %3617, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3623 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3622)
  %3624 = shufflevector <16 x i16> %3617, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3625 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3624)
  %3626 = add nsw <4 x i32> %3331, %3619
  %3627 = add nsw <4 x i32> %3332, %3621
  %3628 = add nsw <4 x i32> %3333, %3623
  %3629 = add nsw <4 x i32> %3334, %3625
  %3630 = add nsw i64 %2010, %1922
  %3631 = shl nsw i64 %3630, 4
  %3632 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3631
  %3633 = bitcast i8* %3632 to <16 x i8>*
  %3634 = load <16 x i8>, <16 x i8>* %3633, align 16, !tbaa !438
  %3635 = zext <16 x i8> %3634 to <16 x i16>
  %3636 = shufflevector <16 x i16> %3635, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3637 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3636)
  %3638 = shufflevector <16 x i16> %3635, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3639 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3638)
  %3640 = shufflevector <16 x i16> %3635, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3641 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3640)
  %3642 = shufflevector <16 x i16> %3635, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3643 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3642)
  %3644 = add nsw <4 x i32> %3349, %3637
  %3645 = add nsw <4 x i32> %3350, %3639
  %3646 = add nsw <4 x i32> %3351, %3641
  %3647 = add nsw <4 x i32> %3352, %3643
  %3648 = add nsw i64 %1946, %1923
  %3649 = shl nsw i64 %3648, 4
  %3650 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3649
  %3651 = bitcast i8* %3650 to <16 x i8>*
  %3652 = load <16 x i8>, <16 x i8>* %3651, align 16, !tbaa !438
  %3653 = zext <16 x i8> %3652 to <16 x i16>
  %3654 = shufflevector <16 x i16> %3653, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3655 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3654)
  %3656 = shufflevector <16 x i16> %3653, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3657 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3656)
  %3658 = shufflevector <16 x i16> %3653, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3659 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3658)
  %3660 = shufflevector <16 x i16> %3653, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3661 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3660)
  %3662 = add nsw <4 x i32> %3367, %3655
  %3663 = add nsw <4 x i32> %3368, %3657
  %3664 = add nsw <4 x i32> %3369, %3659
  %3665 = add nsw <4 x i32> %3370, %3661
  %3666 = add nsw i64 %1970, %1923
  %3667 = shl nsw i64 %3666, 4
  %3668 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3667
  %3669 = bitcast i8* %3668 to <16 x i8>*
  %3670 = load <16 x i8>, <16 x i8>* %3669, align 16, !tbaa !438
  %3671 = zext <16 x i8> %3670 to <16 x i16>
  %3672 = shufflevector <16 x i16> %3671, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3673 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3672)
  %3674 = shufflevector <16 x i16> %3671, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3675 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3674)
  %3676 = shufflevector <16 x i16> %3671, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3677 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3676)
  %3678 = shufflevector <16 x i16> %3671, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3679 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3678)
  %3680 = add nsw <4 x i32> %3385, %3673
  %3681 = add nsw <4 x i32> %3386, %3675
  %3682 = add nsw <4 x i32> %3387, %3677
  %3683 = add nsw <4 x i32> %3388, %3679
  %3684 = add nsw i64 %1990, %1923
  %3685 = shl nsw i64 %3684, 4
  %3686 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3685
  %3687 = bitcast i8* %3686 to <16 x i8>*
  %3688 = load <16 x i8>, <16 x i8>* %3687, align 16, !tbaa !438
  %3689 = zext <16 x i8> %3688 to <16 x i16>
  %3690 = shufflevector <16 x i16> %3689, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3691 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3690)
  %3692 = shufflevector <16 x i16> %3689, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3693 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3692)
  %3694 = shufflevector <16 x i16> %3689, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3695 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3694)
  %3696 = shufflevector <16 x i16> %3689, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3697 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3696)
  %3698 = add nsw <4 x i32> %3403, %3691
  %3699 = add nsw <4 x i32> %3404, %3693
  %3700 = add nsw <4 x i32> %3405, %3695
  %3701 = add nsw <4 x i32> %3406, %3697
  %3702 = add nsw i64 %2010, %1923
  %3703 = shl nsw i64 %3702, 4
  %3704 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3703
  %3705 = bitcast i8* %3704 to <16 x i8>*
  %3706 = load <16 x i8>, <16 x i8>* %3705, align 16, !tbaa !438
  %3707 = zext <16 x i8> %3706 to <16 x i16>
  %3708 = shufflevector <16 x i16> %3707, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3709 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3434, <4 x i16> %3708)
  %3710 = shufflevector <16 x i16> %3707, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3711 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3437, <4 x i16> %3710)
  %3712 = shufflevector <16 x i16> %3707, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3713 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3440, <4 x i16> %3712)
  %3714 = shufflevector <16 x i16> %3707, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3715 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3443, <4 x i16> %3714)
  %3716 = add nsw <4 x i32> %3421, %3709
  %3717 = add nsw <4 x i32> %3422, %3711
  %3718 = add nsw <4 x i32> %3423, %3713
  %3719 = add nsw <4 x i32> %3424, %3715
  %3720 = load <8 x i16>, <8 x i16>* %1738, align 16, !tbaa !395
  %3721 = load <8 x i16>, <8 x i16>* %1740, align 16, !tbaa !395
  %3722 = shufflevector <8 x i16> %3721, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %3723 = add nsw i64 %1946, %1924
  %3724 = shl nsw i64 %3723, 4
  %3725 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3724
  %3726 = bitcast i8* %3725 to <16 x i8>*
  %3727 = load <16 x i8>, <16 x i8>* %3726, align 16, !tbaa !438
  %3728 = zext <16 x i8> %3727 to <16 x i16>
  %3729 = shufflevector <8 x i16> %3720, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3730 = shufflevector <16 x i16> %3728, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3731 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3730)
  %3732 = shufflevector <8 x i16> %3720, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3733 = shufflevector <16 x i16> %3728, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3734 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3733)
  %3735 = shufflevector <8 x i16> %3721, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3736 = shufflevector <16 x i16> %3728, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3737 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3736)
  %3738 = shufflevector <16 x i16> %3722, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3739 = shufflevector <16 x i16> %3728, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3740 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3739)
  %3741 = add nsw <4 x i32> %3446, %3731
  %3742 = add nsw <4 x i32> %3447, %3734
  %3743 = add nsw <4 x i32> %3448, %3737
  %3744 = add nsw <4 x i32> %3449, %3740
  %3745 = add nsw i64 %1970, %1924
  %3746 = shl nsw i64 %3745, 4
  %3747 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3746
  %3748 = bitcast i8* %3747 to <16 x i8>*
  %3749 = load <16 x i8>, <16 x i8>* %3748, align 16, !tbaa !438
  %3750 = zext <16 x i8> %3749 to <16 x i16>
  %3751 = shufflevector <16 x i16> %3750, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3752 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3751)
  %3753 = shufflevector <16 x i16> %3750, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3754 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3753)
  %3755 = shufflevector <16 x i16> %3750, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3756 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3755)
  %3757 = shufflevector <16 x i16> %3750, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3758 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3757)
  %3759 = add nsw <4 x i32> %3464, %3752
  %3760 = add nsw <4 x i32> %3465, %3754
  %3761 = add nsw <4 x i32> %3466, %3756
  %3762 = add nsw <4 x i32> %3467, %3758
  %3763 = add nsw i64 %1990, %1924
  %3764 = shl nsw i64 %3763, 4
  %3765 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3764
  %3766 = bitcast i8* %3765 to <16 x i8>*
  %3767 = load <16 x i8>, <16 x i8>* %3766, align 16, !tbaa !438
  %3768 = zext <16 x i8> %3767 to <16 x i16>
  %3769 = shufflevector <16 x i16> %3768, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3770 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3769)
  %3771 = shufflevector <16 x i16> %3768, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3772 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3771)
  %3773 = shufflevector <16 x i16> %3768, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3774 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3773)
  %3775 = shufflevector <16 x i16> %3768, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3776 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3775)
  %3777 = add nsw <4 x i32> %3482, %3770
  %3778 = add nsw <4 x i32> %3483, %3772
  %3779 = add nsw <4 x i32> %3484, %3774
  %3780 = add nsw <4 x i32> %3485, %3776
  %3781 = add nsw i64 %2010, %1924
  %3782 = shl nsw i64 %3781, 4
  %3783 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3782
  %3784 = bitcast i8* %3783 to <16 x i8>*
  %3785 = load <16 x i8>, <16 x i8>* %3784, align 16, !tbaa !438
  %3786 = zext <16 x i8> %3785 to <16 x i16>
  %3787 = shufflevector <16 x i16> %3786, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3788 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3787)
  %3789 = shufflevector <16 x i16> %3786, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3790 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3789)
  %3791 = shufflevector <16 x i16> %3786, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3792 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3791)
  %3793 = shufflevector <16 x i16> %3786, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3794 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3793)
  %3795 = add nsw <4 x i32> %3500, %3788
  %3796 = add nsw <4 x i32> %3501, %3790
  %3797 = add nsw <4 x i32> %3502, %3792
  %3798 = add nsw <4 x i32> %3503, %3794
  %3799 = add nsw i64 %1946, %1925
  %3800 = shl nsw i64 %3799, 4
  %3801 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3800
  %3802 = bitcast i8* %3801 to <16 x i8>*
  %3803 = load <16 x i8>, <16 x i8>* %3802, align 16, !tbaa !438
  %3804 = zext <16 x i8> %3803 to <16 x i16>
  %3805 = shufflevector <16 x i16> %3804, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3806 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3805)
  %3807 = shufflevector <16 x i16> %3804, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3808 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3807)
  %3809 = shufflevector <16 x i16> %3804, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3810 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3809)
  %3811 = shufflevector <16 x i16> %3804, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3812 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3811)
  %3813 = add nsw <4 x i32> %3518, %3806
  %3814 = add nsw <4 x i32> %3519, %3808
  %3815 = add nsw <4 x i32> %3520, %3810
  %3816 = add nsw <4 x i32> %3521, %3812
  %3817 = add nsw i64 %1970, %1925
  %3818 = shl nsw i64 %3817, 4
  %3819 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3818
  %3820 = bitcast i8* %3819 to <16 x i8>*
  %3821 = load <16 x i8>, <16 x i8>* %3820, align 16, !tbaa !438
  %3822 = zext <16 x i8> %3821 to <16 x i16>
  %3823 = shufflevector <16 x i16> %3822, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3824 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3823)
  %3825 = shufflevector <16 x i16> %3822, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3826 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3825)
  %3827 = shufflevector <16 x i16> %3822, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3828 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3827)
  %3829 = shufflevector <16 x i16> %3822, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3830 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3829)
  %3831 = add nsw <4 x i32> %3536, %3824
  %3832 = add nsw <4 x i32> %3537, %3826
  %3833 = add nsw <4 x i32> %3538, %3828
  %3834 = add nsw <4 x i32> %3539, %3830
  %3835 = add nsw i64 %1990, %1925
  %3836 = shl nsw i64 %3835, 4
  %3837 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3836
  %3838 = bitcast i8* %3837 to <16 x i8>*
  %3839 = load <16 x i8>, <16 x i8>* %3838, align 16, !tbaa !438
  %3840 = zext <16 x i8> %3839 to <16 x i16>
  %3841 = shufflevector <16 x i16> %3840, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3842 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3841)
  %3843 = shufflevector <16 x i16> %3840, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3844 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3843)
  %3845 = shufflevector <16 x i16> %3840, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3846 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3845)
  %3847 = shufflevector <16 x i16> %3840, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3848 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3847)
  %3849 = add nsw <4 x i32> %3554, %3842
  %3850 = add nsw <4 x i32> %3555, %3844
  %3851 = add nsw <4 x i32> %3556, %3846
  %3852 = add nsw <4 x i32> %3557, %3848
  %3853 = add nsw i64 %2010, %1925
  %3854 = shl nsw i64 %3853, 4
  %3855 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3854
  %3856 = bitcast i8* %3855 to <16 x i8>*
  %3857 = load <16 x i8>, <16 x i8>* %3856, align 16, !tbaa !438
  %3858 = zext <16 x i8> %3857 to <16 x i16>
  %3859 = shufflevector <16 x i16> %3858, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3860 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3859)
  %3861 = shufflevector <16 x i16> %3858, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3862 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3861)
  %3863 = shufflevector <16 x i16> %3858, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3864 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3863)
  %3865 = shufflevector <16 x i16> %3858, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3866 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3865)
  %3867 = add nsw <4 x i32> %3572, %3860
  %3868 = add nsw <4 x i32> %3573, %3862
  %3869 = add nsw <4 x i32> %3574, %3864
  %3870 = add nsw <4 x i32> %3575, %3866
  %3871 = add nsw i64 %1946, %1926
  %3872 = shl nsw i64 %3871, 4
  %3873 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3872
  %3874 = bitcast i8* %3873 to <16 x i8>*
  %3875 = load <16 x i8>, <16 x i8>* %3874, align 16, !tbaa !438
  %3876 = zext <16 x i8> %3875 to <16 x i16>
  %3877 = shufflevector <16 x i16> %3876, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3878 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3877)
  %3879 = shufflevector <16 x i16> %3876, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3880 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3879)
  %3881 = shufflevector <16 x i16> %3876, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3882 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3881)
  %3883 = shufflevector <16 x i16> %3876, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3884 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3883)
  %3885 = add nsw <4 x i32> %3590, %3878
  %3886 = add nsw <4 x i32> %3591, %3880
  %3887 = add nsw <4 x i32> %3592, %3882
  %3888 = add nsw <4 x i32> %3593, %3884
  %3889 = add nsw i64 %1970, %1926
  %3890 = shl nsw i64 %3889, 4
  %3891 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3890
  %3892 = bitcast i8* %3891 to <16 x i8>*
  %3893 = load <16 x i8>, <16 x i8>* %3892, align 16, !tbaa !438
  %3894 = zext <16 x i8> %3893 to <16 x i16>
  %3895 = shufflevector <16 x i16> %3894, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3896 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3895)
  %3897 = shufflevector <16 x i16> %3894, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3898 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3897)
  %3899 = shufflevector <16 x i16> %3894, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3900 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3899)
  %3901 = shufflevector <16 x i16> %3894, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3902 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3901)
  %3903 = add nsw <4 x i32> %3608, %3896
  %3904 = add nsw <4 x i32> %3609, %3898
  %3905 = add nsw <4 x i32> %3610, %3900
  %3906 = add nsw <4 x i32> %3611, %3902
  %3907 = add nsw i64 %1990, %1926
  %3908 = shl nsw i64 %3907, 4
  %3909 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3908
  %3910 = bitcast i8* %3909 to <16 x i8>*
  %3911 = load <16 x i8>, <16 x i8>* %3910, align 16, !tbaa !438
  %3912 = zext <16 x i8> %3911 to <16 x i16>
  %3913 = shufflevector <16 x i16> %3912, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3914 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3913)
  %3915 = shufflevector <16 x i16> %3912, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3916 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3915)
  %3917 = shufflevector <16 x i16> %3912, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3918 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3917)
  %3919 = shufflevector <16 x i16> %3912, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3920 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3919)
  %3921 = add nsw <4 x i32> %3626, %3914
  %3922 = add nsw <4 x i32> %3627, %3916
  %3923 = add nsw <4 x i32> %3628, %3918
  %3924 = add nsw <4 x i32> %3629, %3920
  %3925 = add nsw i64 %2010, %1926
  %3926 = shl nsw i64 %3925, 4
  %3927 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3926
  %3928 = bitcast i8* %3927 to <16 x i8>*
  %3929 = load <16 x i8>, <16 x i8>* %3928, align 16, !tbaa !438
  %3930 = zext <16 x i8> %3929 to <16 x i16>
  %3931 = shufflevector <16 x i16> %3930, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3932 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3931)
  %3933 = shufflevector <16 x i16> %3930, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3934 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3933)
  %3935 = shufflevector <16 x i16> %3930, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3936 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3935)
  %3937 = shufflevector <16 x i16> %3930, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3938 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3937)
  %3939 = add nsw <4 x i32> %3644, %3932
  %3940 = add nsw <4 x i32> %3645, %3934
  %3941 = add nsw <4 x i32> %3646, %3936
  %3942 = add nsw <4 x i32> %3647, %3938
  %3943 = add nsw i64 %1946, %1927
  %3944 = shl nsw i64 %3943, 4
  %3945 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3944
  %3946 = bitcast i8* %3945 to <16 x i8>*
  %3947 = load <16 x i8>, <16 x i8>* %3946, align 16, !tbaa !438
  %3948 = zext <16 x i8> %3947 to <16 x i16>
  %3949 = shufflevector <16 x i16> %3948, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3950 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3949)
  %3951 = shufflevector <16 x i16> %3948, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3952 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3951)
  %3953 = shufflevector <16 x i16> %3948, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3954 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3953)
  %3955 = shufflevector <16 x i16> %3948, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3956 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3955)
  %3957 = add nsw <4 x i32> %3662, %3950
  %3958 = add nsw <4 x i32> %3663, %3952
  %3959 = add nsw <4 x i32> %3664, %3954
  %3960 = add nsw <4 x i32> %3665, %3956
  %3961 = add nsw i64 %1970, %1927
  %3962 = shl nsw i64 %3961, 4
  %3963 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3962
  %3964 = bitcast i8* %3963 to <16 x i8>*
  %3965 = load <16 x i8>, <16 x i8>* %3964, align 16, !tbaa !438
  %3966 = zext <16 x i8> %3965 to <16 x i16>
  %3967 = shufflevector <16 x i16> %3966, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3968 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3967)
  %3969 = shufflevector <16 x i16> %3966, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3970 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3969)
  %3971 = shufflevector <16 x i16> %3966, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3972 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3971)
  %3973 = shufflevector <16 x i16> %3966, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3974 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3973)
  %3975 = add nsw <4 x i32> %3680, %3968
  %3976 = add nsw <4 x i32> %3681, %3970
  %3977 = add nsw <4 x i32> %3682, %3972
  %3978 = add nsw <4 x i32> %3683, %3974
  %3979 = add nsw i64 %1990, %1927
  %3980 = shl nsw i64 %3979, 4
  %3981 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3980
  %3982 = bitcast i8* %3981 to <16 x i8>*
  %3983 = load <16 x i8>, <16 x i8>* %3982, align 16, !tbaa !438
  %3984 = zext <16 x i8> %3983 to <16 x i16>
  %3985 = shufflevector <16 x i16> %3984, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3986 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %3985)
  %3987 = shufflevector <16 x i16> %3984, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %3988 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %3987)
  %3989 = shufflevector <16 x i16> %3984, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %3990 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %3989)
  %3991 = shufflevector <16 x i16> %3984, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %3992 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %3991)
  %3993 = add nsw <4 x i32> %3698, %3986
  %3994 = add nsw <4 x i32> %3699, %3988
  %3995 = add nsw <4 x i32> %3700, %3990
  %3996 = add nsw <4 x i32> %3701, %3992
  %3997 = add nsw i64 %2010, %1927
  %3998 = shl nsw i64 %3997, 4
  %3999 = getelementptr inbounds i8, i8* %resampled_input52, i64 %3998
  %4000 = bitcast i8* %3999 to <16 x i8>*
  %4001 = load <16 x i8>, <16 x i8>* %4000, align 16, !tbaa !438
  %4002 = zext <16 x i8> %4001 to <16 x i16>
  %4003 = shufflevector <16 x i16> %4002, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4004 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3729, <4 x i16> %4003)
  %4005 = shufflevector <16 x i16> %4002, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4006 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3732, <4 x i16> %4005)
  %4007 = shufflevector <16 x i16> %4002, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4008 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3735, <4 x i16> %4007)
  %4009 = shufflevector <16 x i16> %4002, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4010 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %3738, <4 x i16> %4009)
  %4011 = add nsw <4 x i32> %3716, %4004
  %4012 = add nsw <4 x i32> %3717, %4006
  %4013 = add nsw <4 x i32> %3718, %4008
  %4014 = add nsw <4 x i32> %3719, %4010
  %4015 = load <8 x i16>, <8 x i16>* %1742, align 16, !tbaa !395
  %4016 = load <8 x i16>, <8 x i16>* %1744, align 16, !tbaa !395
  %4017 = shufflevector <8 x i16> %4016, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %4018 = add nsw i64 %1946, %1928
  %4019 = shl nsw i64 %4018, 4
  %4020 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4019
  %4021 = bitcast i8* %4020 to <16 x i8>*
  %4022 = load <16 x i8>, <16 x i8>* %4021, align 16, !tbaa !438
  %4023 = zext <16 x i8> %4022 to <16 x i16>
  %4024 = shufflevector <8 x i16> %4015, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4025 = shufflevector <16 x i16> %4023, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4026 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4025)
  %4027 = shufflevector <8 x i16> %4015, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4028 = shufflevector <16 x i16> %4023, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4029 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4028)
  %4030 = shufflevector <8 x i16> %4016, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4031 = shufflevector <16 x i16> %4023, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4032 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4031)
  %4033 = shufflevector <16 x i16> %4017, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4034 = shufflevector <16 x i16> %4023, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4035 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4034)
  %4036 = add nsw <4 x i32> %3741, %4026
  %4037 = add nsw <4 x i32> %3742, %4029
  %4038 = add nsw <4 x i32> %3743, %4032
  %4039 = add nsw <4 x i32> %3744, %4035
  %4040 = add nsw i64 %1970, %1928
  %4041 = shl nsw i64 %4040, 4
  %4042 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4041
  %4043 = bitcast i8* %4042 to <16 x i8>*
  %4044 = load <16 x i8>, <16 x i8>* %4043, align 16, !tbaa !438
  %4045 = zext <16 x i8> %4044 to <16 x i16>
  %4046 = shufflevector <16 x i16> %4045, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4047 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4046)
  %4048 = shufflevector <16 x i16> %4045, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4049 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4048)
  %4050 = shufflevector <16 x i16> %4045, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4051 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4050)
  %4052 = shufflevector <16 x i16> %4045, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4053 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4052)
  %4054 = add nsw <4 x i32> %3759, %4047
  %4055 = add nsw <4 x i32> %3760, %4049
  %4056 = add nsw <4 x i32> %3761, %4051
  %4057 = add nsw <4 x i32> %3762, %4053
  %4058 = add nsw i64 %1990, %1928
  %4059 = shl nsw i64 %4058, 4
  %4060 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4059
  %4061 = bitcast i8* %4060 to <16 x i8>*
  %4062 = load <16 x i8>, <16 x i8>* %4061, align 16, !tbaa !438
  %4063 = zext <16 x i8> %4062 to <16 x i16>
  %4064 = shufflevector <16 x i16> %4063, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4065 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4064)
  %4066 = shufflevector <16 x i16> %4063, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4067 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4066)
  %4068 = shufflevector <16 x i16> %4063, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4069 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4068)
  %4070 = shufflevector <16 x i16> %4063, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4071 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4070)
  %4072 = add nsw <4 x i32> %3777, %4065
  %4073 = add nsw <4 x i32> %3778, %4067
  %4074 = add nsw <4 x i32> %3779, %4069
  %4075 = add nsw <4 x i32> %3780, %4071
  %4076 = add nsw i64 %2010, %1928
  %4077 = shl nsw i64 %4076, 4
  %4078 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4077
  %4079 = bitcast i8* %4078 to <16 x i8>*
  %4080 = load <16 x i8>, <16 x i8>* %4079, align 16, !tbaa !438
  %4081 = zext <16 x i8> %4080 to <16 x i16>
  %4082 = shufflevector <16 x i16> %4081, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4083 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4082)
  %4084 = shufflevector <16 x i16> %4081, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4085 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4084)
  %4086 = shufflevector <16 x i16> %4081, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4087 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4086)
  %4088 = shufflevector <16 x i16> %4081, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4089 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4088)
  %4090 = add nsw <4 x i32> %3795, %4083
  %4091 = add nsw <4 x i32> %3796, %4085
  %4092 = add nsw <4 x i32> %3797, %4087
  %4093 = add nsw <4 x i32> %3798, %4089
  %4094 = add nsw i64 %1946, %1929
  %4095 = shl nsw i64 %4094, 4
  %4096 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4095
  %4097 = bitcast i8* %4096 to <16 x i8>*
  %4098 = load <16 x i8>, <16 x i8>* %4097, align 16, !tbaa !438
  %4099 = zext <16 x i8> %4098 to <16 x i16>
  %4100 = shufflevector <16 x i16> %4099, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4101 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4100)
  %4102 = shufflevector <16 x i16> %4099, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4103 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4102)
  %4104 = shufflevector <16 x i16> %4099, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4105 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4104)
  %4106 = shufflevector <16 x i16> %4099, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4107 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4106)
  %4108 = add nsw <4 x i32> %3813, %4101
  %4109 = add nsw <4 x i32> %3814, %4103
  %4110 = add nsw <4 x i32> %3815, %4105
  %4111 = add nsw <4 x i32> %3816, %4107
  %4112 = add nsw i64 %1970, %1929
  %4113 = shl nsw i64 %4112, 4
  %4114 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4113
  %4115 = bitcast i8* %4114 to <16 x i8>*
  %4116 = load <16 x i8>, <16 x i8>* %4115, align 16, !tbaa !438
  %4117 = zext <16 x i8> %4116 to <16 x i16>
  %4118 = shufflevector <16 x i16> %4117, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4119 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4118)
  %4120 = shufflevector <16 x i16> %4117, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4121 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4120)
  %4122 = shufflevector <16 x i16> %4117, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4123 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4122)
  %4124 = shufflevector <16 x i16> %4117, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4125 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4124)
  %4126 = add nsw <4 x i32> %3831, %4119
  %4127 = add nsw <4 x i32> %3832, %4121
  %4128 = add nsw <4 x i32> %3833, %4123
  %4129 = add nsw <4 x i32> %3834, %4125
  %4130 = add nsw i64 %1990, %1929
  %4131 = shl nsw i64 %4130, 4
  %4132 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4131
  %4133 = bitcast i8* %4132 to <16 x i8>*
  %4134 = load <16 x i8>, <16 x i8>* %4133, align 16, !tbaa !438
  %4135 = zext <16 x i8> %4134 to <16 x i16>
  %4136 = shufflevector <16 x i16> %4135, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4137 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4136)
  %4138 = shufflevector <16 x i16> %4135, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4139 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4138)
  %4140 = shufflevector <16 x i16> %4135, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4141 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4140)
  %4142 = shufflevector <16 x i16> %4135, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4143 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4142)
  %4144 = add nsw <4 x i32> %3849, %4137
  %4145 = add nsw <4 x i32> %3850, %4139
  %4146 = add nsw <4 x i32> %3851, %4141
  %4147 = add nsw <4 x i32> %3852, %4143
  %4148 = add nsw i64 %2010, %1929
  %4149 = shl nsw i64 %4148, 4
  %4150 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4149
  %4151 = bitcast i8* %4150 to <16 x i8>*
  %4152 = load <16 x i8>, <16 x i8>* %4151, align 16, !tbaa !438
  %4153 = zext <16 x i8> %4152 to <16 x i16>
  %4154 = shufflevector <16 x i16> %4153, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4155 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4154)
  %4156 = shufflevector <16 x i16> %4153, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4157 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4156)
  %4158 = shufflevector <16 x i16> %4153, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4159 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4158)
  %4160 = shufflevector <16 x i16> %4153, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4161 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4160)
  %4162 = add nsw <4 x i32> %3867, %4155
  %4163 = add nsw <4 x i32> %3868, %4157
  %4164 = add nsw <4 x i32> %3869, %4159
  %4165 = add nsw <4 x i32> %3870, %4161
  %4166 = add nsw i64 %1946, %1930
  %4167 = shl nsw i64 %4166, 4
  %4168 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4167
  %4169 = bitcast i8* %4168 to <16 x i8>*
  %4170 = load <16 x i8>, <16 x i8>* %4169, align 16, !tbaa !438
  %4171 = zext <16 x i8> %4170 to <16 x i16>
  %4172 = shufflevector <16 x i16> %4171, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4173 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4172)
  %4174 = shufflevector <16 x i16> %4171, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4175 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4174)
  %4176 = shufflevector <16 x i16> %4171, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4177 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4176)
  %4178 = shufflevector <16 x i16> %4171, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4179 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4178)
  %4180 = add nsw <4 x i32> %3885, %4173
  %4181 = add nsw <4 x i32> %3886, %4175
  %4182 = add nsw <4 x i32> %3887, %4177
  %4183 = add nsw <4 x i32> %3888, %4179
  %4184 = add nsw i64 %1970, %1930
  %4185 = shl nsw i64 %4184, 4
  %4186 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4185
  %4187 = bitcast i8* %4186 to <16 x i8>*
  %4188 = load <16 x i8>, <16 x i8>* %4187, align 16, !tbaa !438
  %4189 = zext <16 x i8> %4188 to <16 x i16>
  %4190 = shufflevector <16 x i16> %4189, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4191 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4190)
  %4192 = shufflevector <16 x i16> %4189, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4193 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4192)
  %4194 = shufflevector <16 x i16> %4189, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4195 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4194)
  %4196 = shufflevector <16 x i16> %4189, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4197 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4196)
  %4198 = add nsw <4 x i32> %3903, %4191
  %4199 = add nsw <4 x i32> %3904, %4193
  %4200 = add nsw <4 x i32> %3905, %4195
  %4201 = add nsw <4 x i32> %3906, %4197
  %4202 = add nsw i64 %1990, %1930
  %4203 = shl nsw i64 %4202, 4
  %4204 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4203
  %4205 = bitcast i8* %4204 to <16 x i8>*
  %4206 = load <16 x i8>, <16 x i8>* %4205, align 16, !tbaa !438
  %4207 = zext <16 x i8> %4206 to <16 x i16>
  %4208 = shufflevector <16 x i16> %4207, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4209 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4208)
  %4210 = shufflevector <16 x i16> %4207, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4211 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4210)
  %4212 = shufflevector <16 x i16> %4207, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4213 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4212)
  %4214 = shufflevector <16 x i16> %4207, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4215 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4214)
  %4216 = add nsw <4 x i32> %3921, %4209
  %4217 = add nsw <4 x i32> %3922, %4211
  %4218 = add nsw <4 x i32> %3923, %4213
  %4219 = add nsw <4 x i32> %3924, %4215
  %4220 = add nsw i64 %2010, %1930
  %4221 = shl nsw i64 %4220, 4
  %4222 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4221
  %4223 = bitcast i8* %4222 to <16 x i8>*
  %4224 = load <16 x i8>, <16 x i8>* %4223, align 16, !tbaa !438
  %4225 = zext <16 x i8> %4224 to <16 x i16>
  %4226 = shufflevector <16 x i16> %4225, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4227 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4226)
  %4228 = shufflevector <16 x i16> %4225, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4229 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4228)
  %4230 = shufflevector <16 x i16> %4225, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4231 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4230)
  %4232 = shufflevector <16 x i16> %4225, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4233 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4232)
  %4234 = add nsw <4 x i32> %3939, %4227
  %4235 = add nsw <4 x i32> %3940, %4229
  %4236 = add nsw <4 x i32> %3941, %4231
  %4237 = add nsw <4 x i32> %3942, %4233
  %4238 = add nsw i64 %1946, %1931
  %4239 = shl nsw i64 %4238, 4
  %4240 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4239
  %4241 = bitcast i8* %4240 to <16 x i8>*
  %4242 = load <16 x i8>, <16 x i8>* %4241, align 16, !tbaa !438
  %4243 = zext <16 x i8> %4242 to <16 x i16>
  %4244 = shufflevector <16 x i16> %4243, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4245 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4244)
  %4246 = shufflevector <16 x i16> %4243, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4247 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4246)
  %4248 = shufflevector <16 x i16> %4243, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4249 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4248)
  %4250 = shufflevector <16 x i16> %4243, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4251 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4250)
  %4252 = add nsw <4 x i32> %3957, %4245
  %4253 = add nsw <4 x i32> %3958, %4247
  %4254 = add nsw <4 x i32> %3959, %4249
  %4255 = add nsw <4 x i32> %3960, %4251
  %4256 = add nsw i64 %1970, %1931
  %4257 = shl nsw i64 %4256, 4
  %4258 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4257
  %4259 = bitcast i8* %4258 to <16 x i8>*
  %4260 = load <16 x i8>, <16 x i8>* %4259, align 16, !tbaa !438
  %4261 = zext <16 x i8> %4260 to <16 x i16>
  %4262 = shufflevector <16 x i16> %4261, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4263 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4262)
  %4264 = shufflevector <16 x i16> %4261, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4265 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4264)
  %4266 = shufflevector <16 x i16> %4261, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4267 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4266)
  %4268 = shufflevector <16 x i16> %4261, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4269 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4268)
  %4270 = add nsw <4 x i32> %3975, %4263
  %4271 = add nsw <4 x i32> %3976, %4265
  %4272 = add nsw <4 x i32> %3977, %4267
  %4273 = add nsw <4 x i32> %3978, %4269
  %4274 = add nsw i64 %1990, %1931
  %4275 = shl nsw i64 %4274, 4
  %4276 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4275
  %4277 = bitcast i8* %4276 to <16 x i8>*
  %4278 = load <16 x i8>, <16 x i8>* %4277, align 16, !tbaa !438
  %4279 = zext <16 x i8> %4278 to <16 x i16>
  %4280 = shufflevector <16 x i16> %4279, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4281 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4280)
  %4282 = shufflevector <16 x i16> %4279, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4283 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4282)
  %4284 = shufflevector <16 x i16> %4279, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4285 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4284)
  %4286 = shufflevector <16 x i16> %4279, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4287 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4286)
  %4288 = add nsw <4 x i32> %3993, %4281
  %4289 = add nsw <4 x i32> %3994, %4283
  %4290 = add nsw <4 x i32> %3995, %4285
  %4291 = add nsw <4 x i32> %3996, %4287
  %4292 = add nsw i64 %2010, %1931
  %4293 = shl nsw i64 %4292, 4
  %4294 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4293
  %4295 = bitcast i8* %4294 to <16 x i8>*
  %4296 = load <16 x i8>, <16 x i8>* %4295, align 16, !tbaa !438
  %4297 = zext <16 x i8> %4296 to <16 x i16>
  %4298 = shufflevector <16 x i16> %4297, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4299 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4024, <4 x i16> %4298)
  %4300 = shufflevector <16 x i16> %4297, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4301 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4027, <4 x i16> %4300)
  %4302 = shufflevector <16 x i16> %4297, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4303 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4030, <4 x i16> %4302)
  %4304 = shufflevector <16 x i16> %4297, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4305 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4033, <4 x i16> %4304)
  %4306 = add nsw <4 x i32> %4011, %4299
  %4307 = add nsw <4 x i32> %4012, %4301
  %4308 = add nsw <4 x i32> %4013, %4303
  %4309 = add nsw <4 x i32> %4014, %4305
  %4310 = load <8 x i16>, <8 x i16>* %1746, align 16, !tbaa !395
  %4311 = load <8 x i16>, <8 x i16>* %1748, align 16, !tbaa !395
  %4312 = shufflevector <8 x i16> %4311, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %4313 = add nsw i64 %1946, %1932
  %4314 = shl nsw i64 %4313, 4
  %4315 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4314
  %4316 = bitcast i8* %4315 to <16 x i8>*
  %4317 = load <16 x i8>, <16 x i8>* %4316, align 16, !tbaa !438
  %4318 = zext <16 x i8> %4317 to <16 x i16>
  %4319 = shufflevector <8 x i16> %4310, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4320 = shufflevector <16 x i16> %4318, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4321 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4320)
  %4322 = shufflevector <8 x i16> %4310, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4323 = shufflevector <16 x i16> %4318, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4324 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4323)
  %4325 = shufflevector <8 x i16> %4311, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4326 = shufflevector <16 x i16> %4318, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4327 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4326)
  %4328 = shufflevector <16 x i16> %4312, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4329 = shufflevector <16 x i16> %4318, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4330 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4329)
  %4331 = add nsw <4 x i32> %4036, %4321
  %4332 = add nsw <4 x i32> %4037, %4324
  %4333 = add nsw <4 x i32> %4038, %4327
  %4334 = add nsw <4 x i32> %4039, %4330
  %4335 = add nsw i64 %1970, %1932
  %4336 = shl nsw i64 %4335, 4
  %4337 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4336
  %4338 = bitcast i8* %4337 to <16 x i8>*
  %4339 = load <16 x i8>, <16 x i8>* %4338, align 16, !tbaa !438
  %4340 = zext <16 x i8> %4339 to <16 x i16>
  %4341 = shufflevector <16 x i16> %4340, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4342 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4341)
  %4343 = shufflevector <16 x i16> %4340, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4344 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4343)
  %4345 = shufflevector <16 x i16> %4340, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4346 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4345)
  %4347 = shufflevector <16 x i16> %4340, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4348 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4347)
  %4349 = add nsw <4 x i32> %4054, %4342
  %4350 = add nsw <4 x i32> %4055, %4344
  %4351 = add nsw <4 x i32> %4056, %4346
  %4352 = add nsw <4 x i32> %4057, %4348
  %4353 = add nsw i64 %1990, %1932
  %4354 = shl nsw i64 %4353, 4
  %4355 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4354
  %4356 = bitcast i8* %4355 to <16 x i8>*
  %4357 = load <16 x i8>, <16 x i8>* %4356, align 16, !tbaa !438
  %4358 = zext <16 x i8> %4357 to <16 x i16>
  %4359 = shufflevector <16 x i16> %4358, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4360 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4359)
  %4361 = shufflevector <16 x i16> %4358, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4362 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4361)
  %4363 = shufflevector <16 x i16> %4358, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4364 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4363)
  %4365 = shufflevector <16 x i16> %4358, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4366 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4365)
  %4367 = add nsw <4 x i32> %4072, %4360
  %4368 = add nsw <4 x i32> %4073, %4362
  %4369 = add nsw <4 x i32> %4074, %4364
  %4370 = add nsw <4 x i32> %4075, %4366
  %4371 = add nsw i64 %2010, %1932
  %4372 = shl nsw i64 %4371, 4
  %4373 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4372
  %4374 = bitcast i8* %4373 to <16 x i8>*
  %4375 = load <16 x i8>, <16 x i8>* %4374, align 16, !tbaa !438
  %4376 = zext <16 x i8> %4375 to <16 x i16>
  %4377 = shufflevector <16 x i16> %4376, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4378 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4377)
  %4379 = shufflevector <16 x i16> %4376, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4380 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4379)
  %4381 = shufflevector <16 x i16> %4376, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4382 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4381)
  %4383 = shufflevector <16 x i16> %4376, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4384 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4383)
  %4385 = add nsw <4 x i32> %4090, %4378
  %4386 = add nsw <4 x i32> %4091, %4380
  %4387 = add nsw <4 x i32> %4092, %4382
  %4388 = add nsw <4 x i32> %4093, %4384
  %4389 = add nsw i64 %1946, %1933
  %4390 = shl nsw i64 %4389, 4
  %4391 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4390
  %4392 = bitcast i8* %4391 to <16 x i8>*
  %4393 = load <16 x i8>, <16 x i8>* %4392, align 16, !tbaa !438
  %4394 = zext <16 x i8> %4393 to <16 x i16>
  %4395 = shufflevector <16 x i16> %4394, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4396 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4395)
  %4397 = shufflevector <16 x i16> %4394, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4398 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4397)
  %4399 = shufflevector <16 x i16> %4394, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4400 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4399)
  %4401 = shufflevector <16 x i16> %4394, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4402 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4401)
  %4403 = add nsw <4 x i32> %4108, %4396
  %4404 = add nsw <4 x i32> %4109, %4398
  %4405 = add nsw <4 x i32> %4110, %4400
  %4406 = add nsw <4 x i32> %4111, %4402
  %4407 = add nsw i64 %1970, %1933
  %4408 = shl nsw i64 %4407, 4
  %4409 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4408
  %4410 = bitcast i8* %4409 to <16 x i8>*
  %4411 = load <16 x i8>, <16 x i8>* %4410, align 16, !tbaa !438
  %4412 = zext <16 x i8> %4411 to <16 x i16>
  %4413 = shufflevector <16 x i16> %4412, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4414 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4413)
  %4415 = shufflevector <16 x i16> %4412, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4416 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4415)
  %4417 = shufflevector <16 x i16> %4412, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4418 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4417)
  %4419 = shufflevector <16 x i16> %4412, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4420 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4419)
  %4421 = add nsw <4 x i32> %4126, %4414
  %4422 = add nsw <4 x i32> %4127, %4416
  %4423 = add nsw <4 x i32> %4128, %4418
  %4424 = add nsw <4 x i32> %4129, %4420
  %4425 = add nsw i64 %1990, %1933
  %4426 = shl nsw i64 %4425, 4
  %4427 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4426
  %4428 = bitcast i8* %4427 to <16 x i8>*
  %4429 = load <16 x i8>, <16 x i8>* %4428, align 16, !tbaa !438
  %4430 = zext <16 x i8> %4429 to <16 x i16>
  %4431 = shufflevector <16 x i16> %4430, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4432 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4431)
  %4433 = shufflevector <16 x i16> %4430, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4434 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4433)
  %4435 = shufflevector <16 x i16> %4430, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4436 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4435)
  %4437 = shufflevector <16 x i16> %4430, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4438 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4437)
  %4439 = add nsw <4 x i32> %4144, %4432
  %4440 = add nsw <4 x i32> %4145, %4434
  %4441 = add nsw <4 x i32> %4146, %4436
  %4442 = add nsw <4 x i32> %4147, %4438
  %4443 = add nsw i64 %2010, %1933
  %4444 = shl nsw i64 %4443, 4
  %4445 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4444
  %4446 = bitcast i8* %4445 to <16 x i8>*
  %4447 = load <16 x i8>, <16 x i8>* %4446, align 16, !tbaa !438
  %4448 = zext <16 x i8> %4447 to <16 x i16>
  %4449 = shufflevector <16 x i16> %4448, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4450 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4449)
  %4451 = shufflevector <16 x i16> %4448, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4452 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4451)
  %4453 = shufflevector <16 x i16> %4448, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4454 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4453)
  %4455 = shufflevector <16 x i16> %4448, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4456 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4455)
  %4457 = add nsw <4 x i32> %4162, %4450
  %4458 = add nsw <4 x i32> %4163, %4452
  %4459 = add nsw <4 x i32> %4164, %4454
  %4460 = add nsw <4 x i32> %4165, %4456
  %4461 = add nsw i64 %1946, %1934
  %4462 = shl nsw i64 %4461, 4
  %4463 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4462
  %4464 = bitcast i8* %4463 to <16 x i8>*
  %4465 = load <16 x i8>, <16 x i8>* %4464, align 16, !tbaa !438
  %4466 = zext <16 x i8> %4465 to <16 x i16>
  %4467 = shufflevector <16 x i16> %4466, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4468 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4467)
  %4469 = shufflevector <16 x i16> %4466, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4470 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4469)
  %4471 = shufflevector <16 x i16> %4466, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4472 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4471)
  %4473 = shufflevector <16 x i16> %4466, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4474 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4473)
  %4475 = add nsw <4 x i32> %4180, %4468
  %4476 = add nsw <4 x i32> %4181, %4470
  %4477 = add nsw <4 x i32> %4182, %4472
  %4478 = add nsw <4 x i32> %4183, %4474
  %4479 = add nsw i64 %1970, %1934
  %4480 = shl nsw i64 %4479, 4
  %4481 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4480
  %4482 = bitcast i8* %4481 to <16 x i8>*
  %4483 = load <16 x i8>, <16 x i8>* %4482, align 16, !tbaa !438
  %4484 = zext <16 x i8> %4483 to <16 x i16>
  %4485 = shufflevector <16 x i16> %4484, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4486 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4485)
  %4487 = shufflevector <16 x i16> %4484, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4488 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4487)
  %4489 = shufflevector <16 x i16> %4484, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4490 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4489)
  %4491 = shufflevector <16 x i16> %4484, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4492 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4491)
  %4493 = add nsw <4 x i32> %4198, %4486
  %4494 = add nsw <4 x i32> %4199, %4488
  %4495 = add nsw <4 x i32> %4200, %4490
  %4496 = add nsw <4 x i32> %4201, %4492
  %4497 = add nsw i64 %1990, %1934
  %4498 = shl nsw i64 %4497, 4
  %4499 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4498
  %4500 = bitcast i8* %4499 to <16 x i8>*
  %4501 = load <16 x i8>, <16 x i8>* %4500, align 16, !tbaa !438
  %4502 = zext <16 x i8> %4501 to <16 x i16>
  %4503 = shufflevector <16 x i16> %4502, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4504 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4503)
  %4505 = shufflevector <16 x i16> %4502, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4506 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4505)
  %4507 = shufflevector <16 x i16> %4502, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4508 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4507)
  %4509 = shufflevector <16 x i16> %4502, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4510 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4509)
  %4511 = add nsw <4 x i32> %4216, %4504
  %4512 = add nsw <4 x i32> %4217, %4506
  %4513 = add nsw <4 x i32> %4218, %4508
  %4514 = add nsw <4 x i32> %4219, %4510
  %4515 = add nsw i64 %2010, %1934
  %4516 = shl nsw i64 %4515, 4
  %4517 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4516
  %4518 = bitcast i8* %4517 to <16 x i8>*
  %4519 = load <16 x i8>, <16 x i8>* %4518, align 16, !tbaa !438
  %4520 = zext <16 x i8> %4519 to <16 x i16>
  %4521 = shufflevector <16 x i16> %4520, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4522 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4521)
  %4523 = shufflevector <16 x i16> %4520, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4524 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4523)
  %4525 = shufflevector <16 x i16> %4520, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4526 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4525)
  %4527 = shufflevector <16 x i16> %4520, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4528 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4527)
  %4529 = add nsw <4 x i32> %4234, %4522
  %4530 = add nsw <4 x i32> %4235, %4524
  %4531 = add nsw <4 x i32> %4236, %4526
  %4532 = add nsw <4 x i32> %4237, %4528
  %4533 = add nsw i64 %1946, %1935
  %4534 = shl nsw i64 %4533, 4
  %4535 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4534
  %4536 = bitcast i8* %4535 to <16 x i8>*
  %4537 = load <16 x i8>, <16 x i8>* %4536, align 16, !tbaa !438
  %4538 = zext <16 x i8> %4537 to <16 x i16>
  %4539 = shufflevector <16 x i16> %4538, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4540 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4539)
  %4541 = shufflevector <16 x i16> %4538, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4542 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4541)
  %4543 = shufflevector <16 x i16> %4538, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4544 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4543)
  %4545 = shufflevector <16 x i16> %4538, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4546 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4545)
  %4547 = add nsw <4 x i32> %4252, %4540
  %4548 = add nsw <4 x i32> %4253, %4542
  %4549 = add nsw <4 x i32> %4254, %4544
  %4550 = add nsw <4 x i32> %4255, %4546
  %4551 = add nsw i64 %1970, %1935
  %4552 = shl nsw i64 %4551, 4
  %4553 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4552
  %4554 = bitcast i8* %4553 to <16 x i8>*
  %4555 = load <16 x i8>, <16 x i8>* %4554, align 16, !tbaa !438
  %4556 = zext <16 x i8> %4555 to <16 x i16>
  %4557 = shufflevector <16 x i16> %4556, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4558 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4557)
  %4559 = shufflevector <16 x i16> %4556, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4560 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4559)
  %4561 = shufflevector <16 x i16> %4556, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4562 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4561)
  %4563 = shufflevector <16 x i16> %4556, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4564 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4563)
  %4565 = add nsw <4 x i32> %4270, %4558
  %4566 = add nsw <4 x i32> %4271, %4560
  %4567 = add nsw <4 x i32> %4272, %4562
  %4568 = add nsw <4 x i32> %4273, %4564
  %4569 = add nsw i64 %1990, %1935
  %4570 = shl nsw i64 %4569, 4
  %4571 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4570
  %4572 = bitcast i8* %4571 to <16 x i8>*
  %4573 = load <16 x i8>, <16 x i8>* %4572, align 16, !tbaa !438
  %4574 = zext <16 x i8> %4573 to <16 x i16>
  %4575 = shufflevector <16 x i16> %4574, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4576 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4575)
  %4577 = shufflevector <16 x i16> %4574, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4578 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4577)
  %4579 = shufflevector <16 x i16> %4574, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4580 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4579)
  %4581 = shufflevector <16 x i16> %4574, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4582 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4581)
  %4583 = add nsw <4 x i32> %4288, %4576
  %4584 = add nsw <4 x i32> %4289, %4578
  %4585 = add nsw <4 x i32> %4290, %4580
  %4586 = add nsw <4 x i32> %4291, %4582
  %4587 = add nsw i64 %2010, %1935
  %4588 = shl nsw i64 %4587, 4
  %4589 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4588
  %4590 = bitcast i8* %4589 to <16 x i8>*
  %4591 = load <16 x i8>, <16 x i8>* %4590, align 16, !tbaa !438
  %4592 = zext <16 x i8> %4591 to <16 x i16>
  %4593 = shufflevector <16 x i16> %4592, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4594 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4319, <4 x i16> %4593)
  %4595 = shufflevector <16 x i16> %4592, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4596 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4322, <4 x i16> %4595)
  %4597 = shufflevector <16 x i16> %4592, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4598 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4325, <4 x i16> %4597)
  %4599 = shufflevector <16 x i16> %4592, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4600 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4328, <4 x i16> %4599)
  %4601 = add nsw <4 x i32> %4306, %4594
  %4602 = add nsw <4 x i32> %4307, %4596
  %4603 = add nsw <4 x i32> %4308, %4598
  %4604 = add nsw <4 x i32> %4309, %4600
  br label %"consume convolved86"

next_bb79:                                        ; preds = %"for output.s0.x.xo73"
  %4605 = trunc i64 %indvars.iv6351 to i32
  %4606 = shl nsw i32 %4605, 2
  %t2459 = add nsw i32 %4606, %21
  %4607 = mul nsw i32 %t2459, %stride_x
  %t2455 = sub nsw i32 %4607, %t2356
  %4608 = add nsw i32 %t2459, 3
  %4609 = mul nsw i32 %4608, %stride_x
  %t2458 = sub nsw i32 %4609, %t2356
  %4610 = add nsw i32 %t2459, 2
  %4611 = mul nsw i32 %4610, %stride_x
  %t2457 = sub nsw i32 %4611, %t2356
  %4612 = add nsw i32 %t2459, 1
  %4613 = mul nsw i32 %4612, %stride_x
  %t2456 = sub nsw i32 %4613, %t2356
  br i1 %1523, label %"for convolved.s1.r19$y80.preheader", label %"consume convolved86", !prof !391

"for convolved.s1.r19$y80.preheader":             ; preds = %next_bb79
  br i1 %1521, label %"for convolved.s1.r19$y80.us", label %"consume convolved86", !prof !391

"for convolved.s1.r19$y80.us":                    ; preds = %"for convolved.s1.r19$y80.preheader", %"end for convolved.s1.r19$x84.loopexit.us"
  %indvars.iv6347 = phi i64 [ %indvars.iv.next6348, %"end for convolved.s1.r19$x84.loopexit.us" ], [ 0, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3624.0.us = phi <4 x i32> [ %4936, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3568.0.us = phi <4 x i32> [ %4935, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3512.0.us = phi <4 x i32> [ %4934, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3456.0.us = phi <4 x i32> [ %4933, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3400.0.us = phi <4 x i32> [ %4918, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3344.0.us = phi <4 x i32> [ %4917, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3288.0.us = phi <4 x i32> [ %4916, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3232.0.us = phi <4 x i32> [ %4915, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3176.0.us = phi <4 x i32> [ %4900, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3120.0.us = phi <4 x i32> [ %4899, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3064.0.us = phi <4 x i32> [ %4898, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.3008.0.us = phi <4 x i32> [ %4897, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2952.0.us = phi <4 x i32> [ %4882, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2896.0.us = phi <4 x i32> [ %4881, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2840.0.us = phi <4 x i32> [ %4880, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2784.0.us = phi <4 x i32> [ %4879, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2728.0.us = phi <4 x i32> [ %4864, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2672.0.us = phi <4 x i32> [ %4863, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2616.0.us = phi <4 x i32> [ %4862, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2560.0.us = phi <4 x i32> [ %4861, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2504.0.us = phi <4 x i32> [ %4846, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2448.0.us = phi <4 x i32> [ %4845, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2392.0.us = phi <4 x i32> [ %4844, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2336.0.us = phi <4 x i32> [ %4843, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2280.0.us = phi <4 x i32> [ %4828, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2224.0.us = phi <4 x i32> [ %4827, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2168.0.us = phi <4 x i32> [ %4826, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2112.0.us = phi <4 x i32> [ %4825, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2056.0.us = phi <4 x i32> [ %4810, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.2000.0.us = phi <4 x i32> [ %4809, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1944.0.us = phi <4 x i32> [ %4808, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1888.0.us = phi <4 x i32> [ %4807, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1832.0.us = phi <4 x i32> [ %4792, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1776.0.us = phi <4 x i32> [ %4791, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1720.0.us = phi <4 x i32> [ %4790, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1664.0.us = phi <4 x i32> [ %4789, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1608.0.us = phi <4 x i32> [ %4774, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1552.0.us = phi <4 x i32> [ %4773, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1496.0.us = phi <4 x i32> [ %4772, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1440.0.us = phi <4 x i32> [ %4771, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1384.0.us = phi <4 x i32> [ %4756, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1328.0.us = phi <4 x i32> [ %4755, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1272.0.us = phi <4 x i32> [ %4754, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1216.0.us = phi <4 x i32> [ %4753, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1160.0.us = phi <4 x i32> [ %4738, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1104.0.us = phi <4 x i32> [ %4737, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.1048.0.us = phi <4 x i32> [ %4736, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.992.0.us = phi <4 x i32> [ %4735, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.930.0.us = phi <4 x i32> [ %4720, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.868.0.us = phi <4 x i32> [ %4719, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.806.0.us = phi <4 x i32> [ %4718, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.744.0.us = phi <4 x i32> [ %4717, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.682.0.us = phi <4 x i32> [ %4702, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.620.0.us = phi <4 x i32> [ %4701, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.558.0.us = phi <4 x i32> [ %4700, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.496.0.us = phi <4 x i32> [ %4699, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.434.0.us = phi <4 x i32> [ %4684, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.372.0.us = phi <4 x i32> [ %4683, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.310.0.us = phi <4 x i32> [ %4682, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.248.0.us = phi <4 x i32> [ %4681, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.186.0.us = phi <4 x i32> [ %4666, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1885, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.124.0.us = phi <4 x i32> [ %4665, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1884, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.62.0.us = phi <4 x i32> [ %4664, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1883, %"for convolved.s1.r19$y80.preheader" ]
  %convolved944.sroa.0.0.us = phi <4 x i32> [ %4663, %"end for convolved.s1.r19$x84.loopexit.us" ], [ %1882, %"for convolved.s1.r19$y80.preheader" ]
  %"convolved.s1.r19$y82.us" = phi i32 [ %4937, %"end for convolved.s1.r19$x84.loopexit.us" ], [ 0, %"for convolved.s1.r19$y80.preheader" ]
  %t2477.us = mul nsw i32 %"convolved.s1.r19$y82.us", %a613
  %4614 = add nsw i32 %t2477.us, %t2447
  %t2478.us = mul nsw i32 %4614, %a44
  %4615 = add nsw i32 %t2477.us, %t2449
  %t2479.us = mul nsw i32 %4615, %a44
  %4616 = add nsw i32 %t2477.us, %t2451
  %t2480.us = mul nsw i32 %4616, %a44
  %4617 = add nsw i32 %t2477.us, %t2453
  %t2481.us = mul nsw i32 %4617, %a44
  %4618 = mul nsw i64 %indvars.iv6347, %1604
  %t2464.us = add nsw i32 %t2481.us, %t2458
  %t2463.us = add nsw i32 %t2481.us, %t2457
  %t2462.us = add nsw i32 %t2481.us, %t2456
  %t2461.us = add nsw i32 %t2481.us, %t2455
  %t2476.us = add nsw i32 %t2480.us, %t2458
  %t2475.us = add nsw i32 %t2480.us, %t2457
  %t2474.us = add nsw i32 %t2480.us, %t2456
  %t2473.us = add nsw i32 %t2480.us, %t2455
  %t2472.us = add nsw i32 %t2479.us, %t2458
  %t2471.us = add nsw i32 %t2479.us, %t2457
  %t2470.us = add nsw i32 %t2479.us, %t2456
  %t2469.us = add nsw i32 %t2479.us, %t2455
  %t2468.us = add nsw i32 %t2478.us, %t2458
  %t2467.us = add nsw i32 %t2478.us, %t2457
  %t2466.us = add nsw i32 %t2478.us, %t2456
  %t2465.us = add nsw i32 %t2478.us, %t2455
  %4619 = sext i32 %t2461.us to i64
  %4620 = sext i32 %t2462.us to i64
  %4621 = sext i32 %t2463.us to i64
  %4622 = sext i32 %t2464.us to i64
  %4623 = sext i32 %t2465.us to i64
  %4624 = sext i32 %t2466.us to i64
  %4625 = sext i32 %t2467.us to i64
  %4626 = sext i32 %t2468.us to i64
  %4627 = sext i32 %t2469.us to i64
  %4628 = sext i32 %t2470.us to i64
  %4629 = sext i32 %t2471.us to i64
  %4630 = sext i32 %t2472.us to i64
  %4631 = sext i32 %t2473.us to i64
  %4632 = sext i32 %t2474.us to i64
  %4633 = sext i32 %t2475.us to i64
  %4634 = sext i32 %t2476.us to i64
  br label %"for convolved.s1.r19$x83.us"

"for convolved.s1.r19$x83.us":                    ; preds = %"for convolved.s1.r19$y80.us", %"for convolved.s1.r19$x83.us"
  %indvars.iv6345 = phi i64 [ 0, %"for convolved.s1.r19$y80.us" ], [ %indvars.iv.next6346, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3624.1.us = phi <4 x i32> [ %convolved944.sroa.3624.0.us, %"for convolved.s1.r19$y80.us" ], [ %4936, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3568.1.us = phi <4 x i32> [ %convolved944.sroa.3568.0.us, %"for convolved.s1.r19$y80.us" ], [ %4935, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3512.1.us = phi <4 x i32> [ %convolved944.sroa.3512.0.us, %"for convolved.s1.r19$y80.us" ], [ %4934, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3456.1.us = phi <4 x i32> [ %convolved944.sroa.3456.0.us, %"for convolved.s1.r19$y80.us" ], [ %4933, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3400.1.us = phi <4 x i32> [ %convolved944.sroa.3400.0.us, %"for convolved.s1.r19$y80.us" ], [ %4918, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3344.1.us = phi <4 x i32> [ %convolved944.sroa.3344.0.us, %"for convolved.s1.r19$y80.us" ], [ %4917, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3288.1.us = phi <4 x i32> [ %convolved944.sroa.3288.0.us, %"for convolved.s1.r19$y80.us" ], [ %4916, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3232.1.us = phi <4 x i32> [ %convolved944.sroa.3232.0.us, %"for convolved.s1.r19$y80.us" ], [ %4915, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3176.1.us = phi <4 x i32> [ %convolved944.sroa.3176.0.us, %"for convolved.s1.r19$y80.us" ], [ %4900, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3120.1.us = phi <4 x i32> [ %convolved944.sroa.3120.0.us, %"for convolved.s1.r19$y80.us" ], [ %4899, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3064.1.us = phi <4 x i32> [ %convolved944.sroa.3064.0.us, %"for convolved.s1.r19$y80.us" ], [ %4898, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.3008.1.us = phi <4 x i32> [ %convolved944.sroa.3008.0.us, %"for convolved.s1.r19$y80.us" ], [ %4897, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2952.1.us = phi <4 x i32> [ %convolved944.sroa.2952.0.us, %"for convolved.s1.r19$y80.us" ], [ %4882, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2896.1.us = phi <4 x i32> [ %convolved944.sroa.2896.0.us, %"for convolved.s1.r19$y80.us" ], [ %4881, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2840.1.us = phi <4 x i32> [ %convolved944.sroa.2840.0.us, %"for convolved.s1.r19$y80.us" ], [ %4880, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2784.1.us = phi <4 x i32> [ %convolved944.sroa.2784.0.us, %"for convolved.s1.r19$y80.us" ], [ %4879, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2728.1.us = phi <4 x i32> [ %convolved944.sroa.2728.0.us, %"for convolved.s1.r19$y80.us" ], [ %4864, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2672.1.us = phi <4 x i32> [ %convolved944.sroa.2672.0.us, %"for convolved.s1.r19$y80.us" ], [ %4863, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2616.1.us = phi <4 x i32> [ %convolved944.sroa.2616.0.us, %"for convolved.s1.r19$y80.us" ], [ %4862, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2560.1.us = phi <4 x i32> [ %convolved944.sroa.2560.0.us, %"for convolved.s1.r19$y80.us" ], [ %4861, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2504.1.us = phi <4 x i32> [ %convolved944.sroa.2504.0.us, %"for convolved.s1.r19$y80.us" ], [ %4846, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2448.1.us = phi <4 x i32> [ %convolved944.sroa.2448.0.us, %"for convolved.s1.r19$y80.us" ], [ %4845, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2392.1.us = phi <4 x i32> [ %convolved944.sroa.2392.0.us, %"for convolved.s1.r19$y80.us" ], [ %4844, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2336.1.us = phi <4 x i32> [ %convolved944.sroa.2336.0.us, %"for convolved.s1.r19$y80.us" ], [ %4843, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2280.1.us = phi <4 x i32> [ %convolved944.sroa.2280.0.us, %"for convolved.s1.r19$y80.us" ], [ %4828, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2224.1.us = phi <4 x i32> [ %convolved944.sroa.2224.0.us, %"for convolved.s1.r19$y80.us" ], [ %4827, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2168.1.us = phi <4 x i32> [ %convolved944.sroa.2168.0.us, %"for convolved.s1.r19$y80.us" ], [ %4826, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2112.1.us = phi <4 x i32> [ %convolved944.sroa.2112.0.us, %"for convolved.s1.r19$y80.us" ], [ %4825, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2056.1.us = phi <4 x i32> [ %convolved944.sroa.2056.0.us, %"for convolved.s1.r19$y80.us" ], [ %4810, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.2000.1.us = phi <4 x i32> [ %convolved944.sroa.2000.0.us, %"for convolved.s1.r19$y80.us" ], [ %4809, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1944.1.us = phi <4 x i32> [ %convolved944.sroa.1944.0.us, %"for convolved.s1.r19$y80.us" ], [ %4808, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1888.1.us = phi <4 x i32> [ %convolved944.sroa.1888.0.us, %"for convolved.s1.r19$y80.us" ], [ %4807, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1832.1.us = phi <4 x i32> [ %convolved944.sroa.1832.0.us, %"for convolved.s1.r19$y80.us" ], [ %4792, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1776.1.us = phi <4 x i32> [ %convolved944.sroa.1776.0.us, %"for convolved.s1.r19$y80.us" ], [ %4791, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1720.1.us = phi <4 x i32> [ %convolved944.sroa.1720.0.us, %"for convolved.s1.r19$y80.us" ], [ %4790, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1664.1.us = phi <4 x i32> [ %convolved944.sroa.1664.0.us, %"for convolved.s1.r19$y80.us" ], [ %4789, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1608.1.us = phi <4 x i32> [ %convolved944.sroa.1608.0.us, %"for convolved.s1.r19$y80.us" ], [ %4774, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1552.1.us = phi <4 x i32> [ %convolved944.sroa.1552.0.us, %"for convolved.s1.r19$y80.us" ], [ %4773, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1496.1.us = phi <4 x i32> [ %convolved944.sroa.1496.0.us, %"for convolved.s1.r19$y80.us" ], [ %4772, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1440.1.us = phi <4 x i32> [ %convolved944.sroa.1440.0.us, %"for convolved.s1.r19$y80.us" ], [ %4771, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1384.1.us = phi <4 x i32> [ %convolved944.sroa.1384.0.us, %"for convolved.s1.r19$y80.us" ], [ %4756, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1328.1.us = phi <4 x i32> [ %convolved944.sroa.1328.0.us, %"for convolved.s1.r19$y80.us" ], [ %4755, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1272.1.us = phi <4 x i32> [ %convolved944.sroa.1272.0.us, %"for convolved.s1.r19$y80.us" ], [ %4754, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1216.1.us = phi <4 x i32> [ %convolved944.sroa.1216.0.us, %"for convolved.s1.r19$y80.us" ], [ %4753, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1160.1.us = phi <4 x i32> [ %convolved944.sroa.1160.0.us, %"for convolved.s1.r19$y80.us" ], [ %4738, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1104.1.us = phi <4 x i32> [ %convolved944.sroa.1104.0.us, %"for convolved.s1.r19$y80.us" ], [ %4737, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.1048.1.us = phi <4 x i32> [ %convolved944.sroa.1048.0.us, %"for convolved.s1.r19$y80.us" ], [ %4736, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.992.1.us = phi <4 x i32> [ %convolved944.sroa.992.0.us, %"for convolved.s1.r19$y80.us" ], [ %4735, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.930.1.us = phi <4 x i32> [ %convolved944.sroa.930.0.us, %"for convolved.s1.r19$y80.us" ], [ %4720, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.868.1.us = phi <4 x i32> [ %convolved944.sroa.868.0.us, %"for convolved.s1.r19$y80.us" ], [ %4719, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.806.1.us = phi <4 x i32> [ %convolved944.sroa.806.0.us, %"for convolved.s1.r19$y80.us" ], [ %4718, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.744.1.us = phi <4 x i32> [ %convolved944.sroa.744.0.us, %"for convolved.s1.r19$y80.us" ], [ %4717, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.682.1.us = phi <4 x i32> [ %convolved944.sroa.682.0.us, %"for convolved.s1.r19$y80.us" ], [ %4702, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.620.1.us = phi <4 x i32> [ %convolved944.sroa.620.0.us, %"for convolved.s1.r19$y80.us" ], [ %4701, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.558.1.us = phi <4 x i32> [ %convolved944.sroa.558.0.us, %"for convolved.s1.r19$y80.us" ], [ %4700, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.496.1.us = phi <4 x i32> [ %convolved944.sroa.496.0.us, %"for convolved.s1.r19$y80.us" ], [ %4699, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.434.1.us = phi <4 x i32> [ %convolved944.sroa.434.0.us, %"for convolved.s1.r19$y80.us" ], [ %4684, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.372.1.us = phi <4 x i32> [ %convolved944.sroa.372.0.us, %"for convolved.s1.r19$y80.us" ], [ %4683, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.310.1.us = phi <4 x i32> [ %convolved944.sroa.310.0.us, %"for convolved.s1.r19$y80.us" ], [ %4682, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.248.1.us = phi <4 x i32> [ %convolved944.sroa.248.0.us, %"for convolved.s1.r19$y80.us" ], [ %4681, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.186.1.us = phi <4 x i32> [ %convolved944.sroa.186.0.us, %"for convolved.s1.r19$y80.us" ], [ %4666, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.124.1.us = phi <4 x i32> [ %convolved944.sroa.124.0.us, %"for convolved.s1.r19$y80.us" ], [ %4665, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.62.1.us = phi <4 x i32> [ %convolved944.sroa.62.0.us, %"for convolved.s1.r19$y80.us" ], [ %4664, %"for convolved.s1.r19$x83.us" ]
  %convolved944.sroa.0.1.us = phi <4 x i32> [ %convolved944.sroa.0.0.us, %"for convolved.s1.r19$y80.us" ], [ %4663, %"for convolved.s1.r19$x83.us" ]
  %4635 = add nsw i64 %indvars.iv6345, %4618
  %4636 = shl nsw i64 %4635, 4
  %4637 = getelementptr inbounds i16, i16* %filter_zeroed26, i64 %4636
  %4638 = bitcast i16* %4637 to <8 x i16>*
  %4639 = load <8 x i16>, <8 x i16>* %4638, align 16, !tbaa !395
  %4640 = getelementptr inbounds i16, i16* %4637, i64 8
  %4641 = bitcast i16* %4640 to <8 x i16>*
  %4642 = load <8 x i16>, <8 x i16>* %4641, align 16, !tbaa !395
  %4643 = shufflevector <8 x i16> %4642, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %4644 = mul nsw i64 %indvars.iv6345, %1586
  %4645 = add nsw i64 %4644, %4619
  %4646 = shl nsw i64 %4645, 4
  %4647 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4646
  %4648 = bitcast i8* %4647 to <16 x i8>*
  %4649 = load <16 x i8>, <16 x i8>* %4648, align 16, !tbaa !438
  %4650 = zext <16 x i8> %4649 to <16 x i16>
  %4651 = shufflevector <8 x i16> %4639, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4652 = shufflevector <16 x i16> %4650, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4653 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4652)
  %4654 = shufflevector <8 x i16> %4639, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4655 = shufflevector <16 x i16> %4650, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4656 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4655)
  %4657 = shufflevector <8 x i16> %4642, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4658 = shufflevector <16 x i16> %4650, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4659 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4658)
  %4660 = shufflevector <16 x i16> %4643, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4661 = shufflevector <16 x i16> %4650, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4662 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4661)
  %4663 = add nsw <4 x i32> %4653, %convolved944.sroa.0.1.us
  %4664 = add nsw <4 x i32> %4656, %convolved944.sroa.62.1.us
  %4665 = add nsw <4 x i32> %4659, %convolved944.sroa.124.1.us
  %4666 = add nsw <4 x i32> %4662, %convolved944.sroa.186.1.us
  %4667 = add nsw i64 %4644, %4620
  %4668 = shl nsw i64 %4667, 4
  %4669 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4668
  %4670 = bitcast i8* %4669 to <16 x i8>*
  %4671 = load <16 x i8>, <16 x i8>* %4670, align 16, !tbaa !438
  %4672 = zext <16 x i8> %4671 to <16 x i16>
  %4673 = shufflevector <16 x i16> %4672, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4674 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4673)
  %4675 = shufflevector <16 x i16> %4672, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4676 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4675)
  %4677 = shufflevector <16 x i16> %4672, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4678 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4677)
  %4679 = shufflevector <16 x i16> %4672, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4680 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4679)
  %4681 = add nsw <4 x i32> %4674, %convolved944.sroa.248.1.us
  %4682 = add nsw <4 x i32> %4676, %convolved944.sroa.310.1.us
  %4683 = add nsw <4 x i32> %4678, %convolved944.sroa.372.1.us
  %4684 = add nsw <4 x i32> %4680, %convolved944.sroa.434.1.us
  %4685 = add nsw i64 %4644, %4621
  %4686 = shl nsw i64 %4685, 4
  %4687 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4686
  %4688 = bitcast i8* %4687 to <16 x i8>*
  %4689 = load <16 x i8>, <16 x i8>* %4688, align 16, !tbaa !438
  %4690 = zext <16 x i8> %4689 to <16 x i16>
  %4691 = shufflevector <16 x i16> %4690, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4692 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4691)
  %4693 = shufflevector <16 x i16> %4690, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4694 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4693)
  %4695 = shufflevector <16 x i16> %4690, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4696 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4695)
  %4697 = shufflevector <16 x i16> %4690, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4698 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4697)
  %4699 = add nsw <4 x i32> %4692, %convolved944.sroa.496.1.us
  %4700 = add nsw <4 x i32> %4694, %convolved944.sroa.558.1.us
  %4701 = add nsw <4 x i32> %4696, %convolved944.sroa.620.1.us
  %4702 = add nsw <4 x i32> %4698, %convolved944.sroa.682.1.us
  %4703 = add nsw i64 %4644, %4622
  %4704 = shl nsw i64 %4703, 4
  %4705 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4704
  %4706 = bitcast i8* %4705 to <16 x i8>*
  %4707 = load <16 x i8>, <16 x i8>* %4706, align 16, !tbaa !438
  %4708 = zext <16 x i8> %4707 to <16 x i16>
  %4709 = shufflevector <16 x i16> %4708, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4710 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4709)
  %4711 = shufflevector <16 x i16> %4708, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4712 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4711)
  %4713 = shufflevector <16 x i16> %4708, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4714 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4713)
  %4715 = shufflevector <16 x i16> %4708, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4716 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4715)
  %4717 = add nsw <4 x i32> %4710, %convolved944.sroa.744.1.us
  %4718 = add nsw <4 x i32> %4712, %convolved944.sroa.806.1.us
  %4719 = add nsw <4 x i32> %4714, %convolved944.sroa.868.1.us
  %4720 = add nsw <4 x i32> %4716, %convolved944.sroa.930.1.us
  %4721 = add nsw i64 %4644, %4623
  %4722 = shl nsw i64 %4721, 4
  %4723 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4722
  %4724 = bitcast i8* %4723 to <16 x i8>*
  %4725 = load <16 x i8>, <16 x i8>* %4724, align 16, !tbaa !438
  %4726 = zext <16 x i8> %4725 to <16 x i16>
  %4727 = shufflevector <16 x i16> %4726, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4728 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4727)
  %4729 = shufflevector <16 x i16> %4726, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4730 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4729)
  %4731 = shufflevector <16 x i16> %4726, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4732 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4731)
  %4733 = shufflevector <16 x i16> %4726, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4734 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4733)
  %4735 = add nsw <4 x i32> %4728, %convolved944.sroa.992.1.us
  %4736 = add nsw <4 x i32> %4730, %convolved944.sroa.1048.1.us
  %4737 = add nsw <4 x i32> %4732, %convolved944.sroa.1104.1.us
  %4738 = add nsw <4 x i32> %4734, %convolved944.sroa.1160.1.us
  %4739 = add nsw i64 %4644, %4624
  %4740 = shl nsw i64 %4739, 4
  %4741 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4740
  %4742 = bitcast i8* %4741 to <16 x i8>*
  %4743 = load <16 x i8>, <16 x i8>* %4742, align 16, !tbaa !438
  %4744 = zext <16 x i8> %4743 to <16 x i16>
  %4745 = shufflevector <16 x i16> %4744, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4746 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4745)
  %4747 = shufflevector <16 x i16> %4744, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4748 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4747)
  %4749 = shufflevector <16 x i16> %4744, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4750 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4749)
  %4751 = shufflevector <16 x i16> %4744, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4752 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4751)
  %4753 = add nsw <4 x i32> %4746, %convolved944.sroa.1216.1.us
  %4754 = add nsw <4 x i32> %4748, %convolved944.sroa.1272.1.us
  %4755 = add nsw <4 x i32> %4750, %convolved944.sroa.1328.1.us
  %4756 = add nsw <4 x i32> %4752, %convolved944.sroa.1384.1.us
  %4757 = add nsw i64 %4644, %4625
  %4758 = shl nsw i64 %4757, 4
  %4759 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4758
  %4760 = bitcast i8* %4759 to <16 x i8>*
  %4761 = load <16 x i8>, <16 x i8>* %4760, align 16, !tbaa !438
  %4762 = zext <16 x i8> %4761 to <16 x i16>
  %4763 = shufflevector <16 x i16> %4762, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4764 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4763)
  %4765 = shufflevector <16 x i16> %4762, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4766 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4765)
  %4767 = shufflevector <16 x i16> %4762, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4768 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4767)
  %4769 = shufflevector <16 x i16> %4762, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4770 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4769)
  %4771 = add nsw <4 x i32> %4764, %convolved944.sroa.1440.1.us
  %4772 = add nsw <4 x i32> %4766, %convolved944.sroa.1496.1.us
  %4773 = add nsw <4 x i32> %4768, %convolved944.sroa.1552.1.us
  %4774 = add nsw <4 x i32> %4770, %convolved944.sroa.1608.1.us
  %4775 = add nsw i64 %4644, %4626
  %4776 = shl nsw i64 %4775, 4
  %4777 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4776
  %4778 = bitcast i8* %4777 to <16 x i8>*
  %4779 = load <16 x i8>, <16 x i8>* %4778, align 16, !tbaa !438
  %4780 = zext <16 x i8> %4779 to <16 x i16>
  %4781 = shufflevector <16 x i16> %4780, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4782 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4781)
  %4783 = shufflevector <16 x i16> %4780, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4784 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4783)
  %4785 = shufflevector <16 x i16> %4780, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4786 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4785)
  %4787 = shufflevector <16 x i16> %4780, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4788 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4787)
  %4789 = add nsw <4 x i32> %4782, %convolved944.sroa.1664.1.us
  %4790 = add nsw <4 x i32> %4784, %convolved944.sroa.1720.1.us
  %4791 = add nsw <4 x i32> %4786, %convolved944.sroa.1776.1.us
  %4792 = add nsw <4 x i32> %4788, %convolved944.sroa.1832.1.us
  %4793 = add nsw i64 %4644, %4627
  %4794 = shl nsw i64 %4793, 4
  %4795 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4794
  %4796 = bitcast i8* %4795 to <16 x i8>*
  %4797 = load <16 x i8>, <16 x i8>* %4796, align 16, !tbaa !438
  %4798 = zext <16 x i8> %4797 to <16 x i16>
  %4799 = shufflevector <16 x i16> %4798, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4800 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4799)
  %4801 = shufflevector <16 x i16> %4798, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4802 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4801)
  %4803 = shufflevector <16 x i16> %4798, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4804 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4803)
  %4805 = shufflevector <16 x i16> %4798, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4806 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4805)
  %4807 = add nsw <4 x i32> %4800, %convolved944.sroa.1888.1.us
  %4808 = add nsw <4 x i32> %4802, %convolved944.sroa.1944.1.us
  %4809 = add nsw <4 x i32> %4804, %convolved944.sroa.2000.1.us
  %4810 = add nsw <4 x i32> %4806, %convolved944.sroa.2056.1.us
  %4811 = add nsw i64 %4644, %4628
  %4812 = shl nsw i64 %4811, 4
  %4813 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4812
  %4814 = bitcast i8* %4813 to <16 x i8>*
  %4815 = load <16 x i8>, <16 x i8>* %4814, align 16, !tbaa !438
  %4816 = zext <16 x i8> %4815 to <16 x i16>
  %4817 = shufflevector <16 x i16> %4816, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4818 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4817)
  %4819 = shufflevector <16 x i16> %4816, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4820 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4819)
  %4821 = shufflevector <16 x i16> %4816, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4822 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4821)
  %4823 = shufflevector <16 x i16> %4816, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4824 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4823)
  %4825 = add nsw <4 x i32> %4818, %convolved944.sroa.2112.1.us
  %4826 = add nsw <4 x i32> %4820, %convolved944.sroa.2168.1.us
  %4827 = add nsw <4 x i32> %4822, %convolved944.sroa.2224.1.us
  %4828 = add nsw <4 x i32> %4824, %convolved944.sroa.2280.1.us
  %4829 = add nsw i64 %4644, %4629
  %4830 = shl nsw i64 %4829, 4
  %4831 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4830
  %4832 = bitcast i8* %4831 to <16 x i8>*
  %4833 = load <16 x i8>, <16 x i8>* %4832, align 16, !tbaa !438
  %4834 = zext <16 x i8> %4833 to <16 x i16>
  %4835 = shufflevector <16 x i16> %4834, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4836 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4835)
  %4837 = shufflevector <16 x i16> %4834, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4838 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4837)
  %4839 = shufflevector <16 x i16> %4834, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4840 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4839)
  %4841 = shufflevector <16 x i16> %4834, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4842 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4841)
  %4843 = add nsw <4 x i32> %4836, %convolved944.sroa.2336.1.us
  %4844 = add nsw <4 x i32> %4838, %convolved944.sroa.2392.1.us
  %4845 = add nsw <4 x i32> %4840, %convolved944.sroa.2448.1.us
  %4846 = add nsw <4 x i32> %4842, %convolved944.sroa.2504.1.us
  %4847 = add nsw i64 %4644, %4630
  %4848 = shl nsw i64 %4847, 4
  %4849 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4848
  %4850 = bitcast i8* %4849 to <16 x i8>*
  %4851 = load <16 x i8>, <16 x i8>* %4850, align 16, !tbaa !438
  %4852 = zext <16 x i8> %4851 to <16 x i16>
  %4853 = shufflevector <16 x i16> %4852, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4854 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4853)
  %4855 = shufflevector <16 x i16> %4852, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4856 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4855)
  %4857 = shufflevector <16 x i16> %4852, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4858 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4857)
  %4859 = shufflevector <16 x i16> %4852, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4860 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4859)
  %4861 = add nsw <4 x i32> %4854, %convolved944.sroa.2560.1.us
  %4862 = add nsw <4 x i32> %4856, %convolved944.sroa.2616.1.us
  %4863 = add nsw <4 x i32> %4858, %convolved944.sroa.2672.1.us
  %4864 = add nsw <4 x i32> %4860, %convolved944.sroa.2728.1.us
  %4865 = add nsw i64 %4644, %4631
  %4866 = shl nsw i64 %4865, 4
  %4867 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4866
  %4868 = bitcast i8* %4867 to <16 x i8>*
  %4869 = load <16 x i8>, <16 x i8>* %4868, align 16, !tbaa !438
  %4870 = zext <16 x i8> %4869 to <16 x i16>
  %4871 = shufflevector <16 x i16> %4870, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4872 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4871)
  %4873 = shufflevector <16 x i16> %4870, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4874 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4873)
  %4875 = shufflevector <16 x i16> %4870, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4876 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4875)
  %4877 = shufflevector <16 x i16> %4870, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4878 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4877)
  %4879 = add nsw <4 x i32> %4872, %convolved944.sroa.2784.1.us
  %4880 = add nsw <4 x i32> %4874, %convolved944.sroa.2840.1.us
  %4881 = add nsw <4 x i32> %4876, %convolved944.sroa.2896.1.us
  %4882 = add nsw <4 x i32> %4878, %convolved944.sroa.2952.1.us
  %4883 = add nsw i64 %4644, %4632
  %4884 = shl nsw i64 %4883, 4
  %4885 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4884
  %4886 = bitcast i8* %4885 to <16 x i8>*
  %4887 = load <16 x i8>, <16 x i8>* %4886, align 16, !tbaa !438
  %4888 = zext <16 x i8> %4887 to <16 x i16>
  %4889 = shufflevector <16 x i16> %4888, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4890 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4889)
  %4891 = shufflevector <16 x i16> %4888, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4892 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4891)
  %4893 = shufflevector <16 x i16> %4888, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4894 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4893)
  %4895 = shufflevector <16 x i16> %4888, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4896 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4895)
  %4897 = add nsw <4 x i32> %4890, %convolved944.sroa.3008.1.us
  %4898 = add nsw <4 x i32> %4892, %convolved944.sroa.3064.1.us
  %4899 = add nsw <4 x i32> %4894, %convolved944.sroa.3120.1.us
  %4900 = add nsw <4 x i32> %4896, %convolved944.sroa.3176.1.us
  %4901 = add nsw i64 %4644, %4633
  %4902 = shl nsw i64 %4901, 4
  %4903 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4902
  %4904 = bitcast i8* %4903 to <16 x i8>*
  %4905 = load <16 x i8>, <16 x i8>* %4904, align 16, !tbaa !438
  %4906 = zext <16 x i8> %4905 to <16 x i16>
  %4907 = shufflevector <16 x i16> %4906, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4908 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4907)
  %4909 = shufflevector <16 x i16> %4906, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4910 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4909)
  %4911 = shufflevector <16 x i16> %4906, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4912 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4911)
  %4913 = shufflevector <16 x i16> %4906, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4914 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4913)
  %4915 = add nsw <4 x i32> %4908, %convolved944.sroa.3232.1.us
  %4916 = add nsw <4 x i32> %4910, %convolved944.sroa.3288.1.us
  %4917 = add nsw <4 x i32> %4912, %convolved944.sroa.3344.1.us
  %4918 = add nsw <4 x i32> %4914, %convolved944.sroa.3400.1.us
  %4919 = add nsw i64 %4644, %4634
  %4920 = shl nsw i64 %4919, 4
  %4921 = getelementptr inbounds i8, i8* %resampled_input52, i64 %4920
  %4922 = bitcast i8* %4921 to <16 x i8>*
  %4923 = load <16 x i8>, <16 x i8>* %4922, align 16, !tbaa !438
  %4924 = zext <16 x i8> %4923 to <16 x i16>
  %4925 = shufflevector <16 x i16> %4924, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4926 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4651, <4 x i16> %4925)
  %4927 = shufflevector <16 x i16> %4924, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4928 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4654, <4 x i16> %4927)
  %4929 = shufflevector <16 x i16> %4924, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %4930 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4657, <4 x i16> %4929)
  %4931 = shufflevector <16 x i16> %4924, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %4932 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %4660, <4 x i16> %4931)
  %4933 = add nsw <4 x i32> %4926, %convolved944.sroa.3456.1.us
  %4934 = add nsw <4 x i32> %4928, %convolved944.sroa.3512.1.us
  %4935 = add nsw <4 x i32> %4930, %convolved944.sroa.3568.1.us
  %4936 = add nsw <4 x i32> %4932, %convolved944.sroa.3624.1.us
  %indvars.iv.next6346 = add nuw nsw i64 %indvars.iv6345, 1
  %.not2903.us = icmp eq i64 %indvars.iv.next6346, %1603
  br i1 %.not2903.us, label %"end for convolved.s1.r19$x84.loopexit.us", label %"for convolved.s1.r19$x83.us"

"end for convolved.s1.r19$x84.loopexit.us":       ; preds = %"for convolved.s1.r19$x83.us"
  %indvars.iv.next6348 = add nuw nsw i64 %indvars.iv6347, 1
  %4937 = add nuw nsw i32 %"convolved.s1.r19$y82.us", 1
  %.not2902.us = icmp eq i64 %indvars.iv.next6348, %1606
  br i1 %.not2902.us, label %"consume convolved86", label %"for convolved.s1.r19$y80.us"

"consume convolved86":                            ; preds = %"end for convolved.s1.r19$x84.loopexit.us", %"for convolved.s1.r19$y80.preheader", %next_bb79, %then_bb78
  %convolved944.sroa.3624.3 = phi <4 x i32> [ %4604, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4936, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3568.3 = phi <4 x i32> [ %4603, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4935, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3512.3 = phi <4 x i32> [ %4602, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4934, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3456.3 = phi <4 x i32> [ %4601, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4933, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3400.3 = phi <4 x i32> [ %4586, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4918, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3344.3 = phi <4 x i32> [ %4585, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4917, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3288.3 = phi <4 x i32> [ %4584, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4916, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3232.3 = phi <4 x i32> [ %4583, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4915, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3176.3 = phi <4 x i32> [ %4568, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4900, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3120.3 = phi <4 x i32> [ %4567, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4899, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3064.3 = phi <4 x i32> [ %4566, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4898, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.3008.3 = phi <4 x i32> [ %4565, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4897, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2952.3 = phi <4 x i32> [ %4550, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4882, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2896.3 = phi <4 x i32> [ %4549, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4881, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2840.3 = phi <4 x i32> [ %4548, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4880, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2784.3 = phi <4 x i32> [ %4547, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4879, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2728.3 = phi <4 x i32> [ %4532, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4864, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2672.3 = phi <4 x i32> [ %4531, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4863, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2616.3 = phi <4 x i32> [ %4530, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4862, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2560.3 = phi <4 x i32> [ %4529, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4861, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2504.3 = phi <4 x i32> [ %4514, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4846, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2448.3 = phi <4 x i32> [ %4513, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4845, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2392.3 = phi <4 x i32> [ %4512, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4844, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2336.3 = phi <4 x i32> [ %4511, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4843, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2280.3 = phi <4 x i32> [ %4496, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4828, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2224.3 = phi <4 x i32> [ %4495, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4827, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2168.3 = phi <4 x i32> [ %4494, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4826, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2112.3 = phi <4 x i32> [ %4493, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4825, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2056.3 = phi <4 x i32> [ %4478, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4810, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.2000.3 = phi <4 x i32> [ %4477, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4809, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1944.3 = phi <4 x i32> [ %4476, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4808, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1888.3 = phi <4 x i32> [ %4475, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4807, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1832.3 = phi <4 x i32> [ %4460, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4792, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1776.3 = phi <4 x i32> [ %4459, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4791, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1720.3 = phi <4 x i32> [ %4458, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4790, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1664.3 = phi <4 x i32> [ %4457, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4789, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1608.3 = phi <4 x i32> [ %4442, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4774, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1552.3 = phi <4 x i32> [ %4441, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4773, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1496.3 = phi <4 x i32> [ %4440, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4772, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1440.3 = phi <4 x i32> [ %4439, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4771, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1384.3 = phi <4 x i32> [ %4424, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4756, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1328.3 = phi <4 x i32> [ %4423, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4755, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1272.3 = phi <4 x i32> [ %4422, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4754, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1216.3 = phi <4 x i32> [ %4421, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4753, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1160.3 = phi <4 x i32> [ %4406, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4738, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1104.3 = phi <4 x i32> [ %4405, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4737, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.1048.3 = phi <4 x i32> [ %4404, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4736, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.992.3 = phi <4 x i32> [ %4403, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4735, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.930.3 = phi <4 x i32> [ %4388, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4720, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.868.3 = phi <4 x i32> [ %4387, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4719, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.806.3 = phi <4 x i32> [ %4386, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4718, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.744.3 = phi <4 x i32> [ %4385, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4717, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.682.3 = phi <4 x i32> [ %4370, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4702, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.620.3 = phi <4 x i32> [ %4369, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4701, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.558.3 = phi <4 x i32> [ %4368, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4700, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.496.3 = phi <4 x i32> [ %4367, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4699, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.434.3 = phi <4 x i32> [ %4352, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4684, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.372.3 = phi <4 x i32> [ %4351, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4683, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.310.3 = phi <4 x i32> [ %4350, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4682, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.248.3 = phi <4 x i32> [ %4349, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4681, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.186.3 = phi <4 x i32> [ %4334, %then_bb78 ], [ %1885, %next_bb79 ], [ %1885, %"for convolved.s1.r19$y80.preheader" ], [ %4666, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.124.3 = phi <4 x i32> [ %4333, %then_bb78 ], [ %1884, %next_bb79 ], [ %1884, %"for convolved.s1.r19$y80.preheader" ], [ %4665, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.62.3 = phi <4 x i32> [ %4332, %then_bb78 ], [ %1883, %next_bb79 ], [ %1883, %"for convolved.s1.r19$y80.preheader" ], [ %4664, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %convolved944.sroa.0.3 = phi <4 x i32> [ %4331, %then_bb78 ], [ %1882, %next_bb79 ], [ %1882, %"for convolved.s1.r19$y80.preheader" ], [ %4663, %"end for convolved.s1.r19$x84.loopexit.us" ]
  %4938 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.0.3, <4 x i32> %1590)
  %4939 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.62.3, <4 x i32> %1590)
  %4940 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.124.3, <4 x i32> %1590)
  %4941 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.186.3, <4 x i32> %1590)
  %4942 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4938, <4 x i32> %1593)
  %4943 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4939, <4 x i32> %1593)
  %4944 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4940, <4 x i32> %1593)
  %4945 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4941, <4 x i32> %1593)
  %4946 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4942)
  %4947 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4943)
  %4948 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4944)
  %4949 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4945)
  %4950 = shufflevector <4 x i16> %4946, <4 x i16> %4947, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4951 = shufflevector <4 x i16> %4948, <4 x i16> %4949, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4952 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %4950, <8 x i16> %1596)
  %4953 = shufflevector <16 x i16> %4951, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %4954 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %4953, <8 x i16> %1596)
  %4955 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %4952)
  %4956 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %4954)
  %4957 = shufflevector <8 x i8> %4955, <8 x i8> %4956, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %4958 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %4957, <16 x i8> %1598)
  %4959 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %4958, <16 x i8> %1600)
  %4960 = shl nuw nsw i64 %indvars.iv6351, 2
  %4961 = add nsw i64 %4960, %1587
  %4962 = mul nsw i64 %4961, %1601
  %4963 = add nsw i64 %4962, %1936
  %4964 = getelementptr inbounds i8, i8* %19, i64 %4963
  %4965 = bitcast i8* %4964 to <16 x i8>*
  store <16 x i8> %4959, <16 x i8>* %4965, align 1, !tbaa !515
  %4966 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.248.3, <4 x i32> %1590)
  %4967 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.310.3, <4 x i32> %1590)
  %4968 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.372.3, <4 x i32> %1590)
  %4969 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.434.3, <4 x i32> %1590)
  %4970 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4966, <4 x i32> %1593)
  %4971 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4967, <4 x i32> %1593)
  %4972 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4968, <4 x i32> %1593)
  %4973 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4969, <4 x i32> %1593)
  %4974 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4970)
  %4975 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4971)
  %4976 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4972)
  %4977 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4973)
  %4978 = shufflevector <4 x i16> %4974, <4 x i16> %4975, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4979 = shufflevector <4 x i16> %4976, <4 x i16> %4977, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4980 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %4978, <8 x i16> %1596)
  %4981 = shufflevector <16 x i16> %4979, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %4982 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %4981, <8 x i16> %1596)
  %4983 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %4980)
  %4984 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %4982)
  %4985 = shufflevector <8 x i8> %4983, <8 x i8> %4984, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %4986 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %4985, <16 x i8> %1598)
  %4987 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %4986, <16 x i8> %1600)
  %4988 = add nsw i64 %4961, 1
  %4989 = mul nsw i64 %4988, %1601
  %4990 = add nsw i64 %4989, %1936
  %4991 = getelementptr inbounds i8, i8* %19, i64 %4990
  %4992 = bitcast i8* %4991 to <16 x i8>*
  store <16 x i8> %4987, <16 x i8>* %4992, align 1, !tbaa !515
  %4993 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.496.3, <4 x i32> %1590)
  %4994 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.558.3, <4 x i32> %1590)
  %4995 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.620.3, <4 x i32> %1590)
  %4996 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.682.3, <4 x i32> %1590)
  %4997 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4993, <4 x i32> %1593)
  %4998 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4994, <4 x i32> %1593)
  %4999 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4995, <4 x i32> %1593)
  %5000 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %4996, <4 x i32> %1593)
  %5001 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4997)
  %5002 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4998)
  %5003 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %4999)
  %5004 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5000)
  %5005 = shufflevector <4 x i16> %5001, <4 x i16> %5002, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5006 = shufflevector <4 x i16> %5003, <4 x i16> %5004, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5007 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5005, <8 x i16> %1596)
  %5008 = shufflevector <16 x i16> %5006, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5009 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5008, <8 x i16> %1596)
  %5010 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5007)
  %5011 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5009)
  %5012 = shufflevector <8 x i8> %5010, <8 x i8> %5011, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5013 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5012, <16 x i8> %1598)
  %5014 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5013, <16 x i8> %1600)
  %5015 = add nsw i64 %4961, 2
  %5016 = mul nsw i64 %5015, %1601
  %5017 = add nsw i64 %5016, %1936
  %5018 = getelementptr inbounds i8, i8* %19, i64 %5017
  %5019 = bitcast i8* %5018 to <16 x i8>*
  store <16 x i8> %5014, <16 x i8>* %5019, align 1, !tbaa !515
  %5020 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.744.3, <4 x i32> %1590)
  %5021 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.806.3, <4 x i32> %1590)
  %5022 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.868.3, <4 x i32> %1590)
  %5023 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.930.3, <4 x i32> %1590)
  %5024 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5020, <4 x i32> %1593)
  %5025 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5021, <4 x i32> %1593)
  %5026 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5022, <4 x i32> %1593)
  %5027 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5023, <4 x i32> %1593)
  %5028 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5024)
  %5029 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5025)
  %5030 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5026)
  %5031 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5027)
  %5032 = shufflevector <4 x i16> %5028, <4 x i16> %5029, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5033 = shufflevector <4 x i16> %5030, <4 x i16> %5031, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5034 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5032, <8 x i16> %1596)
  %5035 = shufflevector <16 x i16> %5033, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5036 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5035, <8 x i16> %1596)
  %5037 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5034)
  %5038 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5036)
  %5039 = shufflevector <8 x i8> %5037, <8 x i8> %5038, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5040 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5039, <16 x i8> %1598)
  %5041 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5040, <16 x i8> %1600)
  %5042 = add nsw i64 %4961, 3
  %5043 = mul nsw i64 %5042, %1601
  %5044 = add nsw i64 %5043, %1936
  %5045 = getelementptr inbounds i8, i8* %19, i64 %5044
  %5046 = bitcast i8* %5045 to <16 x i8>*
  store <16 x i8> %5041, <16 x i8>* %5046, align 1, !tbaa !515
  %5047 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.992.3, <4 x i32> %1590)
  %5048 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1048.3, <4 x i32> %1590)
  %5049 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1104.3, <4 x i32> %1590)
  %5050 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1160.3, <4 x i32> %1590)
  %5051 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5047, <4 x i32> %1593)
  %5052 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5048, <4 x i32> %1593)
  %5053 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5049, <4 x i32> %1593)
  %5054 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5050, <4 x i32> %1593)
  %5055 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5051)
  %5056 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5052)
  %5057 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5053)
  %5058 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5054)
  %5059 = shufflevector <4 x i16> %5055, <4 x i16> %5056, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5060 = shufflevector <4 x i16> %5057, <4 x i16> %5058, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5061 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5059, <8 x i16> %1596)
  %5062 = shufflevector <16 x i16> %5060, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5063 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5062, <8 x i16> %1596)
  %5064 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5061)
  %5065 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5063)
  %5066 = shufflevector <8 x i8> %5064, <8 x i8> %5065, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5067 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5066, <16 x i8> %1598)
  %5068 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5067, <16 x i8> %1600)
  %5069 = add nsw i64 %4962, %1937
  %5070 = getelementptr inbounds i8, i8* %19, i64 %5069
  %5071 = bitcast i8* %5070 to <16 x i8>*
  store <16 x i8> %5068, <16 x i8>* %5071, align 1, !tbaa !515
  %5072 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1216.3, <4 x i32> %1590)
  %5073 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1272.3, <4 x i32> %1590)
  %5074 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1328.3, <4 x i32> %1590)
  %5075 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1384.3, <4 x i32> %1590)
  %5076 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5072, <4 x i32> %1593)
  %5077 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5073, <4 x i32> %1593)
  %5078 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5074, <4 x i32> %1593)
  %5079 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5075, <4 x i32> %1593)
  %5080 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5076)
  %5081 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5077)
  %5082 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5078)
  %5083 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5079)
  %5084 = shufflevector <4 x i16> %5080, <4 x i16> %5081, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5085 = shufflevector <4 x i16> %5082, <4 x i16> %5083, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5086 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5084, <8 x i16> %1596)
  %5087 = shufflevector <16 x i16> %5085, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5088 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5087, <8 x i16> %1596)
  %5089 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5086)
  %5090 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5088)
  %5091 = shufflevector <8 x i8> %5089, <8 x i8> %5090, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5092 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5091, <16 x i8> %1598)
  %5093 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5092, <16 x i8> %1600)
  %5094 = add nsw i64 %4989, %1937
  %5095 = getelementptr inbounds i8, i8* %19, i64 %5094
  %5096 = bitcast i8* %5095 to <16 x i8>*
  store <16 x i8> %5093, <16 x i8>* %5096, align 1, !tbaa !515
  %5097 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1440.3, <4 x i32> %1590)
  %5098 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1496.3, <4 x i32> %1590)
  %5099 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1552.3, <4 x i32> %1590)
  %5100 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1608.3, <4 x i32> %1590)
  %5101 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5097, <4 x i32> %1593)
  %5102 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5098, <4 x i32> %1593)
  %5103 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5099, <4 x i32> %1593)
  %5104 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5100, <4 x i32> %1593)
  %5105 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5101)
  %5106 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5102)
  %5107 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5103)
  %5108 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5104)
  %5109 = shufflevector <4 x i16> %5105, <4 x i16> %5106, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5110 = shufflevector <4 x i16> %5107, <4 x i16> %5108, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5111 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5109, <8 x i16> %1596)
  %5112 = shufflevector <16 x i16> %5110, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5113 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5112, <8 x i16> %1596)
  %5114 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5111)
  %5115 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5113)
  %5116 = shufflevector <8 x i8> %5114, <8 x i8> %5115, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5117 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5116, <16 x i8> %1598)
  %5118 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5117, <16 x i8> %1600)
  %5119 = add nsw i64 %5016, %1937
  %5120 = getelementptr inbounds i8, i8* %19, i64 %5119
  %5121 = bitcast i8* %5120 to <16 x i8>*
  store <16 x i8> %5118, <16 x i8>* %5121, align 1, !tbaa !515
  %5122 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1664.3, <4 x i32> %1590)
  %5123 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1720.3, <4 x i32> %1590)
  %5124 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1776.3, <4 x i32> %1590)
  %5125 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1832.3, <4 x i32> %1590)
  %5126 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5122, <4 x i32> %1593)
  %5127 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5123, <4 x i32> %1593)
  %5128 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5124, <4 x i32> %1593)
  %5129 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5125, <4 x i32> %1593)
  %5130 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5126)
  %5131 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5127)
  %5132 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5128)
  %5133 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5129)
  %5134 = shufflevector <4 x i16> %5130, <4 x i16> %5131, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5135 = shufflevector <4 x i16> %5132, <4 x i16> %5133, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5136 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5134, <8 x i16> %1596)
  %5137 = shufflevector <16 x i16> %5135, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5138 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5137, <8 x i16> %1596)
  %5139 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5136)
  %5140 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5138)
  %5141 = shufflevector <8 x i8> %5139, <8 x i8> %5140, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5142 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5141, <16 x i8> %1598)
  %5143 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5142, <16 x i8> %1600)
  %5144 = add nsw i64 %5043, %1937
  %5145 = getelementptr inbounds i8, i8* %19, i64 %5144
  %5146 = bitcast i8* %5145 to <16 x i8>*
  store <16 x i8> %5143, <16 x i8>* %5146, align 1, !tbaa !515
  %5147 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1888.3, <4 x i32> %1590)
  %5148 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1944.3, <4 x i32> %1590)
  %5149 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2000.3, <4 x i32> %1590)
  %5150 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2056.3, <4 x i32> %1590)
  %5151 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5147, <4 x i32> %1593)
  %5152 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5148, <4 x i32> %1593)
  %5153 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5149, <4 x i32> %1593)
  %5154 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5150, <4 x i32> %1593)
  %5155 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5151)
  %5156 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5152)
  %5157 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5153)
  %5158 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5154)
  %5159 = shufflevector <4 x i16> %5155, <4 x i16> %5156, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5160 = shufflevector <4 x i16> %5157, <4 x i16> %5158, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5161 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5159, <8 x i16> %1596)
  %5162 = shufflevector <16 x i16> %5160, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5163 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5162, <8 x i16> %1596)
  %5164 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5161)
  %5165 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5163)
  %5166 = shufflevector <8 x i8> %5164, <8 x i8> %5165, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5167 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5166, <16 x i8> %1598)
  %5168 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5167, <16 x i8> %1600)
  %5169 = add nsw i64 %4962, %1938
  %5170 = getelementptr inbounds i8, i8* %19, i64 %5169
  %5171 = bitcast i8* %5170 to <16 x i8>*
  store <16 x i8> %5168, <16 x i8>* %5171, align 1, !tbaa !515
  %5172 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2112.3, <4 x i32> %1590)
  %5173 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2168.3, <4 x i32> %1590)
  %5174 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2224.3, <4 x i32> %1590)
  %5175 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2280.3, <4 x i32> %1590)
  %5176 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5172, <4 x i32> %1593)
  %5177 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5173, <4 x i32> %1593)
  %5178 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5174, <4 x i32> %1593)
  %5179 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5175, <4 x i32> %1593)
  %5180 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5176)
  %5181 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5177)
  %5182 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5178)
  %5183 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5179)
  %5184 = shufflevector <4 x i16> %5180, <4 x i16> %5181, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5185 = shufflevector <4 x i16> %5182, <4 x i16> %5183, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5186 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5184, <8 x i16> %1596)
  %5187 = shufflevector <16 x i16> %5185, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5188 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5187, <8 x i16> %1596)
  %5189 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5186)
  %5190 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5188)
  %5191 = shufflevector <8 x i8> %5189, <8 x i8> %5190, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5192 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5191, <16 x i8> %1598)
  %5193 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5192, <16 x i8> %1600)
  %5194 = add nsw i64 %4989, %1938
  %5195 = getelementptr inbounds i8, i8* %19, i64 %5194
  %5196 = bitcast i8* %5195 to <16 x i8>*
  store <16 x i8> %5193, <16 x i8>* %5196, align 1, !tbaa !515
  %5197 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2336.3, <4 x i32> %1590)
  %5198 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2392.3, <4 x i32> %1590)
  %5199 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2448.3, <4 x i32> %1590)
  %5200 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2504.3, <4 x i32> %1590)
  %5201 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5197, <4 x i32> %1593)
  %5202 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5198, <4 x i32> %1593)
  %5203 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5199, <4 x i32> %1593)
  %5204 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5200, <4 x i32> %1593)
  %5205 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5201)
  %5206 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5202)
  %5207 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5203)
  %5208 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5204)
  %5209 = shufflevector <4 x i16> %5205, <4 x i16> %5206, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5210 = shufflevector <4 x i16> %5207, <4 x i16> %5208, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5211 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5209, <8 x i16> %1596)
  %5212 = shufflevector <16 x i16> %5210, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5213 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5212, <8 x i16> %1596)
  %5214 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5211)
  %5215 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5213)
  %5216 = shufflevector <8 x i8> %5214, <8 x i8> %5215, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5217 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5216, <16 x i8> %1598)
  %5218 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5217, <16 x i8> %1600)
  %5219 = add nsw i64 %5016, %1938
  %5220 = getelementptr inbounds i8, i8* %19, i64 %5219
  %5221 = bitcast i8* %5220 to <16 x i8>*
  store <16 x i8> %5218, <16 x i8>* %5221, align 1, !tbaa !515
  %5222 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2560.3, <4 x i32> %1590)
  %5223 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2616.3, <4 x i32> %1590)
  %5224 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2672.3, <4 x i32> %1590)
  %5225 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2728.3, <4 x i32> %1590)
  %5226 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5222, <4 x i32> %1593)
  %5227 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5223, <4 x i32> %1593)
  %5228 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5224, <4 x i32> %1593)
  %5229 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5225, <4 x i32> %1593)
  %5230 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5226)
  %5231 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5227)
  %5232 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5228)
  %5233 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5229)
  %5234 = shufflevector <4 x i16> %5230, <4 x i16> %5231, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5235 = shufflevector <4 x i16> %5232, <4 x i16> %5233, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5236 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5234, <8 x i16> %1596)
  %5237 = shufflevector <16 x i16> %5235, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5238 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5237, <8 x i16> %1596)
  %5239 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5236)
  %5240 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5238)
  %5241 = shufflevector <8 x i8> %5239, <8 x i8> %5240, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5242 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5241, <16 x i8> %1598)
  %5243 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5242, <16 x i8> %1600)
  %5244 = add nsw i64 %5043, %1938
  %5245 = getelementptr inbounds i8, i8* %19, i64 %5244
  %5246 = bitcast i8* %5245 to <16 x i8>*
  store <16 x i8> %5243, <16 x i8>* %5246, align 1, !tbaa !515
  %5247 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2784.3, <4 x i32> %1590)
  %5248 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2840.3, <4 x i32> %1590)
  %5249 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2896.3, <4 x i32> %1590)
  %5250 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2952.3, <4 x i32> %1590)
  %5251 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5247, <4 x i32> %1593)
  %5252 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5248, <4 x i32> %1593)
  %5253 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5249, <4 x i32> %1593)
  %5254 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5250, <4 x i32> %1593)
  %5255 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5251)
  %5256 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5252)
  %5257 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5253)
  %5258 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5254)
  %5259 = shufflevector <4 x i16> %5255, <4 x i16> %5256, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5260 = shufflevector <4 x i16> %5257, <4 x i16> %5258, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5261 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5259, <8 x i16> %1596)
  %5262 = shufflevector <16 x i16> %5260, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5263 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5262, <8 x i16> %1596)
  %5264 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5261)
  %5265 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5263)
  %5266 = shufflevector <8 x i8> %5264, <8 x i8> %5265, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5267 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5266, <16 x i8> %1598)
  %5268 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5267, <16 x i8> %1600)
  %5269 = add nsw i64 %4962, %1939
  %5270 = getelementptr inbounds i8, i8* %19, i64 %5269
  %5271 = bitcast i8* %5270 to <16 x i8>*
  store <16 x i8> %5268, <16 x i8>* %5271, align 1, !tbaa !515
  %5272 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3008.3, <4 x i32> %1590)
  %5273 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3064.3, <4 x i32> %1590)
  %5274 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3120.3, <4 x i32> %1590)
  %5275 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3176.3, <4 x i32> %1590)
  %5276 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5272, <4 x i32> %1593)
  %5277 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5273, <4 x i32> %1593)
  %5278 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5274, <4 x i32> %1593)
  %5279 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5275, <4 x i32> %1593)
  %5280 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5276)
  %5281 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5277)
  %5282 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5278)
  %5283 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5279)
  %5284 = shufflevector <4 x i16> %5280, <4 x i16> %5281, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5285 = shufflevector <4 x i16> %5282, <4 x i16> %5283, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5286 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5284, <8 x i16> %1596)
  %5287 = shufflevector <16 x i16> %5285, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5288 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5287, <8 x i16> %1596)
  %5289 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5286)
  %5290 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5288)
  %5291 = shufflevector <8 x i8> %5289, <8 x i8> %5290, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5292 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5291, <16 x i8> %1598)
  %5293 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5292, <16 x i8> %1600)
  %5294 = add nsw i64 %4989, %1939
  %5295 = getelementptr inbounds i8, i8* %19, i64 %5294
  %5296 = bitcast i8* %5295 to <16 x i8>*
  store <16 x i8> %5293, <16 x i8>* %5296, align 1, !tbaa !515
  %5297 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3232.3, <4 x i32> %1590)
  %5298 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3288.3, <4 x i32> %1590)
  %5299 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3344.3, <4 x i32> %1590)
  %5300 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3400.3, <4 x i32> %1590)
  %5301 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5297, <4 x i32> %1593)
  %5302 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5298, <4 x i32> %1593)
  %5303 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5299, <4 x i32> %1593)
  %5304 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5300, <4 x i32> %1593)
  %5305 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5301)
  %5306 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5302)
  %5307 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5303)
  %5308 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5304)
  %5309 = shufflevector <4 x i16> %5305, <4 x i16> %5306, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5310 = shufflevector <4 x i16> %5307, <4 x i16> %5308, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5311 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5309, <8 x i16> %1596)
  %5312 = shufflevector <16 x i16> %5310, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5313 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5312, <8 x i16> %1596)
  %5314 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5311)
  %5315 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5313)
  %5316 = shufflevector <8 x i8> %5314, <8 x i8> %5315, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5317 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5316, <16 x i8> %1598)
  %5318 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5317, <16 x i8> %1600)
  %5319 = add nsw i64 %5016, %1939
  %5320 = getelementptr inbounds i8, i8* %19, i64 %5319
  %5321 = bitcast i8* %5320 to <16 x i8>*
  store <16 x i8> %5318, <16 x i8>* %5321, align 1, !tbaa !515
  %5322 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3456.3, <4 x i32> %1590)
  %5323 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3512.3, <4 x i32> %1590)
  %5324 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3568.3, <4 x i32> %1590)
  %5325 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3624.3, <4 x i32> %1590)
  %5326 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5322, <4 x i32> %1593)
  %5327 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5323, <4 x i32> %1593)
  %5328 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5324, <4 x i32> %1593)
  %5329 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %5325, <4 x i32> %1593)
  %5330 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5326)
  %5331 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5327)
  %5332 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5328)
  %5333 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %5329)
  %5334 = shufflevector <4 x i16> %5330, <4 x i16> %5331, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5335 = shufflevector <4 x i16> %5332, <4 x i16> %5333, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5336 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5334, <8 x i16> %1596)
  %5337 = shufflevector <16 x i16> %5335, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5338 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %5337, <8 x i16> %1596)
  %5339 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5336)
  %5340 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %5338)
  %5341 = shufflevector <8 x i8> %5339, <8 x i8> %5340, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5342 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %5341, <16 x i8> %1598)
  %5343 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %5342, <16 x i8> %1600)
  %5344 = add nsw i64 %5043, %1939
  %5345 = getelementptr inbounds i8, i8* %19, i64 %5344
  %5346 = bitcast i8* %5345 to <16 x i8>*
  store <16 x i8> %5343, <16 x i8>* %5346, align 1, !tbaa !515
  %indvars.iv.next6352 = add nuw nsw i64 %indvars.iv6351, 1
  %.not2901 = icmp eq i64 %indvars.iv.next6352, %1614
  br i1 %.not2901, label %"end for output.s0.x.xo74", label %"for output.s0.x.xo73"

if.end.i3133:                                     ; preds = %next_bb19
  %5347 = icmp eq i32 %6, 3
  %5348 = icmp eq i32 %8, 3
  %5349 = and i1 %5347, %5348
  %5350 = icmp sgt i32 %6, 0
  %5351 = select i1 %5350, i32 %6, i32 0
  %t2670 = zext i32 %5351 to i64
  %5352 = icmp sgt i32 %8, 0
  %5353 = select i1 %5352, i32 %8, i32 0
  %t2671 = zext i32 %5353 to i64
  %5354 = shl nuw nsw i64 %t2670, 5
  %5355 = mul i64 %5354, %t2671
  %5356 = or i64 %5355, 6
  %5357 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8*
  store i64 %5356, i64* %.fca.2.gep, align 8, !tbaa !389
  %cmp7.i3132 = icmp ugt i64 %5356, 16384
  br i1 %cmp7.i3132, label %pseudostack_alloc.exit3138, label %pseudostack_alloc.exit3138.thread

pseudostack_alloc.exit3138.thread:                ; preds = %if.end.i3133
  store i64 %5356, i64* %.fca.1.gep, align 8, !tbaa !385
  br label %then_bb90

pseudostack_alloc.exit3138:                       ; preds = %if.end.i3133
  %call.i3134 = tail call i8* @halide_malloc(i8* null, i64 %5356) #15
  store i8* %call.i3134, i8** %.fca.0.gep, align 8, !tbaa !387
  store i64 %5356, i64* %.fca.1.gep, align 8, !tbaa !385
  %5358 = bitcast i8* %call.i3134 to i16*
  %.not2801 = icmp eq i8* %call.i3134, null
  br i1 %.not2801, label %then_bb90, label %"produce filter_zeroed92", !prof !390

then_bb90:                                        ; preds = %pseudostack_alloc.exit3138.thread, %pseudostack_alloc.exit3138
  %5359 = alloca i8*, i64 %5356, align 16
  %5360 = bitcast i8** %5359 to i16*
  %5361 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8***
  store i8** %5359, i8*** %5361, align 8
  br label %"produce filter_zeroed92"

"produce filter_zeroed92":                        ; preds = %pseudostack_alloc.exit3138, %then_bb90
  %filter_zeroed91 = phi i16* [ %5360, %then_bb90 ], [ %5358, %pseudostack_alloc.exit3138 ]
  %t2483 = icmp sgt i32 %5, 8
  %t2482 = icmp sgt i32 %5, 7
  br i1 %5352, label %"for filter_zeroed.s0.y93.preheader", label %"produce sum_filter107", !prof !391

"for filter_zeroed.s0.y93.preheader":             ; preds = %"produce filter_zeroed92"
  %5362 = insertelement <8 x i32> undef, i32 %5, i32 0
  %5363 = shufflevector <8 x i32> %5362, <8 x i32> undef, <8 x i32> zeroinitializer
  %5364 = icmp sgt <8 x i32> %5363, <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5365 = sext i32 %7 to i64
  %5366 = insertelement <8 x i8> undef, i8 %filter_zero, i32 0
  %5367 = shufflevector <8 x i8> %5366, <8 x i8> undef, <8 x i32> zeroinitializer
  %5368 = zext <8 x i8> %5367 to <8 x i16>
  %5369 = icmp sgt <8 x i32> %5363, <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  br i1 %5350, label %"for filter_zeroed.s0.y93.us.preheader", label %"produce sum_filter107", !prof !391

"for filter_zeroed.s0.y93.us.preheader":          ; preds = %"for filter_zeroed.s0.y93.preheader"
  %5370 = zext i32 %6 to i64
  %5371 = zext i32 %6 to i64
  %5372 = sext i32 %9 to i64
  %5373 = zext i32 %8 to i64
  br label %"for filter_zeroed.s0.y93.us"

"for filter_zeroed.s0.y93.us":                    ; preds = %"for filter_zeroed.s0.y93.us.preheader", %"end for filter_zeroed.s0.x97.loopexit.us"
  %indvars.iv6289 = phi i64 [ 0, %"for filter_zeroed.s0.y93.us.preheader" ], [ %indvars.iv.next6290, %"end for filter_zeroed.s0.x97.loopexit.us" ]
  %5374 = mul nsw i64 %indvars.iv6289, %5371
  %5375 = mul nsw i64 %indvars.iv6289, %5372
  br i1 %t2482, label %"for filter_zeroed.s0.x96.preheader.split.us.us", label %"for filter_zeroed.s0.x96.us5500"

"for filter_zeroed.s0.x96.us5500":                ; preds = %"for filter_zeroed.s0.y93.us", %"for filter_zeroed.s0.x96.us5500"
  %indvars.iv = phi i64 [ %indvars.iv.next, %"for filter_zeroed.s0.x96.us5500" ], [ 0, %"for filter_zeroed.s0.y93.us" ]
  %5376 = mul nsw i64 %indvars.iv, %5365
  %5377 = add nsw i64 %5376, %5375
  %5378 = getelementptr inbounds i8, i8* %3, i64 %5377
  %5379 = bitcast i8* %5378 to <8 x i8>*
  %5380 = call <8 x i8> @llvm.masked.load.v8i8.p0v8i8(<8 x i8>* %5379, i32 1, <8 x i1> %5364, <8 x i8> undef), !tbaa !392
  %5381 = zext <8 x i8> %5380 to <8 x i16>
  %5382 = sub nsw <8 x i16> %5381, %5368
  %5383 = add nuw nsw i64 %indvars.iv, %5374
  %5384 = shl nsw i64 %5383, 4
  %5385 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 %5384
  %5386 = bitcast i16* %5385 to <8 x i16>*
  call void @llvm.masked.store.v8i16.p0v8i16(<8 x i16> %5382, <8 x i16>* %5386, i32 16, <8 x i1> %5364), !tbaa !395
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %.not2891.us5502 = icmp eq i64 %indvars.iv.next, %5370
  br i1 %.not2891.us5502, label %"end for filter_zeroed.s0.x97.loopexit.us", label %"for filter_zeroed.s0.x96.us5500"

"end for filter_zeroed.s0.x97.loopexit.us":       ; preds = %"for filter_zeroed.s0.x96.us5500", %"for filter_zeroed.s0.x96.us.us5505", %"for filter_zeroed.s0.x96.us.us.us"
  %indvars.iv.next6290 = add nuw nsw i64 %indvars.iv6289, 1
  %.not2890.us = icmp eq i64 %indvars.iv.next6290, %5373
  br i1 %.not2890.us, label %"produce sum_filter107", label %"for filter_zeroed.s0.y93.us"

"for filter_zeroed.s0.x96.preheader.split.us.us": ; preds = %"for filter_zeroed.s0.y93.us"
  br i1 %t2483, label %"for filter_zeroed.s0.x96.us.us.us", label %"for filter_zeroed.s0.x96.us.us5505"

"for filter_zeroed.s0.x96.us.us5505":             ; preds = %"for filter_zeroed.s0.x96.preheader.split.us.us", %"for filter_zeroed.s0.x96.us.us5505"
  %indvars.iv6285 = phi i64 [ %indvars.iv.next6286, %"for filter_zeroed.s0.x96.us.us5505" ], [ 0, %"for filter_zeroed.s0.x96.preheader.split.us.us" ]
  %5387 = mul nsw i64 %indvars.iv6285, %5365
  %5388 = add nsw i64 %5387, %5375
  %5389 = getelementptr inbounds i8, i8* %3, i64 %5388
  %5390 = bitcast i8* %5389 to <8 x i8>*
  %5391 = load <8 x i8>, <8 x i8>* %5390, align 1, !tbaa !392
  %5392 = zext <8 x i8> %5391 to <8 x i16>
  %5393 = sub nsw <8 x i16> %5392, %5368
  %5394 = add nuw nsw i64 %indvars.iv6285, %5374
  %5395 = shl nsw i64 %5394, 4
  %5396 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 %5395
  %5397 = bitcast i16* %5396 to <8 x i16>*
  store <8 x i16> %5393, <8 x i16>* %5397, align 16, !tbaa !395
  %indvars.iv.next6286 = add nuw nsw i64 %indvars.iv6285, 1
  %.not2891.us.us5507 = icmp eq i64 %indvars.iv.next6286, %5370
  br i1 %.not2891.us.us5507, label %"end for filter_zeroed.s0.x97.loopexit.us", label %"for filter_zeroed.s0.x96.us.us5505"

"for filter_zeroed.s0.x96.us.us.us":              ; preds = %"for filter_zeroed.s0.x96.preheader.split.us.us", %"for filter_zeroed.s0.x96.us.us.us"
  %indvars.iv6287 = phi i64 [ %indvars.iv.next6288, %"for filter_zeroed.s0.x96.us.us.us" ], [ 0, %"for filter_zeroed.s0.x96.preheader.split.us.us" ]
  %5398 = mul nsw i64 %indvars.iv6287, %5365
  %5399 = add nsw i64 %5398, %5375
  %5400 = getelementptr inbounds i8, i8* %3, i64 %5399
  %5401 = bitcast i8* %5400 to <8 x i8>*
  %5402 = load <8 x i8>, <8 x i8>* %5401, align 1, !tbaa !392
  %5403 = zext <8 x i8> %5402 to <8 x i16>
  %5404 = sub nsw <8 x i16> %5403, %5368
  %5405 = add nuw nsw i64 %indvars.iv6287, %5374
  %5406 = shl nsw i64 %5405, 4
  %5407 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 %5406
  %5408 = bitcast i16* %5407 to <8 x i16>*
  store <8 x i16> %5404, <8 x i16>* %5408, align 16, !tbaa !395
  %5409 = getelementptr inbounds i8, i8* %5400, i64 8
  %5410 = bitcast i8* %5409 to <8 x i8>*
  %5411 = call <8 x i8> @llvm.masked.load.v8i8.p0v8i8(<8 x i8>* nonnull %5410, i32 1, <8 x i1> %5369, <8 x i8> undef), !tbaa !392
  %5412 = zext <8 x i8> %5411 to <8 x i16>
  %5413 = sub nsw <8 x i16> %5412, %5368
  %5414 = getelementptr inbounds i16, i16* %5407, i64 8
  %5415 = bitcast i16* %5414 to <8 x i16>*
  call void @llvm.masked.store.v8i16.p0v8i16(<8 x i16> %5413, <8 x i16>* nonnull %5415, i32 16, <8 x i1> %5369), !tbaa !395
  %indvars.iv.next6288 = add nuw nsw i64 %indvars.iv6287, 1
  %.not2891.us.us.us = icmp eq i64 %indvars.iv.next6288, %5370
  br i1 %.not2891.us.us.us, label %"end for filter_zeroed.s0.x97.loopexit.us", label %"for filter_zeroed.s0.x96.us.us.us"

"produce sum_filter107":                          ; preds = %"end for filter_zeroed.s0.x97.loopexit.us", %"for filter_zeroed.s0.y93.preheader", %"produce filter_zeroed92"
  %5416 = insertelement <16 x i32> undef, i32 %5, i32 0
  %5417 = shufflevector <16 x i32> %5416, <16 x i32> undef, <16 x i32> zeroinitializer
  %5418 = icmp sgt <16 x i32> %5417, <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5419 = bitcast [64 x i32]* %sum_filter945 to <4 x i32>*
  %5420 = shufflevector <16 x i1> %5418, <16 x i1> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> zeroinitializer, <4 x i32>* nonnull %5419, i32 16, <4 x i1> %5420), !tbaa !441
  %5421 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 4
  %5422 = bitcast i32* %5421 to <4 x i32>*
  %5423 = shufflevector <16 x i1> %5418, <16 x i1> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> zeroinitializer, <4 x i32>* nonnull %5422, i32 16, <4 x i1> %5423), !tbaa !452
  %5424 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 8
  %5425 = bitcast i32* %5424 to <4 x i32>*
  %5426 = shufflevector <16 x i1> %5418, <16 x i1> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> zeroinitializer, <4 x i32>* nonnull %5425, i32 16, <4 x i1> %5426), !tbaa !454
  %5427 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 12
  %5428 = bitcast i32* %5427 to <4 x i32>*
  %5429 = shufflevector <16 x i1> %5418, <16 x i1> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> zeroinitializer, <4 x i32>* nonnull %5428, i32 16, <4 x i1> %5429), !tbaa !457
  br i1 %5352, label %"for sum_filter.s1.r19$y108.preheader", label %"consume sum_filter114", !prof !391

"for sum_filter.s1.r19$y108.preheader":           ; preds = %"produce sum_filter107"
  %5430 = shufflevector <16 x i1> %5418, <16 x i1> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5431 = shufflevector <16 x i1> %5418, <16 x i1> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  br i1 %5350, label %"for sum_filter.s1.r19$y108.us.preheader", label %"consume sum_filter114", !prof !391

"for sum_filter.s1.r19$y108.us.preheader":        ; preds = %"for sum_filter.s1.r19$y108.preheader"
  %5432 = zext i32 %6 to i64
  %5433 = zext i32 %6 to i64
  %5434 = zext i32 %8 to i64
  br label %"for sum_filter.s1.r19$y108.us"

"for sum_filter.s1.r19$y108.us":                  ; preds = %"for sum_filter.s1.r19$y108.us.preheader", %"end for sum_filter.s1.r19$x112.loopexit.us"
  %indvars.iv6295 = phi i64 [ 0, %"for sum_filter.s1.r19$y108.us.preheader" ], [ %indvars.iv.next6296, %"end for sum_filter.s1.r19$x112.loopexit.us" ]
  %5435 = mul nsw i64 %indvars.iv6295, %5433
  br label %"for sum_filter.s1.r19$x111.us"

"for sum_filter.s1.r19$x111.us":                  ; preds = %"for sum_filter.s1.r19$y108.us", %"for sum_filter.s1.r19$x111.us"
  %indvars.iv6293 = phi i64 [ 0, %"for sum_filter.s1.r19$y108.us" ], [ %indvars.iv.next6294, %"for sum_filter.s1.r19$x111.us" ]
  %unmaskedload2885.us = load <4 x i32>, <4 x i32>* %5419, align 16
  %5436 = select <4 x i1> %5420, <4 x i32> %unmaskedload2885.us, <4 x i32> undef
  %unmaskedload2886.us = load <4 x i32>, <4 x i32>* %5422, align 16
  %5437 = select <4 x i1> %5423, <4 x i32> %unmaskedload2886.us, <4 x i32> undef
  %unmaskedload2887.us = load <4 x i32>, <4 x i32>* %5425, align 16
  %5438 = select <4 x i1> %5426, <4 x i32> %unmaskedload2887.us, <4 x i32> undef
  %unmaskedload2888.us = load <4 x i32>, <4 x i32>* %5428, align 16
  %5439 = select <4 x i1> %5429, <4 x i32> %unmaskedload2888.us, <4 x i32> undef
  %5440 = shufflevector <4 x i32> %5436, <4 x i32> %5437, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5441 = shufflevector <4 x i32> %5438, <4 x i32> %5439, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5442 = shufflevector <8 x i32> %5440, <8 x i32> %5441, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5443 = add nuw nsw i64 %indvars.iv6293, %5435
  %5444 = shl nsw i64 %5443, 4
  %5445 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 %5444
  %5446 = bitcast i16* %5445 to <8 x i16>*
  %5447 = call <8 x i16> @llvm.masked.load.v8i16.p0v8i16(<8 x i16>* %5446, i32 16, <8 x i1> %5430, <8 x i16> undef), !tbaa !395
  %5448 = getelementptr inbounds i16, i16* %5445, i64 8
  %5449 = bitcast i16* %5448 to <8 x i16>*
  %5450 = call <8 x i16> @llvm.masked.load.v8i16.p0v8i16(<8 x i16>* nonnull %5449, i32 16, <8 x i1> %5431, <8 x i16> undef), !tbaa !395
  %5451 = shufflevector <8 x i16> %5447, <8 x i16> %5450, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5452 = sext <16 x i16> %5451 to <16 x i32>
  %5453 = add nsw <16 x i32> %5442, %5452
  %5454 = shufflevector <16 x i32> %5453, <16 x i32> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %5454, <4 x i32>* nonnull %5419, i32 16, <4 x i1> %5420), !tbaa !441
  %5455 = shufflevector <16 x i32> %5453, <16 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %5455, <4 x i32>* nonnull %5422, i32 16, <4 x i1> %5423), !tbaa !452
  %5456 = shufflevector <16 x i32> %5453, <16 x i32> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %5456, <4 x i32>* nonnull %5425, i32 16, <4 x i1> %5426), !tbaa !454
  %5457 = shufflevector <16 x i32> %5453, <16 x i32> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %5457, <4 x i32>* nonnull %5428, i32 16, <4 x i1> %5429), !tbaa !457
  %indvars.iv.next6294 = add nuw nsw i64 %indvars.iv6293, 1
  %.not2889.us = icmp eq i64 %indvars.iv.next6294, %5432
  br i1 %.not2889.us, label %"end for sum_filter.s1.r19$x112.loopexit.us", label %"for sum_filter.s1.r19$x111.us"

"end for sum_filter.s1.r19$x112.loopexit.us":     ; preds = %"for sum_filter.s1.r19$x111.us"
  %indvars.iv.next6296 = add nuw nsw i64 %indvars.iv6295, 1
  %.not2884.us = icmp eq i64 %indvars.iv.next6296, %5434
  br i1 %.not2884.us, label %"consume sum_filter114", label %"for sum_filter.s1.r19$y108.us"

"consume sum_filter114":                          ; preds = %"end for sum_filter.s1.r19$x112.loopexit.us", %"for sum_filter.s1.r19$y108.preheader", %"produce sum_filter107"
  %5458 = bitcast i8* %2 to <4 x i32>*
  %5459 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* %5458, i32 4, <4 x i1> %5420, <4 x i32> undef), !tbaa !517
  %5460 = getelementptr inbounds i8, i8* %2, i64 16
  %5461 = bitcast i8* %5460 to <4 x i32>*
  %5462 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* nonnull %5461, i32 4, <4 x i1> %5423, <4 x i32> undef), !tbaa !527
  %5463 = getelementptr inbounds i8, i8* %2, i64 32
  %5464 = bitcast i8* %5463 to <4 x i32>*
  %5465 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* nonnull %5464, i32 4, <4 x i1> %5426, <4 x i32> undef), !tbaa !529
  %5466 = getelementptr inbounds i8, i8* %2, i64 48
  %5467 = bitcast i8* %5466 to <4 x i32>*
  %5468 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* nonnull %5467, i32 4, <4 x i1> %5429, <4 x i32> undef), !tbaa !532
  %5469 = shufflevector <4 x i32> %5459, <4 x i32> %5462, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5470 = shufflevector <4 x i32> %5465, <4 x i32> %5468, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5471 = shufflevector <8 x i32> %5469, <8 x i32> %5470, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %unmaskedload2802 = load <4 x i32>, <4 x i32>* %5419, align 16
  %5472 = select <4 x i1> %5420, <4 x i32> %unmaskedload2802, <4 x i32> undef
  %unmaskedload2803 = load <4 x i32>, <4 x i32>* %5422, align 16
  %5473 = select <4 x i1> %5423, <4 x i32> %unmaskedload2803, <4 x i32> undef
  %unmaskedload2804 = load <4 x i32>, <4 x i32>* %5425, align 16
  %5474 = select <4 x i1> %5426, <4 x i32> %unmaskedload2804, <4 x i32> undef
  %unmaskedload2805 = load <4 x i32>, <4 x i32>* %5428, align 16
  %5475 = select <4 x i1> %5429, <4 x i32> %unmaskedload2805, <4 x i32> undef
  %5476 = shufflevector <4 x i32> %5472, <4 x i32> %5473, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5477 = shufflevector <4 x i32> %5474, <4 x i32> %5475, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5478 = shufflevector <8 x i32> %5476, <8 x i32> %5477, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5479 = zext i8 %input_zero to i32
  %5480 = insertelement <16 x i32> undef, i32 %5479, i32 0
  %5481 = shufflevector <16 x i32> %5480, <16 x i32> undef, <16 x i32> zeroinitializer
  %5482 = mul nsw <16 x i32> %5478, %5481
  %5483 = sub nsw <16 x i32> %5471, %5482
  %5484 = shufflevector <16 x i32> %5483, <16 x i32> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5485 = bitcast [16 x i32]* %offset_c947 to <4 x i32>*
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %5484, <4 x i32>* nonnull %5485, i32 16, <4 x i1> %5420), !tbaa !417
  %5486 = shufflevector <16 x i32> %5483, <16 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5487 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 4
  %5488 = bitcast i32* %5487 to <4 x i32>*
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %5486, <4 x i32>* nonnull %5488, i32 16, <4 x i1> %5423), !tbaa !428
  %5489 = shufflevector <16 x i32> %5483, <16 x i32> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5490 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 8
  %5491 = bitcast i32* %5490 to <4 x i32>*
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %5489, <4 x i32>* nonnull %5491, i32 16, <4 x i1> %5426), !tbaa !430
  %5492 = shufflevector <16 x i32> %5483, <16 x i32> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5493 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 12
  %5494 = bitcast i32* %5493 to <4 x i32>*
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %5492, <4 x i32>* nonnull %5494, i32 16, <4 x i1> %5429), !tbaa !433
  %t2515 = icmp slt i32 %a614, 0
  %5495 = add nsw i32 %6, -1
  %t2516 = mul nsw i32 %5495, %a614
  %t2517 = icmp slt i32 %stride_x, 0
  %5496 = add nsw i32 %22, 3
  %t2519 = and i32 %5496, -4
  %t2520 = select i1 %t2515, i32 %t2516, i32 0
  %t2521 = icmp slt i32 %a613, 0
  %5497 = add nsw i32 %8, -1
  %t2522 = mul nsw i32 %5497, %a613
  %t2523 = icmp slt i32 %stride_y, 0
  %5498 = add nsw i32 %25, 3
  %t2525 = and i32 %5498, -4
  %t2526 = select i1 %t2521, i32 %t2522, i32 0
  %a614.op2806 = shl i32 %a614, 1
  %t2528 = select i1 %t2515, i32 %a614.op2806, i32 0
  %5499 = add nsw i32 %22, -1
  %t2529 = and i32 %5499, -4
  %a613.op2807 = shl i32 %a613, 1
  %t2530 = select i1 %t2521, i32 %a613.op2807, i32 0
  %5500 = add nsw i32 %25, -1
  %t2531 = and i32 %5500, -4
  %t2506 = icmp eq i32 %depth_multiplier, 1
  %5501 = select i1 %5349, i32 %t2530, i32 %t2526
  %5502 = select i1 %t2523, i32 %t2531, i32 -3
  %5503 = add i32 %24, 3
  %5504 = add i32 %5503, %5502
  %5505 = mul nsw i32 %5504, %stride_y
  %b58 = add nsw i32 %5505, %5501
  %5506 = select i1 %5349, i32 %t2528, i32 %t2520
  %5507 = select i1 %t2517, i32 %t2529, i32 -3
  %5508 = add i32 %21, 3
  %5509 = add i32 %5508, %5507
  %5510 = mul nsw i32 %5509, %stride_x
  %b61 = add nsw i32 %5510, %5506
  %5511 = select i1 %t2523, i32 %t2525, i32 1
  %5512 = add i32 %24, -1
  %5513 = add i32 %5512, %5511
  %5514 = mul nsw i32 %5513, %stride_y
  %b60 = add nsw i32 %5514, %t2526
  %5515 = select i1 %t2521, i32 0, i32 %t2522
  %5516 = add nsw i32 %t2525, -1
  %5517 = select i1 %t2523, i32 0, i32 %5516
  %5518 = add nsw i32 %5517, %24
  %5519 = mul nsw i32 %5518, %stride_y
  %b59 = add nsw i32 %5519, %5515
  %5520 = select i1 %t2517, i32 %t2519, i32 1
  %5521 = add i32 %21, -1
  %5522 = add i32 %5521, %5520
  %5523 = mul nsw i32 %5522, %stride_x
  %b63 = add nsw i32 %5523, %t2520
  %5524 = select i1 %t2515, i32 0, i32 %t2516
  %5525 = add nsw i32 %t2519, -1
  %5526 = select i1 %t2517, i32 0, i32 %5525
  %5527 = add nsw i32 %5526, %21
  %5528 = mul nsw i32 %5527, %stride_x
  %b62 = add nsw i32 %5528, %5524
  %5529 = mul nsw i32 %15, %14
  %5530 = mul nsw i32 %18, %16
  %5531 = mul nsw i32 %13, %12
  %5532 = add i32 %5529, %5531
  %t2507 = add i32 %5532, %5530
  %5533 = icmp sgt i32 %17, 0
  br i1 %5533, label %"for output.s0.b.rebased117.preheader", label %after_bb, !prof !391

"for output.s0.b.rebased117.preheader":           ; preds = %"consume sum_filter114"
  %t2524 = ashr i32 %5498, 2
  %t2518 = ashr i32 %5496, 2
  %5534 = icmp sgt i32 %a614, 0
  %5535 = select i1 %5534, i32 %a614, i32 0
  %t2503 = shl nuw nsw i32 %5535, 1
  %5536 = icmp sgt i32 %a613, 0
  %5537 = select i1 %5536, i32 %a613, i32 0
  %t2496 = shl nuw nsw i32 %5537, 1
  %5538 = icmp slt i32 %b60, %b58
  %resampled_input.y.min_realized = select i1 %5538, i32 %b60, i32 %b58
  %5539 = select i1 %5349, i32 %t2496, i32 %5515
  %5540 = or i32 %5500, 3
  %5541 = select i1 %t2523, i32 0, i32 %5540
  %5542 = add nsw i32 %5541, %24
  %5543 = mul nsw i32 %5542, %stride_y
  %a57 = add nsw i32 %5543, %5539
  %5544 = icmp sgt i32 %a57, %b59
  %5545 = select i1 %5544, i32 %a57, i32 %b59
  %5546 = sub nsw i32 %5545, %resampled_input.y.min_realized
  %a65 = add nsw i32 %5546, 1
  %5547 = icmp slt i32 %b63, %b61
  %resampled_input.x.min_realized = select i1 %5547, i32 %b63, i32 %b61
  %5548 = select i1 %5349, i32 %t2503, i32 %5524
  %5549 = or i32 %5499, 3
  %5550 = select i1 %t2517, i32 0, i32 %5549
  %5551 = add nsw i32 %5550, %21
  %5552 = mul nsw i32 %5551, %stride_x
  %a60 = add nsw i32 %5552, %5548
  %5553 = icmp sgt i32 %a60, %b62
  %5554 = select i1 %5553, i32 %a60, i32 %b62
  %5555 = sub nsw i32 %5554, %resampled_input.x.min_realized
  %a64 = add nsw i32 %5555, 1
  %.inv2808 = icmp slt i32 %5555, 0
  %5556 = select i1 %.inv2808, i32 0, i32 %a64
  %t2672 = zext i32 %5556 to i64
  %.inv2809 = icmp slt i32 %5546, 0
  %5557 = select i1 %.inv2809, i32 0, i32 %a65
  %t2673 = zext i32 %5557 to i64
  %t2674 = shl nuw nsw i64 %t2672, 4
  %5558 = mul i64 %t2674, %t2673
  %5559 = or i64 %5558, 3
  %5560 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8***
  %t2542 = sub i32 %b63, %resampled_input.x.min_realized
  %t2540 = sub nsw i32 %b62, %b63
  %t2543 = sub i32 %b60, %resampled_input.y.min_realized
  %t2539 = sub nsw i32 %b59, %b60
  %5561 = sext i32 %b63 to i64
  %5562 = sext i32 %13 to i64
  %5563 = icmp sgt i32 %5, 1
  %5564 = icmp eq i32 %depth_multiplier, 0
  %t2676 = sext i1 %5564 to i32
  %5565 = sub nsw i32 %depth_multiplier, %t2676
  %5566 = add i32 %5565, 1
  %5567 = icmp ult i32 %5566, 3
  %5568 = select i1 %5567, i32 %5565, i32 0
  %5569 = xor i32 %t2676, -1
  %5570 = and i32 %5568, %5569
  %5571 = sext i32 %5570 to i64
  %.not5145 = icmp eq i32 %5, 2
  %5572 = icmp sgt i32 %5, 3
  %.not5146 = icmp eq i32 %5, 4
  %5573 = icmp sgt i32 %5, 5
  %.not5147 = icmp eq i32 %5, 6
  %.not5142 = icmp eq i32 %5, 9
  %5574 = icmp sgt i32 %5, 10
  %.not5143 = icmp eq i32 %5, 11
  %5575 = icmp sgt i32 %5, 12
  %.not5144 = icmp eq i32 %5, 13
  %5576 = icmp sgt i32 %5, 14
  %.neg = mul i32 %26, %24
  %.neg5148 = mul i32 %23, %21
  %.neg5149 = mul i32 %27, %16
  %reass.add = add i32 %.neg, %.neg5148
  %reass.add5151 = add i32 %reass.add, %.neg5149
  %t2551 = sub nsw i32 %a613.op2807, %resampled_input.y.min_realized
  %t2547 = sub nsw i32 %a614.op2806, %resampled_input.x.min_realized
  %5577 = icmp sgt i32 %25, 0
  %t2605 = sub nsw i32 %a614, %resampled_input.x.min_realized
  %t2606 = sub nsw i32 %a613, %resampled_input.y.min_realized
  %5578 = icmp sgt i32 %22, 0
  %5579 = sext i32 %a614 to i64
  %5580 = bitcast i16* %filter_zeroed91 to <8 x i16>*
  %5581 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 8
  %5582 = bitcast i16* %5581 to <8 x i16>*
  %5583 = sext i32 %21 to i64
  %5584 = sext i32 %stride_x to i64
  %5585 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 16
  %5586 = bitcast i16* %5585 to <8 x i16>*
  %5587 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 24
  %5588 = bitcast i16* %5587 to <8 x i16>*
  %5589 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 32
  %5590 = bitcast i16* %5589 to <8 x i16>*
  %5591 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 40
  %5592 = bitcast i16* %5591 to <8 x i16>*
  %5593 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 48
  %5594 = bitcast i16* %5593 to <8 x i16>*
  %5595 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 56
  %5596 = bitcast i16* %5595 to <8 x i16>*
  %5597 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 64
  %5598 = bitcast i16* %5597 to <8 x i16>*
  %5599 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 72
  %5600 = bitcast i16* %5599 to <8 x i16>*
  %5601 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 80
  %5602 = bitcast i16* %5601 to <8 x i16>*
  %5603 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 88
  %5604 = bitcast i16* %5603 to <8 x i16>*
  %5605 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 96
  %5606 = bitcast i16* %5605 to <8 x i16>*
  %5607 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 104
  %5608 = bitcast i16* %5607 to <8 x i16>*
  %5609 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 112
  %5610 = bitcast i16* %5609 to <8 x i16>*
  %5611 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 120
  %5612 = bitcast i16* %5611 to <8 x i16>*
  %5613 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 128
  %5614 = bitcast i16* %5613 to <8 x i16>*
  %5615 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 136
  %5616 = bitcast i16* %5615 to <8 x i16>*
  %5617 = insertelement <16 x i32> undef, i32 %output_multiplier, i32 0
  %5618 = shufflevector <16 x i32> %5617, <16 x i32> undef, <4 x i32> zeroinitializer
  %5619 = sub nsw i32 0, %output_shift
  %5620 = insertelement <16 x i32> undef, i32 %5619, i32 0
  %5621 = shufflevector <16 x i32> %5620, <16 x i32> undef, <4 x i32> zeroinitializer
  %5622 = zext i8 %output_zero to i16
  %5623 = insertelement <16 x i16> undef, i16 %5622, i32 0
  %5624 = shufflevector <16 x i16> %5623, <16 x i16> undef, <8 x i32> zeroinitializer
  %5625 = insertelement <16 x i8> undef, i8 %output_max, i32 0
  %5626 = shufflevector <16 x i8> %5625, <16 x i8> undef, <16 x i32> zeroinitializer
  %5627 = insertelement <16 x i8> undef, i8 %output_min, i32 0
  %5628 = shufflevector <16 x i8> %5627, <16 x i8> undef, <16 x i32> zeroinitializer
  %5629 = sext i32 %23 to i64
  %5630 = zext i32 %t2540 to i64
  %5631 = sext i32 %t2543 to i64
  %5632 = sext i32 %a64 to i64
  %5633 = sext i32 %t2542 to i64
  %5634 = sext i32 %b60 to i64
  %5635 = sext i32 %15 to i64
  %5636 = zext i32 %t2539 to i64
  %5637 = zext i32 %6 to i64
  %5638 = sext i32 %6 to i64
  %5639 = zext i32 %8 to i64
  %5640 = zext i32 %t2518 to i64
  %5641 = zext i32 %t2524 to i64
  %5642 = zext i32 %16 to i64
  %zext = zext i32 %17 to i64
  %5643 = or i32 %t2539, %t2540
  %5644 = icmp slt i32 %5643, 0
  %.not6878 = icmp sgt i32 %5643, -1
  br label %"for output.s0.b.rebased117"

"for output.s0.b.rebased117":                     ; preds = %"for output.s0.b.rebased117.preheader", %"end for output.s0.y.yo187"
  %indvars.iv6320 = phi i64 [ 0, %"for output.s0.b.rebased117.preheader" ], [ %indvars.iv.next6321, %"end for output.s0.y.yo187" ]
  %5645 = load i64, i64* %.fca.1.gep875, align 8, !tbaa !385
  %cmp.i3140 = icmp ult i64 %5645, %5559
  %5646 = load i8*, i8** %.fca.0.gep874, align 8, !tbaa !387
  br i1 %cmp.i3140, label %if.then.i3143, label %pseudostack_alloc.exit3156, !prof !388

if.then.i3143:                                    ; preds = %"for output.s0.b.rebased117"
  %tobool1.not.i3142 = icmp ne i8* %5646, null
  %5647 = load i64, i64* %.fca.2.gep876, align 8
  %cmp2.i3145 = icmp ugt i64 %5647, 16384
  %or.cond5129 = and i1 %tobool1.not.i3142, %cmp2.i3145
  br i1 %or.cond5129, label %if.then3.i3147, label %if.end.i3151

if.then3.i3147:                                   ; preds = %if.then.i3143
  call void @halide_free(i8* null, i8* nonnull %5646) #15
  %.pre = load i64, i64* %.fca.2.gep876, align 8, !tbaa !389
  br label %if.end.i3151

if.end.i3151:                                     ; preds = %if.then3.i3147, %if.then.i3143
  %5648 = phi i64 [ %.pre, %if.then3.i3147 ], [ %5647, %if.then.i3143 ]
  %add.i3149 = add i64 %5648, %5559
  store i64 %add.i3149, i64* %.fca.2.gep876, align 8, !tbaa !389
  %cmp7.i3150 = icmp ugt i64 %add.i3149, 16384
  br i1 %cmp7.i3150, label %if.then8.i3153, label %if.end11.i3155

if.then8.i3153:                                   ; preds = %if.end.i3151
  %call.i3152 = call i8* @halide_malloc(i8* null, i64 %5559) #15
  br label %if.end11.i3155

if.end11.i3155:                                   ; preds = %if.then8.i3153, %if.end.i3151
  %storemerge.i3154 = phi i8* [ %call.i3152, %if.then8.i3153 ], [ null, %if.end.i3151 ]
  store i8* %storemerge.i3154, i8** %.fca.0.gep874, align 8, !tbaa !387
  store i64 %5559, i64* %.fca.1.gep875, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3156

pseudostack_alloc.exit3156:                       ; preds = %"for output.s0.b.rebased117", %if.end11.i3155
  %5649 = phi i8* [ %storemerge.i3154, %if.end11.i3155 ], [ %5646, %"for output.s0.b.rebased117" ]
  %.not2810 = icmp eq i8* %5649, null
  br i1 %.not2810, label %then_bb121, label %"produce resampled_input123", !prof !390

then_bb121:                                       ; preds = %pseudostack_alloc.exit3156
  %5650 = alloca i8*, i64 %5559, align 16
  %5651 = bitcast i8** %5650 to i8*
  store i8** %5650, i8*** %5560, align 8
  br label %"produce resampled_input123"

"produce resampled_input123":                     ; preds = %pseudostack_alloc.exit3156, %then_bb121
  %resampled_input122 = phi i8* [ %5651, %then_bb121 ], [ %5649, %pseudostack_alloc.exit3156 ]
  %5652 = add nuw i64 %indvars.iv6320, %5642
  br i1 %t2506, label %then_bb125, label %next_bb126

then_bb125:                                       ; preds = %"produce resampled_input123"
  br i1 %.not6878, label %"for resampled_input.s0.y.rebased127.us.preheader", label %"consume resampled_input185", !prof !435

"for resampled_input.s0.y.rebased127.us.preheader": ; preds = %then_bb125
  %5653 = trunc i64 %5652 to i32
  %5654 = mul i32 %18, %5653
  %t2534 = sub i32 %5654, %t2507
  %5655 = sext i32 %t2534 to i64
  br label %"for resampled_input.s0.y.rebased127.us"

"for resampled_input.s0.y.rebased127.us":         ; preds = %"for resampled_input.s0.y.rebased127.us.preheader", %"end for resampled_input.s0.x.rebased131.loopexit.us"
  %indvars.iv6306 = phi i64 [ 0, %"for resampled_input.s0.y.rebased127.us.preheader" ], [ %indvars.iv.next6307, %"end for resampled_input.s0.x.rebased131.loopexit.us" ]
  %5656 = add nsw i64 %indvars.iv6306, %5631
  %5657 = mul nsw i64 %5656, %5632
  %5658 = add nsw i64 %5657, %5633
  %5659 = add nsw i64 %indvars.iv6306, %5634
  %5660 = mul nsw i64 %5659, %5635
  %5661 = add nsw i64 %5660, %5655
  br label %"for resampled_input.s0.x.rebased130.us"

"for resampled_input.s0.x.rebased130.us":         ; preds = %"for resampled_input.s0.y.rebased127.us", %"for resampled_input.s0.x.rebased130.us"
  %indvars.iv6304 = phi i64 [ 0, %"for resampled_input.s0.y.rebased127.us" ], [ %indvars.iv.next6305, %"for resampled_input.s0.x.rebased130.us" ]
  %5662 = add nsw i64 %indvars.iv6304, %5561
  %5663 = mul nsw i64 %5662, %5562
  %5664 = add nsw i64 %5663, %5661
  %5665 = getelementptr inbounds i8, i8* %10, i64 %5664
  %5666 = bitcast i8* %5665 to <16 x i8>*
  %5667 = call <16 x i8> @llvm.masked.load.v16i8.p0v16i8(<16 x i8>* %5666, i32 1, <16 x i1> %5418, <16 x i8> undef), !tbaa !436
  %5668 = add nsw i64 %indvars.iv6304, %5658
  %5669 = shl nsw i64 %5668, 4
  %5670 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5669
  %5671 = bitcast i8* %5670 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %5667, <16 x i8>* %5671, i32 16, <16 x i1> %5418), !tbaa !438
  %indvars.iv.next6305 = add nuw nsw i64 %indvars.iv6304, 1
  %.not2883.us = icmp eq i64 %indvars.iv6304, %5630
  br i1 %.not2883.us, label %"end for resampled_input.s0.x.rebased131.loopexit.us", label %"for resampled_input.s0.x.rebased130.us"

"end for resampled_input.s0.x.rebased131.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased130.us"
  %indvars.iv.next6307 = add nuw nsw i64 %indvars.iv6306, 1
  %.not2882.us = icmp eq i64 %indvars.iv6306, %5636
  br i1 %.not2882.us, label %"consume resampled_input185", label %"for resampled_input.s0.y.rebased127.us"

next_bb126:                                       ; preds = %"produce resampled_input123"
  br i1 %5644, label %"consume resampled_input185", label %"for resampled_input.s0.y.rebased133.us.preheader", !prof !440

"for resampled_input.s0.y.rebased133.us.preheader": ; preds = %next_bb126
  %5672 = trunc i64 %5652 to i32
  %5673 = mul i32 %18, %5672
  %t2541 = sub i32 %5673, %t2507
  %5674 = sext i32 %t2541 to i64
  br label %"for resampled_input.s0.y.rebased133.us"

"for resampled_input.s0.y.rebased133.us":         ; preds = %"for resampled_input.s0.y.rebased133.us.preheader", %"end for resampled_input.s0.x.rebased137.loopexit.us"
  %indvars.iv6300 = phi i64 [ 0, %"for resampled_input.s0.y.rebased133.us.preheader" ], [ %indvars.iv.next6301, %"end for resampled_input.s0.x.rebased137.loopexit.us" ]
  %5675 = add nsw i64 %indvars.iv6300, %5631
  %5676 = mul nsw i64 %5675, %5632
  %5677 = add nsw i64 %5676, %5633
  %5678 = add nsw i64 %indvars.iv6300, %5634
  %5679 = mul nsw i64 %5678, %5635
  %5680 = add nsw i64 %5679, %5674
  br label %"for resampled_input.s0.x.rebased136.us"

"for resampled_input.s0.x.rebased136.us":         ; preds = %"for resampled_input.s0.y.rebased133.us", %after_bb184.us
  %indvars.iv6298 = phi i64 [ 0, %"for resampled_input.s0.y.rebased133.us" ], [ %indvars.iv.next6299, %after_bb184.us ]
  %5681 = add nsw i64 %indvars.iv6298, %5561
  %5682 = mul nsw i64 %5681, %5562
  %5683 = add nsw i64 %5682, %5680
  %5684 = getelementptr inbounds i8, i8* %10, i64 %5683
  %5685 = load i8, i8* %5684, align 1, !tbaa !436
  br i1 %5563, label %after_bb142.us, label %after_bb157.us

after_bb142.us:                                   ; preds = %"for resampled_input.s0.x.rebased136.us"
  %5686 = add nsw i64 %5683, %5571
  %5687 = getelementptr inbounds i8, i8* %10, i64 %5686
  %5688 = load i8, i8* %5687, align 1, !tbaa !436
  br i1 %.not5145, label %after_bb157.us, label %after_bb145.us

after_bb145.us:                                   ; preds = %after_bb142.us
  %5689 = sdiv i32 2, %5565
  %5690 = and i32 %5689, %5569
  %5691 = sext i32 %5690 to i64
  %5692 = add nsw i64 %5683, %5691
  %5693 = getelementptr inbounds i8, i8* %10, i64 %5692
  %5694 = load i8, i8* %5693, align 1, !tbaa !436
  br i1 %5572, label %after_bb148.us, label %after_bb157.us

after_bb148.us:                                   ; preds = %after_bb145.us
  %5695 = sdiv i32 3, %5565
  %5696 = and i32 %5695, %5569
  %5697 = sext i32 %5696 to i64
  %5698 = add nsw i64 %5683, %5697
  %5699 = getelementptr inbounds i8, i8* %10, i64 %5698
  %5700 = load i8, i8* %5699, align 1, !tbaa !436
  br i1 %.not5146, label %after_bb157.us, label %after_bb151.us

after_bb151.us:                                   ; preds = %after_bb148.us
  %5701 = sdiv i32 4, %5565
  %5702 = and i32 %5701, %5569
  %5703 = sext i32 %5702 to i64
  %5704 = add nsw i64 %5683, %5703
  %5705 = getelementptr inbounds i8, i8* %10, i64 %5704
  %5706 = load i8, i8* %5705, align 1, !tbaa !436
  br i1 %5573, label %after_bb154.us, label %after_bb157.us

after_bb154.us:                                   ; preds = %after_bb151.us
  %5707 = sdiv i32 5, %5565
  %5708 = and i32 %5707, %5569
  %5709 = sext i32 %5708 to i64
  %5710 = add nsw i64 %5683, %5709
  %5711 = getelementptr inbounds i8, i8* %10, i64 %5710
  %5712 = load i8, i8* %5711, align 1, !tbaa !436
  br i1 %.not5147, label %after_bb157.us, label %true_bb155.us

true_bb155.us:                                    ; preds = %after_bb154.us
  %5713 = sdiv i32 6, %5565
  %5714 = and i32 %5713, %5569
  %5715 = sext i32 %5714 to i64
  %5716 = add nsw i64 %5683, %5715
  %5717 = getelementptr inbounds i8, i8* %10, i64 %5716
  %5718 = load i8, i8* %5717, align 1, !tbaa !436
  br label %after_bb157.us

after_bb157.us:                                   ; preds = %true_bb155.us, %after_bb154.us, %after_bb151.us, %after_bb148.us, %after_bb145.us, %after_bb142.us, %"for resampled_input.s0.x.rebased136.us"
  %5719 = phi i8 [ %5712, %true_bb155.us ], [ %5712, %after_bb154.us ], [ 0, %after_bb151.us ], [ 0, %after_bb148.us ], [ 0, %after_bb145.us ], [ 0, %after_bb142.us ], [ 0, %"for resampled_input.s0.x.rebased136.us" ]
  %5720 = phi i8 [ %5700, %true_bb155.us ], [ %5700, %after_bb154.us ], [ %5700, %after_bb151.us ], [ %5700, %after_bb148.us ], [ 0, %after_bb145.us ], [ 0, %after_bb142.us ], [ 0, %"for resampled_input.s0.x.rebased136.us" ]
  %5721 = phi i8 [ %5688, %true_bb155.us ], [ %5688, %after_bb154.us ], [ %5688, %after_bb151.us ], [ %5688, %after_bb148.us ], [ %5688, %after_bb145.us ], [ %5688, %after_bb142.us ], [ 0, %"for resampled_input.s0.x.rebased136.us" ]
  %5722 = phi i8 [ %5694, %true_bb155.us ], [ %5694, %after_bb154.us ], [ %5694, %after_bb151.us ], [ %5694, %after_bb148.us ], [ %5694, %after_bb145.us ], [ 0, %after_bb142.us ], [ 0, %"for resampled_input.s0.x.rebased136.us" ]
  %5723 = phi i8 [ %5706, %true_bb155.us ], [ %5706, %after_bb154.us ], [ %5706, %after_bb151.us ], [ 0, %after_bb148.us ], [ 0, %after_bb145.us ], [ 0, %after_bb142.us ], [ 0, %"for resampled_input.s0.x.rebased136.us" ]
  %5724 = phi i8 [ %5718, %true_bb155.us ], [ 0, %after_bb154.us ], [ 0, %after_bb151.us ], [ 0, %after_bb148.us ], [ 0, %after_bb145.us ], [ 0, %after_bb142.us ], [ 0, %"for resampled_input.s0.x.rebased136.us" ]
  br i1 %t2482, label %true_bb158.us, label %after_bb160.us

true_bb158.us:                                    ; preds = %after_bb157.us
  %5725 = sdiv i32 7, %5565
  %5726 = and i32 %5725, %5569
  %5727 = sext i32 %5726 to i64
  %5728 = add nsw i64 %5683, %5727
  %5729 = getelementptr inbounds i8, i8* %10, i64 %5728
  %5730 = load i8, i8* %5729, align 1, !tbaa !436
  br label %after_bb160.us

after_bb160.us:                                   ; preds = %true_bb158.us, %after_bb157.us
  %5731 = phi i8 [ %5730, %true_bb158.us ], [ 0, %after_bb157.us ]
  br i1 %t2483, label %after_bb163.us, label %after_bb184.us

after_bb163.us:                                   ; preds = %after_bb160.us
  %5732 = sdiv i32 8, %5565
  %5733 = and i32 %5732, %5569
  %5734 = sext i32 %5733 to i64
  %5735 = add nsw i64 %5683, %5734
  %5736 = getelementptr inbounds i8, i8* %10, i64 %5735
  %5737 = load i8, i8* %5736, align 1, !tbaa !436
  br i1 %.not5142, label %after_bb184.us, label %after_bb166.us

after_bb166.us:                                   ; preds = %after_bb163.us
  %5738 = sdiv i32 9, %5565
  %5739 = and i32 %5738, %5569
  %5740 = sext i32 %5739 to i64
  %5741 = add nsw i64 %5683, %5740
  %5742 = getelementptr inbounds i8, i8* %10, i64 %5741
  %5743 = load i8, i8* %5742, align 1, !tbaa !436
  br i1 %5574, label %after_bb169.us, label %after_bb184.us

after_bb169.us:                                   ; preds = %after_bb166.us
  %5744 = sdiv i32 10, %5565
  %5745 = and i32 %5744, %5569
  %5746 = sext i32 %5745 to i64
  %5747 = add nsw i64 %5683, %5746
  %5748 = getelementptr inbounds i8, i8* %10, i64 %5747
  %5749 = load i8, i8* %5748, align 1, !tbaa !436
  br i1 %.not5143, label %after_bb184.us, label %after_bb172.us

after_bb172.us:                                   ; preds = %after_bb169.us
  %5750 = sdiv i32 11, %5565
  %5751 = and i32 %5750, %5569
  %5752 = sext i32 %5751 to i64
  %5753 = add nsw i64 %5683, %5752
  %5754 = getelementptr inbounds i8, i8* %10, i64 %5753
  %5755 = load i8, i8* %5754, align 1, !tbaa !436
  br i1 %5575, label %after_bb175.us, label %after_bb184.us

after_bb175.us:                                   ; preds = %after_bb172.us
  %5756 = sdiv i32 12, %5565
  %5757 = and i32 %5756, %5569
  %5758 = sext i32 %5757 to i64
  %5759 = add nsw i64 %5683, %5758
  %5760 = getelementptr inbounds i8, i8* %10, i64 %5759
  %5761 = load i8, i8* %5760, align 1, !tbaa !436
  br i1 %.not5144, label %after_bb184.us, label %after_bb178.us

after_bb178.us:                                   ; preds = %after_bb175.us
  %5762 = sdiv i32 13, %5565
  %5763 = and i32 %5762, %5569
  %5764 = sext i32 %5763 to i64
  %5765 = add nsw i64 %5683, %5764
  %5766 = getelementptr inbounds i8, i8* %10, i64 %5765
  %5767 = load i8, i8* %5766, align 1, !tbaa !436
  br i1 %5576, label %true_bb179.us, label %after_bb184.us

true_bb179.us:                                    ; preds = %after_bb178.us
  %5768 = sdiv i32 14, %5565
  %5769 = and i32 %5768, %5569
  %5770 = sext i32 %5769 to i64
  %5771 = add nsw i64 %5683, %5770
  %5772 = getelementptr inbounds i8, i8* %10, i64 %5771
  %5773 = load i8, i8* %5772, align 1, !tbaa !436
  br label %after_bb184.us

after_bb184.us:                                   ; preds = %true_bb179.us, %after_bb178.us, %after_bb175.us, %after_bb172.us, %after_bb169.us, %after_bb166.us, %after_bb163.us, %after_bb160.us
  %5774 = phi i8 [ %5767, %true_bb179.us ], [ %5767, %after_bb178.us ], [ 0, %after_bb175.us ], [ 0, %after_bb172.us ], [ 0, %after_bb169.us ], [ 0, %after_bb166.us ], [ 0, %after_bb163.us ], [ 0, %after_bb160.us ]
  %5775 = phi i8 [ %5755, %true_bb179.us ], [ %5755, %after_bb178.us ], [ %5755, %after_bb175.us ], [ %5755, %after_bb172.us ], [ 0, %after_bb169.us ], [ 0, %after_bb166.us ], [ 0, %after_bb163.us ], [ 0, %after_bb160.us ]
  %5776 = phi i8 [ %5743, %true_bb179.us ], [ %5743, %after_bb178.us ], [ %5743, %after_bb175.us ], [ %5743, %after_bb172.us ], [ %5743, %after_bb169.us ], [ %5743, %after_bb166.us ], [ 0, %after_bb163.us ], [ 0, %after_bb160.us ]
  %5777 = phi i8 [ %5737, %true_bb179.us ], [ %5737, %after_bb178.us ], [ %5737, %after_bb175.us ], [ %5737, %after_bb172.us ], [ %5737, %after_bb169.us ], [ %5737, %after_bb166.us ], [ %5737, %after_bb163.us ], [ 0, %after_bb160.us ]
  %5778 = phi i8 [ %5749, %true_bb179.us ], [ %5749, %after_bb178.us ], [ %5749, %after_bb175.us ], [ %5749, %after_bb172.us ], [ %5749, %after_bb169.us ], [ 0, %after_bb166.us ], [ 0, %after_bb163.us ], [ 0, %after_bb160.us ]
  %5779 = phi i8 [ %5761, %true_bb179.us ], [ %5761, %after_bb178.us ], [ %5761, %after_bb175.us ], [ 0, %after_bb172.us ], [ 0, %after_bb169.us ], [ 0, %after_bb166.us ], [ 0, %after_bb163.us ], [ 0, %after_bb160.us ]
  %5780 = phi i8 [ %5773, %true_bb179.us ], [ 0, %after_bb178.us ], [ 0, %after_bb175.us ], [ 0, %after_bb172.us ], [ 0, %after_bb169.us ], [ 0, %after_bb166.us ], [ 0, %after_bb163.us ], [ 0, %after_bb160.us ]
  %5781 = insertelement <16 x i8> <i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0>, i8 %5685, i32 0
  %5782 = insertelement <16 x i8> %5781, i8 %5721, i32 1
  %5783 = insertelement <16 x i8> %5782, i8 %5722, i32 2
  %5784 = insertelement <16 x i8> %5783, i8 %5720, i32 3
  %5785 = insertelement <16 x i8> %5784, i8 %5723, i32 4
  %5786 = insertelement <16 x i8> %5785, i8 %5719, i32 5
  %5787 = insertelement <16 x i8> %5786, i8 %5724, i32 6
  %5788 = insertelement <16 x i8> %5787, i8 %5731, i32 7
  %5789 = insertelement <16 x i8> %5788, i8 %5777, i32 8
  %5790 = insertelement <16 x i8> %5789, i8 %5776, i32 9
  %5791 = insertelement <16 x i8> %5790, i8 %5778, i32 10
  %5792 = insertelement <16 x i8> %5791, i8 %5775, i32 11
  %5793 = insertelement <16 x i8> %5792, i8 %5779, i32 12
  %5794 = insertelement <16 x i8> %5793, i8 %5774, i32 13
  %5795 = insertelement <16 x i8> %5794, i8 %5780, i32 14
  %5796 = add nsw i64 %indvars.iv6298, %5677
  %5797 = shl nsw i64 %5796, 4
  %5798 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5797
  %5799 = bitcast i8* %5798 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %5795, <16 x i8>* %5799, i32 16, <16 x i1> %5418), !tbaa !438
  %indvars.iv.next6299 = add nuw nsw i64 %indvars.iv6298, 1
  %.not2881.us = icmp eq i64 %indvars.iv6298, %5630
  br i1 %.not2881.us, label %"end for resampled_input.s0.x.rebased137.loopexit.us", label %"for resampled_input.s0.x.rebased136.us"

"end for resampled_input.s0.x.rebased137.loopexit.us": ; preds = %after_bb184.us
  %indvars.iv.next6301 = add nuw nsw i64 %indvars.iv6300, 1
  %.not2880.us = icmp eq i64 %indvars.iv6300, %5636
  br i1 %.not2880.us, label %"consume resampled_input185", label %"for resampled_input.s0.y.rebased133.us"

"consume resampled_input185":                     ; preds = %"end for resampled_input.s0.x.rebased137.loopexit.us", %"end for resampled_input.s0.x.rebased131.loopexit.us", %next_bb126, %then_bb125
  %5800 = trunc i64 %5652 to i32
  %5801 = mul i32 %27, %5800
  %t2552 = sub i32 %5801, %reass.add5151
  br i1 %5577, label %"for output.s0.y.yo186.preheader", label %"end for output.s0.y.yo187", !prof !391

"for output.s0.y.yo186.preheader":                ; preds = %"consume resampled_input185"
  %5802 = load <4 x i32>, <4 x i32>* %5485, align 16
  %5803 = load <4 x i32>, <4 x i32>* %5488, align 16
  %5804 = load <4 x i32>, <4 x i32>* %5491, align 16
  %5805 = load <4 x i32>, <4 x i32>* %5494, align 16
  br label %"for output.s0.y.yo186"

"for output.s0.y.yo186":                          ; preds = %"for output.s0.y.yo186.preheader", %"end for output.s0.x.xo190"
  %indvars.iv6318 = phi i64 [ 0, %"for output.s0.y.yo186.preheader" ], [ %indvars.iv.next6319, %"end for output.s0.x.xo190" ]
  %5806 = trunc i64 %indvars.iv6318 to i32
  %5807 = shl nsw i32 %5806, 2
  %t2599 = add nsw i32 %5807, %24
  %t2600 = add nsw i32 %t2599, 1
  %t2601 = add nsw i32 %t2599, 2
  %t2602 = add nsw i32 %t2599, 3
  %t2603 = mul nsw i32 %t2600, %stride_y
  %5808 = add nsw i32 %t2603, %t2551
  %t2604 = mul nsw i32 %5808, %a64
  %5809 = add nsw i32 %t2603, %t2606
  %t2607 = mul nsw i32 %5809, %a64
  %t2608 = mul nsw i32 %t2601, %stride_y
  %5810 = add nsw i32 %t2608, %t2551
  %t2609 = mul nsw i32 %5810, %a64
  %5811 = add nsw i32 %t2608, %t2606
  %t2610 = mul nsw i32 %5811, %a64
  %t2611 = mul nsw i32 %t2602, %stride_y
  %5812 = add nsw i32 %t2611, %t2551
  %t2612 = mul nsw i32 %5812, %a64
  %5813 = add nsw i32 %t2611, %t2606
  %t2613 = mul nsw i32 %5813, %a64
  %t2614 = mul nsw i32 %t2599, %stride_y
  %5814 = add nsw i32 %t2614, %t2551
  %t2615 = mul nsw i32 %5814, %a64
  %5815 = add nsw i32 %t2614, %t2606
  %t2616 = mul nsw i32 %5815, %a64
  %t2617 = sub nsw i32 %t2603, %resampled_input.y.min_realized
  %t2618 = mul nsw i32 %t2617, %a64
  %t2619 = sub nsw i32 %t2608, %resampled_input.y.min_realized
  %t2620 = mul nsw i32 %t2619, %a64
  %t2621 = sub nsw i32 %t2611, %resampled_input.y.min_realized
  %t2622 = mul nsw i32 %t2621, %a64
  %t2623 = sub nsw i32 %t2614, %resampled_input.y.min_realized
  %t2624 = mul nsw i32 %t2623, %a64
  br i1 %5578, label %"for output.s0.x.xo189.preheader", label %"end for output.s0.x.xo190", !prof !391

"for output.s0.x.xo189.preheader":                ; preds = %"for output.s0.y.yo186"
  %5816 = mul nsw i32 %t2600, %26
  %t2596 = add nsw i32 %5816, %t2552
  %5817 = mul nsw i32 %t2601, %26
  %t2597 = add nsw i32 %5817, %t2552
  %5818 = mul nsw i32 %t2602, %26
  %t2598 = add nsw i32 %5818, %t2552
  %5819 = mul nsw i32 %t2599, %26
  %t2595 = add nsw i32 %5819, %t2552
  %t2588 = add nsw i32 %t2604, %t2547
  %t2584 = add nsw i32 %t2604, %t2605
  %t2575 = add nsw i32 %t2607, %t2547
  %t2571 = add nsw i32 %t2607, %t2605
  %t2589 = add nsw i32 %t2609, %t2547
  %t2585 = add nsw i32 %t2609, %t2605
  %t2576 = add nsw i32 %t2610, %t2547
  %t2572 = add nsw i32 %t2610, %t2605
  %t2590 = add nsw i32 %t2612, %t2547
  %t2586 = add nsw i32 %t2612, %t2605
  %t2577 = add nsw i32 %t2613, %t2547
  %t2573 = add nsw i32 %t2613, %t2605
  %t2587 = add nsw i32 %t2615, %t2547
  %t2583 = add nsw i32 %t2615, %t2605
  %t2574 = add nsw i32 %t2616, %t2547
  %t2570 = add nsw i32 %t2616, %t2605
  %t2562 = add nsw i32 %t2618, %t2547
  %t2558 = add nsw i32 %t2618, %t2605
  %t2563 = add nsw i32 %t2620, %t2547
  %t2559 = add nsw i32 %t2620, %t2605
  %t2564 = add nsw i32 %t2622, %t2547
  %t2560 = add nsw i32 %t2622, %t2605
  %t2561 = add nsw i32 %t2624, %t2547
  %t2557 = add nsw i32 %t2624, %t2605
  %t2580 = sub nsw i32 %t2604, %resampled_input.x.min_realized
  %t2567 = sub nsw i32 %t2607, %resampled_input.x.min_realized
  %t2581 = sub nsw i32 %t2609, %resampled_input.x.min_realized
  %t2568 = sub nsw i32 %t2610, %resampled_input.x.min_realized
  %t2582 = sub nsw i32 %t2612, %resampled_input.x.min_realized
  %t2569 = sub nsw i32 %t2613, %resampled_input.x.min_realized
  %t2579 = sub nsw i32 %t2615, %resampled_input.x.min_realized
  %t2566 = sub nsw i32 %t2616, %resampled_input.x.min_realized
  %t2554 = sub nsw i32 %t2618, %resampled_input.x.min_realized
  %t2555 = sub nsw i32 %t2620, %resampled_input.x.min_realized
  %t2556 = sub nsw i32 %t2622, %resampled_input.x.min_realized
  %t2553 = sub nsw i32 %t2624, %resampled_input.x.min_realized
  %5820 = sext i32 %t2553 to i64
  %5821 = sext i32 %t2554 to i64
  %5822 = sext i32 %t2555 to i64
  %5823 = sext i32 %t2556 to i64
  %5824 = sext i32 %t2557 to i64
  %5825 = sext i32 %t2558 to i64
  %5826 = sext i32 %t2559 to i64
  %5827 = sext i32 %t2560 to i64
  %5828 = sext i32 %t2561 to i64
  %5829 = sext i32 %t2562 to i64
  %5830 = sext i32 %t2563 to i64
  %5831 = sext i32 %t2564 to i64
  %5832 = sext i32 %t2566 to i64
  %5833 = sext i32 %t2567 to i64
  %5834 = sext i32 %t2568 to i64
  %5835 = sext i32 %t2569 to i64
  %5836 = sext i32 %t2570 to i64
  %5837 = sext i32 %t2571 to i64
  %5838 = sext i32 %t2572 to i64
  %5839 = sext i32 %t2573 to i64
  %5840 = sext i32 %t2574 to i64
  %5841 = sext i32 %t2575 to i64
  %5842 = sext i32 %t2576 to i64
  %5843 = sext i32 %t2577 to i64
  %5844 = sext i32 %t2579 to i64
  %5845 = sext i32 %t2580 to i64
  %5846 = sext i32 %t2581 to i64
  %5847 = sext i32 %t2582 to i64
  %5848 = sext i32 %t2583 to i64
  %5849 = sext i32 %t2584 to i64
  %5850 = sext i32 %t2585 to i64
  %5851 = sext i32 %t2586 to i64
  %5852 = sext i32 %t2587 to i64
  %5853 = sext i32 %t2588 to i64
  %5854 = sext i32 %t2589 to i64
  %5855 = sext i32 %t2590 to i64
  %5856 = sext i32 %t2595 to i64
  %5857 = sext i32 %t2596 to i64
  %5858 = sext i32 %t2597 to i64
  %5859 = sext i32 %t2598 to i64
  br label %"for output.s0.x.xo189"

"end for output.s0.y.yo187":                      ; preds = %"end for output.s0.x.xo190", %"consume resampled_input185"
  %indvars.iv.next6321 = add nuw nsw i64 %indvars.iv6320, 1
  %5860 = icmp eq i64 %indvars.iv.next6321, %zext
  br i1 %5860, label %after_bb.loopexit5218, label %"for output.s0.b.rebased117"

"for output.s0.x.xo189":                          ; preds = %"for output.s0.x.xo189.preheader", %"consume convolved202"
  %indvars.iv6316 = phi i64 [ 0, %"for output.s0.x.xo189.preheader" ], [ %indvars.iv.next6317, %"consume convolved202" ]
  br i1 %5349, label %then_bb194, label %next_bb195

"end for output.s0.x.xo190":                      ; preds = %"consume convolved202", %"for output.s0.y.yo186"
  %indvars.iv.next6319 = add nuw nsw i64 %indvars.iv6318, 1
  %.not2812 = icmp eq i64 %indvars.iv.next6319, %5641
  br i1 %.not2812, label %"end for output.s0.y.yo187", label %"for output.s0.y.yo186"

then_bb194:                                       ; preds = %"for output.s0.x.xo189"
  %5861 = load <8 x i16>, <8 x i16>* %5580, align 16, !tbaa !493
  %5862 = load <8 x i16>, <8 x i16>* %5582, align 16, !tbaa !502
  %5863 = shufflevector <8 x i16> %5862, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %5864 = shl nuw nsw i64 %indvars.iv6316, 2
  %5865 = add nsw i64 %5864, %5583
  %5866 = mul nsw i64 %5865, %5584
  %5867 = add nsw i64 %5866, %5820
  %5868 = shl nsw i64 %5867, 4
  %5869 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5868
  %5870 = bitcast i8* %5869 to <16 x i8>*
  %5871 = load <16 x i8>, <16 x i8>* %5870, align 16, !tbaa !438
  %5872 = zext <16 x i8> %5871 to <16 x i16>
  %5873 = shufflevector <8 x i16> %5861, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5874 = shufflevector <16 x i16> %5872, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5875 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %5874)
  %5876 = shufflevector <8 x i16> %5861, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5877 = shufflevector <16 x i16> %5872, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5878 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %5877)
  %5879 = shufflevector <8 x i16> %5862, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5880 = shufflevector <16 x i16> %5872, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5881 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %5880)
  %5882 = shufflevector <16 x i16> %5863, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5883 = shufflevector <16 x i16> %5872, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5884 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %5883)
  %5885 = add nsw <4 x i32> %5875, %5802
  %5886 = add nsw <4 x i32> %5878, %5803
  %5887 = add nsw <4 x i32> %5881, %5804
  %5888 = add nsw <4 x i32> %5884, %5805
  %5889 = add nsw i64 %5865, 1
  %5890 = mul nsw i64 %5889, %5584
  %5891 = add nsw i64 %5890, %5820
  %5892 = shl nsw i64 %5891, 4
  %5893 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5892
  %5894 = bitcast i8* %5893 to <16 x i8>*
  %5895 = load <16 x i8>, <16 x i8>* %5894, align 16, !tbaa !438
  %5896 = zext <16 x i8> %5895 to <16 x i16>
  %5897 = shufflevector <16 x i16> %5896, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5898 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %5897)
  %5899 = shufflevector <16 x i16> %5896, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5900 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %5899)
  %5901 = shufflevector <16 x i16> %5896, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5902 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %5901)
  %5903 = shufflevector <16 x i16> %5896, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5904 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %5903)
  %5905 = add nsw <4 x i32> %5898, %5802
  %5906 = add nsw <4 x i32> %5900, %5803
  %5907 = add nsw <4 x i32> %5902, %5804
  %5908 = add nsw <4 x i32> %5904, %5805
  %5909 = add nsw i64 %5865, 2
  %5910 = mul nsw i64 %5909, %5584
  %5911 = add nsw i64 %5910, %5820
  %5912 = shl nsw i64 %5911, 4
  %5913 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5912
  %5914 = bitcast i8* %5913 to <16 x i8>*
  %5915 = load <16 x i8>, <16 x i8>* %5914, align 16, !tbaa !438
  %5916 = zext <16 x i8> %5915 to <16 x i16>
  %5917 = shufflevector <16 x i16> %5916, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5918 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %5917)
  %5919 = shufflevector <16 x i16> %5916, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5920 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %5919)
  %5921 = shufflevector <16 x i16> %5916, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5922 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %5921)
  %5923 = shufflevector <16 x i16> %5916, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5924 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %5923)
  %5925 = add nsw <4 x i32> %5918, %5802
  %5926 = add nsw <4 x i32> %5920, %5803
  %5927 = add nsw <4 x i32> %5922, %5804
  %5928 = add nsw <4 x i32> %5924, %5805
  %5929 = add nsw i64 %5865, 3
  %5930 = mul nsw i64 %5929, %5584
  %5931 = add nsw i64 %5930, %5820
  %5932 = shl nsw i64 %5931, 4
  %5933 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5932
  %5934 = bitcast i8* %5933 to <16 x i8>*
  %5935 = load <16 x i8>, <16 x i8>* %5934, align 16, !tbaa !438
  %5936 = zext <16 x i8> %5935 to <16 x i16>
  %5937 = shufflevector <16 x i16> %5936, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5938 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %5937)
  %5939 = shufflevector <16 x i16> %5936, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5940 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %5939)
  %5941 = shufflevector <16 x i16> %5936, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5942 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %5941)
  %5943 = shufflevector <16 x i16> %5936, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5944 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %5943)
  %5945 = add nsw <4 x i32> %5938, %5802
  %5946 = add nsw <4 x i32> %5940, %5803
  %5947 = add nsw <4 x i32> %5942, %5804
  %5948 = add nsw <4 x i32> %5944, %5805
  %5949 = add nsw i64 %5866, %5821
  %5950 = shl nsw i64 %5949, 4
  %5951 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5950
  %5952 = bitcast i8* %5951 to <16 x i8>*
  %5953 = load <16 x i8>, <16 x i8>* %5952, align 16, !tbaa !438
  %5954 = zext <16 x i8> %5953 to <16 x i16>
  %5955 = shufflevector <16 x i16> %5954, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5956 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %5955)
  %5957 = shufflevector <16 x i16> %5954, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5958 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %5957)
  %5959 = shufflevector <16 x i16> %5954, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5960 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %5959)
  %5961 = shufflevector <16 x i16> %5954, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5962 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %5961)
  %5963 = add nsw <4 x i32> %5956, %5802
  %5964 = add nsw <4 x i32> %5958, %5803
  %5965 = add nsw <4 x i32> %5960, %5804
  %5966 = add nsw <4 x i32> %5962, %5805
  %5967 = add nsw i64 %5890, %5821
  %5968 = shl nsw i64 %5967, 4
  %5969 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5968
  %5970 = bitcast i8* %5969 to <16 x i8>*
  %5971 = load <16 x i8>, <16 x i8>* %5970, align 16, !tbaa !438
  %5972 = zext <16 x i8> %5971 to <16 x i16>
  %5973 = shufflevector <16 x i16> %5972, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5974 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %5973)
  %5975 = shufflevector <16 x i16> %5972, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5976 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %5975)
  %5977 = shufflevector <16 x i16> %5972, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5978 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %5977)
  %5979 = shufflevector <16 x i16> %5972, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5980 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %5979)
  %5981 = add nsw <4 x i32> %5974, %5802
  %5982 = add nsw <4 x i32> %5976, %5803
  %5983 = add nsw <4 x i32> %5978, %5804
  %5984 = add nsw <4 x i32> %5980, %5805
  %5985 = add nsw i64 %5910, %5821
  %5986 = shl nsw i64 %5985, 4
  %5987 = getelementptr inbounds i8, i8* %resampled_input122, i64 %5986
  %5988 = bitcast i8* %5987 to <16 x i8>*
  %5989 = load <16 x i8>, <16 x i8>* %5988, align 16, !tbaa !438
  %5990 = zext <16 x i8> %5989 to <16 x i16>
  %5991 = shufflevector <16 x i16> %5990, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5992 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %5991)
  %5993 = shufflevector <16 x i16> %5990, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5994 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %5993)
  %5995 = shufflevector <16 x i16> %5990, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5996 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %5995)
  %5997 = shufflevector <16 x i16> %5990, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %5998 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %5997)
  %5999 = add nsw <4 x i32> %5992, %5802
  %6000 = add nsw <4 x i32> %5994, %5803
  %6001 = add nsw <4 x i32> %5996, %5804
  %6002 = add nsw <4 x i32> %5998, %5805
  %6003 = add nsw i64 %5930, %5821
  %6004 = shl nsw i64 %6003, 4
  %6005 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6004
  %6006 = bitcast i8* %6005 to <16 x i8>*
  %6007 = load <16 x i8>, <16 x i8>* %6006, align 16, !tbaa !438
  %6008 = zext <16 x i8> %6007 to <16 x i16>
  %6009 = shufflevector <16 x i16> %6008, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6010 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6009)
  %6011 = shufflevector <16 x i16> %6008, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6012 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6011)
  %6013 = shufflevector <16 x i16> %6008, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6014 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6013)
  %6015 = shufflevector <16 x i16> %6008, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6016 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6015)
  %6017 = add nsw <4 x i32> %6010, %5802
  %6018 = add nsw <4 x i32> %6012, %5803
  %6019 = add nsw <4 x i32> %6014, %5804
  %6020 = add nsw <4 x i32> %6016, %5805
  %6021 = add nsw i64 %5866, %5822
  %6022 = shl nsw i64 %6021, 4
  %6023 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6022
  %6024 = bitcast i8* %6023 to <16 x i8>*
  %6025 = load <16 x i8>, <16 x i8>* %6024, align 16, !tbaa !438
  %6026 = zext <16 x i8> %6025 to <16 x i16>
  %6027 = shufflevector <16 x i16> %6026, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6028 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6027)
  %6029 = shufflevector <16 x i16> %6026, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6030 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6029)
  %6031 = shufflevector <16 x i16> %6026, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6032 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6031)
  %6033 = shufflevector <16 x i16> %6026, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6034 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6033)
  %6035 = add nsw <4 x i32> %6028, %5802
  %6036 = add nsw <4 x i32> %6030, %5803
  %6037 = add nsw <4 x i32> %6032, %5804
  %6038 = add nsw <4 x i32> %6034, %5805
  %6039 = add nsw i64 %5890, %5822
  %6040 = shl nsw i64 %6039, 4
  %6041 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6040
  %6042 = bitcast i8* %6041 to <16 x i8>*
  %6043 = load <16 x i8>, <16 x i8>* %6042, align 16, !tbaa !438
  %6044 = zext <16 x i8> %6043 to <16 x i16>
  %6045 = shufflevector <16 x i16> %6044, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6046 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6045)
  %6047 = shufflevector <16 x i16> %6044, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6048 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6047)
  %6049 = shufflevector <16 x i16> %6044, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6050 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6049)
  %6051 = shufflevector <16 x i16> %6044, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6052 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6051)
  %6053 = add nsw <4 x i32> %6046, %5802
  %6054 = add nsw <4 x i32> %6048, %5803
  %6055 = add nsw <4 x i32> %6050, %5804
  %6056 = add nsw <4 x i32> %6052, %5805
  %6057 = add nsw i64 %5910, %5822
  %6058 = shl nsw i64 %6057, 4
  %6059 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6058
  %6060 = bitcast i8* %6059 to <16 x i8>*
  %6061 = load <16 x i8>, <16 x i8>* %6060, align 16, !tbaa !438
  %6062 = zext <16 x i8> %6061 to <16 x i16>
  %6063 = shufflevector <16 x i16> %6062, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6064 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6063)
  %6065 = shufflevector <16 x i16> %6062, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6066 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6065)
  %6067 = shufflevector <16 x i16> %6062, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6068 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6067)
  %6069 = shufflevector <16 x i16> %6062, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6070 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6069)
  %6071 = add nsw <4 x i32> %6064, %5802
  %6072 = add nsw <4 x i32> %6066, %5803
  %6073 = add nsw <4 x i32> %6068, %5804
  %6074 = add nsw <4 x i32> %6070, %5805
  %6075 = add nsw i64 %5930, %5822
  %6076 = shl nsw i64 %6075, 4
  %6077 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6076
  %6078 = bitcast i8* %6077 to <16 x i8>*
  %6079 = load <16 x i8>, <16 x i8>* %6078, align 16, !tbaa !438
  %6080 = zext <16 x i8> %6079 to <16 x i16>
  %6081 = shufflevector <16 x i16> %6080, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6082 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6081)
  %6083 = shufflevector <16 x i16> %6080, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6084 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6083)
  %6085 = shufflevector <16 x i16> %6080, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6086 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6085)
  %6087 = shufflevector <16 x i16> %6080, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6088 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6087)
  %6089 = add nsw <4 x i32> %6082, %5802
  %6090 = add nsw <4 x i32> %6084, %5803
  %6091 = add nsw <4 x i32> %6086, %5804
  %6092 = add nsw <4 x i32> %6088, %5805
  %6093 = add nsw i64 %5866, %5823
  %6094 = shl nsw i64 %6093, 4
  %6095 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6094
  %6096 = bitcast i8* %6095 to <16 x i8>*
  %6097 = load <16 x i8>, <16 x i8>* %6096, align 16, !tbaa !438
  %6098 = zext <16 x i8> %6097 to <16 x i16>
  %6099 = shufflevector <16 x i16> %6098, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6100 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6099)
  %6101 = shufflevector <16 x i16> %6098, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6102 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6101)
  %6103 = shufflevector <16 x i16> %6098, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6104 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6103)
  %6105 = shufflevector <16 x i16> %6098, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6106 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6105)
  %6107 = add nsw <4 x i32> %6100, %5802
  %6108 = add nsw <4 x i32> %6102, %5803
  %6109 = add nsw <4 x i32> %6104, %5804
  %6110 = add nsw <4 x i32> %6106, %5805
  %6111 = add nsw i64 %5890, %5823
  %6112 = shl nsw i64 %6111, 4
  %6113 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6112
  %6114 = bitcast i8* %6113 to <16 x i8>*
  %6115 = load <16 x i8>, <16 x i8>* %6114, align 16, !tbaa !438
  %6116 = zext <16 x i8> %6115 to <16 x i16>
  %6117 = shufflevector <16 x i16> %6116, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6118 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6117)
  %6119 = shufflevector <16 x i16> %6116, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6120 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6119)
  %6121 = shufflevector <16 x i16> %6116, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6122 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6121)
  %6123 = shufflevector <16 x i16> %6116, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6124 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6123)
  %6125 = add nsw <4 x i32> %6118, %5802
  %6126 = add nsw <4 x i32> %6120, %5803
  %6127 = add nsw <4 x i32> %6122, %5804
  %6128 = add nsw <4 x i32> %6124, %5805
  %6129 = add nsw i64 %5910, %5823
  %6130 = shl nsw i64 %6129, 4
  %6131 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6130
  %6132 = bitcast i8* %6131 to <16 x i8>*
  %6133 = load <16 x i8>, <16 x i8>* %6132, align 16, !tbaa !438
  %6134 = zext <16 x i8> %6133 to <16 x i16>
  %6135 = shufflevector <16 x i16> %6134, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6136 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6135)
  %6137 = shufflevector <16 x i16> %6134, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6138 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6137)
  %6139 = shufflevector <16 x i16> %6134, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6140 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6139)
  %6141 = shufflevector <16 x i16> %6134, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6142 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6141)
  %6143 = add nsw <4 x i32> %6136, %5802
  %6144 = add nsw <4 x i32> %6138, %5803
  %6145 = add nsw <4 x i32> %6140, %5804
  %6146 = add nsw <4 x i32> %6142, %5805
  %6147 = add nsw i64 %5930, %5823
  %6148 = shl nsw i64 %6147, 4
  %6149 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6148
  %6150 = bitcast i8* %6149 to <16 x i8>*
  %6151 = load <16 x i8>, <16 x i8>* %6150, align 16, !tbaa !438
  %6152 = zext <16 x i8> %6151 to <16 x i16>
  %6153 = shufflevector <16 x i16> %6152, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6154 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5873, <4 x i16> %6153)
  %6155 = shufflevector <16 x i16> %6152, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6156 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5876, <4 x i16> %6155)
  %6157 = shufflevector <16 x i16> %6152, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6158 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5879, <4 x i16> %6157)
  %6159 = shufflevector <16 x i16> %6152, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6160 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %5882, <4 x i16> %6159)
  %6161 = add nsw <4 x i32> %6154, %5802
  %6162 = add nsw <4 x i32> %6156, %5803
  %6163 = add nsw <4 x i32> %6158, %5804
  %6164 = add nsw <4 x i32> %6160, %5805
  %6165 = load <8 x i16>, <8 x i16>* %5586, align 16, !tbaa !504
  %6166 = load <8 x i16>, <8 x i16>* %5588, align 16, !tbaa !507
  %6167 = shufflevector <8 x i16> %6166, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %6168 = add nsw i64 %5866, %5824
  %6169 = shl nsw i64 %6168, 4
  %6170 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6169
  %6171 = bitcast i8* %6170 to <16 x i8>*
  %6172 = load <16 x i8>, <16 x i8>* %6171, align 16, !tbaa !438
  %6173 = zext <16 x i8> %6172 to <16 x i16>
  %6174 = shufflevector <8 x i16> %6165, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6175 = shufflevector <16 x i16> %6173, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6176 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6175)
  %6177 = shufflevector <8 x i16> %6165, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6178 = shufflevector <16 x i16> %6173, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6179 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6178)
  %6180 = shufflevector <8 x i16> %6166, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6181 = shufflevector <16 x i16> %6173, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6182 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6181)
  %6183 = shufflevector <16 x i16> %6167, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6184 = shufflevector <16 x i16> %6173, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6185 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6184)
  %6186 = add nsw <4 x i32> %5885, %6176
  %6187 = add nsw <4 x i32> %5886, %6179
  %6188 = add nsw <4 x i32> %5887, %6182
  %6189 = add nsw <4 x i32> %5888, %6185
  %6190 = add nsw i64 %5890, %5824
  %6191 = shl nsw i64 %6190, 4
  %6192 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6191
  %6193 = bitcast i8* %6192 to <16 x i8>*
  %6194 = load <16 x i8>, <16 x i8>* %6193, align 16, !tbaa !438
  %6195 = zext <16 x i8> %6194 to <16 x i16>
  %6196 = shufflevector <16 x i16> %6195, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6197 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6196)
  %6198 = shufflevector <16 x i16> %6195, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6199 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6198)
  %6200 = shufflevector <16 x i16> %6195, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6201 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6200)
  %6202 = shufflevector <16 x i16> %6195, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6203 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6202)
  %6204 = add nsw <4 x i32> %5905, %6197
  %6205 = add nsw <4 x i32> %5906, %6199
  %6206 = add nsw <4 x i32> %5907, %6201
  %6207 = add nsw <4 x i32> %5908, %6203
  %6208 = add nsw i64 %5910, %5824
  %6209 = shl nsw i64 %6208, 4
  %6210 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6209
  %6211 = bitcast i8* %6210 to <16 x i8>*
  %6212 = load <16 x i8>, <16 x i8>* %6211, align 16, !tbaa !438
  %6213 = zext <16 x i8> %6212 to <16 x i16>
  %6214 = shufflevector <16 x i16> %6213, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6215 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6214)
  %6216 = shufflevector <16 x i16> %6213, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6217 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6216)
  %6218 = shufflevector <16 x i16> %6213, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6219 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6218)
  %6220 = shufflevector <16 x i16> %6213, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6221 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6220)
  %6222 = add nsw <4 x i32> %5925, %6215
  %6223 = add nsw <4 x i32> %5926, %6217
  %6224 = add nsw <4 x i32> %5927, %6219
  %6225 = add nsw <4 x i32> %5928, %6221
  %6226 = add nsw i64 %5930, %5824
  %6227 = shl nsw i64 %6226, 4
  %6228 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6227
  %6229 = bitcast i8* %6228 to <16 x i8>*
  %6230 = load <16 x i8>, <16 x i8>* %6229, align 16, !tbaa !438
  %6231 = zext <16 x i8> %6230 to <16 x i16>
  %6232 = shufflevector <16 x i16> %6231, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6233 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6232)
  %6234 = shufflevector <16 x i16> %6231, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6235 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6234)
  %6236 = shufflevector <16 x i16> %6231, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6237 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6236)
  %6238 = shufflevector <16 x i16> %6231, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6239 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6238)
  %6240 = add nsw <4 x i32> %5945, %6233
  %6241 = add nsw <4 x i32> %5946, %6235
  %6242 = add nsw <4 x i32> %5947, %6237
  %6243 = add nsw <4 x i32> %5948, %6239
  %6244 = add nsw i64 %5866, %5825
  %6245 = shl nsw i64 %6244, 4
  %6246 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6245
  %6247 = bitcast i8* %6246 to <16 x i8>*
  %6248 = load <16 x i8>, <16 x i8>* %6247, align 16, !tbaa !438
  %6249 = zext <16 x i8> %6248 to <16 x i16>
  %6250 = shufflevector <16 x i16> %6249, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6251 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6250)
  %6252 = shufflevector <16 x i16> %6249, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6253 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6252)
  %6254 = shufflevector <16 x i16> %6249, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6255 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6254)
  %6256 = shufflevector <16 x i16> %6249, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6257 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6256)
  %6258 = add nsw <4 x i32> %5963, %6251
  %6259 = add nsw <4 x i32> %5964, %6253
  %6260 = add nsw <4 x i32> %5965, %6255
  %6261 = add nsw <4 x i32> %5966, %6257
  %6262 = add nsw i64 %5890, %5825
  %6263 = shl nsw i64 %6262, 4
  %6264 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6263
  %6265 = bitcast i8* %6264 to <16 x i8>*
  %6266 = load <16 x i8>, <16 x i8>* %6265, align 16, !tbaa !438
  %6267 = zext <16 x i8> %6266 to <16 x i16>
  %6268 = shufflevector <16 x i16> %6267, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6269 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6268)
  %6270 = shufflevector <16 x i16> %6267, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6271 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6270)
  %6272 = shufflevector <16 x i16> %6267, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6273 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6272)
  %6274 = shufflevector <16 x i16> %6267, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6275 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6274)
  %6276 = add nsw <4 x i32> %5981, %6269
  %6277 = add nsw <4 x i32> %5982, %6271
  %6278 = add nsw <4 x i32> %5983, %6273
  %6279 = add nsw <4 x i32> %5984, %6275
  %6280 = add nsw i64 %5910, %5825
  %6281 = shl nsw i64 %6280, 4
  %6282 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6281
  %6283 = bitcast i8* %6282 to <16 x i8>*
  %6284 = load <16 x i8>, <16 x i8>* %6283, align 16, !tbaa !438
  %6285 = zext <16 x i8> %6284 to <16 x i16>
  %6286 = shufflevector <16 x i16> %6285, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6287 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6286)
  %6288 = shufflevector <16 x i16> %6285, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6289 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6288)
  %6290 = shufflevector <16 x i16> %6285, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6291 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6290)
  %6292 = shufflevector <16 x i16> %6285, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6293 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6292)
  %6294 = add nsw <4 x i32> %5999, %6287
  %6295 = add nsw <4 x i32> %6000, %6289
  %6296 = add nsw <4 x i32> %6001, %6291
  %6297 = add nsw <4 x i32> %6002, %6293
  %6298 = add nsw i64 %5930, %5825
  %6299 = shl nsw i64 %6298, 4
  %6300 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6299
  %6301 = bitcast i8* %6300 to <16 x i8>*
  %6302 = load <16 x i8>, <16 x i8>* %6301, align 16, !tbaa !438
  %6303 = zext <16 x i8> %6302 to <16 x i16>
  %6304 = shufflevector <16 x i16> %6303, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6305 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6304)
  %6306 = shufflevector <16 x i16> %6303, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6307 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6306)
  %6308 = shufflevector <16 x i16> %6303, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6309 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6308)
  %6310 = shufflevector <16 x i16> %6303, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6311 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6310)
  %6312 = add nsw <4 x i32> %6017, %6305
  %6313 = add nsw <4 x i32> %6018, %6307
  %6314 = add nsw <4 x i32> %6019, %6309
  %6315 = add nsw <4 x i32> %6020, %6311
  %6316 = add nsw i64 %5866, %5826
  %6317 = shl nsw i64 %6316, 4
  %6318 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6317
  %6319 = bitcast i8* %6318 to <16 x i8>*
  %6320 = load <16 x i8>, <16 x i8>* %6319, align 16, !tbaa !438
  %6321 = zext <16 x i8> %6320 to <16 x i16>
  %6322 = shufflevector <16 x i16> %6321, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6323 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6322)
  %6324 = shufflevector <16 x i16> %6321, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6325 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6324)
  %6326 = shufflevector <16 x i16> %6321, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6327 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6326)
  %6328 = shufflevector <16 x i16> %6321, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6329 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6328)
  %6330 = add nsw <4 x i32> %6035, %6323
  %6331 = add nsw <4 x i32> %6036, %6325
  %6332 = add nsw <4 x i32> %6037, %6327
  %6333 = add nsw <4 x i32> %6038, %6329
  %6334 = add nsw i64 %5890, %5826
  %6335 = shl nsw i64 %6334, 4
  %6336 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6335
  %6337 = bitcast i8* %6336 to <16 x i8>*
  %6338 = load <16 x i8>, <16 x i8>* %6337, align 16, !tbaa !438
  %6339 = zext <16 x i8> %6338 to <16 x i16>
  %6340 = shufflevector <16 x i16> %6339, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6341 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6340)
  %6342 = shufflevector <16 x i16> %6339, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6343 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6342)
  %6344 = shufflevector <16 x i16> %6339, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6345 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6344)
  %6346 = shufflevector <16 x i16> %6339, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6347 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6346)
  %6348 = add nsw <4 x i32> %6053, %6341
  %6349 = add nsw <4 x i32> %6054, %6343
  %6350 = add nsw <4 x i32> %6055, %6345
  %6351 = add nsw <4 x i32> %6056, %6347
  %6352 = add nsw i64 %5910, %5826
  %6353 = shl nsw i64 %6352, 4
  %6354 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6353
  %6355 = bitcast i8* %6354 to <16 x i8>*
  %6356 = load <16 x i8>, <16 x i8>* %6355, align 16, !tbaa !438
  %6357 = zext <16 x i8> %6356 to <16 x i16>
  %6358 = shufflevector <16 x i16> %6357, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6359 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6358)
  %6360 = shufflevector <16 x i16> %6357, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6361 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6360)
  %6362 = shufflevector <16 x i16> %6357, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6363 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6362)
  %6364 = shufflevector <16 x i16> %6357, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6365 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6364)
  %6366 = add nsw <4 x i32> %6071, %6359
  %6367 = add nsw <4 x i32> %6072, %6361
  %6368 = add nsw <4 x i32> %6073, %6363
  %6369 = add nsw <4 x i32> %6074, %6365
  %6370 = add nsw i64 %5930, %5826
  %6371 = shl nsw i64 %6370, 4
  %6372 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6371
  %6373 = bitcast i8* %6372 to <16 x i8>*
  %6374 = load <16 x i8>, <16 x i8>* %6373, align 16, !tbaa !438
  %6375 = zext <16 x i8> %6374 to <16 x i16>
  %6376 = shufflevector <16 x i16> %6375, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6377 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6376)
  %6378 = shufflevector <16 x i16> %6375, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6379 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6378)
  %6380 = shufflevector <16 x i16> %6375, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6381 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6380)
  %6382 = shufflevector <16 x i16> %6375, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6383 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6382)
  %6384 = add nsw <4 x i32> %6089, %6377
  %6385 = add nsw <4 x i32> %6090, %6379
  %6386 = add nsw <4 x i32> %6091, %6381
  %6387 = add nsw <4 x i32> %6092, %6383
  %6388 = add nsw i64 %5866, %5827
  %6389 = shl nsw i64 %6388, 4
  %6390 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6389
  %6391 = bitcast i8* %6390 to <16 x i8>*
  %6392 = load <16 x i8>, <16 x i8>* %6391, align 16, !tbaa !438
  %6393 = zext <16 x i8> %6392 to <16 x i16>
  %6394 = shufflevector <16 x i16> %6393, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6395 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6394)
  %6396 = shufflevector <16 x i16> %6393, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6397 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6396)
  %6398 = shufflevector <16 x i16> %6393, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6399 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6398)
  %6400 = shufflevector <16 x i16> %6393, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6401 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6400)
  %6402 = add nsw <4 x i32> %6107, %6395
  %6403 = add nsw <4 x i32> %6108, %6397
  %6404 = add nsw <4 x i32> %6109, %6399
  %6405 = add nsw <4 x i32> %6110, %6401
  %6406 = add nsw i64 %5890, %5827
  %6407 = shl nsw i64 %6406, 4
  %6408 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6407
  %6409 = bitcast i8* %6408 to <16 x i8>*
  %6410 = load <16 x i8>, <16 x i8>* %6409, align 16, !tbaa !438
  %6411 = zext <16 x i8> %6410 to <16 x i16>
  %6412 = shufflevector <16 x i16> %6411, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6413 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6412)
  %6414 = shufflevector <16 x i16> %6411, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6415 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6414)
  %6416 = shufflevector <16 x i16> %6411, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6417 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6416)
  %6418 = shufflevector <16 x i16> %6411, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6419 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6418)
  %6420 = add nsw <4 x i32> %6125, %6413
  %6421 = add nsw <4 x i32> %6126, %6415
  %6422 = add nsw <4 x i32> %6127, %6417
  %6423 = add nsw <4 x i32> %6128, %6419
  %6424 = add nsw i64 %5910, %5827
  %6425 = shl nsw i64 %6424, 4
  %6426 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6425
  %6427 = bitcast i8* %6426 to <16 x i8>*
  %6428 = load <16 x i8>, <16 x i8>* %6427, align 16, !tbaa !438
  %6429 = zext <16 x i8> %6428 to <16 x i16>
  %6430 = shufflevector <16 x i16> %6429, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6431 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6430)
  %6432 = shufflevector <16 x i16> %6429, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6433 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6432)
  %6434 = shufflevector <16 x i16> %6429, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6435 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6434)
  %6436 = shufflevector <16 x i16> %6429, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6437 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6436)
  %6438 = add nsw <4 x i32> %6143, %6431
  %6439 = add nsw <4 x i32> %6144, %6433
  %6440 = add nsw <4 x i32> %6145, %6435
  %6441 = add nsw <4 x i32> %6146, %6437
  %6442 = add nsw i64 %5930, %5827
  %6443 = shl nsw i64 %6442, 4
  %6444 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6443
  %6445 = bitcast i8* %6444 to <16 x i8>*
  %6446 = load <16 x i8>, <16 x i8>* %6445, align 16, !tbaa !438
  %6447 = zext <16 x i8> %6446 to <16 x i16>
  %6448 = shufflevector <16 x i16> %6447, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6449 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6174, <4 x i16> %6448)
  %6450 = shufflevector <16 x i16> %6447, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6451 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6177, <4 x i16> %6450)
  %6452 = shufflevector <16 x i16> %6447, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6453 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6180, <4 x i16> %6452)
  %6454 = shufflevector <16 x i16> %6447, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6455 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6183, <4 x i16> %6454)
  %6456 = add nsw <4 x i32> %6161, %6449
  %6457 = add nsw <4 x i32> %6162, %6451
  %6458 = add nsw <4 x i32> %6163, %6453
  %6459 = add nsw <4 x i32> %6164, %6455
  %6460 = load <8 x i16>, <8 x i16>* %5590, align 16, !tbaa !509
  %6461 = load <8 x i16>, <8 x i16>* %5592, align 16, !tbaa !513
  %6462 = shufflevector <8 x i16> %6461, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %6463 = add nsw i64 %5866, %5828
  %6464 = shl nsw i64 %6463, 4
  %6465 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6464
  %6466 = bitcast i8* %6465 to <16 x i8>*
  %6467 = load <16 x i8>, <16 x i8>* %6466, align 16, !tbaa !438
  %6468 = zext <16 x i8> %6467 to <16 x i16>
  %6469 = shufflevector <8 x i16> %6460, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6470 = shufflevector <16 x i16> %6468, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6471 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6470)
  %6472 = shufflevector <8 x i16> %6460, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6473 = shufflevector <16 x i16> %6468, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6474 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6473)
  %6475 = shufflevector <8 x i16> %6461, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6476 = shufflevector <16 x i16> %6468, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6477 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6476)
  %6478 = shufflevector <16 x i16> %6462, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6479 = shufflevector <16 x i16> %6468, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6480 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6479)
  %6481 = add nsw <4 x i32> %6186, %6471
  %6482 = add nsw <4 x i32> %6187, %6474
  %6483 = add nsw <4 x i32> %6188, %6477
  %6484 = add nsw <4 x i32> %6189, %6480
  %6485 = add nsw i64 %5890, %5828
  %6486 = shl nsw i64 %6485, 4
  %6487 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6486
  %6488 = bitcast i8* %6487 to <16 x i8>*
  %6489 = load <16 x i8>, <16 x i8>* %6488, align 16, !tbaa !438
  %6490 = zext <16 x i8> %6489 to <16 x i16>
  %6491 = shufflevector <16 x i16> %6490, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6492 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6491)
  %6493 = shufflevector <16 x i16> %6490, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6494 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6493)
  %6495 = shufflevector <16 x i16> %6490, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6496 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6495)
  %6497 = shufflevector <16 x i16> %6490, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6498 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6497)
  %6499 = add nsw <4 x i32> %6204, %6492
  %6500 = add nsw <4 x i32> %6205, %6494
  %6501 = add nsw <4 x i32> %6206, %6496
  %6502 = add nsw <4 x i32> %6207, %6498
  %6503 = add nsw i64 %5910, %5828
  %6504 = shl nsw i64 %6503, 4
  %6505 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6504
  %6506 = bitcast i8* %6505 to <16 x i8>*
  %6507 = load <16 x i8>, <16 x i8>* %6506, align 16, !tbaa !438
  %6508 = zext <16 x i8> %6507 to <16 x i16>
  %6509 = shufflevector <16 x i16> %6508, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6510 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6509)
  %6511 = shufflevector <16 x i16> %6508, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6512 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6511)
  %6513 = shufflevector <16 x i16> %6508, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6514 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6513)
  %6515 = shufflevector <16 x i16> %6508, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6516 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6515)
  %6517 = add nsw <4 x i32> %6222, %6510
  %6518 = add nsw <4 x i32> %6223, %6512
  %6519 = add nsw <4 x i32> %6224, %6514
  %6520 = add nsw <4 x i32> %6225, %6516
  %6521 = add nsw i64 %5930, %5828
  %6522 = shl nsw i64 %6521, 4
  %6523 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6522
  %6524 = bitcast i8* %6523 to <16 x i8>*
  %6525 = load <16 x i8>, <16 x i8>* %6524, align 16, !tbaa !438
  %6526 = zext <16 x i8> %6525 to <16 x i16>
  %6527 = shufflevector <16 x i16> %6526, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6528 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6527)
  %6529 = shufflevector <16 x i16> %6526, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6530 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6529)
  %6531 = shufflevector <16 x i16> %6526, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6532 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6531)
  %6533 = shufflevector <16 x i16> %6526, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6534 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6533)
  %6535 = add nsw <4 x i32> %6240, %6528
  %6536 = add nsw <4 x i32> %6241, %6530
  %6537 = add nsw <4 x i32> %6242, %6532
  %6538 = add nsw <4 x i32> %6243, %6534
  %6539 = add nsw i64 %5866, %5829
  %6540 = shl nsw i64 %6539, 4
  %6541 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6540
  %6542 = bitcast i8* %6541 to <16 x i8>*
  %6543 = load <16 x i8>, <16 x i8>* %6542, align 16, !tbaa !438
  %6544 = zext <16 x i8> %6543 to <16 x i16>
  %6545 = shufflevector <16 x i16> %6544, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6546 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6545)
  %6547 = shufflevector <16 x i16> %6544, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6548 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6547)
  %6549 = shufflevector <16 x i16> %6544, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6550 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6549)
  %6551 = shufflevector <16 x i16> %6544, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6552 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6551)
  %6553 = add nsw <4 x i32> %6258, %6546
  %6554 = add nsw <4 x i32> %6259, %6548
  %6555 = add nsw <4 x i32> %6260, %6550
  %6556 = add nsw <4 x i32> %6261, %6552
  %6557 = add nsw i64 %5890, %5829
  %6558 = shl nsw i64 %6557, 4
  %6559 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6558
  %6560 = bitcast i8* %6559 to <16 x i8>*
  %6561 = load <16 x i8>, <16 x i8>* %6560, align 16, !tbaa !438
  %6562 = zext <16 x i8> %6561 to <16 x i16>
  %6563 = shufflevector <16 x i16> %6562, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6564 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6563)
  %6565 = shufflevector <16 x i16> %6562, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6566 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6565)
  %6567 = shufflevector <16 x i16> %6562, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6568 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6567)
  %6569 = shufflevector <16 x i16> %6562, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6570 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6569)
  %6571 = add nsw <4 x i32> %6276, %6564
  %6572 = add nsw <4 x i32> %6277, %6566
  %6573 = add nsw <4 x i32> %6278, %6568
  %6574 = add nsw <4 x i32> %6279, %6570
  %6575 = add nsw i64 %5910, %5829
  %6576 = shl nsw i64 %6575, 4
  %6577 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6576
  %6578 = bitcast i8* %6577 to <16 x i8>*
  %6579 = load <16 x i8>, <16 x i8>* %6578, align 16, !tbaa !438
  %6580 = zext <16 x i8> %6579 to <16 x i16>
  %6581 = shufflevector <16 x i16> %6580, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6582 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6581)
  %6583 = shufflevector <16 x i16> %6580, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6584 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6583)
  %6585 = shufflevector <16 x i16> %6580, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6586 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6585)
  %6587 = shufflevector <16 x i16> %6580, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6588 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6587)
  %6589 = add nsw <4 x i32> %6294, %6582
  %6590 = add nsw <4 x i32> %6295, %6584
  %6591 = add nsw <4 x i32> %6296, %6586
  %6592 = add nsw <4 x i32> %6297, %6588
  %6593 = add nsw i64 %5930, %5829
  %6594 = shl nsw i64 %6593, 4
  %6595 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6594
  %6596 = bitcast i8* %6595 to <16 x i8>*
  %6597 = load <16 x i8>, <16 x i8>* %6596, align 16, !tbaa !438
  %6598 = zext <16 x i8> %6597 to <16 x i16>
  %6599 = shufflevector <16 x i16> %6598, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6600 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6599)
  %6601 = shufflevector <16 x i16> %6598, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6602 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6601)
  %6603 = shufflevector <16 x i16> %6598, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6604 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6603)
  %6605 = shufflevector <16 x i16> %6598, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6606 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6605)
  %6607 = add nsw <4 x i32> %6312, %6600
  %6608 = add nsw <4 x i32> %6313, %6602
  %6609 = add nsw <4 x i32> %6314, %6604
  %6610 = add nsw <4 x i32> %6315, %6606
  %6611 = add nsw i64 %5866, %5830
  %6612 = shl nsw i64 %6611, 4
  %6613 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6612
  %6614 = bitcast i8* %6613 to <16 x i8>*
  %6615 = load <16 x i8>, <16 x i8>* %6614, align 16, !tbaa !438
  %6616 = zext <16 x i8> %6615 to <16 x i16>
  %6617 = shufflevector <16 x i16> %6616, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6618 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6617)
  %6619 = shufflevector <16 x i16> %6616, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6620 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6619)
  %6621 = shufflevector <16 x i16> %6616, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6622 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6621)
  %6623 = shufflevector <16 x i16> %6616, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6624 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6623)
  %6625 = add nsw <4 x i32> %6330, %6618
  %6626 = add nsw <4 x i32> %6331, %6620
  %6627 = add nsw <4 x i32> %6332, %6622
  %6628 = add nsw <4 x i32> %6333, %6624
  %6629 = add nsw i64 %5890, %5830
  %6630 = shl nsw i64 %6629, 4
  %6631 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6630
  %6632 = bitcast i8* %6631 to <16 x i8>*
  %6633 = load <16 x i8>, <16 x i8>* %6632, align 16, !tbaa !438
  %6634 = zext <16 x i8> %6633 to <16 x i16>
  %6635 = shufflevector <16 x i16> %6634, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6636 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6635)
  %6637 = shufflevector <16 x i16> %6634, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6638 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6637)
  %6639 = shufflevector <16 x i16> %6634, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6640 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6639)
  %6641 = shufflevector <16 x i16> %6634, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6642 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6641)
  %6643 = add nsw <4 x i32> %6348, %6636
  %6644 = add nsw <4 x i32> %6349, %6638
  %6645 = add nsw <4 x i32> %6350, %6640
  %6646 = add nsw <4 x i32> %6351, %6642
  %6647 = add nsw i64 %5910, %5830
  %6648 = shl nsw i64 %6647, 4
  %6649 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6648
  %6650 = bitcast i8* %6649 to <16 x i8>*
  %6651 = load <16 x i8>, <16 x i8>* %6650, align 16, !tbaa !438
  %6652 = zext <16 x i8> %6651 to <16 x i16>
  %6653 = shufflevector <16 x i16> %6652, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6654 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6653)
  %6655 = shufflevector <16 x i16> %6652, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6656 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6655)
  %6657 = shufflevector <16 x i16> %6652, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6658 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6657)
  %6659 = shufflevector <16 x i16> %6652, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6660 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6659)
  %6661 = add nsw <4 x i32> %6366, %6654
  %6662 = add nsw <4 x i32> %6367, %6656
  %6663 = add nsw <4 x i32> %6368, %6658
  %6664 = add nsw <4 x i32> %6369, %6660
  %6665 = add nsw i64 %5930, %5830
  %6666 = shl nsw i64 %6665, 4
  %6667 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6666
  %6668 = bitcast i8* %6667 to <16 x i8>*
  %6669 = load <16 x i8>, <16 x i8>* %6668, align 16, !tbaa !438
  %6670 = zext <16 x i8> %6669 to <16 x i16>
  %6671 = shufflevector <16 x i16> %6670, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6672 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6671)
  %6673 = shufflevector <16 x i16> %6670, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6674 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6673)
  %6675 = shufflevector <16 x i16> %6670, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6676 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6675)
  %6677 = shufflevector <16 x i16> %6670, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6678 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6677)
  %6679 = add nsw <4 x i32> %6384, %6672
  %6680 = add nsw <4 x i32> %6385, %6674
  %6681 = add nsw <4 x i32> %6386, %6676
  %6682 = add nsw <4 x i32> %6387, %6678
  %6683 = add nsw i64 %5866, %5831
  %6684 = shl nsw i64 %6683, 4
  %6685 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6684
  %6686 = bitcast i8* %6685 to <16 x i8>*
  %6687 = load <16 x i8>, <16 x i8>* %6686, align 16, !tbaa !438
  %6688 = zext <16 x i8> %6687 to <16 x i16>
  %6689 = shufflevector <16 x i16> %6688, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6690 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6689)
  %6691 = shufflevector <16 x i16> %6688, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6692 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6691)
  %6693 = shufflevector <16 x i16> %6688, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6694 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6693)
  %6695 = shufflevector <16 x i16> %6688, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6696 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6695)
  %6697 = add nsw <4 x i32> %6402, %6690
  %6698 = add nsw <4 x i32> %6403, %6692
  %6699 = add nsw <4 x i32> %6404, %6694
  %6700 = add nsw <4 x i32> %6405, %6696
  %6701 = add nsw i64 %5890, %5831
  %6702 = shl nsw i64 %6701, 4
  %6703 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6702
  %6704 = bitcast i8* %6703 to <16 x i8>*
  %6705 = load <16 x i8>, <16 x i8>* %6704, align 16, !tbaa !438
  %6706 = zext <16 x i8> %6705 to <16 x i16>
  %6707 = shufflevector <16 x i16> %6706, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6708 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6707)
  %6709 = shufflevector <16 x i16> %6706, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6710 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6709)
  %6711 = shufflevector <16 x i16> %6706, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6712 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6711)
  %6713 = shufflevector <16 x i16> %6706, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6714 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6713)
  %6715 = add nsw <4 x i32> %6420, %6708
  %6716 = add nsw <4 x i32> %6421, %6710
  %6717 = add nsw <4 x i32> %6422, %6712
  %6718 = add nsw <4 x i32> %6423, %6714
  %6719 = add nsw i64 %5910, %5831
  %6720 = shl nsw i64 %6719, 4
  %6721 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6720
  %6722 = bitcast i8* %6721 to <16 x i8>*
  %6723 = load <16 x i8>, <16 x i8>* %6722, align 16, !tbaa !438
  %6724 = zext <16 x i8> %6723 to <16 x i16>
  %6725 = shufflevector <16 x i16> %6724, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6726 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6725)
  %6727 = shufflevector <16 x i16> %6724, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6728 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6727)
  %6729 = shufflevector <16 x i16> %6724, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6730 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6729)
  %6731 = shufflevector <16 x i16> %6724, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6732 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6731)
  %6733 = add nsw <4 x i32> %6438, %6726
  %6734 = add nsw <4 x i32> %6439, %6728
  %6735 = add nsw <4 x i32> %6440, %6730
  %6736 = add nsw <4 x i32> %6441, %6732
  %6737 = add nsw i64 %5930, %5831
  %6738 = shl nsw i64 %6737, 4
  %6739 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6738
  %6740 = bitcast i8* %6739 to <16 x i8>*
  %6741 = load <16 x i8>, <16 x i8>* %6740, align 16, !tbaa !438
  %6742 = zext <16 x i8> %6741 to <16 x i16>
  %6743 = shufflevector <16 x i16> %6742, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6744 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6469, <4 x i16> %6743)
  %6745 = shufflevector <16 x i16> %6742, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6746 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6472, <4 x i16> %6745)
  %6747 = shufflevector <16 x i16> %6742, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6748 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6475, <4 x i16> %6747)
  %6749 = shufflevector <16 x i16> %6742, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6750 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6478, <4 x i16> %6749)
  %6751 = add nsw <4 x i32> %6456, %6744
  %6752 = add nsw <4 x i32> %6457, %6746
  %6753 = add nsw <4 x i32> %6458, %6748
  %6754 = add nsw <4 x i32> %6459, %6750
  %6755 = load <8 x i16>, <8 x i16>* %5594, align 16, !tbaa !395
  %6756 = load <8 x i16>, <8 x i16>* %5596, align 16, !tbaa !395
  %6757 = shufflevector <8 x i16> %6756, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %6758 = add nsw i64 %5866, %5832
  %6759 = shl nsw i64 %6758, 4
  %6760 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6759
  %6761 = bitcast i8* %6760 to <16 x i8>*
  %6762 = load <16 x i8>, <16 x i8>* %6761, align 16, !tbaa !438
  %6763 = zext <16 x i8> %6762 to <16 x i16>
  %6764 = shufflevector <8 x i16> %6755, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6765 = shufflevector <16 x i16> %6763, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6766 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6765)
  %6767 = shufflevector <8 x i16> %6755, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6768 = shufflevector <16 x i16> %6763, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6769 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6768)
  %6770 = shufflevector <8 x i16> %6756, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6771 = shufflevector <16 x i16> %6763, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6772 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6771)
  %6773 = shufflevector <16 x i16> %6757, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6774 = shufflevector <16 x i16> %6763, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6775 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6774)
  %6776 = add nsw <4 x i32> %6481, %6766
  %6777 = add nsw <4 x i32> %6482, %6769
  %6778 = add nsw <4 x i32> %6483, %6772
  %6779 = add nsw <4 x i32> %6484, %6775
  %6780 = add nsw i64 %5890, %5832
  %6781 = shl nsw i64 %6780, 4
  %6782 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6781
  %6783 = bitcast i8* %6782 to <16 x i8>*
  %6784 = load <16 x i8>, <16 x i8>* %6783, align 16, !tbaa !438
  %6785 = zext <16 x i8> %6784 to <16 x i16>
  %6786 = shufflevector <16 x i16> %6785, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6787 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6786)
  %6788 = shufflevector <16 x i16> %6785, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6789 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6788)
  %6790 = shufflevector <16 x i16> %6785, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6791 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6790)
  %6792 = shufflevector <16 x i16> %6785, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6793 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6792)
  %6794 = add nsw <4 x i32> %6499, %6787
  %6795 = add nsw <4 x i32> %6500, %6789
  %6796 = add nsw <4 x i32> %6501, %6791
  %6797 = add nsw <4 x i32> %6502, %6793
  %6798 = add nsw i64 %5910, %5832
  %6799 = shl nsw i64 %6798, 4
  %6800 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6799
  %6801 = bitcast i8* %6800 to <16 x i8>*
  %6802 = load <16 x i8>, <16 x i8>* %6801, align 16, !tbaa !438
  %6803 = zext <16 x i8> %6802 to <16 x i16>
  %6804 = shufflevector <16 x i16> %6803, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6805 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6804)
  %6806 = shufflevector <16 x i16> %6803, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6807 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6806)
  %6808 = shufflevector <16 x i16> %6803, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6809 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6808)
  %6810 = shufflevector <16 x i16> %6803, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6811 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6810)
  %6812 = add nsw <4 x i32> %6517, %6805
  %6813 = add nsw <4 x i32> %6518, %6807
  %6814 = add nsw <4 x i32> %6519, %6809
  %6815 = add nsw <4 x i32> %6520, %6811
  %6816 = add nsw i64 %5930, %5832
  %6817 = shl nsw i64 %6816, 4
  %6818 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6817
  %6819 = bitcast i8* %6818 to <16 x i8>*
  %6820 = load <16 x i8>, <16 x i8>* %6819, align 16, !tbaa !438
  %6821 = zext <16 x i8> %6820 to <16 x i16>
  %6822 = shufflevector <16 x i16> %6821, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6823 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6822)
  %6824 = shufflevector <16 x i16> %6821, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6825 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6824)
  %6826 = shufflevector <16 x i16> %6821, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6827 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6826)
  %6828 = shufflevector <16 x i16> %6821, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6829 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6828)
  %6830 = add nsw <4 x i32> %6535, %6823
  %6831 = add nsw <4 x i32> %6536, %6825
  %6832 = add nsw <4 x i32> %6537, %6827
  %6833 = add nsw <4 x i32> %6538, %6829
  %6834 = add nsw i64 %5866, %5833
  %6835 = shl nsw i64 %6834, 4
  %6836 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6835
  %6837 = bitcast i8* %6836 to <16 x i8>*
  %6838 = load <16 x i8>, <16 x i8>* %6837, align 16, !tbaa !438
  %6839 = zext <16 x i8> %6838 to <16 x i16>
  %6840 = shufflevector <16 x i16> %6839, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6841 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6840)
  %6842 = shufflevector <16 x i16> %6839, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6843 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6842)
  %6844 = shufflevector <16 x i16> %6839, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6845 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6844)
  %6846 = shufflevector <16 x i16> %6839, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6847 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6846)
  %6848 = add nsw <4 x i32> %6553, %6841
  %6849 = add nsw <4 x i32> %6554, %6843
  %6850 = add nsw <4 x i32> %6555, %6845
  %6851 = add nsw <4 x i32> %6556, %6847
  %6852 = add nsw i64 %5890, %5833
  %6853 = shl nsw i64 %6852, 4
  %6854 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6853
  %6855 = bitcast i8* %6854 to <16 x i8>*
  %6856 = load <16 x i8>, <16 x i8>* %6855, align 16, !tbaa !438
  %6857 = zext <16 x i8> %6856 to <16 x i16>
  %6858 = shufflevector <16 x i16> %6857, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6859 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6858)
  %6860 = shufflevector <16 x i16> %6857, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6861 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6860)
  %6862 = shufflevector <16 x i16> %6857, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6863 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6862)
  %6864 = shufflevector <16 x i16> %6857, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6865 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6864)
  %6866 = add nsw <4 x i32> %6571, %6859
  %6867 = add nsw <4 x i32> %6572, %6861
  %6868 = add nsw <4 x i32> %6573, %6863
  %6869 = add nsw <4 x i32> %6574, %6865
  %6870 = add nsw i64 %5910, %5833
  %6871 = shl nsw i64 %6870, 4
  %6872 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6871
  %6873 = bitcast i8* %6872 to <16 x i8>*
  %6874 = load <16 x i8>, <16 x i8>* %6873, align 16, !tbaa !438
  %6875 = zext <16 x i8> %6874 to <16 x i16>
  %6876 = shufflevector <16 x i16> %6875, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6877 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6876)
  %6878 = shufflevector <16 x i16> %6875, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6879 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6878)
  %6880 = shufflevector <16 x i16> %6875, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6881 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6880)
  %6882 = shufflevector <16 x i16> %6875, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6883 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6882)
  %6884 = add nsw <4 x i32> %6589, %6877
  %6885 = add nsw <4 x i32> %6590, %6879
  %6886 = add nsw <4 x i32> %6591, %6881
  %6887 = add nsw <4 x i32> %6592, %6883
  %6888 = add nsw i64 %5930, %5833
  %6889 = shl nsw i64 %6888, 4
  %6890 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6889
  %6891 = bitcast i8* %6890 to <16 x i8>*
  %6892 = load <16 x i8>, <16 x i8>* %6891, align 16, !tbaa !438
  %6893 = zext <16 x i8> %6892 to <16 x i16>
  %6894 = shufflevector <16 x i16> %6893, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6895 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6894)
  %6896 = shufflevector <16 x i16> %6893, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6897 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6896)
  %6898 = shufflevector <16 x i16> %6893, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6899 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6898)
  %6900 = shufflevector <16 x i16> %6893, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6901 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6900)
  %6902 = add nsw <4 x i32> %6607, %6895
  %6903 = add nsw <4 x i32> %6608, %6897
  %6904 = add nsw <4 x i32> %6609, %6899
  %6905 = add nsw <4 x i32> %6610, %6901
  %6906 = add nsw i64 %5866, %5834
  %6907 = shl nsw i64 %6906, 4
  %6908 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6907
  %6909 = bitcast i8* %6908 to <16 x i8>*
  %6910 = load <16 x i8>, <16 x i8>* %6909, align 16, !tbaa !438
  %6911 = zext <16 x i8> %6910 to <16 x i16>
  %6912 = shufflevector <16 x i16> %6911, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6913 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6912)
  %6914 = shufflevector <16 x i16> %6911, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6915 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6914)
  %6916 = shufflevector <16 x i16> %6911, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6917 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6916)
  %6918 = shufflevector <16 x i16> %6911, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6919 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6918)
  %6920 = add nsw <4 x i32> %6625, %6913
  %6921 = add nsw <4 x i32> %6626, %6915
  %6922 = add nsw <4 x i32> %6627, %6917
  %6923 = add nsw <4 x i32> %6628, %6919
  %6924 = add nsw i64 %5890, %5834
  %6925 = shl nsw i64 %6924, 4
  %6926 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6925
  %6927 = bitcast i8* %6926 to <16 x i8>*
  %6928 = load <16 x i8>, <16 x i8>* %6927, align 16, !tbaa !438
  %6929 = zext <16 x i8> %6928 to <16 x i16>
  %6930 = shufflevector <16 x i16> %6929, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6931 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6930)
  %6932 = shufflevector <16 x i16> %6929, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6933 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6932)
  %6934 = shufflevector <16 x i16> %6929, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6935 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6934)
  %6936 = shufflevector <16 x i16> %6929, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6937 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6936)
  %6938 = add nsw <4 x i32> %6643, %6931
  %6939 = add nsw <4 x i32> %6644, %6933
  %6940 = add nsw <4 x i32> %6645, %6935
  %6941 = add nsw <4 x i32> %6646, %6937
  %6942 = add nsw i64 %5910, %5834
  %6943 = shl nsw i64 %6942, 4
  %6944 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6943
  %6945 = bitcast i8* %6944 to <16 x i8>*
  %6946 = load <16 x i8>, <16 x i8>* %6945, align 16, !tbaa !438
  %6947 = zext <16 x i8> %6946 to <16 x i16>
  %6948 = shufflevector <16 x i16> %6947, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6949 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6948)
  %6950 = shufflevector <16 x i16> %6947, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6951 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6950)
  %6952 = shufflevector <16 x i16> %6947, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6953 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6952)
  %6954 = shufflevector <16 x i16> %6947, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6955 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6954)
  %6956 = add nsw <4 x i32> %6661, %6949
  %6957 = add nsw <4 x i32> %6662, %6951
  %6958 = add nsw <4 x i32> %6663, %6953
  %6959 = add nsw <4 x i32> %6664, %6955
  %6960 = add nsw i64 %5930, %5834
  %6961 = shl nsw i64 %6960, 4
  %6962 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6961
  %6963 = bitcast i8* %6962 to <16 x i8>*
  %6964 = load <16 x i8>, <16 x i8>* %6963, align 16, !tbaa !438
  %6965 = zext <16 x i8> %6964 to <16 x i16>
  %6966 = shufflevector <16 x i16> %6965, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6967 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6966)
  %6968 = shufflevector <16 x i16> %6965, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6969 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6968)
  %6970 = shufflevector <16 x i16> %6965, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6971 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6970)
  %6972 = shufflevector <16 x i16> %6965, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6973 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6972)
  %6974 = add nsw <4 x i32> %6679, %6967
  %6975 = add nsw <4 x i32> %6680, %6969
  %6976 = add nsw <4 x i32> %6681, %6971
  %6977 = add nsw <4 x i32> %6682, %6973
  %6978 = add nsw i64 %5866, %5835
  %6979 = shl nsw i64 %6978, 4
  %6980 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6979
  %6981 = bitcast i8* %6980 to <16 x i8>*
  %6982 = load <16 x i8>, <16 x i8>* %6981, align 16, !tbaa !438
  %6983 = zext <16 x i8> %6982 to <16 x i16>
  %6984 = shufflevector <16 x i16> %6983, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %6985 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %6984)
  %6986 = shufflevector <16 x i16> %6983, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6987 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %6986)
  %6988 = shufflevector <16 x i16> %6983, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %6989 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %6988)
  %6990 = shufflevector <16 x i16> %6983, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6991 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %6990)
  %6992 = add nsw <4 x i32> %6697, %6985
  %6993 = add nsw <4 x i32> %6698, %6987
  %6994 = add nsw <4 x i32> %6699, %6989
  %6995 = add nsw <4 x i32> %6700, %6991
  %6996 = add nsw i64 %5890, %5835
  %6997 = shl nsw i64 %6996, 4
  %6998 = getelementptr inbounds i8, i8* %resampled_input122, i64 %6997
  %6999 = bitcast i8* %6998 to <16 x i8>*
  %7000 = load <16 x i8>, <16 x i8>* %6999, align 16, !tbaa !438
  %7001 = zext <16 x i8> %7000 to <16 x i16>
  %7002 = shufflevector <16 x i16> %7001, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7003 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %7002)
  %7004 = shufflevector <16 x i16> %7001, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7005 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %7004)
  %7006 = shufflevector <16 x i16> %7001, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7007 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %7006)
  %7008 = shufflevector <16 x i16> %7001, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7009 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %7008)
  %7010 = add nsw <4 x i32> %6715, %7003
  %7011 = add nsw <4 x i32> %6716, %7005
  %7012 = add nsw <4 x i32> %6717, %7007
  %7013 = add nsw <4 x i32> %6718, %7009
  %7014 = add nsw i64 %5910, %5835
  %7015 = shl nsw i64 %7014, 4
  %7016 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7015
  %7017 = bitcast i8* %7016 to <16 x i8>*
  %7018 = load <16 x i8>, <16 x i8>* %7017, align 16, !tbaa !438
  %7019 = zext <16 x i8> %7018 to <16 x i16>
  %7020 = shufflevector <16 x i16> %7019, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7021 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %7020)
  %7022 = shufflevector <16 x i16> %7019, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7023 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %7022)
  %7024 = shufflevector <16 x i16> %7019, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7025 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %7024)
  %7026 = shufflevector <16 x i16> %7019, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7027 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %7026)
  %7028 = add nsw <4 x i32> %6733, %7021
  %7029 = add nsw <4 x i32> %6734, %7023
  %7030 = add nsw <4 x i32> %6735, %7025
  %7031 = add nsw <4 x i32> %6736, %7027
  %7032 = add nsw i64 %5930, %5835
  %7033 = shl nsw i64 %7032, 4
  %7034 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7033
  %7035 = bitcast i8* %7034 to <16 x i8>*
  %7036 = load <16 x i8>, <16 x i8>* %7035, align 16, !tbaa !438
  %7037 = zext <16 x i8> %7036 to <16 x i16>
  %7038 = shufflevector <16 x i16> %7037, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7039 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6764, <4 x i16> %7038)
  %7040 = shufflevector <16 x i16> %7037, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7041 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6767, <4 x i16> %7040)
  %7042 = shufflevector <16 x i16> %7037, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7043 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6770, <4 x i16> %7042)
  %7044 = shufflevector <16 x i16> %7037, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7045 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %6773, <4 x i16> %7044)
  %7046 = add nsw <4 x i32> %6751, %7039
  %7047 = add nsw <4 x i32> %6752, %7041
  %7048 = add nsw <4 x i32> %6753, %7043
  %7049 = add nsw <4 x i32> %6754, %7045
  %7050 = load <8 x i16>, <8 x i16>* %5598, align 16, !tbaa !395
  %7051 = load <8 x i16>, <8 x i16>* %5600, align 16, !tbaa !395
  %7052 = shufflevector <8 x i16> %7051, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %7053 = add nsw i64 %5866, %5836
  %7054 = shl nsw i64 %7053, 4
  %7055 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7054
  %7056 = bitcast i8* %7055 to <16 x i8>*
  %7057 = load <16 x i8>, <16 x i8>* %7056, align 16, !tbaa !438
  %7058 = zext <16 x i8> %7057 to <16 x i16>
  %7059 = shufflevector <8 x i16> %7050, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7060 = shufflevector <16 x i16> %7058, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7061 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7060)
  %7062 = shufflevector <8 x i16> %7050, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7063 = shufflevector <16 x i16> %7058, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7064 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7063)
  %7065 = shufflevector <8 x i16> %7051, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7066 = shufflevector <16 x i16> %7058, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7067 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7066)
  %7068 = shufflevector <16 x i16> %7052, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7069 = shufflevector <16 x i16> %7058, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7070 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7069)
  %7071 = add nsw <4 x i32> %6776, %7061
  %7072 = add nsw <4 x i32> %6777, %7064
  %7073 = add nsw <4 x i32> %6778, %7067
  %7074 = add nsw <4 x i32> %6779, %7070
  %7075 = add nsw i64 %5890, %5836
  %7076 = shl nsw i64 %7075, 4
  %7077 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7076
  %7078 = bitcast i8* %7077 to <16 x i8>*
  %7079 = load <16 x i8>, <16 x i8>* %7078, align 16, !tbaa !438
  %7080 = zext <16 x i8> %7079 to <16 x i16>
  %7081 = shufflevector <16 x i16> %7080, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7082 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7081)
  %7083 = shufflevector <16 x i16> %7080, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7084 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7083)
  %7085 = shufflevector <16 x i16> %7080, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7086 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7085)
  %7087 = shufflevector <16 x i16> %7080, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7088 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7087)
  %7089 = add nsw <4 x i32> %6794, %7082
  %7090 = add nsw <4 x i32> %6795, %7084
  %7091 = add nsw <4 x i32> %6796, %7086
  %7092 = add nsw <4 x i32> %6797, %7088
  %7093 = add nsw i64 %5910, %5836
  %7094 = shl nsw i64 %7093, 4
  %7095 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7094
  %7096 = bitcast i8* %7095 to <16 x i8>*
  %7097 = load <16 x i8>, <16 x i8>* %7096, align 16, !tbaa !438
  %7098 = zext <16 x i8> %7097 to <16 x i16>
  %7099 = shufflevector <16 x i16> %7098, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7100 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7099)
  %7101 = shufflevector <16 x i16> %7098, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7102 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7101)
  %7103 = shufflevector <16 x i16> %7098, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7104 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7103)
  %7105 = shufflevector <16 x i16> %7098, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7106 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7105)
  %7107 = add nsw <4 x i32> %6812, %7100
  %7108 = add nsw <4 x i32> %6813, %7102
  %7109 = add nsw <4 x i32> %6814, %7104
  %7110 = add nsw <4 x i32> %6815, %7106
  %7111 = add nsw i64 %5930, %5836
  %7112 = shl nsw i64 %7111, 4
  %7113 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7112
  %7114 = bitcast i8* %7113 to <16 x i8>*
  %7115 = load <16 x i8>, <16 x i8>* %7114, align 16, !tbaa !438
  %7116 = zext <16 x i8> %7115 to <16 x i16>
  %7117 = shufflevector <16 x i16> %7116, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7118 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7117)
  %7119 = shufflevector <16 x i16> %7116, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7120 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7119)
  %7121 = shufflevector <16 x i16> %7116, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7122 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7121)
  %7123 = shufflevector <16 x i16> %7116, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7124 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7123)
  %7125 = add nsw <4 x i32> %6830, %7118
  %7126 = add nsw <4 x i32> %6831, %7120
  %7127 = add nsw <4 x i32> %6832, %7122
  %7128 = add nsw <4 x i32> %6833, %7124
  %7129 = add nsw i64 %5866, %5837
  %7130 = shl nsw i64 %7129, 4
  %7131 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7130
  %7132 = bitcast i8* %7131 to <16 x i8>*
  %7133 = load <16 x i8>, <16 x i8>* %7132, align 16, !tbaa !438
  %7134 = zext <16 x i8> %7133 to <16 x i16>
  %7135 = shufflevector <16 x i16> %7134, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7136 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7135)
  %7137 = shufflevector <16 x i16> %7134, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7138 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7137)
  %7139 = shufflevector <16 x i16> %7134, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7140 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7139)
  %7141 = shufflevector <16 x i16> %7134, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7142 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7141)
  %7143 = add nsw <4 x i32> %6848, %7136
  %7144 = add nsw <4 x i32> %6849, %7138
  %7145 = add nsw <4 x i32> %6850, %7140
  %7146 = add nsw <4 x i32> %6851, %7142
  %7147 = add nsw i64 %5890, %5837
  %7148 = shl nsw i64 %7147, 4
  %7149 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7148
  %7150 = bitcast i8* %7149 to <16 x i8>*
  %7151 = load <16 x i8>, <16 x i8>* %7150, align 16, !tbaa !438
  %7152 = zext <16 x i8> %7151 to <16 x i16>
  %7153 = shufflevector <16 x i16> %7152, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7154 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7153)
  %7155 = shufflevector <16 x i16> %7152, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7156 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7155)
  %7157 = shufflevector <16 x i16> %7152, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7158 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7157)
  %7159 = shufflevector <16 x i16> %7152, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7160 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7159)
  %7161 = add nsw <4 x i32> %6866, %7154
  %7162 = add nsw <4 x i32> %6867, %7156
  %7163 = add nsw <4 x i32> %6868, %7158
  %7164 = add nsw <4 x i32> %6869, %7160
  %7165 = add nsw i64 %5910, %5837
  %7166 = shl nsw i64 %7165, 4
  %7167 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7166
  %7168 = bitcast i8* %7167 to <16 x i8>*
  %7169 = load <16 x i8>, <16 x i8>* %7168, align 16, !tbaa !438
  %7170 = zext <16 x i8> %7169 to <16 x i16>
  %7171 = shufflevector <16 x i16> %7170, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7172 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7171)
  %7173 = shufflevector <16 x i16> %7170, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7174 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7173)
  %7175 = shufflevector <16 x i16> %7170, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7176 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7175)
  %7177 = shufflevector <16 x i16> %7170, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7178 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7177)
  %7179 = add nsw <4 x i32> %6884, %7172
  %7180 = add nsw <4 x i32> %6885, %7174
  %7181 = add nsw <4 x i32> %6886, %7176
  %7182 = add nsw <4 x i32> %6887, %7178
  %7183 = add nsw i64 %5930, %5837
  %7184 = shl nsw i64 %7183, 4
  %7185 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7184
  %7186 = bitcast i8* %7185 to <16 x i8>*
  %7187 = load <16 x i8>, <16 x i8>* %7186, align 16, !tbaa !438
  %7188 = zext <16 x i8> %7187 to <16 x i16>
  %7189 = shufflevector <16 x i16> %7188, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7190 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7189)
  %7191 = shufflevector <16 x i16> %7188, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7192 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7191)
  %7193 = shufflevector <16 x i16> %7188, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7194 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7193)
  %7195 = shufflevector <16 x i16> %7188, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7196 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7195)
  %7197 = add nsw <4 x i32> %6902, %7190
  %7198 = add nsw <4 x i32> %6903, %7192
  %7199 = add nsw <4 x i32> %6904, %7194
  %7200 = add nsw <4 x i32> %6905, %7196
  %7201 = add nsw i64 %5866, %5838
  %7202 = shl nsw i64 %7201, 4
  %7203 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7202
  %7204 = bitcast i8* %7203 to <16 x i8>*
  %7205 = load <16 x i8>, <16 x i8>* %7204, align 16, !tbaa !438
  %7206 = zext <16 x i8> %7205 to <16 x i16>
  %7207 = shufflevector <16 x i16> %7206, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7208 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7207)
  %7209 = shufflevector <16 x i16> %7206, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7210 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7209)
  %7211 = shufflevector <16 x i16> %7206, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7212 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7211)
  %7213 = shufflevector <16 x i16> %7206, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7214 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7213)
  %7215 = add nsw <4 x i32> %6920, %7208
  %7216 = add nsw <4 x i32> %6921, %7210
  %7217 = add nsw <4 x i32> %6922, %7212
  %7218 = add nsw <4 x i32> %6923, %7214
  %7219 = add nsw i64 %5890, %5838
  %7220 = shl nsw i64 %7219, 4
  %7221 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7220
  %7222 = bitcast i8* %7221 to <16 x i8>*
  %7223 = load <16 x i8>, <16 x i8>* %7222, align 16, !tbaa !438
  %7224 = zext <16 x i8> %7223 to <16 x i16>
  %7225 = shufflevector <16 x i16> %7224, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7226 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7225)
  %7227 = shufflevector <16 x i16> %7224, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7228 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7227)
  %7229 = shufflevector <16 x i16> %7224, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7230 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7229)
  %7231 = shufflevector <16 x i16> %7224, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7232 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7231)
  %7233 = add nsw <4 x i32> %6938, %7226
  %7234 = add nsw <4 x i32> %6939, %7228
  %7235 = add nsw <4 x i32> %6940, %7230
  %7236 = add nsw <4 x i32> %6941, %7232
  %7237 = add nsw i64 %5910, %5838
  %7238 = shl nsw i64 %7237, 4
  %7239 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7238
  %7240 = bitcast i8* %7239 to <16 x i8>*
  %7241 = load <16 x i8>, <16 x i8>* %7240, align 16, !tbaa !438
  %7242 = zext <16 x i8> %7241 to <16 x i16>
  %7243 = shufflevector <16 x i16> %7242, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7244 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7243)
  %7245 = shufflevector <16 x i16> %7242, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7246 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7245)
  %7247 = shufflevector <16 x i16> %7242, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7248 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7247)
  %7249 = shufflevector <16 x i16> %7242, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7250 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7249)
  %7251 = add nsw <4 x i32> %6956, %7244
  %7252 = add nsw <4 x i32> %6957, %7246
  %7253 = add nsw <4 x i32> %6958, %7248
  %7254 = add nsw <4 x i32> %6959, %7250
  %7255 = add nsw i64 %5930, %5838
  %7256 = shl nsw i64 %7255, 4
  %7257 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7256
  %7258 = bitcast i8* %7257 to <16 x i8>*
  %7259 = load <16 x i8>, <16 x i8>* %7258, align 16, !tbaa !438
  %7260 = zext <16 x i8> %7259 to <16 x i16>
  %7261 = shufflevector <16 x i16> %7260, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7262 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7261)
  %7263 = shufflevector <16 x i16> %7260, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7264 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7263)
  %7265 = shufflevector <16 x i16> %7260, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7266 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7265)
  %7267 = shufflevector <16 x i16> %7260, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7268 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7267)
  %7269 = add nsw <4 x i32> %6974, %7262
  %7270 = add nsw <4 x i32> %6975, %7264
  %7271 = add nsw <4 x i32> %6976, %7266
  %7272 = add nsw <4 x i32> %6977, %7268
  %7273 = add nsw i64 %5866, %5839
  %7274 = shl nsw i64 %7273, 4
  %7275 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7274
  %7276 = bitcast i8* %7275 to <16 x i8>*
  %7277 = load <16 x i8>, <16 x i8>* %7276, align 16, !tbaa !438
  %7278 = zext <16 x i8> %7277 to <16 x i16>
  %7279 = shufflevector <16 x i16> %7278, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7280 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7279)
  %7281 = shufflevector <16 x i16> %7278, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7282 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7281)
  %7283 = shufflevector <16 x i16> %7278, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7284 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7283)
  %7285 = shufflevector <16 x i16> %7278, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7286 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7285)
  %7287 = add nsw <4 x i32> %6992, %7280
  %7288 = add nsw <4 x i32> %6993, %7282
  %7289 = add nsw <4 x i32> %6994, %7284
  %7290 = add nsw <4 x i32> %6995, %7286
  %7291 = add nsw i64 %5890, %5839
  %7292 = shl nsw i64 %7291, 4
  %7293 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7292
  %7294 = bitcast i8* %7293 to <16 x i8>*
  %7295 = load <16 x i8>, <16 x i8>* %7294, align 16, !tbaa !438
  %7296 = zext <16 x i8> %7295 to <16 x i16>
  %7297 = shufflevector <16 x i16> %7296, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7298 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7297)
  %7299 = shufflevector <16 x i16> %7296, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7300 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7299)
  %7301 = shufflevector <16 x i16> %7296, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7302 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7301)
  %7303 = shufflevector <16 x i16> %7296, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7304 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7303)
  %7305 = add nsw <4 x i32> %7010, %7298
  %7306 = add nsw <4 x i32> %7011, %7300
  %7307 = add nsw <4 x i32> %7012, %7302
  %7308 = add nsw <4 x i32> %7013, %7304
  %7309 = add nsw i64 %5910, %5839
  %7310 = shl nsw i64 %7309, 4
  %7311 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7310
  %7312 = bitcast i8* %7311 to <16 x i8>*
  %7313 = load <16 x i8>, <16 x i8>* %7312, align 16, !tbaa !438
  %7314 = zext <16 x i8> %7313 to <16 x i16>
  %7315 = shufflevector <16 x i16> %7314, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7316 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7315)
  %7317 = shufflevector <16 x i16> %7314, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7318 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7317)
  %7319 = shufflevector <16 x i16> %7314, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7320 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7319)
  %7321 = shufflevector <16 x i16> %7314, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7322 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7321)
  %7323 = add nsw <4 x i32> %7028, %7316
  %7324 = add nsw <4 x i32> %7029, %7318
  %7325 = add nsw <4 x i32> %7030, %7320
  %7326 = add nsw <4 x i32> %7031, %7322
  %7327 = add nsw i64 %5930, %5839
  %7328 = shl nsw i64 %7327, 4
  %7329 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7328
  %7330 = bitcast i8* %7329 to <16 x i8>*
  %7331 = load <16 x i8>, <16 x i8>* %7330, align 16, !tbaa !438
  %7332 = zext <16 x i8> %7331 to <16 x i16>
  %7333 = shufflevector <16 x i16> %7332, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7334 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7059, <4 x i16> %7333)
  %7335 = shufflevector <16 x i16> %7332, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7336 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7062, <4 x i16> %7335)
  %7337 = shufflevector <16 x i16> %7332, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7338 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7065, <4 x i16> %7337)
  %7339 = shufflevector <16 x i16> %7332, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7340 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7068, <4 x i16> %7339)
  %7341 = add nsw <4 x i32> %7046, %7334
  %7342 = add nsw <4 x i32> %7047, %7336
  %7343 = add nsw <4 x i32> %7048, %7338
  %7344 = add nsw <4 x i32> %7049, %7340
  %7345 = load <8 x i16>, <8 x i16>* %5602, align 16, !tbaa !395
  %7346 = load <8 x i16>, <8 x i16>* %5604, align 16, !tbaa !395
  %7347 = shufflevector <8 x i16> %7346, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %7348 = add nsw i64 %5866, %5840
  %7349 = shl nsw i64 %7348, 4
  %7350 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7349
  %7351 = bitcast i8* %7350 to <16 x i8>*
  %7352 = load <16 x i8>, <16 x i8>* %7351, align 16, !tbaa !438
  %7353 = zext <16 x i8> %7352 to <16 x i16>
  %7354 = shufflevector <8 x i16> %7345, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7355 = shufflevector <16 x i16> %7353, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7356 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7355)
  %7357 = shufflevector <8 x i16> %7345, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7358 = shufflevector <16 x i16> %7353, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7359 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7358)
  %7360 = shufflevector <8 x i16> %7346, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7361 = shufflevector <16 x i16> %7353, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7362 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7361)
  %7363 = shufflevector <16 x i16> %7347, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7364 = shufflevector <16 x i16> %7353, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7365 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7364)
  %7366 = add nsw <4 x i32> %7071, %7356
  %7367 = add nsw <4 x i32> %7072, %7359
  %7368 = add nsw <4 x i32> %7073, %7362
  %7369 = add nsw <4 x i32> %7074, %7365
  %7370 = add nsw i64 %5890, %5840
  %7371 = shl nsw i64 %7370, 4
  %7372 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7371
  %7373 = bitcast i8* %7372 to <16 x i8>*
  %7374 = load <16 x i8>, <16 x i8>* %7373, align 16, !tbaa !438
  %7375 = zext <16 x i8> %7374 to <16 x i16>
  %7376 = shufflevector <16 x i16> %7375, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7377 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7376)
  %7378 = shufflevector <16 x i16> %7375, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7379 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7378)
  %7380 = shufflevector <16 x i16> %7375, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7381 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7380)
  %7382 = shufflevector <16 x i16> %7375, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7383 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7382)
  %7384 = add nsw <4 x i32> %7089, %7377
  %7385 = add nsw <4 x i32> %7090, %7379
  %7386 = add nsw <4 x i32> %7091, %7381
  %7387 = add nsw <4 x i32> %7092, %7383
  %7388 = add nsw i64 %5910, %5840
  %7389 = shl nsw i64 %7388, 4
  %7390 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7389
  %7391 = bitcast i8* %7390 to <16 x i8>*
  %7392 = load <16 x i8>, <16 x i8>* %7391, align 16, !tbaa !438
  %7393 = zext <16 x i8> %7392 to <16 x i16>
  %7394 = shufflevector <16 x i16> %7393, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7395 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7394)
  %7396 = shufflevector <16 x i16> %7393, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7397 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7396)
  %7398 = shufflevector <16 x i16> %7393, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7399 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7398)
  %7400 = shufflevector <16 x i16> %7393, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7401 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7400)
  %7402 = add nsw <4 x i32> %7107, %7395
  %7403 = add nsw <4 x i32> %7108, %7397
  %7404 = add nsw <4 x i32> %7109, %7399
  %7405 = add nsw <4 x i32> %7110, %7401
  %7406 = add nsw i64 %5930, %5840
  %7407 = shl nsw i64 %7406, 4
  %7408 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7407
  %7409 = bitcast i8* %7408 to <16 x i8>*
  %7410 = load <16 x i8>, <16 x i8>* %7409, align 16, !tbaa !438
  %7411 = zext <16 x i8> %7410 to <16 x i16>
  %7412 = shufflevector <16 x i16> %7411, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7413 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7412)
  %7414 = shufflevector <16 x i16> %7411, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7415 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7414)
  %7416 = shufflevector <16 x i16> %7411, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7417 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7416)
  %7418 = shufflevector <16 x i16> %7411, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7419 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7418)
  %7420 = add nsw <4 x i32> %7125, %7413
  %7421 = add nsw <4 x i32> %7126, %7415
  %7422 = add nsw <4 x i32> %7127, %7417
  %7423 = add nsw <4 x i32> %7128, %7419
  %7424 = add nsw i64 %5866, %5841
  %7425 = shl nsw i64 %7424, 4
  %7426 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7425
  %7427 = bitcast i8* %7426 to <16 x i8>*
  %7428 = load <16 x i8>, <16 x i8>* %7427, align 16, !tbaa !438
  %7429 = zext <16 x i8> %7428 to <16 x i16>
  %7430 = shufflevector <16 x i16> %7429, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7431 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7430)
  %7432 = shufflevector <16 x i16> %7429, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7433 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7432)
  %7434 = shufflevector <16 x i16> %7429, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7435 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7434)
  %7436 = shufflevector <16 x i16> %7429, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7437 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7436)
  %7438 = add nsw <4 x i32> %7143, %7431
  %7439 = add nsw <4 x i32> %7144, %7433
  %7440 = add nsw <4 x i32> %7145, %7435
  %7441 = add nsw <4 x i32> %7146, %7437
  %7442 = add nsw i64 %5890, %5841
  %7443 = shl nsw i64 %7442, 4
  %7444 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7443
  %7445 = bitcast i8* %7444 to <16 x i8>*
  %7446 = load <16 x i8>, <16 x i8>* %7445, align 16, !tbaa !438
  %7447 = zext <16 x i8> %7446 to <16 x i16>
  %7448 = shufflevector <16 x i16> %7447, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7449 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7448)
  %7450 = shufflevector <16 x i16> %7447, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7451 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7450)
  %7452 = shufflevector <16 x i16> %7447, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7453 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7452)
  %7454 = shufflevector <16 x i16> %7447, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7455 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7454)
  %7456 = add nsw <4 x i32> %7161, %7449
  %7457 = add nsw <4 x i32> %7162, %7451
  %7458 = add nsw <4 x i32> %7163, %7453
  %7459 = add nsw <4 x i32> %7164, %7455
  %7460 = add nsw i64 %5910, %5841
  %7461 = shl nsw i64 %7460, 4
  %7462 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7461
  %7463 = bitcast i8* %7462 to <16 x i8>*
  %7464 = load <16 x i8>, <16 x i8>* %7463, align 16, !tbaa !438
  %7465 = zext <16 x i8> %7464 to <16 x i16>
  %7466 = shufflevector <16 x i16> %7465, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7467 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7466)
  %7468 = shufflevector <16 x i16> %7465, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7469 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7468)
  %7470 = shufflevector <16 x i16> %7465, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7471 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7470)
  %7472 = shufflevector <16 x i16> %7465, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7473 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7472)
  %7474 = add nsw <4 x i32> %7179, %7467
  %7475 = add nsw <4 x i32> %7180, %7469
  %7476 = add nsw <4 x i32> %7181, %7471
  %7477 = add nsw <4 x i32> %7182, %7473
  %7478 = add nsw i64 %5930, %5841
  %7479 = shl nsw i64 %7478, 4
  %7480 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7479
  %7481 = bitcast i8* %7480 to <16 x i8>*
  %7482 = load <16 x i8>, <16 x i8>* %7481, align 16, !tbaa !438
  %7483 = zext <16 x i8> %7482 to <16 x i16>
  %7484 = shufflevector <16 x i16> %7483, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7485 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7484)
  %7486 = shufflevector <16 x i16> %7483, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7487 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7486)
  %7488 = shufflevector <16 x i16> %7483, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7489 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7488)
  %7490 = shufflevector <16 x i16> %7483, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7491 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7490)
  %7492 = add nsw <4 x i32> %7197, %7485
  %7493 = add nsw <4 x i32> %7198, %7487
  %7494 = add nsw <4 x i32> %7199, %7489
  %7495 = add nsw <4 x i32> %7200, %7491
  %7496 = add nsw i64 %5866, %5842
  %7497 = shl nsw i64 %7496, 4
  %7498 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7497
  %7499 = bitcast i8* %7498 to <16 x i8>*
  %7500 = load <16 x i8>, <16 x i8>* %7499, align 16, !tbaa !438
  %7501 = zext <16 x i8> %7500 to <16 x i16>
  %7502 = shufflevector <16 x i16> %7501, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7503 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7502)
  %7504 = shufflevector <16 x i16> %7501, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7505 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7504)
  %7506 = shufflevector <16 x i16> %7501, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7507 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7506)
  %7508 = shufflevector <16 x i16> %7501, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7509 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7508)
  %7510 = add nsw <4 x i32> %7215, %7503
  %7511 = add nsw <4 x i32> %7216, %7505
  %7512 = add nsw <4 x i32> %7217, %7507
  %7513 = add nsw <4 x i32> %7218, %7509
  %7514 = add nsw i64 %5890, %5842
  %7515 = shl nsw i64 %7514, 4
  %7516 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7515
  %7517 = bitcast i8* %7516 to <16 x i8>*
  %7518 = load <16 x i8>, <16 x i8>* %7517, align 16, !tbaa !438
  %7519 = zext <16 x i8> %7518 to <16 x i16>
  %7520 = shufflevector <16 x i16> %7519, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7521 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7520)
  %7522 = shufflevector <16 x i16> %7519, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7523 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7522)
  %7524 = shufflevector <16 x i16> %7519, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7525 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7524)
  %7526 = shufflevector <16 x i16> %7519, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7527 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7526)
  %7528 = add nsw <4 x i32> %7233, %7521
  %7529 = add nsw <4 x i32> %7234, %7523
  %7530 = add nsw <4 x i32> %7235, %7525
  %7531 = add nsw <4 x i32> %7236, %7527
  %7532 = add nsw i64 %5910, %5842
  %7533 = shl nsw i64 %7532, 4
  %7534 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7533
  %7535 = bitcast i8* %7534 to <16 x i8>*
  %7536 = load <16 x i8>, <16 x i8>* %7535, align 16, !tbaa !438
  %7537 = zext <16 x i8> %7536 to <16 x i16>
  %7538 = shufflevector <16 x i16> %7537, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7539 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7538)
  %7540 = shufflevector <16 x i16> %7537, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7541 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7540)
  %7542 = shufflevector <16 x i16> %7537, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7543 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7542)
  %7544 = shufflevector <16 x i16> %7537, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7545 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7544)
  %7546 = add nsw <4 x i32> %7251, %7539
  %7547 = add nsw <4 x i32> %7252, %7541
  %7548 = add nsw <4 x i32> %7253, %7543
  %7549 = add nsw <4 x i32> %7254, %7545
  %7550 = add nsw i64 %5930, %5842
  %7551 = shl nsw i64 %7550, 4
  %7552 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7551
  %7553 = bitcast i8* %7552 to <16 x i8>*
  %7554 = load <16 x i8>, <16 x i8>* %7553, align 16, !tbaa !438
  %7555 = zext <16 x i8> %7554 to <16 x i16>
  %7556 = shufflevector <16 x i16> %7555, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7557 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7556)
  %7558 = shufflevector <16 x i16> %7555, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7559 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7558)
  %7560 = shufflevector <16 x i16> %7555, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7561 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7560)
  %7562 = shufflevector <16 x i16> %7555, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7563 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7562)
  %7564 = add nsw <4 x i32> %7269, %7557
  %7565 = add nsw <4 x i32> %7270, %7559
  %7566 = add nsw <4 x i32> %7271, %7561
  %7567 = add nsw <4 x i32> %7272, %7563
  %7568 = add nsw i64 %5866, %5843
  %7569 = shl nsw i64 %7568, 4
  %7570 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7569
  %7571 = bitcast i8* %7570 to <16 x i8>*
  %7572 = load <16 x i8>, <16 x i8>* %7571, align 16, !tbaa !438
  %7573 = zext <16 x i8> %7572 to <16 x i16>
  %7574 = shufflevector <16 x i16> %7573, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7575 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7574)
  %7576 = shufflevector <16 x i16> %7573, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7577 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7576)
  %7578 = shufflevector <16 x i16> %7573, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7579 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7578)
  %7580 = shufflevector <16 x i16> %7573, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7581 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7580)
  %7582 = add nsw <4 x i32> %7287, %7575
  %7583 = add nsw <4 x i32> %7288, %7577
  %7584 = add nsw <4 x i32> %7289, %7579
  %7585 = add nsw <4 x i32> %7290, %7581
  %7586 = add nsw i64 %5890, %5843
  %7587 = shl nsw i64 %7586, 4
  %7588 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7587
  %7589 = bitcast i8* %7588 to <16 x i8>*
  %7590 = load <16 x i8>, <16 x i8>* %7589, align 16, !tbaa !438
  %7591 = zext <16 x i8> %7590 to <16 x i16>
  %7592 = shufflevector <16 x i16> %7591, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7593 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7592)
  %7594 = shufflevector <16 x i16> %7591, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7595 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7594)
  %7596 = shufflevector <16 x i16> %7591, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7597 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7596)
  %7598 = shufflevector <16 x i16> %7591, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7599 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7598)
  %7600 = add nsw <4 x i32> %7305, %7593
  %7601 = add nsw <4 x i32> %7306, %7595
  %7602 = add nsw <4 x i32> %7307, %7597
  %7603 = add nsw <4 x i32> %7308, %7599
  %7604 = add nsw i64 %5910, %5843
  %7605 = shl nsw i64 %7604, 4
  %7606 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7605
  %7607 = bitcast i8* %7606 to <16 x i8>*
  %7608 = load <16 x i8>, <16 x i8>* %7607, align 16, !tbaa !438
  %7609 = zext <16 x i8> %7608 to <16 x i16>
  %7610 = shufflevector <16 x i16> %7609, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7611 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7610)
  %7612 = shufflevector <16 x i16> %7609, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7613 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7612)
  %7614 = shufflevector <16 x i16> %7609, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7615 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7614)
  %7616 = shufflevector <16 x i16> %7609, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7617 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7616)
  %7618 = add nsw <4 x i32> %7323, %7611
  %7619 = add nsw <4 x i32> %7324, %7613
  %7620 = add nsw <4 x i32> %7325, %7615
  %7621 = add nsw <4 x i32> %7326, %7617
  %7622 = add nsw i64 %5930, %5843
  %7623 = shl nsw i64 %7622, 4
  %7624 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7623
  %7625 = bitcast i8* %7624 to <16 x i8>*
  %7626 = load <16 x i8>, <16 x i8>* %7625, align 16, !tbaa !438
  %7627 = zext <16 x i8> %7626 to <16 x i16>
  %7628 = shufflevector <16 x i16> %7627, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7629 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7354, <4 x i16> %7628)
  %7630 = shufflevector <16 x i16> %7627, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7631 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7357, <4 x i16> %7630)
  %7632 = shufflevector <16 x i16> %7627, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7633 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7360, <4 x i16> %7632)
  %7634 = shufflevector <16 x i16> %7627, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7635 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7363, <4 x i16> %7634)
  %7636 = add nsw <4 x i32> %7341, %7629
  %7637 = add nsw <4 x i32> %7342, %7631
  %7638 = add nsw <4 x i32> %7343, %7633
  %7639 = add nsw <4 x i32> %7344, %7635
  %7640 = load <8 x i16>, <8 x i16>* %5606, align 16, !tbaa !395
  %7641 = load <8 x i16>, <8 x i16>* %5608, align 16, !tbaa !395
  %7642 = shufflevector <8 x i16> %7641, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %7643 = add nsw i64 %5866, %5844
  %7644 = shl nsw i64 %7643, 4
  %7645 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7644
  %7646 = bitcast i8* %7645 to <16 x i8>*
  %7647 = load <16 x i8>, <16 x i8>* %7646, align 16, !tbaa !438
  %7648 = zext <16 x i8> %7647 to <16 x i16>
  %7649 = shufflevector <8 x i16> %7640, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7650 = shufflevector <16 x i16> %7648, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7651 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7650)
  %7652 = shufflevector <8 x i16> %7640, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7653 = shufflevector <16 x i16> %7648, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7654 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7653)
  %7655 = shufflevector <8 x i16> %7641, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7656 = shufflevector <16 x i16> %7648, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7657 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7656)
  %7658 = shufflevector <16 x i16> %7642, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7659 = shufflevector <16 x i16> %7648, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7660 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7659)
  %7661 = add nsw <4 x i32> %7366, %7651
  %7662 = add nsw <4 x i32> %7367, %7654
  %7663 = add nsw <4 x i32> %7368, %7657
  %7664 = add nsw <4 x i32> %7369, %7660
  %7665 = add nsw i64 %5890, %5844
  %7666 = shl nsw i64 %7665, 4
  %7667 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7666
  %7668 = bitcast i8* %7667 to <16 x i8>*
  %7669 = load <16 x i8>, <16 x i8>* %7668, align 16, !tbaa !438
  %7670 = zext <16 x i8> %7669 to <16 x i16>
  %7671 = shufflevector <16 x i16> %7670, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7672 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7671)
  %7673 = shufflevector <16 x i16> %7670, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7674 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7673)
  %7675 = shufflevector <16 x i16> %7670, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7676 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7675)
  %7677 = shufflevector <16 x i16> %7670, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7678 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7677)
  %7679 = add nsw <4 x i32> %7384, %7672
  %7680 = add nsw <4 x i32> %7385, %7674
  %7681 = add nsw <4 x i32> %7386, %7676
  %7682 = add nsw <4 x i32> %7387, %7678
  %7683 = add nsw i64 %5910, %5844
  %7684 = shl nsw i64 %7683, 4
  %7685 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7684
  %7686 = bitcast i8* %7685 to <16 x i8>*
  %7687 = load <16 x i8>, <16 x i8>* %7686, align 16, !tbaa !438
  %7688 = zext <16 x i8> %7687 to <16 x i16>
  %7689 = shufflevector <16 x i16> %7688, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7690 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7689)
  %7691 = shufflevector <16 x i16> %7688, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7692 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7691)
  %7693 = shufflevector <16 x i16> %7688, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7694 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7693)
  %7695 = shufflevector <16 x i16> %7688, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7696 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7695)
  %7697 = add nsw <4 x i32> %7402, %7690
  %7698 = add nsw <4 x i32> %7403, %7692
  %7699 = add nsw <4 x i32> %7404, %7694
  %7700 = add nsw <4 x i32> %7405, %7696
  %7701 = add nsw i64 %5930, %5844
  %7702 = shl nsw i64 %7701, 4
  %7703 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7702
  %7704 = bitcast i8* %7703 to <16 x i8>*
  %7705 = load <16 x i8>, <16 x i8>* %7704, align 16, !tbaa !438
  %7706 = zext <16 x i8> %7705 to <16 x i16>
  %7707 = shufflevector <16 x i16> %7706, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7708 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7707)
  %7709 = shufflevector <16 x i16> %7706, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7710 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7709)
  %7711 = shufflevector <16 x i16> %7706, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7712 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7711)
  %7713 = shufflevector <16 x i16> %7706, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7714 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7713)
  %7715 = add nsw <4 x i32> %7420, %7708
  %7716 = add nsw <4 x i32> %7421, %7710
  %7717 = add nsw <4 x i32> %7422, %7712
  %7718 = add nsw <4 x i32> %7423, %7714
  %7719 = add nsw i64 %5866, %5845
  %7720 = shl nsw i64 %7719, 4
  %7721 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7720
  %7722 = bitcast i8* %7721 to <16 x i8>*
  %7723 = load <16 x i8>, <16 x i8>* %7722, align 16, !tbaa !438
  %7724 = zext <16 x i8> %7723 to <16 x i16>
  %7725 = shufflevector <16 x i16> %7724, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7726 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7725)
  %7727 = shufflevector <16 x i16> %7724, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7728 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7727)
  %7729 = shufflevector <16 x i16> %7724, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7730 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7729)
  %7731 = shufflevector <16 x i16> %7724, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7732 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7731)
  %7733 = add nsw <4 x i32> %7438, %7726
  %7734 = add nsw <4 x i32> %7439, %7728
  %7735 = add nsw <4 x i32> %7440, %7730
  %7736 = add nsw <4 x i32> %7441, %7732
  %7737 = add nsw i64 %5890, %5845
  %7738 = shl nsw i64 %7737, 4
  %7739 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7738
  %7740 = bitcast i8* %7739 to <16 x i8>*
  %7741 = load <16 x i8>, <16 x i8>* %7740, align 16, !tbaa !438
  %7742 = zext <16 x i8> %7741 to <16 x i16>
  %7743 = shufflevector <16 x i16> %7742, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7744 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7743)
  %7745 = shufflevector <16 x i16> %7742, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7746 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7745)
  %7747 = shufflevector <16 x i16> %7742, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7748 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7747)
  %7749 = shufflevector <16 x i16> %7742, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7750 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7749)
  %7751 = add nsw <4 x i32> %7456, %7744
  %7752 = add nsw <4 x i32> %7457, %7746
  %7753 = add nsw <4 x i32> %7458, %7748
  %7754 = add nsw <4 x i32> %7459, %7750
  %7755 = add nsw i64 %5910, %5845
  %7756 = shl nsw i64 %7755, 4
  %7757 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7756
  %7758 = bitcast i8* %7757 to <16 x i8>*
  %7759 = load <16 x i8>, <16 x i8>* %7758, align 16, !tbaa !438
  %7760 = zext <16 x i8> %7759 to <16 x i16>
  %7761 = shufflevector <16 x i16> %7760, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7762 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7761)
  %7763 = shufflevector <16 x i16> %7760, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7764 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7763)
  %7765 = shufflevector <16 x i16> %7760, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7766 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7765)
  %7767 = shufflevector <16 x i16> %7760, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7768 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7767)
  %7769 = add nsw <4 x i32> %7474, %7762
  %7770 = add nsw <4 x i32> %7475, %7764
  %7771 = add nsw <4 x i32> %7476, %7766
  %7772 = add nsw <4 x i32> %7477, %7768
  %7773 = add nsw i64 %5930, %5845
  %7774 = shl nsw i64 %7773, 4
  %7775 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7774
  %7776 = bitcast i8* %7775 to <16 x i8>*
  %7777 = load <16 x i8>, <16 x i8>* %7776, align 16, !tbaa !438
  %7778 = zext <16 x i8> %7777 to <16 x i16>
  %7779 = shufflevector <16 x i16> %7778, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7780 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7779)
  %7781 = shufflevector <16 x i16> %7778, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7782 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7781)
  %7783 = shufflevector <16 x i16> %7778, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7784 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7783)
  %7785 = shufflevector <16 x i16> %7778, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7786 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7785)
  %7787 = add nsw <4 x i32> %7492, %7780
  %7788 = add nsw <4 x i32> %7493, %7782
  %7789 = add nsw <4 x i32> %7494, %7784
  %7790 = add nsw <4 x i32> %7495, %7786
  %7791 = add nsw i64 %5866, %5846
  %7792 = shl nsw i64 %7791, 4
  %7793 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7792
  %7794 = bitcast i8* %7793 to <16 x i8>*
  %7795 = load <16 x i8>, <16 x i8>* %7794, align 16, !tbaa !438
  %7796 = zext <16 x i8> %7795 to <16 x i16>
  %7797 = shufflevector <16 x i16> %7796, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7798 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7797)
  %7799 = shufflevector <16 x i16> %7796, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7800 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7799)
  %7801 = shufflevector <16 x i16> %7796, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7802 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7801)
  %7803 = shufflevector <16 x i16> %7796, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7804 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7803)
  %7805 = add nsw <4 x i32> %7510, %7798
  %7806 = add nsw <4 x i32> %7511, %7800
  %7807 = add nsw <4 x i32> %7512, %7802
  %7808 = add nsw <4 x i32> %7513, %7804
  %7809 = add nsw i64 %5890, %5846
  %7810 = shl nsw i64 %7809, 4
  %7811 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7810
  %7812 = bitcast i8* %7811 to <16 x i8>*
  %7813 = load <16 x i8>, <16 x i8>* %7812, align 16, !tbaa !438
  %7814 = zext <16 x i8> %7813 to <16 x i16>
  %7815 = shufflevector <16 x i16> %7814, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7816 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7815)
  %7817 = shufflevector <16 x i16> %7814, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7818 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7817)
  %7819 = shufflevector <16 x i16> %7814, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7820 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7819)
  %7821 = shufflevector <16 x i16> %7814, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7822 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7821)
  %7823 = add nsw <4 x i32> %7528, %7816
  %7824 = add nsw <4 x i32> %7529, %7818
  %7825 = add nsw <4 x i32> %7530, %7820
  %7826 = add nsw <4 x i32> %7531, %7822
  %7827 = add nsw i64 %5910, %5846
  %7828 = shl nsw i64 %7827, 4
  %7829 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7828
  %7830 = bitcast i8* %7829 to <16 x i8>*
  %7831 = load <16 x i8>, <16 x i8>* %7830, align 16, !tbaa !438
  %7832 = zext <16 x i8> %7831 to <16 x i16>
  %7833 = shufflevector <16 x i16> %7832, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7834 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7833)
  %7835 = shufflevector <16 x i16> %7832, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7836 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7835)
  %7837 = shufflevector <16 x i16> %7832, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7838 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7837)
  %7839 = shufflevector <16 x i16> %7832, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7840 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7839)
  %7841 = add nsw <4 x i32> %7546, %7834
  %7842 = add nsw <4 x i32> %7547, %7836
  %7843 = add nsw <4 x i32> %7548, %7838
  %7844 = add nsw <4 x i32> %7549, %7840
  %7845 = add nsw i64 %5930, %5846
  %7846 = shl nsw i64 %7845, 4
  %7847 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7846
  %7848 = bitcast i8* %7847 to <16 x i8>*
  %7849 = load <16 x i8>, <16 x i8>* %7848, align 16, !tbaa !438
  %7850 = zext <16 x i8> %7849 to <16 x i16>
  %7851 = shufflevector <16 x i16> %7850, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7852 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7851)
  %7853 = shufflevector <16 x i16> %7850, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7854 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7853)
  %7855 = shufflevector <16 x i16> %7850, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7856 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7855)
  %7857 = shufflevector <16 x i16> %7850, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7858 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7857)
  %7859 = add nsw <4 x i32> %7564, %7852
  %7860 = add nsw <4 x i32> %7565, %7854
  %7861 = add nsw <4 x i32> %7566, %7856
  %7862 = add nsw <4 x i32> %7567, %7858
  %7863 = add nsw i64 %5866, %5847
  %7864 = shl nsw i64 %7863, 4
  %7865 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7864
  %7866 = bitcast i8* %7865 to <16 x i8>*
  %7867 = load <16 x i8>, <16 x i8>* %7866, align 16, !tbaa !438
  %7868 = zext <16 x i8> %7867 to <16 x i16>
  %7869 = shufflevector <16 x i16> %7868, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7870 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7869)
  %7871 = shufflevector <16 x i16> %7868, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7872 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7871)
  %7873 = shufflevector <16 x i16> %7868, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7874 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7873)
  %7875 = shufflevector <16 x i16> %7868, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7876 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7875)
  %7877 = add nsw <4 x i32> %7582, %7870
  %7878 = add nsw <4 x i32> %7583, %7872
  %7879 = add nsw <4 x i32> %7584, %7874
  %7880 = add nsw <4 x i32> %7585, %7876
  %7881 = add nsw i64 %5890, %5847
  %7882 = shl nsw i64 %7881, 4
  %7883 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7882
  %7884 = bitcast i8* %7883 to <16 x i8>*
  %7885 = load <16 x i8>, <16 x i8>* %7884, align 16, !tbaa !438
  %7886 = zext <16 x i8> %7885 to <16 x i16>
  %7887 = shufflevector <16 x i16> %7886, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7888 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7887)
  %7889 = shufflevector <16 x i16> %7886, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7890 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7889)
  %7891 = shufflevector <16 x i16> %7886, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7892 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7891)
  %7893 = shufflevector <16 x i16> %7886, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7894 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7893)
  %7895 = add nsw <4 x i32> %7600, %7888
  %7896 = add nsw <4 x i32> %7601, %7890
  %7897 = add nsw <4 x i32> %7602, %7892
  %7898 = add nsw <4 x i32> %7603, %7894
  %7899 = add nsw i64 %5910, %5847
  %7900 = shl nsw i64 %7899, 4
  %7901 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7900
  %7902 = bitcast i8* %7901 to <16 x i8>*
  %7903 = load <16 x i8>, <16 x i8>* %7902, align 16, !tbaa !438
  %7904 = zext <16 x i8> %7903 to <16 x i16>
  %7905 = shufflevector <16 x i16> %7904, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7906 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7905)
  %7907 = shufflevector <16 x i16> %7904, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7908 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7907)
  %7909 = shufflevector <16 x i16> %7904, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7910 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7909)
  %7911 = shufflevector <16 x i16> %7904, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7912 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7911)
  %7913 = add nsw <4 x i32> %7618, %7906
  %7914 = add nsw <4 x i32> %7619, %7908
  %7915 = add nsw <4 x i32> %7620, %7910
  %7916 = add nsw <4 x i32> %7621, %7912
  %7917 = add nsw i64 %5930, %5847
  %7918 = shl nsw i64 %7917, 4
  %7919 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7918
  %7920 = bitcast i8* %7919 to <16 x i8>*
  %7921 = load <16 x i8>, <16 x i8>* %7920, align 16, !tbaa !438
  %7922 = zext <16 x i8> %7921 to <16 x i16>
  %7923 = shufflevector <16 x i16> %7922, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7924 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7649, <4 x i16> %7923)
  %7925 = shufflevector <16 x i16> %7922, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7926 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7652, <4 x i16> %7925)
  %7927 = shufflevector <16 x i16> %7922, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7928 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7655, <4 x i16> %7927)
  %7929 = shufflevector <16 x i16> %7922, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7930 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7658, <4 x i16> %7929)
  %7931 = add nsw <4 x i32> %7636, %7924
  %7932 = add nsw <4 x i32> %7637, %7926
  %7933 = add nsw <4 x i32> %7638, %7928
  %7934 = add nsw <4 x i32> %7639, %7930
  %7935 = load <8 x i16>, <8 x i16>* %5610, align 16, !tbaa !395
  %7936 = load <8 x i16>, <8 x i16>* %5612, align 16, !tbaa !395
  %7937 = shufflevector <8 x i16> %7936, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %7938 = add nsw i64 %5866, %5848
  %7939 = shl nsw i64 %7938, 4
  %7940 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7939
  %7941 = bitcast i8* %7940 to <16 x i8>*
  %7942 = load <16 x i8>, <16 x i8>* %7941, align 16, !tbaa !438
  %7943 = zext <16 x i8> %7942 to <16 x i16>
  %7944 = shufflevector <8 x i16> %7935, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7945 = shufflevector <16 x i16> %7943, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7946 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %7945)
  %7947 = shufflevector <8 x i16> %7935, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7948 = shufflevector <16 x i16> %7943, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7949 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %7948)
  %7950 = shufflevector <8 x i16> %7936, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7951 = shufflevector <16 x i16> %7943, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7952 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %7951)
  %7953 = shufflevector <16 x i16> %7937, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7954 = shufflevector <16 x i16> %7943, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7955 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %7954)
  %7956 = add nsw <4 x i32> %7661, %7946
  %7957 = add nsw <4 x i32> %7662, %7949
  %7958 = add nsw <4 x i32> %7663, %7952
  %7959 = add nsw <4 x i32> %7664, %7955
  %7960 = add nsw i64 %5890, %5848
  %7961 = shl nsw i64 %7960, 4
  %7962 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7961
  %7963 = bitcast i8* %7962 to <16 x i8>*
  %7964 = load <16 x i8>, <16 x i8>* %7963, align 16, !tbaa !438
  %7965 = zext <16 x i8> %7964 to <16 x i16>
  %7966 = shufflevector <16 x i16> %7965, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7967 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %7966)
  %7968 = shufflevector <16 x i16> %7965, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7969 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %7968)
  %7970 = shufflevector <16 x i16> %7965, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7971 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %7970)
  %7972 = shufflevector <16 x i16> %7965, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7973 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %7972)
  %7974 = add nsw <4 x i32> %7679, %7967
  %7975 = add nsw <4 x i32> %7680, %7969
  %7976 = add nsw <4 x i32> %7681, %7971
  %7977 = add nsw <4 x i32> %7682, %7973
  %7978 = add nsw i64 %5910, %5848
  %7979 = shl nsw i64 %7978, 4
  %7980 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7979
  %7981 = bitcast i8* %7980 to <16 x i8>*
  %7982 = load <16 x i8>, <16 x i8>* %7981, align 16, !tbaa !438
  %7983 = zext <16 x i8> %7982 to <16 x i16>
  %7984 = shufflevector <16 x i16> %7983, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %7985 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %7984)
  %7986 = shufflevector <16 x i16> %7983, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7987 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %7986)
  %7988 = shufflevector <16 x i16> %7983, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7989 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %7988)
  %7990 = shufflevector <16 x i16> %7983, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %7991 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %7990)
  %7992 = add nsw <4 x i32> %7697, %7985
  %7993 = add nsw <4 x i32> %7698, %7987
  %7994 = add nsw <4 x i32> %7699, %7989
  %7995 = add nsw <4 x i32> %7700, %7991
  %7996 = add nsw i64 %5930, %5848
  %7997 = shl nsw i64 %7996, 4
  %7998 = getelementptr inbounds i8, i8* %resampled_input122, i64 %7997
  %7999 = bitcast i8* %7998 to <16 x i8>*
  %8000 = load <16 x i8>, <16 x i8>* %7999, align 16, !tbaa !438
  %8001 = zext <16 x i8> %8000 to <16 x i16>
  %8002 = shufflevector <16 x i16> %8001, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8003 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8002)
  %8004 = shufflevector <16 x i16> %8001, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8005 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8004)
  %8006 = shufflevector <16 x i16> %8001, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8007 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8006)
  %8008 = shufflevector <16 x i16> %8001, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8009 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8008)
  %8010 = add nsw <4 x i32> %7715, %8003
  %8011 = add nsw <4 x i32> %7716, %8005
  %8012 = add nsw <4 x i32> %7717, %8007
  %8013 = add nsw <4 x i32> %7718, %8009
  %8014 = add nsw i64 %5866, %5849
  %8015 = shl nsw i64 %8014, 4
  %8016 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8015
  %8017 = bitcast i8* %8016 to <16 x i8>*
  %8018 = load <16 x i8>, <16 x i8>* %8017, align 16, !tbaa !438
  %8019 = zext <16 x i8> %8018 to <16 x i16>
  %8020 = shufflevector <16 x i16> %8019, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8021 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8020)
  %8022 = shufflevector <16 x i16> %8019, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8023 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8022)
  %8024 = shufflevector <16 x i16> %8019, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8025 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8024)
  %8026 = shufflevector <16 x i16> %8019, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8027 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8026)
  %8028 = add nsw <4 x i32> %7733, %8021
  %8029 = add nsw <4 x i32> %7734, %8023
  %8030 = add nsw <4 x i32> %7735, %8025
  %8031 = add nsw <4 x i32> %7736, %8027
  %8032 = add nsw i64 %5890, %5849
  %8033 = shl nsw i64 %8032, 4
  %8034 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8033
  %8035 = bitcast i8* %8034 to <16 x i8>*
  %8036 = load <16 x i8>, <16 x i8>* %8035, align 16, !tbaa !438
  %8037 = zext <16 x i8> %8036 to <16 x i16>
  %8038 = shufflevector <16 x i16> %8037, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8039 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8038)
  %8040 = shufflevector <16 x i16> %8037, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8041 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8040)
  %8042 = shufflevector <16 x i16> %8037, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8043 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8042)
  %8044 = shufflevector <16 x i16> %8037, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8045 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8044)
  %8046 = add nsw <4 x i32> %7751, %8039
  %8047 = add nsw <4 x i32> %7752, %8041
  %8048 = add nsw <4 x i32> %7753, %8043
  %8049 = add nsw <4 x i32> %7754, %8045
  %8050 = add nsw i64 %5910, %5849
  %8051 = shl nsw i64 %8050, 4
  %8052 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8051
  %8053 = bitcast i8* %8052 to <16 x i8>*
  %8054 = load <16 x i8>, <16 x i8>* %8053, align 16, !tbaa !438
  %8055 = zext <16 x i8> %8054 to <16 x i16>
  %8056 = shufflevector <16 x i16> %8055, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8057 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8056)
  %8058 = shufflevector <16 x i16> %8055, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8059 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8058)
  %8060 = shufflevector <16 x i16> %8055, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8061 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8060)
  %8062 = shufflevector <16 x i16> %8055, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8063 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8062)
  %8064 = add nsw <4 x i32> %7769, %8057
  %8065 = add nsw <4 x i32> %7770, %8059
  %8066 = add nsw <4 x i32> %7771, %8061
  %8067 = add nsw <4 x i32> %7772, %8063
  %8068 = add nsw i64 %5930, %5849
  %8069 = shl nsw i64 %8068, 4
  %8070 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8069
  %8071 = bitcast i8* %8070 to <16 x i8>*
  %8072 = load <16 x i8>, <16 x i8>* %8071, align 16, !tbaa !438
  %8073 = zext <16 x i8> %8072 to <16 x i16>
  %8074 = shufflevector <16 x i16> %8073, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8075 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8074)
  %8076 = shufflevector <16 x i16> %8073, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8077 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8076)
  %8078 = shufflevector <16 x i16> %8073, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8079 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8078)
  %8080 = shufflevector <16 x i16> %8073, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8081 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8080)
  %8082 = add nsw <4 x i32> %7787, %8075
  %8083 = add nsw <4 x i32> %7788, %8077
  %8084 = add nsw <4 x i32> %7789, %8079
  %8085 = add nsw <4 x i32> %7790, %8081
  %8086 = add nsw i64 %5866, %5850
  %8087 = shl nsw i64 %8086, 4
  %8088 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8087
  %8089 = bitcast i8* %8088 to <16 x i8>*
  %8090 = load <16 x i8>, <16 x i8>* %8089, align 16, !tbaa !438
  %8091 = zext <16 x i8> %8090 to <16 x i16>
  %8092 = shufflevector <16 x i16> %8091, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8093 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8092)
  %8094 = shufflevector <16 x i16> %8091, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8095 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8094)
  %8096 = shufflevector <16 x i16> %8091, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8097 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8096)
  %8098 = shufflevector <16 x i16> %8091, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8099 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8098)
  %8100 = add nsw <4 x i32> %7805, %8093
  %8101 = add nsw <4 x i32> %7806, %8095
  %8102 = add nsw <4 x i32> %7807, %8097
  %8103 = add nsw <4 x i32> %7808, %8099
  %8104 = add nsw i64 %5890, %5850
  %8105 = shl nsw i64 %8104, 4
  %8106 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8105
  %8107 = bitcast i8* %8106 to <16 x i8>*
  %8108 = load <16 x i8>, <16 x i8>* %8107, align 16, !tbaa !438
  %8109 = zext <16 x i8> %8108 to <16 x i16>
  %8110 = shufflevector <16 x i16> %8109, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8111 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8110)
  %8112 = shufflevector <16 x i16> %8109, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8113 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8112)
  %8114 = shufflevector <16 x i16> %8109, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8115 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8114)
  %8116 = shufflevector <16 x i16> %8109, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8117 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8116)
  %8118 = add nsw <4 x i32> %7823, %8111
  %8119 = add nsw <4 x i32> %7824, %8113
  %8120 = add nsw <4 x i32> %7825, %8115
  %8121 = add nsw <4 x i32> %7826, %8117
  %8122 = add nsw i64 %5910, %5850
  %8123 = shl nsw i64 %8122, 4
  %8124 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8123
  %8125 = bitcast i8* %8124 to <16 x i8>*
  %8126 = load <16 x i8>, <16 x i8>* %8125, align 16, !tbaa !438
  %8127 = zext <16 x i8> %8126 to <16 x i16>
  %8128 = shufflevector <16 x i16> %8127, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8129 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8128)
  %8130 = shufflevector <16 x i16> %8127, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8131 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8130)
  %8132 = shufflevector <16 x i16> %8127, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8133 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8132)
  %8134 = shufflevector <16 x i16> %8127, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8135 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8134)
  %8136 = add nsw <4 x i32> %7841, %8129
  %8137 = add nsw <4 x i32> %7842, %8131
  %8138 = add nsw <4 x i32> %7843, %8133
  %8139 = add nsw <4 x i32> %7844, %8135
  %8140 = add nsw i64 %5930, %5850
  %8141 = shl nsw i64 %8140, 4
  %8142 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8141
  %8143 = bitcast i8* %8142 to <16 x i8>*
  %8144 = load <16 x i8>, <16 x i8>* %8143, align 16, !tbaa !438
  %8145 = zext <16 x i8> %8144 to <16 x i16>
  %8146 = shufflevector <16 x i16> %8145, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8147 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8146)
  %8148 = shufflevector <16 x i16> %8145, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8149 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8148)
  %8150 = shufflevector <16 x i16> %8145, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8151 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8150)
  %8152 = shufflevector <16 x i16> %8145, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8153 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8152)
  %8154 = add nsw <4 x i32> %7859, %8147
  %8155 = add nsw <4 x i32> %7860, %8149
  %8156 = add nsw <4 x i32> %7861, %8151
  %8157 = add nsw <4 x i32> %7862, %8153
  %8158 = add nsw i64 %5866, %5851
  %8159 = shl nsw i64 %8158, 4
  %8160 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8159
  %8161 = bitcast i8* %8160 to <16 x i8>*
  %8162 = load <16 x i8>, <16 x i8>* %8161, align 16, !tbaa !438
  %8163 = zext <16 x i8> %8162 to <16 x i16>
  %8164 = shufflevector <16 x i16> %8163, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8165 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8164)
  %8166 = shufflevector <16 x i16> %8163, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8167 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8166)
  %8168 = shufflevector <16 x i16> %8163, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8169 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8168)
  %8170 = shufflevector <16 x i16> %8163, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8171 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8170)
  %8172 = add nsw <4 x i32> %7877, %8165
  %8173 = add nsw <4 x i32> %7878, %8167
  %8174 = add nsw <4 x i32> %7879, %8169
  %8175 = add nsw <4 x i32> %7880, %8171
  %8176 = add nsw i64 %5890, %5851
  %8177 = shl nsw i64 %8176, 4
  %8178 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8177
  %8179 = bitcast i8* %8178 to <16 x i8>*
  %8180 = load <16 x i8>, <16 x i8>* %8179, align 16, !tbaa !438
  %8181 = zext <16 x i8> %8180 to <16 x i16>
  %8182 = shufflevector <16 x i16> %8181, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8183 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8182)
  %8184 = shufflevector <16 x i16> %8181, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8185 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8184)
  %8186 = shufflevector <16 x i16> %8181, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8187 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8186)
  %8188 = shufflevector <16 x i16> %8181, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8189 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8188)
  %8190 = add nsw <4 x i32> %7895, %8183
  %8191 = add nsw <4 x i32> %7896, %8185
  %8192 = add nsw <4 x i32> %7897, %8187
  %8193 = add nsw <4 x i32> %7898, %8189
  %8194 = add nsw i64 %5910, %5851
  %8195 = shl nsw i64 %8194, 4
  %8196 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8195
  %8197 = bitcast i8* %8196 to <16 x i8>*
  %8198 = load <16 x i8>, <16 x i8>* %8197, align 16, !tbaa !438
  %8199 = zext <16 x i8> %8198 to <16 x i16>
  %8200 = shufflevector <16 x i16> %8199, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8201 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8200)
  %8202 = shufflevector <16 x i16> %8199, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8203 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8202)
  %8204 = shufflevector <16 x i16> %8199, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8205 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8204)
  %8206 = shufflevector <16 x i16> %8199, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8207 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8206)
  %8208 = add nsw <4 x i32> %7913, %8201
  %8209 = add nsw <4 x i32> %7914, %8203
  %8210 = add nsw <4 x i32> %7915, %8205
  %8211 = add nsw <4 x i32> %7916, %8207
  %8212 = add nsw i64 %5930, %5851
  %8213 = shl nsw i64 %8212, 4
  %8214 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8213
  %8215 = bitcast i8* %8214 to <16 x i8>*
  %8216 = load <16 x i8>, <16 x i8>* %8215, align 16, !tbaa !438
  %8217 = zext <16 x i8> %8216 to <16 x i16>
  %8218 = shufflevector <16 x i16> %8217, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8219 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7944, <4 x i16> %8218)
  %8220 = shufflevector <16 x i16> %8217, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8221 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7947, <4 x i16> %8220)
  %8222 = shufflevector <16 x i16> %8217, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8223 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7950, <4 x i16> %8222)
  %8224 = shufflevector <16 x i16> %8217, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8225 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %7953, <4 x i16> %8224)
  %8226 = add nsw <4 x i32> %7931, %8219
  %8227 = add nsw <4 x i32> %7932, %8221
  %8228 = add nsw <4 x i32> %7933, %8223
  %8229 = add nsw <4 x i32> %7934, %8225
  %8230 = load <8 x i16>, <8 x i16>* %5614, align 16, !tbaa !395
  %8231 = load <8 x i16>, <8 x i16>* %5616, align 16, !tbaa !395
  %8232 = shufflevector <8 x i16> %8231, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %8233 = add nsw i64 %5866, %5852
  %8234 = shl nsw i64 %8233, 4
  %8235 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8234
  %8236 = bitcast i8* %8235 to <16 x i8>*
  %8237 = load <16 x i8>, <16 x i8>* %8236, align 16, !tbaa !438
  %8238 = zext <16 x i8> %8237 to <16 x i16>
  %8239 = shufflevector <8 x i16> %8230, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8240 = shufflevector <16 x i16> %8238, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8241 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8240)
  %8242 = shufflevector <8 x i16> %8230, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8243 = shufflevector <16 x i16> %8238, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8244 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8243)
  %8245 = shufflevector <8 x i16> %8231, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8246 = shufflevector <16 x i16> %8238, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8247 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8246)
  %8248 = shufflevector <16 x i16> %8232, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8249 = shufflevector <16 x i16> %8238, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8250 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8249)
  %8251 = add nsw <4 x i32> %7956, %8241
  %8252 = add nsw <4 x i32> %7957, %8244
  %8253 = add nsw <4 x i32> %7958, %8247
  %8254 = add nsw <4 x i32> %7959, %8250
  %8255 = add nsw i64 %5890, %5852
  %8256 = shl nsw i64 %8255, 4
  %8257 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8256
  %8258 = bitcast i8* %8257 to <16 x i8>*
  %8259 = load <16 x i8>, <16 x i8>* %8258, align 16, !tbaa !438
  %8260 = zext <16 x i8> %8259 to <16 x i16>
  %8261 = shufflevector <16 x i16> %8260, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8262 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8261)
  %8263 = shufflevector <16 x i16> %8260, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8264 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8263)
  %8265 = shufflevector <16 x i16> %8260, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8266 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8265)
  %8267 = shufflevector <16 x i16> %8260, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8268 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8267)
  %8269 = add nsw <4 x i32> %7974, %8262
  %8270 = add nsw <4 x i32> %7975, %8264
  %8271 = add nsw <4 x i32> %7976, %8266
  %8272 = add nsw <4 x i32> %7977, %8268
  %8273 = add nsw i64 %5910, %5852
  %8274 = shl nsw i64 %8273, 4
  %8275 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8274
  %8276 = bitcast i8* %8275 to <16 x i8>*
  %8277 = load <16 x i8>, <16 x i8>* %8276, align 16, !tbaa !438
  %8278 = zext <16 x i8> %8277 to <16 x i16>
  %8279 = shufflevector <16 x i16> %8278, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8280 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8279)
  %8281 = shufflevector <16 x i16> %8278, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8282 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8281)
  %8283 = shufflevector <16 x i16> %8278, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8284 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8283)
  %8285 = shufflevector <16 x i16> %8278, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8286 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8285)
  %8287 = add nsw <4 x i32> %7992, %8280
  %8288 = add nsw <4 x i32> %7993, %8282
  %8289 = add nsw <4 x i32> %7994, %8284
  %8290 = add nsw <4 x i32> %7995, %8286
  %8291 = add nsw i64 %5930, %5852
  %8292 = shl nsw i64 %8291, 4
  %8293 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8292
  %8294 = bitcast i8* %8293 to <16 x i8>*
  %8295 = load <16 x i8>, <16 x i8>* %8294, align 16, !tbaa !438
  %8296 = zext <16 x i8> %8295 to <16 x i16>
  %8297 = shufflevector <16 x i16> %8296, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8298 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8297)
  %8299 = shufflevector <16 x i16> %8296, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8300 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8299)
  %8301 = shufflevector <16 x i16> %8296, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8302 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8301)
  %8303 = shufflevector <16 x i16> %8296, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8304 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8303)
  %8305 = add nsw <4 x i32> %8010, %8298
  %8306 = add nsw <4 x i32> %8011, %8300
  %8307 = add nsw <4 x i32> %8012, %8302
  %8308 = add nsw <4 x i32> %8013, %8304
  %8309 = add nsw i64 %5866, %5853
  %8310 = shl nsw i64 %8309, 4
  %8311 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8310
  %8312 = bitcast i8* %8311 to <16 x i8>*
  %8313 = load <16 x i8>, <16 x i8>* %8312, align 16, !tbaa !438
  %8314 = zext <16 x i8> %8313 to <16 x i16>
  %8315 = shufflevector <16 x i16> %8314, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8316 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8315)
  %8317 = shufflevector <16 x i16> %8314, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8318 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8317)
  %8319 = shufflevector <16 x i16> %8314, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8320 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8319)
  %8321 = shufflevector <16 x i16> %8314, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8322 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8321)
  %8323 = add nsw <4 x i32> %8028, %8316
  %8324 = add nsw <4 x i32> %8029, %8318
  %8325 = add nsw <4 x i32> %8030, %8320
  %8326 = add nsw <4 x i32> %8031, %8322
  %8327 = add nsw i64 %5890, %5853
  %8328 = shl nsw i64 %8327, 4
  %8329 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8328
  %8330 = bitcast i8* %8329 to <16 x i8>*
  %8331 = load <16 x i8>, <16 x i8>* %8330, align 16, !tbaa !438
  %8332 = zext <16 x i8> %8331 to <16 x i16>
  %8333 = shufflevector <16 x i16> %8332, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8334 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8333)
  %8335 = shufflevector <16 x i16> %8332, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8336 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8335)
  %8337 = shufflevector <16 x i16> %8332, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8338 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8337)
  %8339 = shufflevector <16 x i16> %8332, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8340 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8339)
  %8341 = add nsw <4 x i32> %8046, %8334
  %8342 = add nsw <4 x i32> %8047, %8336
  %8343 = add nsw <4 x i32> %8048, %8338
  %8344 = add nsw <4 x i32> %8049, %8340
  %8345 = add nsw i64 %5910, %5853
  %8346 = shl nsw i64 %8345, 4
  %8347 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8346
  %8348 = bitcast i8* %8347 to <16 x i8>*
  %8349 = load <16 x i8>, <16 x i8>* %8348, align 16, !tbaa !438
  %8350 = zext <16 x i8> %8349 to <16 x i16>
  %8351 = shufflevector <16 x i16> %8350, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8352 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8351)
  %8353 = shufflevector <16 x i16> %8350, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8354 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8353)
  %8355 = shufflevector <16 x i16> %8350, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8356 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8355)
  %8357 = shufflevector <16 x i16> %8350, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8358 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8357)
  %8359 = add nsw <4 x i32> %8064, %8352
  %8360 = add nsw <4 x i32> %8065, %8354
  %8361 = add nsw <4 x i32> %8066, %8356
  %8362 = add nsw <4 x i32> %8067, %8358
  %8363 = add nsw i64 %5930, %5853
  %8364 = shl nsw i64 %8363, 4
  %8365 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8364
  %8366 = bitcast i8* %8365 to <16 x i8>*
  %8367 = load <16 x i8>, <16 x i8>* %8366, align 16, !tbaa !438
  %8368 = zext <16 x i8> %8367 to <16 x i16>
  %8369 = shufflevector <16 x i16> %8368, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8370 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8369)
  %8371 = shufflevector <16 x i16> %8368, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8372 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8371)
  %8373 = shufflevector <16 x i16> %8368, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8374 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8373)
  %8375 = shufflevector <16 x i16> %8368, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8376 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8375)
  %8377 = add nsw <4 x i32> %8082, %8370
  %8378 = add nsw <4 x i32> %8083, %8372
  %8379 = add nsw <4 x i32> %8084, %8374
  %8380 = add nsw <4 x i32> %8085, %8376
  %8381 = add nsw i64 %5866, %5854
  %8382 = shl nsw i64 %8381, 4
  %8383 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8382
  %8384 = bitcast i8* %8383 to <16 x i8>*
  %8385 = load <16 x i8>, <16 x i8>* %8384, align 16, !tbaa !438
  %8386 = zext <16 x i8> %8385 to <16 x i16>
  %8387 = shufflevector <16 x i16> %8386, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8388 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8387)
  %8389 = shufflevector <16 x i16> %8386, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8390 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8389)
  %8391 = shufflevector <16 x i16> %8386, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8392 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8391)
  %8393 = shufflevector <16 x i16> %8386, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8394 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8393)
  %8395 = add nsw <4 x i32> %8100, %8388
  %8396 = add nsw <4 x i32> %8101, %8390
  %8397 = add nsw <4 x i32> %8102, %8392
  %8398 = add nsw <4 x i32> %8103, %8394
  %8399 = add nsw i64 %5890, %5854
  %8400 = shl nsw i64 %8399, 4
  %8401 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8400
  %8402 = bitcast i8* %8401 to <16 x i8>*
  %8403 = load <16 x i8>, <16 x i8>* %8402, align 16, !tbaa !438
  %8404 = zext <16 x i8> %8403 to <16 x i16>
  %8405 = shufflevector <16 x i16> %8404, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8406 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8405)
  %8407 = shufflevector <16 x i16> %8404, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8408 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8407)
  %8409 = shufflevector <16 x i16> %8404, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8410 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8409)
  %8411 = shufflevector <16 x i16> %8404, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8412 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8411)
  %8413 = add nsw <4 x i32> %8118, %8406
  %8414 = add nsw <4 x i32> %8119, %8408
  %8415 = add nsw <4 x i32> %8120, %8410
  %8416 = add nsw <4 x i32> %8121, %8412
  %8417 = add nsw i64 %5910, %5854
  %8418 = shl nsw i64 %8417, 4
  %8419 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8418
  %8420 = bitcast i8* %8419 to <16 x i8>*
  %8421 = load <16 x i8>, <16 x i8>* %8420, align 16, !tbaa !438
  %8422 = zext <16 x i8> %8421 to <16 x i16>
  %8423 = shufflevector <16 x i16> %8422, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8424 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8423)
  %8425 = shufflevector <16 x i16> %8422, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8426 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8425)
  %8427 = shufflevector <16 x i16> %8422, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8428 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8427)
  %8429 = shufflevector <16 x i16> %8422, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8430 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8429)
  %8431 = add nsw <4 x i32> %8136, %8424
  %8432 = add nsw <4 x i32> %8137, %8426
  %8433 = add nsw <4 x i32> %8138, %8428
  %8434 = add nsw <4 x i32> %8139, %8430
  %8435 = add nsw i64 %5930, %5854
  %8436 = shl nsw i64 %8435, 4
  %8437 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8436
  %8438 = bitcast i8* %8437 to <16 x i8>*
  %8439 = load <16 x i8>, <16 x i8>* %8438, align 16, !tbaa !438
  %8440 = zext <16 x i8> %8439 to <16 x i16>
  %8441 = shufflevector <16 x i16> %8440, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8442 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8441)
  %8443 = shufflevector <16 x i16> %8440, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8444 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8443)
  %8445 = shufflevector <16 x i16> %8440, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8446 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8445)
  %8447 = shufflevector <16 x i16> %8440, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8448 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8447)
  %8449 = add nsw <4 x i32> %8154, %8442
  %8450 = add nsw <4 x i32> %8155, %8444
  %8451 = add nsw <4 x i32> %8156, %8446
  %8452 = add nsw <4 x i32> %8157, %8448
  %8453 = add nsw i64 %5866, %5855
  %8454 = shl nsw i64 %8453, 4
  %8455 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8454
  %8456 = bitcast i8* %8455 to <16 x i8>*
  %8457 = load <16 x i8>, <16 x i8>* %8456, align 16, !tbaa !438
  %8458 = zext <16 x i8> %8457 to <16 x i16>
  %8459 = shufflevector <16 x i16> %8458, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8460 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8459)
  %8461 = shufflevector <16 x i16> %8458, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8462 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8461)
  %8463 = shufflevector <16 x i16> %8458, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8464 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8463)
  %8465 = shufflevector <16 x i16> %8458, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8466 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8465)
  %8467 = add nsw <4 x i32> %8172, %8460
  %8468 = add nsw <4 x i32> %8173, %8462
  %8469 = add nsw <4 x i32> %8174, %8464
  %8470 = add nsw <4 x i32> %8175, %8466
  %8471 = add nsw i64 %5890, %5855
  %8472 = shl nsw i64 %8471, 4
  %8473 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8472
  %8474 = bitcast i8* %8473 to <16 x i8>*
  %8475 = load <16 x i8>, <16 x i8>* %8474, align 16, !tbaa !438
  %8476 = zext <16 x i8> %8475 to <16 x i16>
  %8477 = shufflevector <16 x i16> %8476, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8478 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8477)
  %8479 = shufflevector <16 x i16> %8476, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8480 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8479)
  %8481 = shufflevector <16 x i16> %8476, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8482 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8481)
  %8483 = shufflevector <16 x i16> %8476, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8484 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8483)
  %8485 = add nsw <4 x i32> %8190, %8478
  %8486 = add nsw <4 x i32> %8191, %8480
  %8487 = add nsw <4 x i32> %8192, %8482
  %8488 = add nsw <4 x i32> %8193, %8484
  %8489 = add nsw i64 %5910, %5855
  %8490 = shl nsw i64 %8489, 4
  %8491 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8490
  %8492 = bitcast i8* %8491 to <16 x i8>*
  %8493 = load <16 x i8>, <16 x i8>* %8492, align 16, !tbaa !438
  %8494 = zext <16 x i8> %8493 to <16 x i16>
  %8495 = shufflevector <16 x i16> %8494, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8496 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8495)
  %8497 = shufflevector <16 x i16> %8494, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8498 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8497)
  %8499 = shufflevector <16 x i16> %8494, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8500 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8499)
  %8501 = shufflevector <16 x i16> %8494, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8502 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8501)
  %8503 = add nsw <4 x i32> %8208, %8496
  %8504 = add nsw <4 x i32> %8209, %8498
  %8505 = add nsw <4 x i32> %8210, %8500
  %8506 = add nsw <4 x i32> %8211, %8502
  %8507 = add nsw i64 %5930, %5855
  %8508 = shl nsw i64 %8507, 4
  %8509 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8508
  %8510 = bitcast i8* %8509 to <16 x i8>*
  %8511 = load <16 x i8>, <16 x i8>* %8510, align 16, !tbaa !438
  %8512 = zext <16 x i8> %8511 to <16 x i16>
  %8513 = shufflevector <16 x i16> %8512, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8514 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8239, <4 x i16> %8513)
  %8515 = shufflevector <16 x i16> %8512, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8516 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8242, <4 x i16> %8515)
  %8517 = shufflevector <16 x i16> %8512, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8518 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8245, <4 x i16> %8517)
  %8519 = shufflevector <16 x i16> %8512, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8520 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8248, <4 x i16> %8519)
  %8521 = add nsw <4 x i32> %8226, %8514
  %8522 = add nsw <4 x i32> %8227, %8516
  %8523 = add nsw <4 x i32> %8228, %8518
  %8524 = add nsw <4 x i32> %8229, %8520
  br label %"consume convolved202"

next_bb195:                                       ; preds = %"for output.s0.x.xo189"
  %8525 = trunc i64 %indvars.iv6316 to i32
  %8526 = shl nsw i32 %8525, 2
  %t2629 = add nsw i32 %8526, %21
  %8527 = mul nsw i32 %t2629, %stride_x
  %t2625 = sub nsw i32 %8527, %resampled_input.x.min_realized
  %8528 = add nsw i32 %t2629, 3
  %8529 = mul nsw i32 %8528, %stride_x
  %t2628 = sub nsw i32 %8529, %resampled_input.x.min_realized
  %8530 = add nsw i32 %t2629, 2
  %8531 = mul nsw i32 %8530, %stride_x
  %t2627 = sub nsw i32 %8531, %resampled_input.x.min_realized
  %8532 = add nsw i32 %t2629, 1
  %8533 = mul nsw i32 %8532, %stride_x
  %t2626 = sub nsw i32 %8533, %resampled_input.x.min_realized
  br i1 %5352, label %"for convolved.s1.r19$y196.preheader", label %"consume convolved202", !prof !391

"for convolved.s1.r19$y196.preheader":            ; preds = %next_bb195
  br i1 %5350, label %"for convolved.s1.r19$y196.us", label %"consume convolved202", !prof !391

"for convolved.s1.r19$y196.us":                   ; preds = %"for convolved.s1.r19$y196.preheader", %"end for convolved.s1.r19$x200.loopexit.us"
  %indvars.iv6312 = phi i64 [ %indvars.iv.next6313, %"end for convolved.s1.r19$x200.loopexit.us" ], [ 0, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3624.4.us = phi <4 x i32> [ %8856, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3568.4.us = phi <4 x i32> [ %8855, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3512.4.us = phi <4 x i32> [ %8854, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3456.4.us = phi <4 x i32> [ %8853, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3400.4.us = phi <4 x i32> [ %8838, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3344.4.us = phi <4 x i32> [ %8837, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3288.4.us = phi <4 x i32> [ %8836, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3232.4.us = phi <4 x i32> [ %8835, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3176.4.us = phi <4 x i32> [ %8820, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3120.4.us = phi <4 x i32> [ %8819, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3064.4.us = phi <4 x i32> [ %8818, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.3008.4.us = phi <4 x i32> [ %8817, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2952.4.us = phi <4 x i32> [ %8802, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2896.4.us = phi <4 x i32> [ %8801, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2840.4.us = phi <4 x i32> [ %8800, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2784.4.us = phi <4 x i32> [ %8799, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2728.4.us = phi <4 x i32> [ %8784, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2672.4.us = phi <4 x i32> [ %8783, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2616.4.us = phi <4 x i32> [ %8782, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2560.4.us = phi <4 x i32> [ %8781, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2504.4.us = phi <4 x i32> [ %8766, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2448.4.us = phi <4 x i32> [ %8765, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2392.4.us = phi <4 x i32> [ %8764, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2336.4.us = phi <4 x i32> [ %8763, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2280.4.us = phi <4 x i32> [ %8748, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2224.4.us = phi <4 x i32> [ %8747, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2168.4.us = phi <4 x i32> [ %8746, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2112.4.us = phi <4 x i32> [ %8745, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2056.4.us = phi <4 x i32> [ %8730, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.2000.4.us = phi <4 x i32> [ %8729, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1944.4.us = phi <4 x i32> [ %8728, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1888.4.us = phi <4 x i32> [ %8727, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1832.4.us = phi <4 x i32> [ %8712, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1776.4.us = phi <4 x i32> [ %8711, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1720.4.us = phi <4 x i32> [ %8710, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1664.4.us = phi <4 x i32> [ %8709, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1608.4.us = phi <4 x i32> [ %8694, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1552.4.us = phi <4 x i32> [ %8693, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1496.4.us = phi <4 x i32> [ %8692, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1440.4.us = phi <4 x i32> [ %8691, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1384.4.us = phi <4 x i32> [ %8676, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1328.4.us = phi <4 x i32> [ %8675, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1272.4.us = phi <4 x i32> [ %8674, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1216.4.us = phi <4 x i32> [ %8673, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1160.4.us = phi <4 x i32> [ %8658, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1104.4.us = phi <4 x i32> [ %8657, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.1048.4.us = phi <4 x i32> [ %8656, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.992.4.us = phi <4 x i32> [ %8655, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.930.4.us = phi <4 x i32> [ %8640, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.868.4.us = phi <4 x i32> [ %8639, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.806.4.us = phi <4 x i32> [ %8638, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.744.4.us = phi <4 x i32> [ %8637, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.682.4.us = phi <4 x i32> [ %8622, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.620.4.us = phi <4 x i32> [ %8621, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.558.4.us = phi <4 x i32> [ %8620, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.496.4.us = phi <4 x i32> [ %8619, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.434.4.us = phi <4 x i32> [ %8604, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.372.4.us = phi <4 x i32> [ %8603, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.310.4.us = phi <4 x i32> [ %8602, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.248.4.us = phi <4 x i32> [ %8601, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.186.4.us = phi <4 x i32> [ %8586, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5805, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.124.4.us = phi <4 x i32> [ %8585, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5804, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.62.4.us = phi <4 x i32> [ %8584, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5803, %"for convolved.s1.r19$y196.preheader" ]
  %convolved944.sroa.0.4.us = phi <4 x i32> [ %8583, %"end for convolved.s1.r19$x200.loopexit.us" ], [ %5802, %"for convolved.s1.r19$y196.preheader" ]
  %"convolved.s1.r19$y198.us" = phi i32 [ %8857, %"end for convolved.s1.r19$x200.loopexit.us" ], [ 0, %"for convolved.s1.r19$y196.preheader" ]
  %t2647.us = mul nsw i32 %"convolved.s1.r19$y198.us", %a613
  %8534 = add nsw i32 %t2647.us, %t2617
  %t2648.us = mul nsw i32 %8534, %a64
  %8535 = add nsw i32 %t2647.us, %t2619
  %t2649.us = mul nsw i32 %8535, %a64
  %8536 = add nsw i32 %t2647.us, %t2621
  %t2650.us = mul nsw i32 %8536, %a64
  %8537 = add nsw i32 %t2647.us, %t2623
  %t2651.us = mul nsw i32 %8537, %a64
  %8538 = mul nsw i64 %indvars.iv6312, %5638
  %t2634.us = add nsw i32 %t2651.us, %t2628
  %t2633.us = add nsw i32 %t2651.us, %t2627
  %t2632.us = add nsw i32 %t2651.us, %t2626
  %t2631.us = add nsw i32 %t2651.us, %t2625
  %t2646.us = add nsw i32 %t2650.us, %t2628
  %t2645.us = add nsw i32 %t2650.us, %t2627
  %t2644.us = add nsw i32 %t2650.us, %t2626
  %t2643.us = add nsw i32 %t2650.us, %t2625
  %t2642.us = add nsw i32 %t2649.us, %t2628
  %t2641.us = add nsw i32 %t2649.us, %t2627
  %t2640.us = add nsw i32 %t2649.us, %t2626
  %t2639.us = add nsw i32 %t2649.us, %t2625
  %t2638.us = add nsw i32 %t2648.us, %t2628
  %t2637.us = add nsw i32 %t2648.us, %t2627
  %t2636.us = add nsw i32 %t2648.us, %t2626
  %t2635.us = add nsw i32 %t2648.us, %t2625
  %8539 = sext i32 %t2631.us to i64
  %8540 = sext i32 %t2632.us to i64
  %8541 = sext i32 %t2633.us to i64
  %8542 = sext i32 %t2634.us to i64
  %8543 = sext i32 %t2635.us to i64
  %8544 = sext i32 %t2636.us to i64
  %8545 = sext i32 %t2637.us to i64
  %8546 = sext i32 %t2638.us to i64
  %8547 = sext i32 %t2639.us to i64
  %8548 = sext i32 %t2640.us to i64
  %8549 = sext i32 %t2641.us to i64
  %8550 = sext i32 %t2642.us to i64
  %8551 = sext i32 %t2643.us to i64
  %8552 = sext i32 %t2644.us to i64
  %8553 = sext i32 %t2645.us to i64
  %8554 = sext i32 %t2646.us to i64
  br label %"for convolved.s1.r19$x199.us"

"for convolved.s1.r19$x199.us":                   ; preds = %"for convolved.s1.r19$y196.us", %"for convolved.s1.r19$x199.us"
  %indvars.iv6310 = phi i64 [ 0, %"for convolved.s1.r19$y196.us" ], [ %indvars.iv.next6311, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3624.5.us = phi <4 x i32> [ %convolved944.sroa.3624.4.us, %"for convolved.s1.r19$y196.us" ], [ %8856, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3568.5.us = phi <4 x i32> [ %convolved944.sroa.3568.4.us, %"for convolved.s1.r19$y196.us" ], [ %8855, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3512.5.us = phi <4 x i32> [ %convolved944.sroa.3512.4.us, %"for convolved.s1.r19$y196.us" ], [ %8854, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3456.5.us = phi <4 x i32> [ %convolved944.sroa.3456.4.us, %"for convolved.s1.r19$y196.us" ], [ %8853, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3400.5.us = phi <4 x i32> [ %convolved944.sroa.3400.4.us, %"for convolved.s1.r19$y196.us" ], [ %8838, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3344.5.us = phi <4 x i32> [ %convolved944.sroa.3344.4.us, %"for convolved.s1.r19$y196.us" ], [ %8837, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3288.5.us = phi <4 x i32> [ %convolved944.sroa.3288.4.us, %"for convolved.s1.r19$y196.us" ], [ %8836, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3232.5.us = phi <4 x i32> [ %convolved944.sroa.3232.4.us, %"for convolved.s1.r19$y196.us" ], [ %8835, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3176.5.us = phi <4 x i32> [ %convolved944.sroa.3176.4.us, %"for convolved.s1.r19$y196.us" ], [ %8820, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3120.5.us = phi <4 x i32> [ %convolved944.sroa.3120.4.us, %"for convolved.s1.r19$y196.us" ], [ %8819, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3064.5.us = phi <4 x i32> [ %convolved944.sroa.3064.4.us, %"for convolved.s1.r19$y196.us" ], [ %8818, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.3008.5.us = phi <4 x i32> [ %convolved944.sroa.3008.4.us, %"for convolved.s1.r19$y196.us" ], [ %8817, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2952.5.us = phi <4 x i32> [ %convolved944.sroa.2952.4.us, %"for convolved.s1.r19$y196.us" ], [ %8802, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2896.5.us = phi <4 x i32> [ %convolved944.sroa.2896.4.us, %"for convolved.s1.r19$y196.us" ], [ %8801, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2840.5.us = phi <4 x i32> [ %convolved944.sroa.2840.4.us, %"for convolved.s1.r19$y196.us" ], [ %8800, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2784.5.us = phi <4 x i32> [ %convolved944.sroa.2784.4.us, %"for convolved.s1.r19$y196.us" ], [ %8799, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2728.5.us = phi <4 x i32> [ %convolved944.sroa.2728.4.us, %"for convolved.s1.r19$y196.us" ], [ %8784, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2672.5.us = phi <4 x i32> [ %convolved944.sroa.2672.4.us, %"for convolved.s1.r19$y196.us" ], [ %8783, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2616.5.us = phi <4 x i32> [ %convolved944.sroa.2616.4.us, %"for convolved.s1.r19$y196.us" ], [ %8782, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2560.5.us = phi <4 x i32> [ %convolved944.sroa.2560.4.us, %"for convolved.s1.r19$y196.us" ], [ %8781, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2504.5.us = phi <4 x i32> [ %convolved944.sroa.2504.4.us, %"for convolved.s1.r19$y196.us" ], [ %8766, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2448.5.us = phi <4 x i32> [ %convolved944.sroa.2448.4.us, %"for convolved.s1.r19$y196.us" ], [ %8765, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2392.5.us = phi <4 x i32> [ %convolved944.sroa.2392.4.us, %"for convolved.s1.r19$y196.us" ], [ %8764, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2336.5.us = phi <4 x i32> [ %convolved944.sroa.2336.4.us, %"for convolved.s1.r19$y196.us" ], [ %8763, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2280.5.us = phi <4 x i32> [ %convolved944.sroa.2280.4.us, %"for convolved.s1.r19$y196.us" ], [ %8748, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2224.5.us = phi <4 x i32> [ %convolved944.sroa.2224.4.us, %"for convolved.s1.r19$y196.us" ], [ %8747, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2168.5.us = phi <4 x i32> [ %convolved944.sroa.2168.4.us, %"for convolved.s1.r19$y196.us" ], [ %8746, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2112.5.us = phi <4 x i32> [ %convolved944.sroa.2112.4.us, %"for convolved.s1.r19$y196.us" ], [ %8745, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2056.5.us = phi <4 x i32> [ %convolved944.sroa.2056.4.us, %"for convolved.s1.r19$y196.us" ], [ %8730, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.2000.5.us = phi <4 x i32> [ %convolved944.sroa.2000.4.us, %"for convolved.s1.r19$y196.us" ], [ %8729, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1944.5.us = phi <4 x i32> [ %convolved944.sroa.1944.4.us, %"for convolved.s1.r19$y196.us" ], [ %8728, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1888.5.us = phi <4 x i32> [ %convolved944.sroa.1888.4.us, %"for convolved.s1.r19$y196.us" ], [ %8727, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1832.5.us = phi <4 x i32> [ %convolved944.sroa.1832.4.us, %"for convolved.s1.r19$y196.us" ], [ %8712, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1776.5.us = phi <4 x i32> [ %convolved944.sroa.1776.4.us, %"for convolved.s1.r19$y196.us" ], [ %8711, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1720.5.us = phi <4 x i32> [ %convolved944.sroa.1720.4.us, %"for convolved.s1.r19$y196.us" ], [ %8710, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1664.5.us = phi <4 x i32> [ %convolved944.sroa.1664.4.us, %"for convolved.s1.r19$y196.us" ], [ %8709, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1608.5.us = phi <4 x i32> [ %convolved944.sroa.1608.4.us, %"for convolved.s1.r19$y196.us" ], [ %8694, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1552.5.us = phi <4 x i32> [ %convolved944.sroa.1552.4.us, %"for convolved.s1.r19$y196.us" ], [ %8693, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1496.5.us = phi <4 x i32> [ %convolved944.sroa.1496.4.us, %"for convolved.s1.r19$y196.us" ], [ %8692, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1440.5.us = phi <4 x i32> [ %convolved944.sroa.1440.4.us, %"for convolved.s1.r19$y196.us" ], [ %8691, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1384.5.us = phi <4 x i32> [ %convolved944.sroa.1384.4.us, %"for convolved.s1.r19$y196.us" ], [ %8676, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1328.5.us = phi <4 x i32> [ %convolved944.sroa.1328.4.us, %"for convolved.s1.r19$y196.us" ], [ %8675, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1272.5.us = phi <4 x i32> [ %convolved944.sroa.1272.4.us, %"for convolved.s1.r19$y196.us" ], [ %8674, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1216.5.us = phi <4 x i32> [ %convolved944.sroa.1216.4.us, %"for convolved.s1.r19$y196.us" ], [ %8673, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1160.5.us = phi <4 x i32> [ %convolved944.sroa.1160.4.us, %"for convolved.s1.r19$y196.us" ], [ %8658, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1104.5.us = phi <4 x i32> [ %convolved944.sroa.1104.4.us, %"for convolved.s1.r19$y196.us" ], [ %8657, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.1048.5.us = phi <4 x i32> [ %convolved944.sroa.1048.4.us, %"for convolved.s1.r19$y196.us" ], [ %8656, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.992.5.us = phi <4 x i32> [ %convolved944.sroa.992.4.us, %"for convolved.s1.r19$y196.us" ], [ %8655, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.930.5.us = phi <4 x i32> [ %convolved944.sroa.930.4.us, %"for convolved.s1.r19$y196.us" ], [ %8640, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.868.5.us = phi <4 x i32> [ %convolved944.sroa.868.4.us, %"for convolved.s1.r19$y196.us" ], [ %8639, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.806.5.us = phi <4 x i32> [ %convolved944.sroa.806.4.us, %"for convolved.s1.r19$y196.us" ], [ %8638, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.744.5.us = phi <4 x i32> [ %convolved944.sroa.744.4.us, %"for convolved.s1.r19$y196.us" ], [ %8637, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.682.5.us = phi <4 x i32> [ %convolved944.sroa.682.4.us, %"for convolved.s1.r19$y196.us" ], [ %8622, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.620.5.us = phi <4 x i32> [ %convolved944.sroa.620.4.us, %"for convolved.s1.r19$y196.us" ], [ %8621, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.558.5.us = phi <4 x i32> [ %convolved944.sroa.558.4.us, %"for convolved.s1.r19$y196.us" ], [ %8620, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.496.5.us = phi <4 x i32> [ %convolved944.sroa.496.4.us, %"for convolved.s1.r19$y196.us" ], [ %8619, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.434.5.us = phi <4 x i32> [ %convolved944.sroa.434.4.us, %"for convolved.s1.r19$y196.us" ], [ %8604, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.372.5.us = phi <4 x i32> [ %convolved944.sroa.372.4.us, %"for convolved.s1.r19$y196.us" ], [ %8603, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.310.5.us = phi <4 x i32> [ %convolved944.sroa.310.4.us, %"for convolved.s1.r19$y196.us" ], [ %8602, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.248.5.us = phi <4 x i32> [ %convolved944.sroa.248.4.us, %"for convolved.s1.r19$y196.us" ], [ %8601, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.186.5.us = phi <4 x i32> [ %convolved944.sroa.186.4.us, %"for convolved.s1.r19$y196.us" ], [ %8586, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.124.5.us = phi <4 x i32> [ %convolved944.sroa.124.4.us, %"for convolved.s1.r19$y196.us" ], [ %8585, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.62.5.us = phi <4 x i32> [ %convolved944.sroa.62.4.us, %"for convolved.s1.r19$y196.us" ], [ %8584, %"for convolved.s1.r19$x199.us" ]
  %convolved944.sroa.0.5.us = phi <4 x i32> [ %convolved944.sroa.0.4.us, %"for convolved.s1.r19$y196.us" ], [ %8583, %"for convolved.s1.r19$x199.us" ]
  %8555 = add nsw i64 %indvars.iv6310, %8538
  %8556 = shl nsw i64 %8555, 4
  %8557 = getelementptr inbounds i16, i16* %filter_zeroed91, i64 %8556
  %8558 = bitcast i16* %8557 to <8 x i16>*
  %8559 = load <8 x i16>, <8 x i16>* %8558, align 16, !tbaa !395
  %8560 = getelementptr inbounds i16, i16* %8557, i64 8
  %8561 = bitcast i16* %8560 to <8 x i16>*
  %8562 = load <8 x i16>, <8 x i16>* %8561, align 16, !tbaa !395
  %8563 = shufflevector <8 x i16> %8562, <8 x i16> poison, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 4, i32 5, i32 6, i32 7>
  %8564 = mul nsw i64 %indvars.iv6310, %5579
  %8565 = add nsw i64 %8564, %8539
  %8566 = shl nsw i64 %8565, 4
  %8567 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8566
  %8568 = bitcast i8* %8567 to <16 x i8>*
  %8569 = load <16 x i8>, <16 x i8>* %8568, align 16, !tbaa !438
  %8570 = zext <16 x i8> %8569 to <16 x i16>
  %8571 = shufflevector <8 x i16> %8559, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8572 = shufflevector <16 x i16> %8570, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8573 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8572)
  %8574 = shufflevector <8 x i16> %8559, <8 x i16> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8575 = shufflevector <16 x i16> %8570, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8576 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8575)
  %8577 = shufflevector <8 x i16> %8562, <8 x i16> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8578 = shufflevector <16 x i16> %8570, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8579 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8578)
  %8580 = shufflevector <16 x i16> %8563, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8581 = shufflevector <16 x i16> %8570, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8582 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8581)
  %8583 = add nsw <4 x i32> %8573, %convolved944.sroa.0.5.us
  %8584 = add nsw <4 x i32> %8576, %convolved944.sroa.62.5.us
  %8585 = add nsw <4 x i32> %8579, %convolved944.sroa.124.5.us
  %8586 = add nsw <4 x i32> %8582, %convolved944.sroa.186.5.us
  %8587 = add nsw i64 %8564, %8540
  %8588 = shl nsw i64 %8587, 4
  %8589 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8588
  %8590 = bitcast i8* %8589 to <16 x i8>*
  %8591 = load <16 x i8>, <16 x i8>* %8590, align 16, !tbaa !438
  %8592 = zext <16 x i8> %8591 to <16 x i16>
  %8593 = shufflevector <16 x i16> %8592, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8594 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8593)
  %8595 = shufflevector <16 x i16> %8592, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8596 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8595)
  %8597 = shufflevector <16 x i16> %8592, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8598 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8597)
  %8599 = shufflevector <16 x i16> %8592, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8600 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8599)
  %8601 = add nsw <4 x i32> %8594, %convolved944.sroa.248.5.us
  %8602 = add nsw <4 x i32> %8596, %convolved944.sroa.310.5.us
  %8603 = add nsw <4 x i32> %8598, %convolved944.sroa.372.5.us
  %8604 = add nsw <4 x i32> %8600, %convolved944.sroa.434.5.us
  %8605 = add nsw i64 %8564, %8541
  %8606 = shl nsw i64 %8605, 4
  %8607 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8606
  %8608 = bitcast i8* %8607 to <16 x i8>*
  %8609 = load <16 x i8>, <16 x i8>* %8608, align 16, !tbaa !438
  %8610 = zext <16 x i8> %8609 to <16 x i16>
  %8611 = shufflevector <16 x i16> %8610, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8612 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8611)
  %8613 = shufflevector <16 x i16> %8610, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8614 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8613)
  %8615 = shufflevector <16 x i16> %8610, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8616 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8615)
  %8617 = shufflevector <16 x i16> %8610, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8618 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8617)
  %8619 = add nsw <4 x i32> %8612, %convolved944.sroa.496.5.us
  %8620 = add nsw <4 x i32> %8614, %convolved944.sroa.558.5.us
  %8621 = add nsw <4 x i32> %8616, %convolved944.sroa.620.5.us
  %8622 = add nsw <4 x i32> %8618, %convolved944.sroa.682.5.us
  %8623 = add nsw i64 %8564, %8542
  %8624 = shl nsw i64 %8623, 4
  %8625 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8624
  %8626 = bitcast i8* %8625 to <16 x i8>*
  %8627 = load <16 x i8>, <16 x i8>* %8626, align 16, !tbaa !438
  %8628 = zext <16 x i8> %8627 to <16 x i16>
  %8629 = shufflevector <16 x i16> %8628, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8630 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8629)
  %8631 = shufflevector <16 x i16> %8628, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8632 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8631)
  %8633 = shufflevector <16 x i16> %8628, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8634 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8633)
  %8635 = shufflevector <16 x i16> %8628, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8636 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8635)
  %8637 = add nsw <4 x i32> %8630, %convolved944.sroa.744.5.us
  %8638 = add nsw <4 x i32> %8632, %convolved944.sroa.806.5.us
  %8639 = add nsw <4 x i32> %8634, %convolved944.sroa.868.5.us
  %8640 = add nsw <4 x i32> %8636, %convolved944.sroa.930.5.us
  %8641 = add nsw i64 %8564, %8543
  %8642 = shl nsw i64 %8641, 4
  %8643 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8642
  %8644 = bitcast i8* %8643 to <16 x i8>*
  %8645 = load <16 x i8>, <16 x i8>* %8644, align 16, !tbaa !438
  %8646 = zext <16 x i8> %8645 to <16 x i16>
  %8647 = shufflevector <16 x i16> %8646, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8648 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8647)
  %8649 = shufflevector <16 x i16> %8646, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8650 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8649)
  %8651 = shufflevector <16 x i16> %8646, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8652 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8651)
  %8653 = shufflevector <16 x i16> %8646, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8654 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8653)
  %8655 = add nsw <4 x i32> %8648, %convolved944.sroa.992.5.us
  %8656 = add nsw <4 x i32> %8650, %convolved944.sroa.1048.5.us
  %8657 = add nsw <4 x i32> %8652, %convolved944.sroa.1104.5.us
  %8658 = add nsw <4 x i32> %8654, %convolved944.sroa.1160.5.us
  %8659 = add nsw i64 %8564, %8544
  %8660 = shl nsw i64 %8659, 4
  %8661 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8660
  %8662 = bitcast i8* %8661 to <16 x i8>*
  %8663 = load <16 x i8>, <16 x i8>* %8662, align 16, !tbaa !438
  %8664 = zext <16 x i8> %8663 to <16 x i16>
  %8665 = shufflevector <16 x i16> %8664, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8666 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8665)
  %8667 = shufflevector <16 x i16> %8664, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8668 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8667)
  %8669 = shufflevector <16 x i16> %8664, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8670 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8669)
  %8671 = shufflevector <16 x i16> %8664, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8672 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8671)
  %8673 = add nsw <4 x i32> %8666, %convolved944.sroa.1216.5.us
  %8674 = add nsw <4 x i32> %8668, %convolved944.sroa.1272.5.us
  %8675 = add nsw <4 x i32> %8670, %convolved944.sroa.1328.5.us
  %8676 = add nsw <4 x i32> %8672, %convolved944.sroa.1384.5.us
  %8677 = add nsw i64 %8564, %8545
  %8678 = shl nsw i64 %8677, 4
  %8679 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8678
  %8680 = bitcast i8* %8679 to <16 x i8>*
  %8681 = load <16 x i8>, <16 x i8>* %8680, align 16, !tbaa !438
  %8682 = zext <16 x i8> %8681 to <16 x i16>
  %8683 = shufflevector <16 x i16> %8682, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8684 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8683)
  %8685 = shufflevector <16 x i16> %8682, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8686 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8685)
  %8687 = shufflevector <16 x i16> %8682, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8688 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8687)
  %8689 = shufflevector <16 x i16> %8682, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8690 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8689)
  %8691 = add nsw <4 x i32> %8684, %convolved944.sroa.1440.5.us
  %8692 = add nsw <4 x i32> %8686, %convolved944.sroa.1496.5.us
  %8693 = add nsw <4 x i32> %8688, %convolved944.sroa.1552.5.us
  %8694 = add nsw <4 x i32> %8690, %convolved944.sroa.1608.5.us
  %8695 = add nsw i64 %8564, %8546
  %8696 = shl nsw i64 %8695, 4
  %8697 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8696
  %8698 = bitcast i8* %8697 to <16 x i8>*
  %8699 = load <16 x i8>, <16 x i8>* %8698, align 16, !tbaa !438
  %8700 = zext <16 x i8> %8699 to <16 x i16>
  %8701 = shufflevector <16 x i16> %8700, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8702 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8701)
  %8703 = shufflevector <16 x i16> %8700, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8704 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8703)
  %8705 = shufflevector <16 x i16> %8700, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8706 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8705)
  %8707 = shufflevector <16 x i16> %8700, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8708 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8707)
  %8709 = add nsw <4 x i32> %8702, %convolved944.sroa.1664.5.us
  %8710 = add nsw <4 x i32> %8704, %convolved944.sroa.1720.5.us
  %8711 = add nsw <4 x i32> %8706, %convolved944.sroa.1776.5.us
  %8712 = add nsw <4 x i32> %8708, %convolved944.sroa.1832.5.us
  %8713 = add nsw i64 %8564, %8547
  %8714 = shl nsw i64 %8713, 4
  %8715 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8714
  %8716 = bitcast i8* %8715 to <16 x i8>*
  %8717 = load <16 x i8>, <16 x i8>* %8716, align 16, !tbaa !438
  %8718 = zext <16 x i8> %8717 to <16 x i16>
  %8719 = shufflevector <16 x i16> %8718, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8720 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8719)
  %8721 = shufflevector <16 x i16> %8718, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8722 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8721)
  %8723 = shufflevector <16 x i16> %8718, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8724 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8723)
  %8725 = shufflevector <16 x i16> %8718, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8726 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8725)
  %8727 = add nsw <4 x i32> %8720, %convolved944.sroa.1888.5.us
  %8728 = add nsw <4 x i32> %8722, %convolved944.sroa.1944.5.us
  %8729 = add nsw <4 x i32> %8724, %convolved944.sroa.2000.5.us
  %8730 = add nsw <4 x i32> %8726, %convolved944.sroa.2056.5.us
  %8731 = add nsw i64 %8564, %8548
  %8732 = shl nsw i64 %8731, 4
  %8733 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8732
  %8734 = bitcast i8* %8733 to <16 x i8>*
  %8735 = load <16 x i8>, <16 x i8>* %8734, align 16, !tbaa !438
  %8736 = zext <16 x i8> %8735 to <16 x i16>
  %8737 = shufflevector <16 x i16> %8736, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8738 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8737)
  %8739 = shufflevector <16 x i16> %8736, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8740 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8739)
  %8741 = shufflevector <16 x i16> %8736, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8742 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8741)
  %8743 = shufflevector <16 x i16> %8736, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8744 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8743)
  %8745 = add nsw <4 x i32> %8738, %convolved944.sroa.2112.5.us
  %8746 = add nsw <4 x i32> %8740, %convolved944.sroa.2168.5.us
  %8747 = add nsw <4 x i32> %8742, %convolved944.sroa.2224.5.us
  %8748 = add nsw <4 x i32> %8744, %convolved944.sroa.2280.5.us
  %8749 = add nsw i64 %8564, %8549
  %8750 = shl nsw i64 %8749, 4
  %8751 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8750
  %8752 = bitcast i8* %8751 to <16 x i8>*
  %8753 = load <16 x i8>, <16 x i8>* %8752, align 16, !tbaa !438
  %8754 = zext <16 x i8> %8753 to <16 x i16>
  %8755 = shufflevector <16 x i16> %8754, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8756 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8755)
  %8757 = shufflevector <16 x i16> %8754, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8758 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8757)
  %8759 = shufflevector <16 x i16> %8754, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8760 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8759)
  %8761 = shufflevector <16 x i16> %8754, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8762 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8761)
  %8763 = add nsw <4 x i32> %8756, %convolved944.sroa.2336.5.us
  %8764 = add nsw <4 x i32> %8758, %convolved944.sroa.2392.5.us
  %8765 = add nsw <4 x i32> %8760, %convolved944.sroa.2448.5.us
  %8766 = add nsw <4 x i32> %8762, %convolved944.sroa.2504.5.us
  %8767 = add nsw i64 %8564, %8550
  %8768 = shl nsw i64 %8767, 4
  %8769 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8768
  %8770 = bitcast i8* %8769 to <16 x i8>*
  %8771 = load <16 x i8>, <16 x i8>* %8770, align 16, !tbaa !438
  %8772 = zext <16 x i8> %8771 to <16 x i16>
  %8773 = shufflevector <16 x i16> %8772, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8774 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8773)
  %8775 = shufflevector <16 x i16> %8772, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8776 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8775)
  %8777 = shufflevector <16 x i16> %8772, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8778 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8777)
  %8779 = shufflevector <16 x i16> %8772, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8780 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8779)
  %8781 = add nsw <4 x i32> %8774, %convolved944.sroa.2560.5.us
  %8782 = add nsw <4 x i32> %8776, %convolved944.sroa.2616.5.us
  %8783 = add nsw <4 x i32> %8778, %convolved944.sroa.2672.5.us
  %8784 = add nsw <4 x i32> %8780, %convolved944.sroa.2728.5.us
  %8785 = add nsw i64 %8564, %8551
  %8786 = shl nsw i64 %8785, 4
  %8787 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8786
  %8788 = bitcast i8* %8787 to <16 x i8>*
  %8789 = load <16 x i8>, <16 x i8>* %8788, align 16, !tbaa !438
  %8790 = zext <16 x i8> %8789 to <16 x i16>
  %8791 = shufflevector <16 x i16> %8790, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8792 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8791)
  %8793 = shufflevector <16 x i16> %8790, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8794 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8793)
  %8795 = shufflevector <16 x i16> %8790, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8796 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8795)
  %8797 = shufflevector <16 x i16> %8790, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8798 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8797)
  %8799 = add nsw <4 x i32> %8792, %convolved944.sroa.2784.5.us
  %8800 = add nsw <4 x i32> %8794, %convolved944.sroa.2840.5.us
  %8801 = add nsw <4 x i32> %8796, %convolved944.sroa.2896.5.us
  %8802 = add nsw <4 x i32> %8798, %convolved944.sroa.2952.5.us
  %8803 = add nsw i64 %8564, %8552
  %8804 = shl nsw i64 %8803, 4
  %8805 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8804
  %8806 = bitcast i8* %8805 to <16 x i8>*
  %8807 = load <16 x i8>, <16 x i8>* %8806, align 16, !tbaa !438
  %8808 = zext <16 x i8> %8807 to <16 x i16>
  %8809 = shufflevector <16 x i16> %8808, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8810 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8809)
  %8811 = shufflevector <16 x i16> %8808, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8812 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8811)
  %8813 = shufflevector <16 x i16> %8808, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8814 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8813)
  %8815 = shufflevector <16 x i16> %8808, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8816 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8815)
  %8817 = add nsw <4 x i32> %8810, %convolved944.sroa.3008.5.us
  %8818 = add nsw <4 x i32> %8812, %convolved944.sroa.3064.5.us
  %8819 = add nsw <4 x i32> %8814, %convolved944.sroa.3120.5.us
  %8820 = add nsw <4 x i32> %8816, %convolved944.sroa.3176.5.us
  %8821 = add nsw i64 %8564, %8553
  %8822 = shl nsw i64 %8821, 4
  %8823 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8822
  %8824 = bitcast i8* %8823 to <16 x i8>*
  %8825 = load <16 x i8>, <16 x i8>* %8824, align 16, !tbaa !438
  %8826 = zext <16 x i8> %8825 to <16 x i16>
  %8827 = shufflevector <16 x i16> %8826, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8828 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8827)
  %8829 = shufflevector <16 x i16> %8826, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8830 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8829)
  %8831 = shufflevector <16 x i16> %8826, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8832 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8831)
  %8833 = shufflevector <16 x i16> %8826, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8834 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8833)
  %8835 = add nsw <4 x i32> %8828, %convolved944.sroa.3232.5.us
  %8836 = add nsw <4 x i32> %8830, %convolved944.sroa.3288.5.us
  %8837 = add nsw <4 x i32> %8832, %convolved944.sroa.3344.5.us
  %8838 = add nsw <4 x i32> %8834, %convolved944.sroa.3400.5.us
  %8839 = add nsw i64 %8564, %8554
  %8840 = shl nsw i64 %8839, 4
  %8841 = getelementptr inbounds i8, i8* %resampled_input122, i64 %8840
  %8842 = bitcast i8* %8841 to <16 x i8>*
  %8843 = load <16 x i8>, <16 x i8>* %8842, align 16, !tbaa !438
  %8844 = zext <16 x i8> %8843 to <16 x i16>
  %8845 = shufflevector <16 x i16> %8844, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %8846 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8571, <4 x i16> %8845)
  %8847 = shufflevector <16 x i16> %8844, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8848 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8574, <4 x i16> %8847)
  %8849 = shufflevector <16 x i16> %8844, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8850 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8577, <4 x i16> %8849)
  %8851 = shufflevector <16 x i16> %8844, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %8852 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %8580, <4 x i16> %8851)
  %8853 = add nsw <4 x i32> %8846, %convolved944.sroa.3456.5.us
  %8854 = add nsw <4 x i32> %8848, %convolved944.sroa.3512.5.us
  %8855 = add nsw <4 x i32> %8850, %convolved944.sroa.3568.5.us
  %8856 = add nsw <4 x i32> %8852, %convolved944.sroa.3624.5.us
  %indvars.iv.next6311 = add nuw nsw i64 %indvars.iv6310, 1
  %.not2879.us = icmp eq i64 %indvars.iv.next6311, %5637
  br i1 %.not2879.us, label %"end for convolved.s1.r19$x200.loopexit.us", label %"for convolved.s1.r19$x199.us"

"end for convolved.s1.r19$x200.loopexit.us":      ; preds = %"for convolved.s1.r19$x199.us"
  %indvars.iv.next6313 = add nuw nsw i64 %indvars.iv6312, 1
  %8857 = add nuw nsw i32 %"convolved.s1.r19$y198.us", 1
  %.not2878.us = icmp eq i64 %indvars.iv.next6313, %5639
  br i1 %.not2878.us, label %"consume convolved202", label %"for convolved.s1.r19$y196.us"

"consume convolved202":                           ; preds = %"end for convolved.s1.r19$x200.loopexit.us", %"for convolved.s1.r19$y196.preheader", %next_bb195, %then_bb194
  %convolved944.sroa.3624.7 = phi <4 x i32> [ %8524, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8856, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3568.7 = phi <4 x i32> [ %8523, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8855, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3512.7 = phi <4 x i32> [ %8522, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8854, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3456.7 = phi <4 x i32> [ %8521, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8853, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3400.7 = phi <4 x i32> [ %8506, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8838, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3344.7 = phi <4 x i32> [ %8505, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8837, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3288.7 = phi <4 x i32> [ %8504, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8836, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3232.7 = phi <4 x i32> [ %8503, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8835, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3176.7 = phi <4 x i32> [ %8488, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8820, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3120.7 = phi <4 x i32> [ %8487, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8819, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3064.7 = phi <4 x i32> [ %8486, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8818, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.3008.7 = phi <4 x i32> [ %8485, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8817, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2952.7 = phi <4 x i32> [ %8470, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8802, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2896.7 = phi <4 x i32> [ %8469, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8801, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2840.7 = phi <4 x i32> [ %8468, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8800, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2784.7 = phi <4 x i32> [ %8467, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8799, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2728.7 = phi <4 x i32> [ %8452, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8784, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2672.7 = phi <4 x i32> [ %8451, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8783, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2616.7 = phi <4 x i32> [ %8450, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8782, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2560.7 = phi <4 x i32> [ %8449, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8781, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2504.7 = phi <4 x i32> [ %8434, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8766, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2448.7 = phi <4 x i32> [ %8433, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8765, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2392.7 = phi <4 x i32> [ %8432, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8764, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2336.7 = phi <4 x i32> [ %8431, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8763, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2280.7 = phi <4 x i32> [ %8416, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8748, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2224.7 = phi <4 x i32> [ %8415, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8747, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2168.7 = phi <4 x i32> [ %8414, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8746, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2112.7 = phi <4 x i32> [ %8413, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8745, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2056.7 = phi <4 x i32> [ %8398, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8730, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.2000.7 = phi <4 x i32> [ %8397, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8729, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1944.7 = phi <4 x i32> [ %8396, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8728, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1888.7 = phi <4 x i32> [ %8395, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8727, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1832.7 = phi <4 x i32> [ %8380, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8712, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1776.7 = phi <4 x i32> [ %8379, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8711, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1720.7 = phi <4 x i32> [ %8378, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8710, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1664.7 = phi <4 x i32> [ %8377, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8709, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1608.7 = phi <4 x i32> [ %8362, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8694, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1552.7 = phi <4 x i32> [ %8361, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8693, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1496.7 = phi <4 x i32> [ %8360, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8692, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1440.7 = phi <4 x i32> [ %8359, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8691, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1384.7 = phi <4 x i32> [ %8344, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8676, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1328.7 = phi <4 x i32> [ %8343, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8675, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1272.7 = phi <4 x i32> [ %8342, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8674, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1216.7 = phi <4 x i32> [ %8341, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8673, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1160.7 = phi <4 x i32> [ %8326, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8658, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1104.7 = phi <4 x i32> [ %8325, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8657, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.1048.7 = phi <4 x i32> [ %8324, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8656, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.992.7 = phi <4 x i32> [ %8323, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8655, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.930.7 = phi <4 x i32> [ %8308, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8640, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.868.7 = phi <4 x i32> [ %8307, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8639, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.806.7 = phi <4 x i32> [ %8306, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8638, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.744.7 = phi <4 x i32> [ %8305, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8637, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.682.7 = phi <4 x i32> [ %8290, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8622, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.620.7 = phi <4 x i32> [ %8289, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8621, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.558.7 = phi <4 x i32> [ %8288, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8620, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.496.7 = phi <4 x i32> [ %8287, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8619, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.434.7 = phi <4 x i32> [ %8272, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8604, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.372.7 = phi <4 x i32> [ %8271, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8603, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.310.7 = phi <4 x i32> [ %8270, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8602, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.248.7 = phi <4 x i32> [ %8269, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8601, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.186.7 = phi <4 x i32> [ %8254, %then_bb194 ], [ %5805, %next_bb195 ], [ %5805, %"for convolved.s1.r19$y196.preheader" ], [ %8586, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.124.7 = phi <4 x i32> [ %8253, %then_bb194 ], [ %5804, %next_bb195 ], [ %5804, %"for convolved.s1.r19$y196.preheader" ], [ %8585, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.62.7 = phi <4 x i32> [ %8252, %then_bb194 ], [ %5803, %next_bb195 ], [ %5803, %"for convolved.s1.r19$y196.preheader" ], [ %8584, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %convolved944.sroa.0.7 = phi <4 x i32> [ %8251, %then_bb194 ], [ %5802, %next_bb195 ], [ %5802, %"for convolved.s1.r19$y196.preheader" ], [ %8583, %"end for convolved.s1.r19$x200.loopexit.us" ]
  %8858 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.0.7, <4 x i32> undef
  %8859 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.62.7, <4 x i32> undef
  %8860 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.124.7, <4 x i32> undef
  %8861 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.186.7, <4 x i32> undef
  %8862 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8858, <4 x i32> %5618)
  %8863 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8859, <4 x i32> %5618)
  %8864 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8860, <4 x i32> %5618)
  %8865 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8861, <4 x i32> %5618)
  %8866 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8862, <4 x i32> %5621)
  %8867 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8863, <4 x i32> %5621)
  %8868 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8864, <4 x i32> %5621)
  %8869 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8865, <4 x i32> %5621)
  %8870 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8866)
  %8871 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8867)
  %8872 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8868)
  %8873 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8869)
  %8874 = shufflevector <4 x i16> %8870, <4 x i16> %8871, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %8875 = shufflevector <4 x i16> %8872, <4 x i16> %8873, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %8876 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8874, <8 x i16> %5624)
  %8877 = shufflevector <16 x i16> %8875, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8878 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8877, <8 x i16> %5624)
  %8879 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %8876)
  %8880 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %8878)
  %8881 = shufflevector <8 x i8> %8879, <8 x i8> %8880, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8882 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %8881, <16 x i8> %5626)
  %8883 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %8882, <16 x i8> %5628)
  %8884 = shl nuw nsw i64 %indvars.iv6316, 2
  %8885 = add nsw i64 %8884, %5583
  %8886 = mul nsw i64 %8885, %5629
  %8887 = add nsw i64 %8886, %5856
  %8888 = getelementptr inbounds i8, i8* %19, i64 %8887
  %8889 = bitcast i8* %8888 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %8883, <16 x i8>* %8889, i32 1, <16 x i1> %5418), !tbaa !515
  %8890 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.248.7, <4 x i32> undef
  %8891 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.310.7, <4 x i32> undef
  %8892 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.372.7, <4 x i32> undef
  %8893 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.434.7, <4 x i32> undef
  %8894 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8890, <4 x i32> %5618)
  %8895 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8891, <4 x i32> %5618)
  %8896 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8892, <4 x i32> %5618)
  %8897 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8893, <4 x i32> %5618)
  %8898 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8894, <4 x i32> %5621)
  %8899 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8895, <4 x i32> %5621)
  %8900 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8896, <4 x i32> %5621)
  %8901 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8897, <4 x i32> %5621)
  %8902 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8898)
  %8903 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8899)
  %8904 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8900)
  %8905 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8901)
  %8906 = shufflevector <4 x i16> %8902, <4 x i16> %8903, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %8907 = shufflevector <4 x i16> %8904, <4 x i16> %8905, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %8908 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8906, <8 x i16> %5624)
  %8909 = shufflevector <16 x i16> %8907, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8910 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8909, <8 x i16> %5624)
  %8911 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %8908)
  %8912 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %8910)
  %8913 = shufflevector <8 x i8> %8911, <8 x i8> %8912, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8914 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %8913, <16 x i8> %5626)
  %8915 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %8914, <16 x i8> %5628)
  %8916 = add nsw i64 %8885, 1
  %8917 = mul nsw i64 %8916, %5629
  %8918 = add nsw i64 %8917, %5856
  %8919 = getelementptr inbounds i8, i8* %19, i64 %8918
  %8920 = bitcast i8* %8919 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %8915, <16 x i8>* %8920, i32 1, <16 x i1> %5418), !tbaa !515
  %8921 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.496.7, <4 x i32> undef
  %8922 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.558.7, <4 x i32> undef
  %8923 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.620.7, <4 x i32> undef
  %8924 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.682.7, <4 x i32> undef
  %8925 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8921, <4 x i32> %5618)
  %8926 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8922, <4 x i32> %5618)
  %8927 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8923, <4 x i32> %5618)
  %8928 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8924, <4 x i32> %5618)
  %8929 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8925, <4 x i32> %5621)
  %8930 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8926, <4 x i32> %5621)
  %8931 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8927, <4 x i32> %5621)
  %8932 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8928, <4 x i32> %5621)
  %8933 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8929)
  %8934 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8930)
  %8935 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8931)
  %8936 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8932)
  %8937 = shufflevector <4 x i16> %8933, <4 x i16> %8934, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %8938 = shufflevector <4 x i16> %8935, <4 x i16> %8936, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %8939 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8937, <8 x i16> %5624)
  %8940 = shufflevector <16 x i16> %8938, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8941 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8940, <8 x i16> %5624)
  %8942 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %8939)
  %8943 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %8941)
  %8944 = shufflevector <8 x i8> %8942, <8 x i8> %8943, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8945 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %8944, <16 x i8> %5626)
  %8946 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %8945, <16 x i8> %5628)
  %8947 = add nsw i64 %8885, 2
  %8948 = mul nsw i64 %8947, %5629
  %8949 = add nsw i64 %8948, %5856
  %8950 = getelementptr inbounds i8, i8* %19, i64 %8949
  %8951 = bitcast i8* %8950 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %8946, <16 x i8>* %8951, i32 1, <16 x i1> %5418), !tbaa !515
  %8952 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.744.7, <4 x i32> undef
  %8953 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.806.7, <4 x i32> undef
  %8954 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.868.7, <4 x i32> undef
  %8955 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.930.7, <4 x i32> undef
  %8956 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8952, <4 x i32> %5618)
  %8957 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8953, <4 x i32> %5618)
  %8958 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8954, <4 x i32> %5618)
  %8959 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8955, <4 x i32> %5618)
  %8960 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8956, <4 x i32> %5621)
  %8961 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8957, <4 x i32> %5621)
  %8962 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8958, <4 x i32> %5621)
  %8963 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8959, <4 x i32> %5621)
  %8964 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8960)
  %8965 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8961)
  %8966 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8962)
  %8967 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8963)
  %8968 = shufflevector <4 x i16> %8964, <4 x i16> %8965, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %8969 = shufflevector <4 x i16> %8966, <4 x i16> %8967, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %8970 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8968, <8 x i16> %5624)
  %8971 = shufflevector <16 x i16> %8969, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8972 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8971, <8 x i16> %5624)
  %8973 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %8970)
  %8974 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %8972)
  %8975 = shufflevector <8 x i8> %8973, <8 x i8> %8974, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8976 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %8975, <16 x i8> %5626)
  %8977 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %8976, <16 x i8> %5628)
  %8978 = add nsw i64 %8885, 3
  %8979 = mul nsw i64 %8978, %5629
  %8980 = add nsw i64 %8979, %5856
  %8981 = getelementptr inbounds i8, i8* %19, i64 %8980
  %8982 = bitcast i8* %8981 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %8977, <16 x i8>* %8982, i32 1, <16 x i1> %5418), !tbaa !515
  %8983 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.992.7, <4 x i32> undef
  %8984 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.1048.7, <4 x i32> undef
  %8985 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.1104.7, <4 x i32> undef
  %8986 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.1160.7, <4 x i32> undef
  %8987 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8983, <4 x i32> %5618)
  %8988 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8984, <4 x i32> %5618)
  %8989 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8985, <4 x i32> %5618)
  %8990 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %8986, <4 x i32> %5618)
  %8991 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8987, <4 x i32> %5621)
  %8992 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8988, <4 x i32> %5621)
  %8993 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8989, <4 x i32> %5621)
  %8994 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %8990, <4 x i32> %5621)
  %8995 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8991)
  %8996 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8992)
  %8997 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8993)
  %8998 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %8994)
  %8999 = shufflevector <4 x i16> %8995, <4 x i16> %8996, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9000 = shufflevector <4 x i16> %8997, <4 x i16> %8998, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9001 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %8999, <8 x i16> %5624)
  %9002 = shufflevector <16 x i16> %9000, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9003 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9002, <8 x i16> %5624)
  %9004 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9001)
  %9005 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9003)
  %9006 = shufflevector <8 x i8> %9004, <8 x i8> %9005, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9007 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9006, <16 x i8> %5626)
  %9008 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9007, <16 x i8> %5628)
  %9009 = add nsw i64 %8886, %5857
  %9010 = getelementptr inbounds i8, i8* %19, i64 %9009
  %9011 = bitcast i8* %9010 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9008, <16 x i8>* %9011, i32 1, <16 x i1> %5418), !tbaa !515
  %9012 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.1216.7, <4 x i32> undef
  %9013 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.1272.7, <4 x i32> undef
  %9014 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.1328.7, <4 x i32> undef
  %9015 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.1384.7, <4 x i32> undef
  %9016 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9012, <4 x i32> %5618)
  %9017 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9013, <4 x i32> %5618)
  %9018 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9014, <4 x i32> %5618)
  %9019 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9015, <4 x i32> %5618)
  %9020 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9016, <4 x i32> %5621)
  %9021 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9017, <4 x i32> %5621)
  %9022 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9018, <4 x i32> %5621)
  %9023 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9019, <4 x i32> %5621)
  %9024 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9020)
  %9025 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9021)
  %9026 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9022)
  %9027 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9023)
  %9028 = shufflevector <4 x i16> %9024, <4 x i16> %9025, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9029 = shufflevector <4 x i16> %9026, <4 x i16> %9027, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9030 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9028, <8 x i16> %5624)
  %9031 = shufflevector <16 x i16> %9029, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9032 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9031, <8 x i16> %5624)
  %9033 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9030)
  %9034 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9032)
  %9035 = shufflevector <8 x i8> %9033, <8 x i8> %9034, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9036 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9035, <16 x i8> %5626)
  %9037 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9036, <16 x i8> %5628)
  %9038 = add nsw i64 %8917, %5857
  %9039 = getelementptr inbounds i8, i8* %19, i64 %9038
  %9040 = bitcast i8* %9039 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9037, <16 x i8>* %9040, i32 1, <16 x i1> %5418), !tbaa !515
  %9041 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.1440.7, <4 x i32> undef
  %9042 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.1496.7, <4 x i32> undef
  %9043 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.1552.7, <4 x i32> undef
  %9044 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.1608.7, <4 x i32> undef
  %9045 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9041, <4 x i32> %5618)
  %9046 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9042, <4 x i32> %5618)
  %9047 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9043, <4 x i32> %5618)
  %9048 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9044, <4 x i32> %5618)
  %9049 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9045, <4 x i32> %5621)
  %9050 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9046, <4 x i32> %5621)
  %9051 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9047, <4 x i32> %5621)
  %9052 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9048, <4 x i32> %5621)
  %9053 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9049)
  %9054 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9050)
  %9055 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9051)
  %9056 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9052)
  %9057 = shufflevector <4 x i16> %9053, <4 x i16> %9054, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9058 = shufflevector <4 x i16> %9055, <4 x i16> %9056, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9059 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9057, <8 x i16> %5624)
  %9060 = shufflevector <16 x i16> %9058, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9061 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9060, <8 x i16> %5624)
  %9062 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9059)
  %9063 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9061)
  %9064 = shufflevector <8 x i8> %9062, <8 x i8> %9063, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9065 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9064, <16 x i8> %5626)
  %9066 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9065, <16 x i8> %5628)
  %9067 = add nsw i64 %8948, %5857
  %9068 = getelementptr inbounds i8, i8* %19, i64 %9067
  %9069 = bitcast i8* %9068 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9066, <16 x i8>* %9069, i32 1, <16 x i1> %5418), !tbaa !515
  %9070 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.1664.7, <4 x i32> undef
  %9071 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.1720.7, <4 x i32> undef
  %9072 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.1776.7, <4 x i32> undef
  %9073 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.1832.7, <4 x i32> undef
  %9074 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9070, <4 x i32> %5618)
  %9075 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9071, <4 x i32> %5618)
  %9076 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9072, <4 x i32> %5618)
  %9077 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9073, <4 x i32> %5618)
  %9078 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9074, <4 x i32> %5621)
  %9079 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9075, <4 x i32> %5621)
  %9080 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9076, <4 x i32> %5621)
  %9081 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9077, <4 x i32> %5621)
  %9082 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9078)
  %9083 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9079)
  %9084 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9080)
  %9085 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9081)
  %9086 = shufflevector <4 x i16> %9082, <4 x i16> %9083, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9087 = shufflevector <4 x i16> %9084, <4 x i16> %9085, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9088 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9086, <8 x i16> %5624)
  %9089 = shufflevector <16 x i16> %9087, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9090 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9089, <8 x i16> %5624)
  %9091 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9088)
  %9092 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9090)
  %9093 = shufflevector <8 x i8> %9091, <8 x i8> %9092, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9094 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9093, <16 x i8> %5626)
  %9095 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9094, <16 x i8> %5628)
  %9096 = add nsw i64 %8979, %5857
  %9097 = getelementptr inbounds i8, i8* %19, i64 %9096
  %9098 = bitcast i8* %9097 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9095, <16 x i8>* %9098, i32 1, <16 x i1> %5418), !tbaa !515
  %9099 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.1888.7, <4 x i32> undef
  %9100 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.1944.7, <4 x i32> undef
  %9101 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.2000.7, <4 x i32> undef
  %9102 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.2056.7, <4 x i32> undef
  %9103 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9099, <4 x i32> %5618)
  %9104 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9100, <4 x i32> %5618)
  %9105 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9101, <4 x i32> %5618)
  %9106 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9102, <4 x i32> %5618)
  %9107 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9103, <4 x i32> %5621)
  %9108 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9104, <4 x i32> %5621)
  %9109 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9105, <4 x i32> %5621)
  %9110 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9106, <4 x i32> %5621)
  %9111 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9107)
  %9112 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9108)
  %9113 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9109)
  %9114 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9110)
  %9115 = shufflevector <4 x i16> %9111, <4 x i16> %9112, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9116 = shufflevector <4 x i16> %9113, <4 x i16> %9114, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9117 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9115, <8 x i16> %5624)
  %9118 = shufflevector <16 x i16> %9116, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9119 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9118, <8 x i16> %5624)
  %9120 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9117)
  %9121 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9119)
  %9122 = shufflevector <8 x i8> %9120, <8 x i8> %9121, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9123 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9122, <16 x i8> %5626)
  %9124 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9123, <16 x i8> %5628)
  %9125 = add nsw i64 %8886, %5858
  %9126 = getelementptr inbounds i8, i8* %19, i64 %9125
  %9127 = bitcast i8* %9126 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9124, <16 x i8>* %9127, i32 1, <16 x i1> %5418), !tbaa !515
  %9128 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.2112.7, <4 x i32> undef
  %9129 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.2168.7, <4 x i32> undef
  %9130 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.2224.7, <4 x i32> undef
  %9131 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.2280.7, <4 x i32> undef
  %9132 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9128, <4 x i32> %5618)
  %9133 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9129, <4 x i32> %5618)
  %9134 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9130, <4 x i32> %5618)
  %9135 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9131, <4 x i32> %5618)
  %9136 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9132, <4 x i32> %5621)
  %9137 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9133, <4 x i32> %5621)
  %9138 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9134, <4 x i32> %5621)
  %9139 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9135, <4 x i32> %5621)
  %9140 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9136)
  %9141 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9137)
  %9142 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9138)
  %9143 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9139)
  %9144 = shufflevector <4 x i16> %9140, <4 x i16> %9141, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9145 = shufflevector <4 x i16> %9142, <4 x i16> %9143, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9146 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9144, <8 x i16> %5624)
  %9147 = shufflevector <16 x i16> %9145, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9148 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9147, <8 x i16> %5624)
  %9149 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9146)
  %9150 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9148)
  %9151 = shufflevector <8 x i8> %9149, <8 x i8> %9150, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9152 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9151, <16 x i8> %5626)
  %9153 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9152, <16 x i8> %5628)
  %9154 = add nsw i64 %8917, %5858
  %9155 = getelementptr inbounds i8, i8* %19, i64 %9154
  %9156 = bitcast i8* %9155 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9153, <16 x i8>* %9156, i32 1, <16 x i1> %5418), !tbaa !515
  %9157 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.2336.7, <4 x i32> undef
  %9158 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.2392.7, <4 x i32> undef
  %9159 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.2448.7, <4 x i32> undef
  %9160 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.2504.7, <4 x i32> undef
  %9161 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9157, <4 x i32> %5618)
  %9162 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9158, <4 x i32> %5618)
  %9163 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9159, <4 x i32> %5618)
  %9164 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9160, <4 x i32> %5618)
  %9165 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9161, <4 x i32> %5621)
  %9166 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9162, <4 x i32> %5621)
  %9167 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9163, <4 x i32> %5621)
  %9168 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9164, <4 x i32> %5621)
  %9169 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9165)
  %9170 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9166)
  %9171 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9167)
  %9172 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9168)
  %9173 = shufflevector <4 x i16> %9169, <4 x i16> %9170, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9174 = shufflevector <4 x i16> %9171, <4 x i16> %9172, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9175 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9173, <8 x i16> %5624)
  %9176 = shufflevector <16 x i16> %9174, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9177 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9176, <8 x i16> %5624)
  %9178 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9175)
  %9179 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9177)
  %9180 = shufflevector <8 x i8> %9178, <8 x i8> %9179, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9181 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9180, <16 x i8> %5626)
  %9182 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9181, <16 x i8> %5628)
  %9183 = add nsw i64 %8948, %5858
  %9184 = getelementptr inbounds i8, i8* %19, i64 %9183
  %9185 = bitcast i8* %9184 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9182, <16 x i8>* %9185, i32 1, <16 x i1> %5418), !tbaa !515
  %9186 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.2560.7, <4 x i32> undef
  %9187 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.2616.7, <4 x i32> undef
  %9188 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.2672.7, <4 x i32> undef
  %9189 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.2728.7, <4 x i32> undef
  %9190 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9186, <4 x i32> %5618)
  %9191 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9187, <4 x i32> %5618)
  %9192 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9188, <4 x i32> %5618)
  %9193 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9189, <4 x i32> %5618)
  %9194 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9190, <4 x i32> %5621)
  %9195 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9191, <4 x i32> %5621)
  %9196 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9192, <4 x i32> %5621)
  %9197 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9193, <4 x i32> %5621)
  %9198 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9194)
  %9199 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9195)
  %9200 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9196)
  %9201 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9197)
  %9202 = shufflevector <4 x i16> %9198, <4 x i16> %9199, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9203 = shufflevector <4 x i16> %9200, <4 x i16> %9201, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9204 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9202, <8 x i16> %5624)
  %9205 = shufflevector <16 x i16> %9203, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9206 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9205, <8 x i16> %5624)
  %9207 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9204)
  %9208 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9206)
  %9209 = shufflevector <8 x i8> %9207, <8 x i8> %9208, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9210 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9209, <16 x i8> %5626)
  %9211 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9210, <16 x i8> %5628)
  %9212 = add nsw i64 %8979, %5858
  %9213 = getelementptr inbounds i8, i8* %19, i64 %9212
  %9214 = bitcast i8* %9213 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9211, <16 x i8>* %9214, i32 1, <16 x i1> %5418), !tbaa !515
  %9215 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.2784.7, <4 x i32> undef
  %9216 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.2840.7, <4 x i32> undef
  %9217 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.2896.7, <4 x i32> undef
  %9218 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.2952.7, <4 x i32> undef
  %9219 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9215, <4 x i32> %5618)
  %9220 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9216, <4 x i32> %5618)
  %9221 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9217, <4 x i32> %5618)
  %9222 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9218, <4 x i32> %5618)
  %9223 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9219, <4 x i32> %5621)
  %9224 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9220, <4 x i32> %5621)
  %9225 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9221, <4 x i32> %5621)
  %9226 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9222, <4 x i32> %5621)
  %9227 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9223)
  %9228 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9224)
  %9229 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9225)
  %9230 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9226)
  %9231 = shufflevector <4 x i16> %9227, <4 x i16> %9228, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9232 = shufflevector <4 x i16> %9229, <4 x i16> %9230, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9233 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9231, <8 x i16> %5624)
  %9234 = shufflevector <16 x i16> %9232, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9235 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9234, <8 x i16> %5624)
  %9236 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9233)
  %9237 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9235)
  %9238 = shufflevector <8 x i8> %9236, <8 x i8> %9237, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9239 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9238, <16 x i8> %5626)
  %9240 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9239, <16 x i8> %5628)
  %9241 = add nsw i64 %8886, %5859
  %9242 = getelementptr inbounds i8, i8* %19, i64 %9241
  %9243 = bitcast i8* %9242 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9240, <16 x i8>* %9243, i32 1, <16 x i1> %5418), !tbaa !515
  %9244 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.3008.7, <4 x i32> undef
  %9245 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.3064.7, <4 x i32> undef
  %9246 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.3120.7, <4 x i32> undef
  %9247 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.3176.7, <4 x i32> undef
  %9248 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9244, <4 x i32> %5618)
  %9249 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9245, <4 x i32> %5618)
  %9250 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9246, <4 x i32> %5618)
  %9251 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9247, <4 x i32> %5618)
  %9252 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9248, <4 x i32> %5621)
  %9253 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9249, <4 x i32> %5621)
  %9254 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9250, <4 x i32> %5621)
  %9255 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9251, <4 x i32> %5621)
  %9256 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9252)
  %9257 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9253)
  %9258 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9254)
  %9259 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9255)
  %9260 = shufflevector <4 x i16> %9256, <4 x i16> %9257, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9261 = shufflevector <4 x i16> %9258, <4 x i16> %9259, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9262 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9260, <8 x i16> %5624)
  %9263 = shufflevector <16 x i16> %9261, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9264 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9263, <8 x i16> %5624)
  %9265 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9262)
  %9266 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9264)
  %9267 = shufflevector <8 x i8> %9265, <8 x i8> %9266, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9268 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9267, <16 x i8> %5626)
  %9269 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9268, <16 x i8> %5628)
  %9270 = add nsw i64 %8917, %5859
  %9271 = getelementptr inbounds i8, i8* %19, i64 %9270
  %9272 = bitcast i8* %9271 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9269, <16 x i8>* %9272, i32 1, <16 x i1> %5418), !tbaa !515
  %9273 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.3232.7, <4 x i32> undef
  %9274 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.3288.7, <4 x i32> undef
  %9275 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.3344.7, <4 x i32> undef
  %9276 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.3400.7, <4 x i32> undef
  %9277 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9273, <4 x i32> %5618)
  %9278 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9274, <4 x i32> %5618)
  %9279 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9275, <4 x i32> %5618)
  %9280 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9276, <4 x i32> %5618)
  %9281 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9277, <4 x i32> %5621)
  %9282 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9278, <4 x i32> %5621)
  %9283 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9279, <4 x i32> %5621)
  %9284 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9280, <4 x i32> %5621)
  %9285 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9281)
  %9286 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9282)
  %9287 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9283)
  %9288 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9284)
  %9289 = shufflevector <4 x i16> %9285, <4 x i16> %9286, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9290 = shufflevector <4 x i16> %9287, <4 x i16> %9288, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9291 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9289, <8 x i16> %5624)
  %9292 = shufflevector <16 x i16> %9290, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9293 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9292, <8 x i16> %5624)
  %9294 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9291)
  %9295 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9293)
  %9296 = shufflevector <8 x i8> %9294, <8 x i8> %9295, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9297 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9296, <16 x i8> %5626)
  %9298 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9297, <16 x i8> %5628)
  %9299 = add nsw i64 %8948, %5859
  %9300 = getelementptr inbounds i8, i8* %19, i64 %9299
  %9301 = bitcast i8* %9300 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9298, <16 x i8>* %9301, i32 1, <16 x i1> %5418), !tbaa !515
  %9302 = select <4 x i1> %5420, <4 x i32> %convolved944.sroa.3456.7, <4 x i32> undef
  %9303 = select <4 x i1> %5423, <4 x i32> %convolved944.sroa.3512.7, <4 x i32> undef
  %9304 = select <4 x i1> %5426, <4 x i32> %convolved944.sroa.3568.7, <4 x i32> undef
  %9305 = select <4 x i1> %5429, <4 x i32> %convolved944.sroa.3624.7, <4 x i32> undef
  %9306 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9302, <4 x i32> %5618)
  %9307 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9303, <4 x i32> %5618)
  %9308 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9304, <4 x i32> %5618)
  %9309 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %9305, <4 x i32> %5618)
  %9310 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9306, <4 x i32> %5621)
  %9311 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9307, <4 x i32> %5621)
  %9312 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9308, <4 x i32> %5621)
  %9313 = call <4 x i32> @llvm.aarch64.neon.srshl.v4i32(<4 x i32> %9309, <4 x i32> %5621)
  %9314 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9310)
  %9315 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9311)
  %9316 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9312)
  %9317 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %9313)
  %9318 = shufflevector <4 x i16> %9314, <4 x i16> %9315, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9319 = shufflevector <4 x i16> %9316, <4 x i16> %9317, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %9320 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9318, <8 x i16> %5624)
  %9321 = shufflevector <16 x i16> %9319, <16 x i16> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9322 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %9321, <8 x i16> %5624)
  %9323 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9320)
  %9324 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %9322)
  %9325 = shufflevector <8 x i8> %9323, <8 x i8> %9324, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %9326 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9325, <16 x i8> %5626)
  %9327 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %9326, <16 x i8> %5628)
  %9328 = add nsw i64 %8979, %5859
  %9329 = getelementptr inbounds i8, i8* %19, i64 %9328
  %9330 = bitcast i8* %9329 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %9327, <16 x i8>* %9330, i32 1, <16 x i1> %5418), !tbaa !515
  %indvars.iv.next6317 = add nuw nsw i64 %indvars.iv6316, 1
  %.not2877 = icmp eq i64 %indvars.iv.next6317, %5640
  br i1 %.not2877, label %"end for output.s0.x.xo190", label %"for output.s0.x.xo189"

after_bb236.loopexit:                             ; preds = %"end for output.s0.b.rebased308"
  %9331 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8*
  br label %after_bb236

after_bb236.loopexit5212:                         ; preds = %"end for output.s0.b.rebased469"
  %9332 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8*
  br label %after_bb236

after_bb236:                                      ; preds = %after_bb236.loopexit5212, %after_bb236.loopexit, %"consume sum_filter648", %next_bb408
  %.5 = phi i8* [ %.05065, %"consume sum_filter648" ], [ %.05065, %next_bb408 ], [ %.7, %after_bb236.loopexit ], [ %.9, %after_bb236.loopexit5212 ]
  %.1 = phi i8* [ %16475, %"consume sum_filter648" ], [ %.0, %next_bb408 ], [ %9331, %after_bb236.loopexit ], [ %9332, %after_bb236.loopexit5212 ]
  %tobool.not.i.not = icmp eq i8* %.5, null
  br i1 %tobool.not.i.not, label %call_destructor.exit, label %if.then.i3157

if.then.i3157.loopexit:                           ; preds = %"end for output.s0.y.yo765"
  %9333 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8*
  br label %if.then.i3157

if.then.i3157:                                    ; preds = %if.then.i3157.loopexit, %after_bb236
  %.15099 = phi i8* [ %.1, %after_bb236 ], [ %16475, %if.then.i3157.loopexit ]
  %.55096 = phi i8* [ %.5, %after_bb236 ], [ %9333, %if.then.i3157.loopexit ]
  %ptr1.i = bitcast i8* %.55096 to i8**
  %9334 = load i8*, i8** %ptr1.i, align 8, !tbaa !387
  %tobool.not.i3271 = icmp eq i8* %9334, null
  br i1 %tobool.not.i3271, label %pseudostack_free.exit, label %land.lhs.true.i3274

land.lhs.true.i3274:                              ; preds = %if.then.i3157
  %cumulative_size.i3272 = getelementptr inbounds i8, i8* %.55096, i64 16
  %9335 = bitcast i8* %cumulative_size.i3272 to i64*
  %9336 = load i64, i64* %9335, align 8, !tbaa !389
  %cmp.i3273 = icmp ugt i64 %9336, 16384
  br i1 %cmp.i3273, label %if.then.i3275, label %pseudostack_free.exit

if.then.i3275:                                    ; preds = %land.lhs.true.i3274
  call void @halide_free(i8* null, i8* nonnull %9334) #15
  br label %pseudostack_free.exit

pseudostack_free.exit:                            ; preds = %if.then.i3157, %land.lhs.true.i3274, %if.then.i3275
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 dereferenceable(24) %.55096, i8 0, i64 24, i1 false)
  br label %call_destructor.exit

call_destructor.exit:                             ; preds = %after_bb236, %pseudostack_free.exit
  %.15098 = phi i8* [ %.1, %after_bb236 ], [ %.15099, %pseudostack_free.exit ]
  %tobool.not.i3159.not = icmp eq i8* %.15098, null
  br i1 %tobool.not.i3159.not, label %call_destructor.exit3162, label %if.then.i3160

if.then.i3160:                                    ; preds = %call_destructor.exit
  %ptr1.i3278 = bitcast i8* %.15098 to i8**
  %9337 = load i8*, i8** %ptr1.i3278, align 8, !tbaa !387
  %tobool.not.i3279 = icmp eq i8* %9337, null
  br i1 %tobool.not.i3279, label %pseudostack_free.exit3287, label %land.lhs.true.i3282

land.lhs.true.i3282:                              ; preds = %if.then.i3160
  %cumulative_size.i3280 = getelementptr inbounds i8, i8* %.15098, i64 16
  %9338 = bitcast i8* %cumulative_size.i3280 to i64*
  %9339 = load i64, i64* %9338, align 8, !tbaa !389
  %cmp.i3281 = icmp ugt i64 %9339, 16384
  br i1 %cmp.i3281, label %if.then.i3283, label %pseudostack_free.exit3287

if.then.i3283:                                    ; preds = %land.lhs.true.i3282
  call void @halide_free(i8* null, i8* nonnull %9337) #15
  br label %pseudostack_free.exit3287

pseudostack_free.exit3287:                        ; preds = %if.then.i3160, %land.lhs.true.i3282, %if.then.i3283
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 dereferenceable(24) %.15098, i8 0, i64 24, i1 false)
  br label %call_destructor.exit3162

call_destructor.exit3162:                         ; preds = %call_destructor.exit, %pseudostack_free.exit3287
  ret i32 0

then_bb237:                                       ; preds = %after_bb
  %t2201239 = icmp slt i32 %a614, 0
  %9340 = add nsw i32 %46, -1
  %t2202240 = mul nsw i32 %9340, %a614
  %t2203241 = icmp slt i32 %stride_x, 0
  %9341 = icmp slt i32 %62, 2
  %9342 = select i1 %9341, i32 %62, i32 2
  %t2205243 = add nsw i32 %9342, -2
  %t2206244 = add nsw i32 %62, -1
  %t2207245 = select i1 %t2201239, i32 %t2202240, i32 0
  %9343 = select i1 %t2203241, i32 %t2206244, i32 %t2205243
  %9344 = add nsw i32 %9343, %61
  %t2209247 = icmp slt i32 %a613, 0
  %9345 = add nsw i32 %48, -1
  %t2210248 = mul nsw i32 %9345, %a613
  %t2211249 = icmp slt i32 %stride_y, 0
  %9346 = icmp slt i32 %65, 2
  %9347 = select i1 %9346, i32 %65, i32 2
  %t2213251 = add nsw i32 %9347, -2
  %t2214252 = add nsw i32 %65, -1
  %t2215253 = select i1 %t2209247, i32 %t2210248, i32 0
  %9348 = select i1 %t2211249, i32 %t2214252, i32 %t2213251
  %9349 = add nsw i32 %9348, %64
  %9350 = icmp eq i32 %46, 3
  %9351 = icmp eq i32 %48, 3
  %t2217255 = and i1 %9350, %9351
  %a614.op2587 = shl i32 %a614, 1
  %t2218256 = select i1 %t2201239, i32 %a614.op2587, i32 0
  %a613.op2588 = shl i32 %a613, 1
  %t2219257 = select i1 %t2209247, i32 %a613.op2588, i32 0
  %t2194258 = icmp eq i32 %depth_multiplier, 1
  %9352 = add nuw nsw i32 %65, 1
  %t2196259 = ashr i32 %9352, 1
  %9353 = add nuw nsw i32 %62, 1
  %t2197260 = ashr i32 %9353, 1
  %9354 = add nuw nsw i32 %45, 15
  %t2172261 = ashr i32 %9354, 4
  %9355 = icmp sgt i32 %a613, 0
  %9356 = select i1 %9355, i32 %a613, i32 0
  %t2185262 = shl nuw nsw i32 %9356, 1
  %9357 = icmp sgt i32 %a614, 0
  %9358 = select i1 %9357, i32 %a614, i32 0
  %t2192263 = shl nuw nsw i32 %9358, 1
  %9359 = select i1 %t2217255, i32 %t2219257, i32 %t2215253
  %9360 = mul nsw i32 %9349, %stride_y
  %9361 = select i1 %t2217255, i32 %t2218256, i32 %t2207245
  %9362 = mul nsw i32 %9344, %stride_x
  %9363 = select i1 %t2209247, i32 0, i32 %t2210248
  %9364 = select i1 %t2211249, i32 %t2213251, i32 %t2214252
  %9365 = add nsw i32 %9364, %64
  %9366 = mul nsw i32 %9365, %stride_y
  %t2178 = add nsw i32 %9362, %t2207245
  %9367 = select i1 %t2201239, i32 0, i32 %t2202240
  %9368 = select i1 %t2203241, i32 %t2205243, i32 %t2206244
  %9369 = add nsw i32 %9368, %61
  %9370 = mul nsw i32 %9369, %stride_x
  %9371 = mul nsw i32 %55, %54
  %9372 = mul nsw i32 %58, %56
  %9373 = mul nsw i32 %53, %52
  %9374 = add nsw i32 %9372, %9373
  %b78 = add nsw i32 %45, -16
  %9375 = icmp sgt i32 %46, 0
  %9376 = select i1 %9375, i32 %46, i32 0
  %t4597 = zext i32 %9376 to i64
  %9377 = icmp sgt i32 %48, 0
  %9378 = select i1 %9377, i32 %48, i32 0
  %t4598 = zext i32 %9378 to i64
  %9379 = shl nuw nsw i64 %t4597, 5
  %9380 = mul i64 %9379, %t4598
  %9381 = or i64 %9380, 6
  %9382 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8***
  %9383 = sext i32 %47 to i64
  %9384 = zext i8 %filter_zero to i16
  %9385 = insertelement <8 x i16> undef, i16 %9384, i32 0
  %9386 = shufflevector <8 x i16> %9385, <8 x i16> undef, <8 x i32> zeroinitializer
  %9387 = bitcast [64 x i32]* %sum_filter945 to <4 x i32>*
  %9388 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 4
  %9389 = bitcast i32* %9388 to <4 x i32>*
  %9390 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 8
  %9391 = bitcast i32* %9390 to <4 x i32>*
  %9392 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 12
  %9393 = bitcast i32* %9392 to <4 x i32>*
  %9394 = bitcast i8* %42 to i32*
  %9395 = zext i8 %input_zero to i32
  %9396 = insertelement <4 x i32> undef, i32 %9395, i32 0
  %9397 = shufflevector <4 x i32> %9396, <4 x i32> undef, <4 x i32> zeroinitializer
  %9398 = bitcast [16 x i32]* %offset_c636946 to <4 x i32>*
  %9399 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 4
  %9400 = bitcast i32* %9399 to <4 x i32>*
  %9401 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 8
  %9402 = bitcast i32* %9401 to <4 x i32>*
  %9403 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 12
  %9404 = bitcast i32* %9403 to <4 x i32>*
  %b88 = add nsw i32 %9362, %9361
  %a87 = add nsw i32 %9360, %t2215253
  %b89 = add nsw i32 %9360, %9359
  %9405 = select i1 %t2217255, i32 %t2192263, i32 %9367
  %a88 = add nsw i32 %9370, %9405
  %b90 = add nsw i32 %9370, %9367
  %9406 = icmp sgt i32 %9405, %9367
  %9407 = select i1 %9406, i32 %a88, i32 %b90
  %9408 = icmp slt i32 %9361, %t2207245
  %9409 = select i1 %9408, i32 %b88, i32 %t2178
  %9410 = select i1 %t2217255, i32 %t2185262, i32 %9363
  %a90 = add nsw i32 %9366, %9410
  %b92 = add nsw i32 %9366, %9363
  %9411 = icmp sgt i32 %9410, %9363
  %9412 = select i1 %9411, i32 %a90, i32 %b92
  %9413 = icmp slt i32 %9359, %t2215253
  %9414 = select i1 %9413, i32 %b89, i32 %a87
  %t2228304 = sub nsw i32 %b90, %t2178
  %t2227305 = sub nsw i32 %b92, %a87
  %9415 = icmp sgt i32 %57, 0
  %9416 = sub nsw i32 %9412, %9414
  %a100 = add nsw i32 %9416, 1
  %9417 = sub nsw i32 %9407, %9409
  %a99 = add nsw i32 %9417, 1
  %.inv2591 = icmp slt i32 %9417, 0
  %9418 = select i1 %.inv2591, i32 0, i32 %a99
  %t4599 = zext i32 %9418 to i64
  %.inv2592 = icmp slt i32 %9416, 0
  %9419 = select i1 %.inv2592, i32 0, i32 %a100
  %t4600 = zext i32 %9419 to i64
  %t4601 = shl nuw nsw i64 %t4599, 4
  %9420 = mul i64 %t4601, %t4600
  %9421 = or i64 %9420, 3
  %9422 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8***
  %9423 = icmp sgt i32 %t2227305, -1
  %9424 = sub i32 %a87, %9414
  %9425 = sub i32 %t2178, %9409
  %9426 = add i32 %9371, %9374
  %9427 = icmp sgt i32 %t2228304, -1
  %9428 = icmp eq i32 %depth_multiplier, 0
  %t4604 = select i1 %9428, <16 x i32> <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>, <16 x i32> zeroinitializer
  %depth_multiplier.lobit2793 = ashr i32 %depth_multiplier, 31
  %9429 = insertelement <16 x i32> undef, i32 %depth_multiplier, i32 0
  %9430 = shufflevector <16 x i32> %9429, <16 x i32> undef, <16 x i32> zeroinitializer
  %9431 = sub nsw <16 x i32> %9430, %t4604
  %9432 = xor i32 %depth_multiplier.lobit2793, -1
  %9433 = sub nsw i32 %9432, %depth_multiplier.lobit2793
  %9434 = insertelement <16 x i32> undef, i32 %9433, i32 0
  %9435 = shufflevector <16 x i32> %9434, <16 x i32> undef, <16 x i32> zeroinitializer
  %9436 = xor <16 x i32> %t4604, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  %9437 = sext i32 %t2178 to i64
  %9438 = sext i32 %53 to i64
  %.neg5202 = mul i32 %66, %64
  %.neg5203 = mul i32 %63, %61
  %.neg5204 = mul i32 %67, %56
  %9439 = icmp sgt i32 %65, 0
  %b120 = add nsw i32 %65, -2
  %9440 = sub nsw i32 %a613.op2588, %9414
  %9441 = sub nsw i32 %a613, %9414
  %9442 = sub nsw i32 %a614, %9409
  %9443 = sub nsw i32 %a614.op2587, %9409
  %reass.add5207 = add i32 %.neg5202, %.neg5203
  %reass.add5208 = add i32 %reass.add5207, %.neg5204
  %9444 = icmp sgt i32 %62, 0
  %b249 = add nsw i32 %62, -2
  %9445 = add nsw i32 %t2228304, 1
  %t3007 = shl nuw i32 1, %output_shift
  %9446 = insertelement <4 x i32> undef, i32 %output_multiplier, i32 0
  %9447 = shufflevector <4 x i32> %9446, <4 x i32> undef, <4 x i32> zeroinitializer
  %9448 = ashr i32 %t3007, 1
  %9449 = insertelement <4 x i32> undef, i32 %9448, i32 0
  %9450 = shufflevector <4 x i32> %9449, <4 x i32> undef, <4 x i32> zeroinitializer
  %9451 = sub i32 0, %output_shift
  %9452 = insertelement <4 x i32> undef, i32 %9451, i32 0
  %9453 = shufflevector <4 x i32> %9452, <4 x i32> undef, <4 x i32> zeroinitializer
  %9454 = zext i8 %output_zero to i16
  %9455 = insertelement <8 x i16> undef, i16 %9454, i32 0
  %9456 = shufflevector <8 x i16> %9455, <8 x i16> undef, <8 x i32> zeroinitializer
  %9457 = insertelement <16 x i8> undef, i8 %output_max, i32 0
  %9458 = shufflevector <16 x i8> %9457, <16 x i8> undef, <16 x i32> zeroinitializer
  %9459 = insertelement <16 x i8> undef, i8 %output_min, i32 0
  %9460 = shufflevector <16 x i8> %9459, <16 x i8> undef, <16 x i32> zeroinitializer
  %9461 = sext i32 %61 to i64
  %9462 = sext i32 %63 to i64
  %9463 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8*
  %9464 = zext i32 %46 to i64
  %9465 = sext i32 %46 to i64
  %9466 = sext i32 %49 to i64
  %9467 = zext i32 %48 to i64
  %9468 = zext i32 %t2228304 to i64
  %9469 = sext i32 %9424 to i64
  %9470 = sext i32 %a99 to i64
  %9471 = sext i32 %9425 to i64
  %9472 = sext i32 %a87 to i64
  %9473 = sext i32 %55 to i64
  %9474 = zext i32 %t2227305 to i64
  %9475 = zext i32 %a613 to i64
  %9476 = sext i32 %56 to i64
  %9477 = sext i32 %58 to i64
  %9478 = bitcast [64 x i32]* %sum_filter945 to i8*
  %9479 = bitcast [64 x i32]* %sum_filter945 to i8*
  %9480 = bitcast [64 x i32]* %sum_filter945 to i8*
  %zext6502 = zext i32 %57 to i64
  %9481 = or i32 %t2227305, %t2228304
  %9482 = icmp slt i32 %9481, 0
  br label %"for output.s0.c.co266"

next_bb238:                                       ; preds = %after_bb
  br i1 %77, label %then_bb407, label %next_bb408

"for output.s0.c.co266":                          ; preds = %then_bb237, %"end for output.s0.b.rebased308"
  %.6 = phi i8* [ %.05065, %then_bb237 ], [ %.7, %"end for output.s0.b.rebased308" ]
  %output.s0.c.co268 = phi i32 [ 0, %then_bb237 ], [ %9649, %"end for output.s0.b.rebased308" ]
  %a76 = shl nsw i32 %output.s0.c.co268, 4
  %9483 = icmp slt i32 %a76, %b78
  %output.s0.c.c.base269 = select i1 %9483, i32 %a76, i32 %b78
  %9484 = load i64, i64* %.fca.1.gep, align 8, !tbaa !385
  %cmp.i3164 = icmp ult i64 %9484, %9381
  %9485 = load i8*, i8** %.fca.0.gep, align 8, !tbaa !387
  br i1 %cmp.i3164, label %if.then.i3167, label %pseudostack_alloc.exit3180, !prof !388

if.then.i3167:                                    ; preds = %"for output.s0.c.co266"
  %tobool1.not.i3166 = icmp ne i8* %9485, null
  %9486 = load i64, i64* %.fca.2.gep, align 8
  %cmp2.i3169 = icmp ugt i64 %9486, 16384
  %or.cond5130 = and i1 %tobool1.not.i3166, %cmp2.i3169
  br i1 %or.cond5130, label %if.then3.i3171, label %if.end.i3175

if.then3.i3171:                                   ; preds = %if.then.i3167
  call void @halide_free(i8* null, i8* nonnull %9485) #15
  %.pre6511 = load i64, i64* %.fca.2.gep, align 8, !tbaa !389
  br label %if.end.i3175

if.end.i3175:                                     ; preds = %if.then3.i3171, %if.then.i3167
  %9487 = phi i64 [ %.pre6511, %if.then3.i3171 ], [ %9486, %if.then.i3167 ]
  %add.i3173 = add i64 %9487, %9381
  store i64 %add.i3173, i64* %.fca.2.gep, align 8, !tbaa !389
  %cmp7.i3174 = icmp ugt i64 %add.i3173, 16384
  br i1 %cmp7.i3174, label %if.then8.i3177, label %if.end11.i3179

if.then8.i3177:                                   ; preds = %if.end.i3175
  %call.i3176 = call i8* @halide_malloc(i8* null, i64 %9381) #15
  br label %if.end11.i3179

if.end11.i3179:                                   ; preds = %if.then8.i3177, %if.end.i3175
  %storemerge.i3178 = phi i8* [ %call.i3176, %if.then8.i3177 ], [ null, %if.end.i3175 ]
  store i8* %storemerge.i3178, i8** %.fca.0.gep, align 8, !tbaa !387
  store i64 %9381, i64* %.fca.1.gep, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3180

pseudostack_alloc.exit3180:                       ; preds = %"for output.s0.c.co266", %if.end11.i3179
  %9488 = phi i8* [ %storemerge.i3178, %if.end11.i3179 ], [ %9485, %"for output.s0.c.co266" ]
  %9489 = bitcast i8* %9488 to i16*
  %.not2589 = icmp eq i8* %9488, null
  br i1 %.not2589, label %then_bb271, label %"produce filter_zeroed273", !prof !390

then_bb271:                                       ; preds = %pseudostack_alloc.exit3180
  %9490 = alloca i8*, i64 %9381, align 16
  %9491 = bitcast i8** %9490 to i16*
  store i8** %9490, i8*** %9382, align 8
  br label %"produce filter_zeroed273"

"produce filter_zeroed273":                       ; preds = %pseudostack_alloc.exit3180, %then_bb271
  %filter_zeroed272 = phi i16* [ %9491, %then_bb271 ], [ %9489, %pseudostack_alloc.exit3180 ]
  br i1 %9377, label %"for filter_zeroed.s0.y274.preheader", label %"consume sum_filter293.critedge", !prof !391

"for filter_zeroed.s0.y274.preheader":            ; preds = %"produce filter_zeroed273"
  br i1 %9375, label %"for filter_zeroed.s0.y274.us.preheader", label %"for sum_filter.s1.r19$y286.preheader.thread", !prof !391

"for sum_filter.s1.r19$y286.preheader.thread":    ; preds = %"for filter_zeroed.s0.y274.preheader"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %9479, i8 0, i64 64, i1 false)
  br label %"consume sum_filter293"

"for filter_zeroed.s0.y274.us.preheader":         ; preds = %"for filter_zeroed.s0.y274.preheader"
  %9492 = sext i32 %output.s0.c.c.base269 to i64
  br label %"for filter_zeroed.s0.y274.us"

"for filter_zeroed.s0.y274.us":                   ; preds = %"for filter_zeroed.s0.y274.us.preheader", %"end for filter_zeroed.s0.x280.loopexit.us"
  %indvars.iv6472 = phi i64 [ 0, %"for filter_zeroed.s0.y274.us.preheader" ], [ %indvars.iv.next6473, %"end for filter_zeroed.s0.x280.loopexit.us" ]
  %9493 = mul nsw i64 %indvars.iv6472, %9465
  %9494 = mul nsw i64 %indvars.iv6472, %9466
  %9495 = add nsw i64 %9494, %9492
  br label %"for filter_zeroed.s0.x279.us"

"for filter_zeroed.s0.x279.us":                   ; preds = %"for filter_zeroed.s0.y274.us", %"for filter_zeroed.s0.x279.us"
  %indvars.iv6470 = phi i64 [ 0, %"for filter_zeroed.s0.y274.us" ], [ %indvars.iv.next6471, %"for filter_zeroed.s0.x279.us" ]
  %9496 = mul nsw i64 %indvars.iv6470, %9383
  %9497 = add nsw i64 %9496, %9495
  %9498 = getelementptr inbounds i8, i8* %43, i64 %9497
  %9499 = bitcast i8* %9498 to <8 x i8>*
  %9500 = load <8 x i8>, <8 x i8>* %9499, align 1, !tbaa !392
  %9501 = zext <8 x i8> %9500 to <8 x i16>
  %9502 = sub nsw <8 x i16> %9501, %9386
  %9503 = add nsw i64 %indvars.iv6470, %9493
  %9504 = shl nsw i64 %9503, 4
  %9505 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 %9504
  %9506 = bitcast i16* %9505 to <8 x i16>*
  store <8 x i16> %9502, <8 x i16>* %9506, align 16, !tbaa !395
  %9507 = getelementptr inbounds i8, i8* %9498, i64 8
  %9508 = bitcast i8* %9507 to <8 x i8>*
  %9509 = load <8 x i8>, <8 x i8>* %9508, align 1, !tbaa !392
  %9510 = zext <8 x i8> %9509 to <8 x i16>
  %9511 = sub nsw <8 x i16> %9510, %9386
  %9512 = getelementptr inbounds i16, i16* %9505, i64 8
  %9513 = bitcast i16* %9512 to <8 x i16>*
  store <8 x i16> %9511, <8 x i16>* %9513, align 16, !tbaa !395
  %indvars.iv.next6471 = add nuw nsw i64 %indvars.iv6470, 1
  %.not2800.us = icmp eq i64 %indvars.iv.next6471, %9464
  br i1 %.not2800.us, label %"end for filter_zeroed.s0.x280.loopexit.us", label %"for filter_zeroed.s0.x279.us"

"end for filter_zeroed.s0.x280.loopexit.us":      ; preds = %"for filter_zeroed.s0.x279.us"
  %indvars.iv.next6473 = add nuw nsw i64 %indvars.iv6472, 1
  %.not2799.us = icmp eq i64 %indvars.iv.next6473, %9467
  br i1 %.not2799.us, label %"for sum_filter.s1.r19$y286.preheader", label %"for filter_zeroed.s0.y274.us"

"for sum_filter.s1.r19$y286.preheader":           ; preds = %"end for filter_zeroed.s0.x280.loopexit.us"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %9480, i8 0, i64 64, i1 false)
  br i1 %9375, label %"for sum_filter.s1.r19$y286.us", label %"consume sum_filter293", !prof !391

"for sum_filter.s1.r19$y286.us":                  ; preds = %"for sum_filter.s1.r19$y286.preheader", %"end for sum_filter.s1.r19$x291.loopexit.us"
  %indvars.iv6479 = phi i64 [ %indvars.iv.next6480, %"end for sum_filter.s1.r19$x291.loopexit.us" ], [ 0, %"for sum_filter.s1.r19$y286.preheader" ]
  %.lcssa5892.us5900 = phi <4 x i32> [ %9540, %"end for sum_filter.s1.r19$x291.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader" ]
  %.lcssa5890.us5898 = phi <4 x i32> [ %9535, %"end for sum_filter.s1.r19$x291.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader" ]
  %.lcssa5888.us5896 = phi <4 x i32> [ %9530, %"end for sum_filter.s1.r19$x291.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader" ]
  %.lcssa5886.us5894 = phi <4 x i32> [ %9525, %"end for sum_filter.s1.r19$x291.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader" ]
  %9514 = mul nsw i64 %indvars.iv6479, %9465
  br label %"for sum_filter.s1.r19$x290.us"

"for sum_filter.s1.r19$x290.us":                  ; preds = %"for sum_filter.s1.r19$y286.us", %"for sum_filter.s1.r19$x290.us"
  %indvars.iv6476 = phi i64 [ 0, %"for sum_filter.s1.r19$y286.us" ], [ %indvars.iv.next6477, %"for sum_filter.s1.r19$x290.us" ]
  %9515 = phi <4 x i32> [ %.lcssa5892.us5900, %"for sum_filter.s1.r19$y286.us" ], [ %9540, %"for sum_filter.s1.r19$x290.us" ]
  %9516 = phi <4 x i32> [ %.lcssa5890.us5898, %"for sum_filter.s1.r19$y286.us" ], [ %9535, %"for sum_filter.s1.r19$x290.us" ]
  %9517 = phi <4 x i32> [ %.lcssa5888.us5896, %"for sum_filter.s1.r19$y286.us" ], [ %9530, %"for sum_filter.s1.r19$x290.us" ]
  %9518 = phi <4 x i32> [ %.lcssa5886.us5894, %"for sum_filter.s1.r19$y286.us" ], [ %9525, %"for sum_filter.s1.r19$x290.us" ]
  %9519 = add nsw i64 %indvars.iv6476, %9514
  %9520 = shl nsw i64 %9519, 4
  %9521 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 %9520
  %9522 = bitcast i16* %9521 to <4 x i16>*
  %9523 = load <4 x i16>, <4 x i16>* %9522, align 16, !tbaa !395
  %9524 = sext <4 x i16> %9523 to <4 x i32>
  %9525 = add <4 x i32> %9518, %9524
  %9526 = getelementptr inbounds i16, i16* %9521, i64 4
  %9527 = bitcast i16* %9526 to <4 x i16>*
  %9528 = load <4 x i16>, <4 x i16>* %9527, align 8, !tbaa !395
  %9529 = sext <4 x i16> %9528 to <4 x i32>
  %9530 = add <4 x i32> %9517, %9529
  %9531 = getelementptr inbounds i16, i16* %9521, i64 8
  %9532 = bitcast i16* %9531 to <4 x i16>*
  %9533 = load <4 x i16>, <4 x i16>* %9532, align 16, !tbaa !395
  %9534 = sext <4 x i16> %9533 to <4 x i32>
  %9535 = add <4 x i32> %9516, %9534
  %9536 = getelementptr inbounds i16, i16* %9521, i64 12
  %9537 = bitcast i16* %9536 to <4 x i16>*
  %9538 = load <4 x i16>, <4 x i16>* %9537, align 8, !tbaa !395
  %9539 = sext <4 x i16> %9538 to <4 x i32>
  %9540 = add <4 x i32> %9515, %9539
  %indvars.iv.next6477 = add nuw nsw i64 %indvars.iv6476, 1
  %.not2798.us = icmp eq i64 %indvars.iv.next6477, %9464
  br i1 %.not2798.us, label %"end for sum_filter.s1.r19$x291.loopexit.us", label %"for sum_filter.s1.r19$x290.us"

"end for sum_filter.s1.r19$x291.loopexit.us":     ; preds = %"for sum_filter.s1.r19$x290.us"
  %indvars.iv.next6480 = add nuw nsw i64 %indvars.iv6479, 1
  %.not2797.us = icmp eq i64 %indvars.iv.next6480, %9467
  br i1 %.not2797.us, label %"consume sum_filter293.loopexit.split.us", label %"for sum_filter.s1.r19$y286.us"

"consume sum_filter293.loopexit.split.us":        ; preds = %"end for sum_filter.s1.r19$x291.loopexit.us"
  store <4 x i32> %9525, <4 x i32>* %9387, align 16, !tbaa !441
  store <4 x i32> %9530, <4 x i32>* %9389, align 16, !tbaa !452
  store <4 x i32> %9535, <4 x i32>* %9391, align 16, !tbaa !454
  store <4 x i32> %9540, <4 x i32>* %9393, align 16, !tbaa !457
  br label %"consume sum_filter293"

"consume sum_filter293.critedge":                 ; preds = %"produce filter_zeroed273"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %9478, i8 0, i64 64, i1 false)
  br label %"consume sum_filter293"

"consume sum_filter293":                          ; preds = %"for sum_filter.s1.r19$y286.preheader", %"for sum_filter.s1.r19$y286.preheader.thread", %"consume sum_filter293.loopexit.split.us", %"consume sum_filter293.critedge"
  %9541 = phi <4 x i32> [ %9540, %"consume sum_filter293.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter293.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader" ]
  %9542 = phi <4 x i32> [ %9535, %"consume sum_filter293.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter293.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader" ]
  %9543 = phi <4 x i32> [ %9530, %"consume sum_filter293.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter293.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader" ]
  %9544 = phi <4 x i32> [ %9525, %"consume sum_filter293.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter293.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y286.preheader" ]
  %9545 = sext i32 %output.s0.c.c.base269 to i64
  %9546 = getelementptr inbounds i32, i32* %9394, i64 %9545
  %9547 = bitcast i32* %9546 to <4 x i32>*
  %9548 = load <4 x i32>, <4 x i32>* %9547, align 4, !tbaa !415
  %9549 = mul <4 x i32> %9544, %9397
  %9550 = sub <4 x i32> %9548, %9549
  %9551 = getelementptr inbounds i32, i32* %9546, i64 4
  %9552 = bitcast i32* %9551 to <4 x i32>*
  %9553 = load <4 x i32>, <4 x i32>* %9552, align 4, !tbaa !415
  %9554 = mul <4 x i32> %9543, %9397
  %9555 = sub <4 x i32> %9553, %9554
  %9556 = getelementptr inbounds i32, i32* %9546, i64 8
  %9557 = bitcast i32* %9556 to <4 x i32>*
  %9558 = load <4 x i32>, <4 x i32>* %9557, align 4, !tbaa !415
  %9559 = mul <4 x i32> %9542, %9397
  %9560 = sub <4 x i32> %9558, %9559
  %9561 = getelementptr inbounds i32, i32* %9546, i64 12
  %9562 = bitcast i32* %9561 to <4 x i32>*
  %9563 = load <4 x i32>, <4 x i32>* %9562, align 4, !tbaa !415
  %9564 = mul <4 x i32> %9541, %9397
  %9565 = sub <4 x i32> %9563, %9564
  store <4 x i32> %9550, <4 x i32>* %9398, align 16, !tbaa !397
  store <4 x i32> %9555, <4 x i32>* %9400, align 16, !tbaa !408
  store <4 x i32> %9560, <4 x i32>* %9402, align 16, !tbaa !410
  store <4 x i32> %9565, <4 x i32>* %9404, align 16, !tbaa !413
  br i1 %9415, label %"for output.s0.b.rebased307.preheader", label %"end for output.s0.b.rebased308", !prof !391

"for output.s0.b.rebased307.preheader":           ; preds = %"consume sum_filter293"
  %9566 = insertelement <16 x i32> undef, i32 %output.s0.c.c.base269, i32 0
  %9567 = shufflevector <16 x i32> %9566, <16 x i32> undef, <16 x i32> zeroinitializer
  %9568 = add nsw <16 x i32> %9567, <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %.lobit2792 = ashr <16 x i32> %9568, <i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31>
  %9569 = sub nsw <16 x i32> %9568, %.lobit2792
  %9570 = and <16 x i32> %.lobit2792, %9435
  %9571 = sub i32 %output.s0.c.c.base269, %9426
  %9572 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 96
  %9573 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 128
  %9574 = bitcast i16* %9573 to <4 x i16>*
  %9575 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 112
  %9576 = bitcast i16* %9575 to <4 x i16>*
  %9577 = bitcast i16* %9572 to <4 x i16>*
  %9578 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 48
  %9579 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 80
  %9580 = bitcast i16* %9579 to <4 x i16>*
  %9581 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 64
  %9582 = bitcast i16* %9581 to <4 x i16>*
  %9583 = bitcast i16* %9578 to <4 x i16>*
  %9584 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 32
  %9585 = bitcast i16* %9584 to <4 x i16>*
  %9586 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 16
  %9587 = bitcast i16* %9586 to <4 x i16>*
  %9588 = bitcast i16* %filter_zeroed272 to <4 x i16>*
  %9589 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 132
  %9590 = bitcast i16* %9589 to <4 x i16>*
  %9591 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 116
  %9592 = bitcast i16* %9591 to <4 x i16>*
  %9593 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 100
  %9594 = bitcast i16* %9593 to <4 x i16>*
  %9595 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 84
  %9596 = bitcast i16* %9595 to <4 x i16>*
  %9597 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 68
  %9598 = bitcast i16* %9597 to <4 x i16>*
  %9599 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 52
  %9600 = bitcast i16* %9599 to <4 x i16>*
  %9601 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 36
  %9602 = bitcast i16* %9601 to <4 x i16>*
  %9603 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 20
  %9604 = bitcast i16* %9603 to <4 x i16>*
  %9605 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 4
  %9606 = bitcast i16* %9605 to <4 x i16>*
  %9607 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 136
  %9608 = bitcast i16* %9607 to <4 x i16>*
  %9609 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 120
  %9610 = bitcast i16* %9609 to <4 x i16>*
  %9611 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 104
  %9612 = bitcast i16* %9611 to <4 x i16>*
  %9613 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 88
  %9614 = bitcast i16* %9613 to <4 x i16>*
  %9615 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 72
  %9616 = bitcast i16* %9615 to <4 x i16>*
  %9617 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 56
  %9618 = bitcast i16* %9617 to <4 x i16>*
  %9619 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 40
  %9620 = bitcast i16* %9619 to <4 x i16>*
  %9621 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 24
  %9622 = bitcast i16* %9621 to <4 x i16>*
  %9623 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 8
  %9624 = bitcast i16* %9623 to <4 x i16>*
  %9625 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 140
  %9626 = bitcast i16* %9625 to <4 x i16>*
  %9627 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 124
  %9628 = bitcast i16* %9627 to <4 x i16>*
  %9629 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 108
  %9630 = bitcast i16* %9629 to <4 x i16>*
  %9631 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 92
  %9632 = bitcast i16* %9631 to <4 x i16>*
  %9633 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 76
  %9634 = bitcast i16* %9633 to <4 x i16>*
  %9635 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 60
  %9636 = bitcast i16* %9635 to <4 x i16>*
  %9637 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 44
  %9638 = bitcast i16* %9637 to <4 x i16>*
  %9639 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 28
  %9640 = bitcast i16* %9639 to <4 x i16>*
  %9641 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 12
  %9642 = bitcast i16* %9641 to <4 x i16>*
  %9643 = sext i32 %9571 to i64
  br label %"for output.s0.b.rebased307"

"for output.s0.b.rebased307":                     ; preds = %"for output.s0.b.rebased307.preheader", %"end for output.s0.y.yo344"
  %indvars.iv6500 = phi i64 [ 0, %"for output.s0.b.rebased307.preheader" ], [ %indvars.iv.next6501, %"end for output.s0.y.yo344" ]
  %9644 = load i64, i64* %.fca.1.gep875, align 8, !tbaa !385
  %cmp.i3182 = icmp ult i64 %9644, %9421
  %9645 = load i8*, i8** %.fca.0.gep874, align 8, !tbaa !387
  br i1 %cmp.i3182, label %if.then.i3185, label %pseudostack_alloc.exit3198, !prof !388

if.then.i3185:                                    ; preds = %"for output.s0.b.rebased307"
  %tobool1.not.i3184 = icmp ne i8* %9645, null
  %9646 = load i64, i64* %.fca.2.gep876, align 8
  %cmp2.i3187 = icmp ugt i64 %9646, 16384
  %or.cond5131 = and i1 %tobool1.not.i3184, %cmp2.i3187
  br i1 %or.cond5131, label %if.then3.i3189, label %if.end.i3193

if.then3.i3189:                                   ; preds = %if.then.i3185
  call void @halide_free(i8* null, i8* nonnull %9645) #15
  %.pre6512 = load i64, i64* %.fca.2.gep876, align 8, !tbaa !389
  br label %if.end.i3193

if.end.i3193:                                     ; preds = %if.then3.i3189, %if.then.i3185
  %9647 = phi i64 [ %.pre6512, %if.then3.i3189 ], [ %9646, %if.then.i3185 ]
  %add.i3191 = add i64 %9647, %9421
  store i64 %add.i3191, i64* %.fca.2.gep876, align 8, !tbaa !389
  %cmp7.i3192 = icmp ugt i64 %add.i3191, 16384
  br i1 %cmp7.i3192, label %if.then8.i3195, label %if.end11.i3197

if.then8.i3195:                                   ; preds = %if.end.i3193
  %call.i3194 = call i8* @halide_malloc(i8* null, i64 %9421) #15
  br label %if.end11.i3197

if.end11.i3197:                                   ; preds = %if.then8.i3195, %if.end.i3193
  %storemerge.i3196 = phi i8* [ %call.i3194, %if.then8.i3195 ], [ null, %if.end.i3193 ]
  store i8* %storemerge.i3196, i8** %.fca.0.gep874, align 8, !tbaa !387
  store i64 %9421, i64* %.fca.1.gep875, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3198

pseudostack_alloc.exit3198:                       ; preds = %"for output.s0.b.rebased307", %if.end11.i3197
  %9648 = phi i8* [ %storemerge.i3196, %if.end11.i3197 ], [ %9645, %"for output.s0.b.rebased307" ]
  %.not2593 = icmp eq i8* %9648, null
  br i1 %.not2593, label %then_bb311, label %"produce resampled_input313", !prof !390

"end for output.s0.b.rebased308":                 ; preds = %"end for output.s0.y.yo344", %"consume sum_filter293"
  %.7 = phi i8* [ %.6, %"consume sum_filter293" ], [ %9463, %"end for output.s0.y.yo344" ]
  %9649 = add nuw nsw i32 %output.s0.c.co268, 1
  %.not2590 = icmp eq i32 %9649, %t2172261
  br i1 %.not2590, label %after_bb236.loopexit, label %"for output.s0.c.co266"

then_bb311:                                       ; preds = %pseudostack_alloc.exit3198
  %9650 = alloca i8*, i64 %9421, align 16
  %9651 = bitcast i8** %9650 to i8*
  store i8** %9650, i8*** %9422, align 8
  br label %"produce resampled_input313"

"produce resampled_input313":                     ; preds = %pseudostack_alloc.exit3198, %then_bb311
  %resampled_input312 = phi i8* [ %9651, %then_bb311 ], [ %9648, %pseudostack_alloc.exit3198 ]
  br i1 %t2194258, label %then_bb315, label %next_bb316

then_bb315:                                       ; preds = %"produce resampled_input313"
  br i1 %9423, label %"for resampled_input.s0.y.rebased320.preheader", label %"consume resampled_input339", !prof !391

"for resampled_input.s0.y.rebased320.preheader":  ; preds = %then_bb315
  %9652 = add nsw i64 %indvars.iv6500, %9476
  %9653 = mul nsw i64 %9652, %9477
  %9654 = add nsw i64 %9653, %9643
  br i1 %9427, label %"for resampled_input.s0.y.rebased320.us", label %"consume resampled_input339", !prof !391

"for resampled_input.s0.y.rebased320.us":         ; preds = %"for resampled_input.s0.y.rebased320.preheader", %"end for resampled_input.s0.x.rebased326.loopexit.us"
  %indvars.iv6490 = phi i64 [ %indvars.iv.next6491, %"end for resampled_input.s0.x.rebased326.loopexit.us" ], [ 0, %"for resampled_input.s0.y.rebased320.preheader" ]
  %9655 = add nsw i64 %indvars.iv6490, %9469
  %9656 = mul nsw i64 %9655, %9470
  %9657 = add nsw i64 %9656, %9471
  %9658 = add nsw i64 %indvars.iv6490, %9472
  %9659 = mul nsw i64 %9658, %9473
  %9660 = add nsw i64 %9654, %9659
  br label %"for resampled_input.s0.x.rebased325.us"

"for resampled_input.s0.x.rebased325.us":         ; preds = %"for resampled_input.s0.y.rebased320.us", %"for resampled_input.s0.x.rebased325.us"
  %indvars.iv6488 = phi i64 [ 0, %"for resampled_input.s0.y.rebased320.us" ], [ %indvars.iv.next6489, %"for resampled_input.s0.x.rebased325.us" ]
  %9661 = add nsw i64 %indvars.iv6488, %9437
  %9662 = mul nsw i64 %9661, %9438
  %9663 = add nsw i64 %9662, %9660
  %9664 = getelementptr inbounds i8, i8* %50, i64 %9663
  %9665 = bitcast i8* %9664 to <16 x i8>*
  %9666 = load <16 x i8>, <16 x i8>* %9665, align 1, !tbaa !436
  %9667 = add nsw i64 %indvars.iv6488, %9657
  %9668 = shl nsw i64 %9667, 4
  %9669 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9668
  %9670 = bitcast i8* %9669 to <16 x i8>*
  store <16 x i8> %9666, <16 x i8>* %9670, align 16, !tbaa !438
  %indvars.iv.next6489 = add nuw nsw i64 %indvars.iv6488, 1
  %.not2796.us = icmp eq i64 %indvars.iv6488, %9468
  br i1 %.not2796.us, label %"end for resampled_input.s0.x.rebased326.loopexit.us", label %"for resampled_input.s0.x.rebased325.us"

"end for resampled_input.s0.x.rebased326.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased325.us"
  %indvars.iv.next6491 = add nuw nsw i64 %indvars.iv6490, 1
  %.not2795.us = icmp eq i64 %indvars.iv6490, %9474
  br i1 %.not2795.us, label %"consume resampled_input339", label %"for resampled_input.s0.y.rebased320.us"

next_bb316:                                       ; preds = %"produce resampled_input313"
  br i1 %9482, label %"consume resampled_input339", label %"for resampled_input.s0.y.rebased331.preheader.split.us", !prof !440

"for resampled_input.s0.y.rebased331.preheader.split.us": ; preds = %next_bb316
  %9671 = trunc i64 %indvars.iv6500 to i32
  %9672 = add i32 %56, %9671
  %9673 = mul i32 %9672, %58
  %9674 = sub i32 %9673, %9426
  %9675 = sdiv <16 x i32> %9569, %9431
  %9676 = add nsw <16 x i32> %9675, %9570
  %9677 = and <16 x i32> %9676, %9436
  br label %"for resampled_input.s0.y.rebased331.us"

"for resampled_input.s0.y.rebased331.us":         ; preds = %"end for resampled_input.s0.x.rebased337.loopexit.us", %"for resampled_input.s0.y.rebased331.preheader.split.us"
  %indvars.iv6484 = phi i64 [ %indvars.iv.next6485, %"end for resampled_input.s0.x.rebased337.loopexit.us" ], [ 0, %"for resampled_input.s0.y.rebased331.preheader.split.us" ]
  %9678 = add nsw i64 %indvars.iv6484, %9469
  %9679 = mul nsw i64 %9678, %9470
  %9680 = add nsw i64 %9679, %9471
  %9681 = trunc i64 %indvars.iv6484 to i32
  %9682 = add i32 %a87, %9681
  %9683 = mul i32 %9682, %55
  %9684 = add i32 %9683, %9674
  br label %"for resampled_input.s0.x.rebased336.us"

"for resampled_input.s0.x.rebased336.us":         ; preds = %"for resampled_input.s0.y.rebased331.us", %"for resampled_input.s0.x.rebased336.us"
  %indvars.iv6482 = phi i64 [ 0, %"for resampled_input.s0.y.rebased331.us" ], [ %indvars.iv.next6483, %"for resampled_input.s0.x.rebased336.us" ]
  %9685 = trunc i64 %indvars.iv6482 to i32
  %9686 = add nsw i32 %t2178, %9685
  %9687 = mul nsw i32 %9686, %53
  %9688 = add nsw i32 %9684, %9687
  %9689 = insertelement <16 x i32> undef, i32 %9688, i32 0
  %9690 = shufflevector <16 x i32> %9689, <16 x i32> undef, <16 x i32> zeroinitializer
  %9691 = add nsw <16 x i32> %9690, %9677
  %9692 = extractelement <16 x i32> %9691, i32 0
  %9693 = sext i32 %9692 to i64
  %9694 = getelementptr inbounds i8, i8* %50, i64 %9693
  %9695 = load i8, i8* %9694, align 1, !tbaa !436
  %9696 = insertelement <16 x i8> undef, i8 %9695, i32 0
  %9697 = extractelement <16 x i32> %9691, i32 1
  %9698 = sext i32 %9697 to i64
  %9699 = getelementptr inbounds i8, i8* %50, i64 %9698
  %9700 = load i8, i8* %9699, align 1, !tbaa !436
  %9701 = insertelement <16 x i8> %9696, i8 %9700, i32 1
  %9702 = extractelement <16 x i32> %9691, i32 2
  %9703 = sext i32 %9702 to i64
  %9704 = getelementptr inbounds i8, i8* %50, i64 %9703
  %9705 = load i8, i8* %9704, align 1, !tbaa !436
  %9706 = insertelement <16 x i8> %9701, i8 %9705, i32 2
  %9707 = extractelement <16 x i32> %9691, i32 3
  %9708 = sext i32 %9707 to i64
  %9709 = getelementptr inbounds i8, i8* %50, i64 %9708
  %9710 = load i8, i8* %9709, align 1, !tbaa !436
  %9711 = insertelement <16 x i8> %9706, i8 %9710, i32 3
  %9712 = extractelement <16 x i32> %9691, i32 4
  %9713 = sext i32 %9712 to i64
  %9714 = getelementptr inbounds i8, i8* %50, i64 %9713
  %9715 = load i8, i8* %9714, align 1, !tbaa !436
  %9716 = insertelement <16 x i8> %9711, i8 %9715, i32 4
  %9717 = extractelement <16 x i32> %9691, i32 5
  %9718 = sext i32 %9717 to i64
  %9719 = getelementptr inbounds i8, i8* %50, i64 %9718
  %9720 = load i8, i8* %9719, align 1, !tbaa !436
  %9721 = insertelement <16 x i8> %9716, i8 %9720, i32 5
  %9722 = extractelement <16 x i32> %9691, i32 6
  %9723 = sext i32 %9722 to i64
  %9724 = getelementptr inbounds i8, i8* %50, i64 %9723
  %9725 = load i8, i8* %9724, align 1, !tbaa !436
  %9726 = insertelement <16 x i8> %9721, i8 %9725, i32 6
  %9727 = extractelement <16 x i32> %9691, i32 7
  %9728 = sext i32 %9727 to i64
  %9729 = getelementptr inbounds i8, i8* %50, i64 %9728
  %9730 = load i8, i8* %9729, align 1, !tbaa !436
  %9731 = insertelement <16 x i8> %9726, i8 %9730, i32 7
  %9732 = extractelement <16 x i32> %9691, i32 8
  %9733 = sext i32 %9732 to i64
  %9734 = getelementptr inbounds i8, i8* %50, i64 %9733
  %9735 = load i8, i8* %9734, align 1, !tbaa !436
  %9736 = insertelement <16 x i8> %9731, i8 %9735, i32 8
  %9737 = extractelement <16 x i32> %9691, i32 9
  %9738 = sext i32 %9737 to i64
  %9739 = getelementptr inbounds i8, i8* %50, i64 %9738
  %9740 = load i8, i8* %9739, align 1, !tbaa !436
  %9741 = insertelement <16 x i8> %9736, i8 %9740, i32 9
  %9742 = extractelement <16 x i32> %9691, i32 10
  %9743 = sext i32 %9742 to i64
  %9744 = getelementptr inbounds i8, i8* %50, i64 %9743
  %9745 = load i8, i8* %9744, align 1, !tbaa !436
  %9746 = insertelement <16 x i8> %9741, i8 %9745, i32 10
  %9747 = extractelement <16 x i32> %9691, i32 11
  %9748 = sext i32 %9747 to i64
  %9749 = getelementptr inbounds i8, i8* %50, i64 %9748
  %9750 = load i8, i8* %9749, align 1, !tbaa !436
  %9751 = insertelement <16 x i8> %9746, i8 %9750, i32 11
  %9752 = extractelement <16 x i32> %9691, i32 12
  %9753 = sext i32 %9752 to i64
  %9754 = getelementptr inbounds i8, i8* %50, i64 %9753
  %9755 = load i8, i8* %9754, align 1, !tbaa !436
  %9756 = insertelement <16 x i8> %9751, i8 %9755, i32 12
  %9757 = extractelement <16 x i32> %9691, i32 13
  %9758 = sext i32 %9757 to i64
  %9759 = getelementptr inbounds i8, i8* %50, i64 %9758
  %9760 = load i8, i8* %9759, align 1, !tbaa !436
  %9761 = insertelement <16 x i8> %9756, i8 %9760, i32 13
  %9762 = extractelement <16 x i32> %9691, i32 14
  %9763 = sext i32 %9762 to i64
  %9764 = getelementptr inbounds i8, i8* %50, i64 %9763
  %9765 = load i8, i8* %9764, align 1, !tbaa !436
  %9766 = insertelement <16 x i8> %9761, i8 %9765, i32 14
  %9767 = extractelement <16 x i32> %9691, i32 15
  %9768 = sext i32 %9767 to i64
  %9769 = getelementptr inbounds i8, i8* %50, i64 %9768
  %9770 = load i8, i8* %9769, align 1, !tbaa !436
  %9771 = insertelement <16 x i8> %9766, i8 %9770, i32 15
  %9772 = add nsw i64 %indvars.iv6482, %9680
  %9773 = shl nsw i64 %9772, 4
  %9774 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9773
  %9775 = bitcast i8* %9774 to <16 x i8>*
  store <16 x i8> %9771, <16 x i8>* %9775, align 16, !tbaa !438
  %indvars.iv.next6483 = add nuw nsw i64 %indvars.iv6482, 1
  %.not2794.us = icmp eq i64 %indvars.iv6482, %9468
  br i1 %.not2794.us, label %"end for resampled_input.s0.x.rebased337.loopexit.us", label %"for resampled_input.s0.x.rebased336.us"

"end for resampled_input.s0.x.rebased337.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased336.us"
  %indvars.iv.next6485 = add nuw nsw i64 %indvars.iv6484, 1
  %.not2791.us = icmp eq i64 %indvars.iv6484, %9474
  br i1 %.not2791.us, label %"consume resampled_input339", label %"for resampled_input.s0.y.rebased331.us"

"consume resampled_input339":                     ; preds = %"end for resampled_input.s0.x.rebased337.loopexit.us", %"end for resampled_input.s0.x.rebased326.loopexit.us", %next_bb316, %"for resampled_input.s0.y.rebased320.preheader", %then_bb315
  br i1 %9439, label %"for output.s0.y.yo343.preheader", label %"end for output.s0.y.yo344", !prof !391

"for output.s0.y.yo343.preheader":                ; preds = %"consume resampled_input339"
  %9776 = trunc i64 %indvars.iv6500 to i32
  %9777 = add i32 %56, %9776
  %9778 = mul i32 %9777, %67
  %9779 = add i32 %9778, %output.s0.c.c.base269
  %9780 = sub i32 %9779, %reass.add5208
  %9781 = load <4 x i32>, <4 x i32>* %9398, align 16
  %9782 = load <4 x i32>, <4 x i32>* %9400, align 16
  %9783 = load <4 x i32>, <4 x i32>* %9402, align 16
  %9784 = load <4 x i32>, <4 x i32>* %9404, align 16
  br label %"for output.s0.y.yo343"

"for output.s0.y.yo343":                          ; preds = %"for output.s0.y.yo343.preheader", %"end for output.s0.x.xo382"
  %output.s0.y.yo345 = phi i32 [ %9812, %"end for output.s0.x.xo382" ], [ 0, %"for output.s0.y.yo343.preheader" ]
  %a118 = shl nuw nsw i32 %output.s0.y.yo345, 1
  %9785 = icmp slt i32 %a118, %b120
  %9786 = select i1 %9785, i32 %a118, i32 %b120
  %9787 = add nsw i32 %9786, %64
  %9788 = mul nsw i32 %9787, %stride_y
  %9789 = add nsw i32 %9788, %9440
  %9790 = add nsw i32 %9788, %9441
  %9791 = add nsw i32 %9787, 1
  %9792 = mul nsw i32 %9791, %stride_y
  %9793 = add nsw i32 %9792, %9440
  %9794 = add nsw i32 %9792, %9441
  %9795 = sub nsw i32 %9788, %9414
  %9796 = sub nsw i32 %9792, %9414
  %9797 = mul nsw i32 %9796, %a99
  %t2257361 = sub nsw i32 %9797, %9409
  %9798 = mul nsw i32 %9795, %a99
  %t2256362 = sub nsw i32 %9798, %9409
  %9799 = mul nsw i32 %9794, %a99
  %t2264363 = sub nsw i32 %9799, %9409
  %9800 = mul nsw i32 %9793, %a99
  %t2271364 = sub nsw i32 %9800, %9409
  %9801 = mul nsw i32 %9790, %a99
  %t2263365 = sub nsw i32 %9801, %9409
  %9802 = mul nsw i32 %9789, %a99
  %t2270366 = sub nsw i32 %9802, %9409
  %t2259367 = add nsw i32 %9797, %9442
  %t2261368 = add nsw i32 %9797, %9443
  %t2258369 = add nsw i32 %9798, %9442
  %t2260370 = add nsw i32 %9798, %9443
  %t2266371 = add nsw i32 %9799, %9442
  %t2268372 = add nsw i32 %9799, %9443
  %t2273373 = add nsw i32 %9800, %9442
  %t2275374 = add nsw i32 %9800, %9443
  %t2265375 = add nsw i32 %9801, %9442
  %t2267376 = add nsw i32 %9801, %9443
  %t2272377 = add nsw i32 %9802, %9442
  %t2274378 = add nsw i32 %9802, %9443
  br i1 %9444, label %"for output.s0.x.xo381.preheader", label %"end for output.s0.x.xo382", !prof !391

"for output.s0.x.xo381.preheader":                ; preds = %"for output.s0.y.yo343"
  %9803 = mul nsw i32 %9787, %66
  %t2278380 = add nsw i32 %9803, %9780
  %9804 = mul nsw i32 %9791, %66
  %t2279379 = add nsw i32 %9804, %9780
  %9805 = sub i32 %9788, %a87
  %9806 = sub i32 %9792, %a87
  %9807 = sext i32 %t2278380 to i64
  %9808 = sext i32 %t2279379 to i64
  br label %"for output.s0.x.xo381"

"end for output.s0.y.yo344":                      ; preds = %"end for output.s0.x.xo382", %"consume resampled_input339"
  %indvars.iv.next6501 = add nuw nsw i64 %indvars.iv6500, 1
  %9809 = icmp eq i64 %indvars.iv.next6501, %zext6502
  br i1 %9809, label %"end for output.s0.b.rebased308", label %"for output.s0.b.rebased307"

"for output.s0.x.xo381":                          ; preds = %"for output.s0.x.xo381.preheader", %"consume convolved406"
  %output.s0.x.xo383 = phi i32 [ %11092, %"consume convolved406" ], [ 0, %"for output.s0.x.xo381.preheader" ]
  %a247 = shl nuw nsw i32 %output.s0.x.xo383, 1
  %9810 = icmp slt i32 %a247, %b249
  %output.s0.x.x.base.s384 = select i1 %9810, i32 %a247, i32 %b249
  %9811 = add nsw i32 %output.s0.x.x.base.s384, %61
  br i1 %t2217255, label %then_bb387, label %next_bb388

"end for output.s0.x.xo382":                      ; preds = %"consume convolved406", %"for output.s0.y.yo343"
  %9812 = add nuw nsw i32 %output.s0.y.yo345, 1
  %.not2595 = icmp eq i32 %9812, %t2196259
  br i1 %.not2595, label %"end for output.s0.y.yo344", label %"for output.s0.y.yo343"

then_bb387:                                       ; preds = %"for output.s0.x.xo381"
  %t2851 = mul nsw i32 %9811, %stride_x
  %t2852 = add nsw i32 %t2851, %t2274378
  %9813 = sext i32 %t2852 to i64
  %9814 = shl nsw i64 %9813, 4
  %9815 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9814
  %9816 = bitcast i8* %9815 to <8 x i8>*
  %t2853 = load <8 x i8>, <8 x i8>* %9816, align 16, !tbaa !438
  %t2854 = add nsw i32 %t2851, %t2272377
  %9817 = sext i32 %t2854 to i64
  %9818 = shl nsw i64 %9817, 4
  %9819 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9818
  %9820 = bitcast i8* %9819 to <8 x i8>*
  %t2855 = load <8 x i8>, <8 x i8>* %9820, align 16, !tbaa !438
  %t2856 = add nsw i32 %t2851, %t2270366
  %9821 = sext i32 %t2856 to i64
  %9822 = shl nsw i64 %9821, 4
  %9823 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9822
  %9824 = bitcast i8* %9823 to <8 x i8>*
  %t2857 = load <8 x i8>, <8 x i8>* %9824, align 16, !tbaa !438
  %t2858 = add nsw i32 %t2851, %t2267376
  %9825 = sext i32 %t2858 to i64
  %9826 = shl nsw i64 %9825, 4
  %9827 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9826
  %9828 = bitcast i8* %9827 to <8 x i8>*
  %t2859 = load <8 x i8>, <8 x i8>* %9828, align 16, !tbaa !438
  %t2860 = add nsw i32 %t2851, %t2265375
  %9829 = sext i32 %t2860 to i64
  %9830 = shl nsw i64 %9829, 4
  %9831 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9830
  %9832 = bitcast i8* %9831 to <8 x i8>*
  %t2861 = load <8 x i8>, <8 x i8>* %9832, align 16, !tbaa !438
  %t2862 = add nsw i32 %t2851, %t2263365
  %9833 = sext i32 %t2862 to i64
  %9834 = shl nsw i64 %9833, 4
  %9835 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9834
  %9836 = bitcast i8* %9835 to <8 x i8>*
  %t2863 = load <8 x i8>, <8 x i8>* %9836, align 16, !tbaa !438
  %t2864 = add nsw i32 %t2851, %t2260370
  %9837 = sext i32 %t2864 to i64
  %9838 = shl nsw i64 %9837, 4
  %9839 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9838
  %9840 = bitcast i8* %9839 to <8 x i8>*
  %t2865 = load <8 x i8>, <8 x i8>* %9840, align 16, !tbaa !438
  %t2866 = add nsw i32 %t2851, %t2258369
  %9841 = sext i32 %t2866 to i64
  %9842 = shl nsw i64 %9841, 4
  %9843 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9842
  %9844 = bitcast i8* %9843 to <8 x i8>*
  %t2867 = load <8 x i8>, <8 x i8>* %9844, align 16, !tbaa !438
  %t2868 = add nsw i32 %t2851, %t2256362
  %9845 = sext i32 %t2868 to i64
  %9846 = shl nsw i64 %9845, 4
  %9847 = getelementptr inbounds i8, i8* %resampled_input312, i64 %9846
  %9848 = bitcast i8* %9847 to <8 x i8>*
  %t2869 = load <8 x i8>, <8 x i8>* %9848, align 16, !tbaa !438
  %9849 = getelementptr inbounds i8, i8* %9815, i64 8
  %9850 = bitcast i8* %9849 to <8 x i8>*
  %t2870 = load <8 x i8>, <8 x i8>* %9850, align 8, !tbaa !438
  %9851 = getelementptr inbounds i8, i8* %9819, i64 8
  %9852 = bitcast i8* %9851 to <8 x i8>*
  %t2871 = load <8 x i8>, <8 x i8>* %9852, align 8, !tbaa !438
  %9853 = getelementptr inbounds i8, i8* %9823, i64 8
  %9854 = bitcast i8* %9853 to <8 x i8>*
  %t2872 = load <8 x i8>, <8 x i8>* %9854, align 8, !tbaa !438
  %9855 = getelementptr inbounds i8, i8* %9827, i64 8
  %9856 = bitcast i8* %9855 to <8 x i8>*
  %t2873 = load <8 x i8>, <8 x i8>* %9856, align 8, !tbaa !438
  %9857 = getelementptr inbounds i8, i8* %9831, i64 8
  %9858 = bitcast i8* %9857 to <8 x i8>*
  %t2874 = load <8 x i8>, <8 x i8>* %9858, align 8, !tbaa !438
  %9859 = getelementptr inbounds i8, i8* %9835, i64 8
  %9860 = bitcast i8* %9859 to <8 x i8>*
  %t2875 = load <8 x i8>, <8 x i8>* %9860, align 8, !tbaa !438
  %9861 = getelementptr inbounds i8, i8* %9839, i64 8
  %9862 = bitcast i8* %9861 to <8 x i8>*
  %t2876 = load <8 x i8>, <8 x i8>* %9862, align 8, !tbaa !438
  %9863 = getelementptr inbounds i8, i8* %9843, i64 8
  %9864 = bitcast i8* %9863 to <8 x i8>*
  %t2877 = load <8 x i8>, <8 x i8>* %9864, align 8, !tbaa !438
  %9865 = getelementptr inbounds i8, i8* %9847, i64 8
  %9866 = bitcast i8* %9865 to <8 x i8>*
  %t2878 = load <8 x i8>, <8 x i8>* %9866, align 8, !tbaa !438
  %9867 = load <4 x i16>, <4 x i16>* %9574, align 16, !tbaa !395
  %9868 = zext <8 x i8> %t2853 to <8 x i16>
  %9869 = bitcast <8 x i16> %9868 to <2 x i64>
  %9870 = shufflevector <2 x i64> %9869, <2 x i64> undef, <1 x i32> zeroinitializer
  %9871 = bitcast <1 x i64> %9870 to <4 x i16>
  %9872 = load <4 x i16>, <4 x i16>* %9576, align 16, !tbaa !395
  %9873 = zext <8 x i8> %t2855 to <8 x i16>
  %9874 = bitcast <8 x i16> %9873 to <2 x i64>
  %9875 = shufflevector <2 x i64> %9874, <2 x i64> undef, <1 x i32> zeroinitializer
  %9876 = load <4 x i16>, <4 x i16>* %9577, align 16, !tbaa !395
  %9877 = zext <8 x i8> %t2857 to <8 x i16>
  %9878 = bitcast <8 x i16> %9877 to <2 x i64>
  %9879 = shufflevector <2 x i64> %9878, <2 x i64> undef, <1 x i32> zeroinitializer
  %9880 = bitcast <1 x i64> %9879 to <4 x i16>
  %9881 = load <4 x i16>, <4 x i16>* %9580, align 16, !tbaa !395
  %9882 = zext <8 x i8> %t2859 to <8 x i16>
  %9883 = bitcast <8 x i16> %9882 to <2 x i64>
  %9884 = shufflevector <2 x i64> %9883, <2 x i64> undef, <1 x i32> zeroinitializer
  %9885 = load <4 x i16>, <4 x i16>* %9582, align 16, !tbaa !395
  %9886 = zext <8 x i8> %t2861 to <8 x i16>
  %9887 = bitcast <8 x i16> %9886 to <2 x i64>
  %9888 = shufflevector <2 x i64> %9887, <2 x i64> undef, <1 x i32> zeroinitializer
  %9889 = bitcast <1 x i64> %9888 to <4 x i16>
  %9890 = load <4 x i16>, <4 x i16>* %9583, align 16, !tbaa !395
  %9891 = zext <8 x i8> %t2863 to <8 x i16>
  %9892 = bitcast <8 x i16> %9891 to <2 x i64>
  %9893 = shufflevector <2 x i64> %9892, <2 x i64> undef, <1 x i32> zeroinitializer
  %9894 = load <4 x i16>, <4 x i16>* %9585, align 16, !tbaa !534
  %9895 = zext <8 x i8> %t2865 to <8 x i16>
  %9896 = bitcast <8 x i16> %9895 to <2 x i64>
  %9897 = shufflevector <2 x i64> %9896, <2 x i64> undef, <1 x i32> zeroinitializer
  %9898 = bitcast <1 x i64> %9897 to <4 x i16>
  %9899 = load <4 x i16>, <4 x i16>* %9587, align 16, !tbaa !536
  %9900 = zext <8 x i8> %t2867 to <8 x i16>
  %9901 = bitcast <8 x i16> %9900 to <2 x i64>
  %9902 = shufflevector <2 x i64> %9901, <2 x i64> undef, <1 x i32> zeroinitializer
  %9903 = load <4 x i16>, <4 x i16>* %9588, align 16, !tbaa !538
  %9904 = zext <8 x i8> %t2869 to <8 x i16>
  %9905 = bitcast <8 x i16> %9904 to <2 x i64>
  %9906 = shufflevector <2 x i64> %9905, <2 x i64> undef, <1 x i32> zeroinitializer
  %9907 = bitcast <1 x i64> %9906 to <4 x i16>
  %.cast2601 = bitcast <1 x i64> %9875 to <4 x i16>
  %9908 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2601, <4 x i16> %9872) #11
  %9909 = sext <4 x i16> %9871 to <4 x i32>
  %9910 = sext <4 x i16> %9867 to <4 x i32>
  %9911 = mul nsw <4 x i32> %9910, %9909
  %.cast2604 = bitcast <1 x i64> %9884 to <4 x i16>
  %9912 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2604, <4 x i16> %9881) #11
  %9913 = sext <4 x i16> %9880 to <4 x i32>
  %9914 = sext <4 x i16> %9876 to <4 x i32>
  %9915 = mul nsw <4 x i32> %9914, %9913
  %.cast2607 = bitcast <1 x i64> %9893 to <4 x i16>
  %9916 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2607, <4 x i16> %9890) #11
  %9917 = sext <4 x i16> %9889 to <4 x i32>
  %9918 = sext <4 x i16> %9885 to <4 x i32>
  %9919 = mul nsw <4 x i32> %9918, %9917
  %.cast2610 = bitcast <1 x i64> %9902 to <4 x i16>
  %9920 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2610, <4 x i16> %9899) #11
  %9921 = sext <4 x i16> %9898 to <4 x i32>
  %9922 = sext <4 x i16> %9894 to <4 x i32>
  %9923 = mul nsw <4 x i32> %9922, %9921
  %9924 = sext <4 x i16> %9903 to <4 x i32>
  %9925 = sext <4 x i16> %9907 to <4 x i32>
  %9926 = mul nsw <4 x i32> %9924, %9925
  %9927 = add <4 x i32> %9911, %9781
  %9928 = add <4 x i32> %9927, %9915
  %9929 = add <4 x i32> %9928, %9919
  %9930 = add <4 x i32> %9929, %9923
  %9931 = add <4 x i32> %9930, %9908
  %9932 = add <4 x i32> %9931, %9912
  %9933 = add <4 x i32> %9932, %9926
  %9934 = add <4 x i32> %9933, %9916
  %9935 = add <4 x i32> %9934, %9920
  %9936 = load <4 x i16>, <4 x i16>* %9590, align 8, !tbaa !395
  %9937 = shufflevector <2 x i64> %9869, <2 x i64> undef, <1 x i32> <i32 1>
  %9938 = bitcast <1 x i64> %9937 to <4 x i16>
  %9939 = load <4 x i16>, <4 x i16>* %9592, align 8, !tbaa !395
  %9940 = shufflevector <2 x i64> %9874, <2 x i64> undef, <1 x i32> <i32 1>
  %9941 = load <4 x i16>, <4 x i16>* %9594, align 8, !tbaa !395
  %9942 = shufflevector <2 x i64> %9878, <2 x i64> undef, <1 x i32> <i32 1>
  %9943 = bitcast <1 x i64> %9942 to <4 x i16>
  %9944 = load <4 x i16>, <4 x i16>* %9596, align 8, !tbaa !395
  %9945 = shufflevector <2 x i64> %9883, <2 x i64> undef, <1 x i32> <i32 1>
  %9946 = load <4 x i16>, <4 x i16>* %9598, align 8, !tbaa !395
  %9947 = shufflevector <2 x i64> %9887, <2 x i64> undef, <1 x i32> <i32 1>
  %9948 = bitcast <1 x i64> %9947 to <4 x i16>
  %9949 = load <4 x i16>, <4 x i16>* %9600, align 8, !tbaa !395
  %9950 = shufflevector <2 x i64> %9892, <2 x i64> undef, <1 x i32> <i32 1>
  %9951 = load <4 x i16>, <4 x i16>* %9602, align 16, !tbaa !540
  %9952 = shufflevector <2 x i64> %9896, <2 x i64> undef, <1 x i32> <i32 1>
  %9953 = bitcast <1 x i64> %9952 to <4 x i16>
  %9954 = load <4 x i16>, <4 x i16>* %9604, align 16, !tbaa !542
  %9955 = shufflevector <2 x i64> %9901, <2 x i64> undef, <1 x i32> <i32 1>
  %9956 = load <4 x i16>, <4 x i16>* %9606, align 16, !tbaa !544
  %9957 = shufflevector <2 x i64> %9905, <2 x i64> undef, <1 x i32> <i32 1>
  %9958 = bitcast <1 x i64> %9957 to <4 x i16>
  %.cast2613 = bitcast <1 x i64> %9940 to <4 x i16>
  %9959 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2613, <4 x i16> %9939) #11
  %9960 = sext <4 x i16> %9938 to <4 x i32>
  %9961 = sext <4 x i16> %9936 to <4 x i32>
  %9962 = mul nsw <4 x i32> %9961, %9960
  %.cast2616 = bitcast <1 x i64> %9945 to <4 x i16>
  %9963 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2616, <4 x i16> %9944) #11
  %9964 = sext <4 x i16> %9943 to <4 x i32>
  %9965 = sext <4 x i16> %9941 to <4 x i32>
  %9966 = mul nsw <4 x i32> %9965, %9964
  %.cast2619 = bitcast <1 x i64> %9950 to <4 x i16>
  %9967 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2619, <4 x i16> %9949) #11
  %9968 = sext <4 x i16> %9948 to <4 x i32>
  %9969 = sext <4 x i16> %9946 to <4 x i32>
  %9970 = mul nsw <4 x i32> %9969, %9968
  %.cast2622 = bitcast <1 x i64> %9955 to <4 x i16>
  %9971 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2622, <4 x i16> %9954) #11
  %9972 = sext <4 x i16> %9953 to <4 x i32>
  %9973 = sext <4 x i16> %9951 to <4 x i32>
  %9974 = mul nsw <4 x i32> %9973, %9972
  %9975 = sext <4 x i16> %9956 to <4 x i32>
  %9976 = sext <4 x i16> %9958 to <4 x i32>
  %9977 = mul nsw <4 x i32> %9975, %9976
  %9978 = add <4 x i32> %9962, %9782
  %9979 = add <4 x i32> %9978, %9966
  %9980 = add <4 x i32> %9979, %9970
  %9981 = add <4 x i32> %9980, %9974
  %9982 = add <4 x i32> %9981, %9959
  %9983 = add <4 x i32> %9982, %9963
  %9984 = add <4 x i32> %9983, %9977
  %9985 = add <4 x i32> %9984, %9967
  %9986 = add <4 x i32> %9985, %9971
  %9987 = load <4 x i16>, <4 x i16>* %9608, align 16, !tbaa !395
  %9988 = zext <8 x i8> %t2870 to <8 x i16>
  %9989 = bitcast <8 x i16> %9988 to <2 x i64>
  %9990 = shufflevector <2 x i64> %9989, <2 x i64> undef, <1 x i32> zeroinitializer
  %9991 = bitcast <1 x i64> %9990 to <4 x i16>
  %9992 = load <4 x i16>, <4 x i16>* %9610, align 16, !tbaa !395
  %9993 = zext <8 x i8> %t2871 to <8 x i16>
  %9994 = bitcast <8 x i16> %9993 to <2 x i64>
  %9995 = shufflevector <2 x i64> %9994, <2 x i64> undef, <1 x i32> zeroinitializer
  %9996 = load <4 x i16>, <4 x i16>* %9612, align 16, !tbaa !395
  %9997 = zext <8 x i8> %t2872 to <8 x i16>
  %9998 = bitcast <8 x i16> %9997 to <2 x i64>
  %9999 = shufflevector <2 x i64> %9998, <2 x i64> undef, <1 x i32> zeroinitializer
  %10000 = bitcast <1 x i64> %9999 to <4 x i16>
  %10001 = load <4 x i16>, <4 x i16>* %9614, align 16, !tbaa !395
  %10002 = zext <8 x i8> %t2873 to <8 x i16>
  %10003 = bitcast <8 x i16> %10002 to <2 x i64>
  %10004 = shufflevector <2 x i64> %10003, <2 x i64> undef, <1 x i32> zeroinitializer
  %10005 = load <4 x i16>, <4 x i16>* %9616, align 16, !tbaa !395
  %10006 = zext <8 x i8> %t2874 to <8 x i16>
  %10007 = bitcast <8 x i16> %10006 to <2 x i64>
  %10008 = shufflevector <2 x i64> %10007, <2 x i64> undef, <1 x i32> zeroinitializer
  %10009 = bitcast <1 x i64> %10008 to <4 x i16>
  %10010 = load <4 x i16>, <4 x i16>* %9618, align 16, !tbaa !395
  %10011 = zext <8 x i8> %t2875 to <8 x i16>
  %10012 = bitcast <8 x i16> %10011 to <2 x i64>
  %10013 = shufflevector <2 x i64> %10012, <2 x i64> undef, <1 x i32> zeroinitializer
  %10014 = load <4 x i16>, <4 x i16>* %9620, align 16, !tbaa !546
  %10015 = zext <8 x i8> %t2876 to <8 x i16>
  %10016 = bitcast <8 x i16> %10015 to <2 x i64>
  %10017 = shufflevector <2 x i64> %10016, <2 x i64> undef, <1 x i32> zeroinitializer
  %10018 = bitcast <1 x i64> %10017 to <4 x i16>
  %10019 = load <4 x i16>, <4 x i16>* %9622, align 16, !tbaa !548
  %10020 = zext <8 x i8> %t2877 to <8 x i16>
  %10021 = bitcast <8 x i16> %10020 to <2 x i64>
  %10022 = shufflevector <2 x i64> %10021, <2 x i64> undef, <1 x i32> zeroinitializer
  %10023 = load <4 x i16>, <4 x i16>* %9624, align 16, !tbaa !550
  %10024 = zext <8 x i8> %t2878 to <8 x i16>
  %10025 = bitcast <8 x i16> %10024 to <2 x i64>
  %10026 = shufflevector <2 x i64> %10025, <2 x i64> undef, <1 x i32> zeroinitializer
  %10027 = bitcast <1 x i64> %10026 to <4 x i16>
  %.cast2625 = bitcast <1 x i64> %9995 to <4 x i16>
  %10028 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2625, <4 x i16> %9992) #11
  %10029 = sext <4 x i16> %9991 to <4 x i32>
  %10030 = sext <4 x i16> %9987 to <4 x i32>
  %10031 = mul nsw <4 x i32> %10030, %10029
  %.cast2628 = bitcast <1 x i64> %10004 to <4 x i16>
  %10032 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2628, <4 x i16> %10001) #11
  %10033 = sext <4 x i16> %10000 to <4 x i32>
  %10034 = sext <4 x i16> %9996 to <4 x i32>
  %10035 = mul nsw <4 x i32> %10034, %10033
  %.cast2631 = bitcast <1 x i64> %10013 to <4 x i16>
  %10036 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2631, <4 x i16> %10010) #11
  %10037 = sext <4 x i16> %10009 to <4 x i32>
  %10038 = sext <4 x i16> %10005 to <4 x i32>
  %10039 = mul nsw <4 x i32> %10038, %10037
  %.cast2634 = bitcast <1 x i64> %10022 to <4 x i16>
  %10040 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2634, <4 x i16> %10019) #11
  %10041 = sext <4 x i16> %10018 to <4 x i32>
  %10042 = sext <4 x i16> %10014 to <4 x i32>
  %10043 = mul nsw <4 x i32> %10042, %10041
  %10044 = sext <4 x i16> %10023 to <4 x i32>
  %10045 = sext <4 x i16> %10027 to <4 x i32>
  %10046 = mul nsw <4 x i32> %10044, %10045
  %10047 = add <4 x i32> %10031, %9783
  %10048 = add <4 x i32> %10047, %10035
  %10049 = add <4 x i32> %10048, %10039
  %10050 = add <4 x i32> %10049, %10043
  %10051 = add <4 x i32> %10050, %10028
  %10052 = add <4 x i32> %10051, %10032
  %10053 = add <4 x i32> %10052, %10046
  %10054 = add <4 x i32> %10053, %10036
  %10055 = add <4 x i32> %10054, %10040
  %10056 = load <4 x i16>, <4 x i16>* %9626, align 8, !tbaa !395
  %10057 = shufflevector <2 x i64> %9989, <2 x i64> undef, <1 x i32> <i32 1>
  %10058 = bitcast <1 x i64> %10057 to <4 x i16>
  %10059 = load <4 x i16>, <4 x i16>* %9628, align 8, !tbaa !395
  %10060 = shufflevector <2 x i64> %9994, <2 x i64> undef, <1 x i32> <i32 1>
  %10061 = load <4 x i16>, <4 x i16>* %9630, align 8, !tbaa !395
  %10062 = shufflevector <2 x i64> %9998, <2 x i64> undef, <1 x i32> <i32 1>
  %10063 = bitcast <1 x i64> %10062 to <4 x i16>
  %10064 = load <4 x i16>, <4 x i16>* %9632, align 8, !tbaa !395
  %10065 = shufflevector <2 x i64> %10003, <2 x i64> undef, <1 x i32> <i32 1>
  %10066 = load <4 x i16>, <4 x i16>* %9634, align 8, !tbaa !395
  %10067 = shufflevector <2 x i64> %10007, <2 x i64> undef, <1 x i32> <i32 1>
  %10068 = bitcast <1 x i64> %10067 to <4 x i16>
  %10069 = load <4 x i16>, <4 x i16>* %9636, align 8, !tbaa !395
  %10070 = shufflevector <2 x i64> %10012, <2 x i64> undef, <1 x i32> <i32 1>
  %10071 = load <4 x i16>, <4 x i16>* %9638, align 16, !tbaa !552
  %10072 = shufflevector <2 x i64> %10016, <2 x i64> undef, <1 x i32> <i32 1>
  %10073 = bitcast <1 x i64> %10072 to <4 x i16>
  %10074 = load <4 x i16>, <4 x i16>* %9640, align 16, !tbaa !554
  %10075 = shufflevector <2 x i64> %10021, <2 x i64> undef, <1 x i32> <i32 1>
  %10076 = load <4 x i16>, <4 x i16>* %9642, align 16, !tbaa !556
  %10077 = shufflevector <2 x i64> %10025, <2 x i64> undef, <1 x i32> <i32 1>
  %10078 = bitcast <1 x i64> %10077 to <4 x i16>
  %.cast2637 = bitcast <1 x i64> %10060 to <4 x i16>
  %10079 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2637, <4 x i16> %10059) #11
  %10080 = sext <4 x i16> %10058 to <4 x i32>
  %10081 = sext <4 x i16> %10056 to <4 x i32>
  %10082 = mul nsw <4 x i32> %10081, %10080
  %.cast2640 = bitcast <1 x i64> %10065 to <4 x i16>
  %10083 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2640, <4 x i16> %10064) #11
  %10084 = sext <4 x i16> %10063 to <4 x i32>
  %10085 = sext <4 x i16> %10061 to <4 x i32>
  %10086 = mul nsw <4 x i32> %10085, %10084
  %.cast2643 = bitcast <1 x i64> %10070 to <4 x i16>
  %10087 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2643, <4 x i16> %10069) #11
  %10088 = sext <4 x i16> %10068 to <4 x i32>
  %10089 = sext <4 x i16> %10066 to <4 x i32>
  %10090 = mul nsw <4 x i32> %10089, %10088
  %.cast2646 = bitcast <1 x i64> %10075 to <4 x i16>
  %10091 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2646, <4 x i16> %10074) #11
  %10092 = sext <4 x i16> %10073 to <4 x i32>
  %10093 = sext <4 x i16> %10071 to <4 x i32>
  %10094 = mul nsw <4 x i32> %10093, %10092
  %10095 = sext <4 x i16> %10076 to <4 x i32>
  %10096 = sext <4 x i16> %10078 to <4 x i32>
  %10097 = mul nsw <4 x i32> %10095, %10096
  %10098 = add <4 x i32> %10082, %9784
  %10099 = add <4 x i32> %10098, %10086
  %10100 = add <4 x i32> %10099, %10090
  %10101 = add <4 x i32> %10100, %10094
  %10102 = add <4 x i32> %10101, %10079
  %10103 = add <4 x i32> %10102, %10083
  %10104 = add <4 x i32> %10103, %10097
  %10105 = add <4 x i32> %10104, %10087
  %10106 = add <4 x i32> %10105, %10091
  %10107 = add nsw i32 %9811, 1
  %t2879 = mul nsw i32 %10107, %stride_x
  %t2880 = add nsw i32 %t2879, %t2274378
  %10108 = sext i32 %t2880 to i64
  %10109 = shl nsw i64 %10108, 4
  %10110 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10109
  %10111 = bitcast i8* %10110 to <8 x i8>*
  %t2881 = load <8 x i8>, <8 x i8>* %10111, align 16, !tbaa !438
  %t2882 = add nsw i32 %t2879, %t2272377
  %10112 = sext i32 %t2882 to i64
  %10113 = shl nsw i64 %10112, 4
  %10114 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10113
  %10115 = bitcast i8* %10114 to <8 x i8>*
  %t2883 = load <8 x i8>, <8 x i8>* %10115, align 16, !tbaa !438
  %t2884 = add nsw i32 %t2879, %t2270366
  %10116 = sext i32 %t2884 to i64
  %10117 = shl nsw i64 %10116, 4
  %10118 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10117
  %10119 = bitcast i8* %10118 to <8 x i8>*
  %t2885 = load <8 x i8>, <8 x i8>* %10119, align 16, !tbaa !438
  %t2886 = add nsw i32 %t2879, %t2267376
  %10120 = sext i32 %t2886 to i64
  %10121 = shl nsw i64 %10120, 4
  %10122 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10121
  %10123 = bitcast i8* %10122 to <8 x i8>*
  %t2887 = load <8 x i8>, <8 x i8>* %10123, align 16, !tbaa !438
  %t2888 = add nsw i32 %t2879, %t2265375
  %10124 = sext i32 %t2888 to i64
  %10125 = shl nsw i64 %10124, 4
  %10126 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10125
  %10127 = bitcast i8* %10126 to <8 x i8>*
  %t2889 = load <8 x i8>, <8 x i8>* %10127, align 16, !tbaa !438
  %t2890 = add nsw i32 %t2879, %t2263365
  %10128 = sext i32 %t2890 to i64
  %10129 = shl nsw i64 %10128, 4
  %10130 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10129
  %10131 = bitcast i8* %10130 to <8 x i8>*
  %t2891 = load <8 x i8>, <8 x i8>* %10131, align 16, !tbaa !438
  %t2892 = add nsw i32 %t2879, %t2260370
  %10132 = sext i32 %t2892 to i64
  %10133 = shl nsw i64 %10132, 4
  %10134 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10133
  %10135 = bitcast i8* %10134 to <8 x i8>*
  %t2893 = load <8 x i8>, <8 x i8>* %10135, align 16, !tbaa !438
  %t2894 = add nsw i32 %t2879, %t2258369
  %10136 = sext i32 %t2894 to i64
  %10137 = shl nsw i64 %10136, 4
  %10138 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10137
  %10139 = bitcast i8* %10138 to <8 x i8>*
  %t2895 = load <8 x i8>, <8 x i8>* %10139, align 16, !tbaa !438
  %t2896 = add nsw i32 %t2879, %t2256362
  %10140 = sext i32 %t2896 to i64
  %10141 = shl nsw i64 %10140, 4
  %10142 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10141
  %10143 = bitcast i8* %10142 to <8 x i8>*
  %t2897 = load <8 x i8>, <8 x i8>* %10143, align 16, !tbaa !438
  %10144 = getelementptr inbounds i8, i8* %10110, i64 8
  %10145 = bitcast i8* %10144 to <8 x i8>*
  %t2898 = load <8 x i8>, <8 x i8>* %10145, align 8, !tbaa !438
  %10146 = getelementptr inbounds i8, i8* %10114, i64 8
  %10147 = bitcast i8* %10146 to <8 x i8>*
  %t2899 = load <8 x i8>, <8 x i8>* %10147, align 8, !tbaa !438
  %10148 = getelementptr inbounds i8, i8* %10118, i64 8
  %10149 = bitcast i8* %10148 to <8 x i8>*
  %t2900 = load <8 x i8>, <8 x i8>* %10149, align 8, !tbaa !438
  %10150 = getelementptr inbounds i8, i8* %10122, i64 8
  %10151 = bitcast i8* %10150 to <8 x i8>*
  %t2901 = load <8 x i8>, <8 x i8>* %10151, align 8, !tbaa !438
  %10152 = getelementptr inbounds i8, i8* %10126, i64 8
  %10153 = bitcast i8* %10152 to <8 x i8>*
  %t2902 = load <8 x i8>, <8 x i8>* %10153, align 8, !tbaa !438
  %10154 = getelementptr inbounds i8, i8* %10130, i64 8
  %10155 = bitcast i8* %10154 to <8 x i8>*
  %t2903 = load <8 x i8>, <8 x i8>* %10155, align 8, !tbaa !438
  %10156 = getelementptr inbounds i8, i8* %10134, i64 8
  %10157 = bitcast i8* %10156 to <8 x i8>*
  %t2904 = load <8 x i8>, <8 x i8>* %10157, align 8, !tbaa !438
  %10158 = getelementptr inbounds i8, i8* %10138, i64 8
  %10159 = bitcast i8* %10158 to <8 x i8>*
  %t2905 = load <8 x i8>, <8 x i8>* %10159, align 8, !tbaa !438
  %10160 = getelementptr inbounds i8, i8* %10142, i64 8
  %10161 = bitcast i8* %10160 to <8 x i8>*
  %t2906 = load <8 x i8>, <8 x i8>* %10161, align 8, !tbaa !438
  %10162 = zext <8 x i8> %t2881 to <8 x i16>
  %10163 = bitcast <8 x i16> %10162 to <2 x i64>
  %10164 = shufflevector <2 x i64> %10163, <2 x i64> undef, <1 x i32> zeroinitializer
  %10165 = bitcast <1 x i64> %10164 to <4 x i16>
  %10166 = zext <8 x i8> %t2883 to <8 x i16>
  %10167 = bitcast <8 x i16> %10166 to <2 x i64>
  %10168 = shufflevector <2 x i64> %10167, <2 x i64> undef, <1 x i32> zeroinitializer
  %10169 = zext <8 x i8> %t2885 to <8 x i16>
  %10170 = bitcast <8 x i16> %10169 to <2 x i64>
  %10171 = shufflevector <2 x i64> %10170, <2 x i64> undef, <1 x i32> zeroinitializer
  %10172 = bitcast <1 x i64> %10171 to <4 x i16>
  %10173 = zext <8 x i8> %t2887 to <8 x i16>
  %10174 = bitcast <8 x i16> %10173 to <2 x i64>
  %10175 = shufflevector <2 x i64> %10174, <2 x i64> undef, <1 x i32> zeroinitializer
  %10176 = zext <8 x i8> %t2889 to <8 x i16>
  %10177 = bitcast <8 x i16> %10176 to <2 x i64>
  %10178 = shufflevector <2 x i64> %10177, <2 x i64> undef, <1 x i32> zeroinitializer
  %10179 = bitcast <1 x i64> %10178 to <4 x i16>
  %10180 = zext <8 x i8> %t2891 to <8 x i16>
  %10181 = bitcast <8 x i16> %10180 to <2 x i64>
  %10182 = shufflevector <2 x i64> %10181, <2 x i64> undef, <1 x i32> zeroinitializer
  %10183 = zext <8 x i8> %t2893 to <8 x i16>
  %10184 = bitcast <8 x i16> %10183 to <2 x i64>
  %10185 = shufflevector <2 x i64> %10184, <2 x i64> undef, <1 x i32> zeroinitializer
  %10186 = bitcast <1 x i64> %10185 to <4 x i16>
  %10187 = zext <8 x i8> %t2895 to <8 x i16>
  %10188 = bitcast <8 x i16> %10187 to <2 x i64>
  %10189 = shufflevector <2 x i64> %10188, <2 x i64> undef, <1 x i32> zeroinitializer
  %10190 = zext <8 x i8> %t2897 to <8 x i16>
  %10191 = bitcast <8 x i16> %10190 to <2 x i64>
  %10192 = shufflevector <2 x i64> %10191, <2 x i64> undef, <1 x i32> zeroinitializer
  %10193 = bitcast <1 x i64> %10192 to <4 x i16>
  %.cast2649 = bitcast <1 x i64> %10168 to <4 x i16>
  %10194 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2649, <4 x i16> %9872) #11
  %10195 = sext <4 x i16> %10165 to <4 x i32>
  %10196 = mul nsw <4 x i32> %10195, %9910
  %.cast2652 = bitcast <1 x i64> %10175 to <4 x i16>
  %10197 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2652, <4 x i16> %9881) #11
  %10198 = sext <4 x i16> %10172 to <4 x i32>
  %10199 = mul nsw <4 x i32> %10198, %9914
  %.cast2655 = bitcast <1 x i64> %10182 to <4 x i16>
  %10200 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2655, <4 x i16> %9890) #11
  %10201 = sext <4 x i16> %10179 to <4 x i32>
  %10202 = mul nsw <4 x i32> %10201, %9918
  %.cast2658 = bitcast <1 x i64> %10189 to <4 x i16>
  %10203 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2658, <4 x i16> %9899) #11
  %10204 = sext <4 x i16> %10186 to <4 x i32>
  %10205 = mul nsw <4 x i32> %10204, %9922
  %10206 = sext <4 x i16> %10193 to <4 x i32>
  %10207 = mul nsw <4 x i32> %10206, %9924
  %10208 = add <4 x i32> %10196, %9781
  %10209 = add <4 x i32> %10208, %10199
  %10210 = add <4 x i32> %10209, %10202
  %10211 = add <4 x i32> %10210, %10205
  %10212 = add <4 x i32> %10211, %10207
  %10213 = add <4 x i32> %10212, %10194
  %10214 = add <4 x i32> %10213, %10197
  %10215 = add <4 x i32> %10214, %10200
  %10216 = add <4 x i32> %10215, %10203
  %10217 = shufflevector <2 x i64> %10163, <2 x i64> undef, <1 x i32> <i32 1>
  %10218 = bitcast <1 x i64> %10217 to <4 x i16>
  %10219 = shufflevector <2 x i64> %10167, <2 x i64> undef, <1 x i32> <i32 1>
  %10220 = shufflevector <2 x i64> %10170, <2 x i64> undef, <1 x i32> <i32 1>
  %10221 = bitcast <1 x i64> %10220 to <4 x i16>
  %10222 = shufflevector <2 x i64> %10174, <2 x i64> undef, <1 x i32> <i32 1>
  %10223 = shufflevector <2 x i64> %10177, <2 x i64> undef, <1 x i32> <i32 1>
  %10224 = bitcast <1 x i64> %10223 to <4 x i16>
  %10225 = shufflevector <2 x i64> %10181, <2 x i64> undef, <1 x i32> <i32 1>
  %10226 = shufflevector <2 x i64> %10184, <2 x i64> undef, <1 x i32> <i32 1>
  %10227 = bitcast <1 x i64> %10226 to <4 x i16>
  %10228 = shufflevector <2 x i64> %10188, <2 x i64> undef, <1 x i32> <i32 1>
  %10229 = shufflevector <2 x i64> %10191, <2 x i64> undef, <1 x i32> <i32 1>
  %10230 = bitcast <1 x i64> %10229 to <4 x i16>
  %.cast2661 = bitcast <1 x i64> %10219 to <4 x i16>
  %10231 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2661, <4 x i16> %9939) #11
  %10232 = sext <4 x i16> %10218 to <4 x i32>
  %10233 = mul nsw <4 x i32> %10232, %9961
  %.cast2664 = bitcast <1 x i64> %10222 to <4 x i16>
  %10234 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2664, <4 x i16> %9944) #11
  %10235 = sext <4 x i16> %10221 to <4 x i32>
  %10236 = mul nsw <4 x i32> %10235, %9965
  %.cast2667 = bitcast <1 x i64> %10225 to <4 x i16>
  %10237 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2667, <4 x i16> %9949) #11
  %10238 = sext <4 x i16> %10224 to <4 x i32>
  %10239 = mul nsw <4 x i32> %10238, %9969
  %.cast2670 = bitcast <1 x i64> %10228 to <4 x i16>
  %10240 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2670, <4 x i16> %9954) #11
  %10241 = sext <4 x i16> %10227 to <4 x i32>
  %10242 = mul nsw <4 x i32> %10241, %9973
  %10243 = sext <4 x i16> %10230 to <4 x i32>
  %10244 = mul nsw <4 x i32> %10243, %9975
  %10245 = add <4 x i32> %10233, %9782
  %10246 = add <4 x i32> %10245, %10236
  %10247 = add <4 x i32> %10246, %10239
  %10248 = add <4 x i32> %10247, %10242
  %10249 = add <4 x i32> %10248, %10244
  %10250 = add <4 x i32> %10249, %10231
  %10251 = add <4 x i32> %10250, %10234
  %10252 = add <4 x i32> %10251, %10237
  %10253 = add <4 x i32> %10252, %10240
  %10254 = zext <8 x i8> %t2898 to <8 x i16>
  %10255 = bitcast <8 x i16> %10254 to <2 x i64>
  %10256 = shufflevector <2 x i64> %10255, <2 x i64> undef, <1 x i32> zeroinitializer
  %10257 = bitcast <1 x i64> %10256 to <4 x i16>
  %10258 = zext <8 x i8> %t2899 to <8 x i16>
  %10259 = bitcast <8 x i16> %10258 to <2 x i64>
  %10260 = shufflevector <2 x i64> %10259, <2 x i64> undef, <1 x i32> zeroinitializer
  %10261 = zext <8 x i8> %t2900 to <8 x i16>
  %10262 = bitcast <8 x i16> %10261 to <2 x i64>
  %10263 = shufflevector <2 x i64> %10262, <2 x i64> undef, <1 x i32> zeroinitializer
  %10264 = bitcast <1 x i64> %10263 to <4 x i16>
  %10265 = zext <8 x i8> %t2901 to <8 x i16>
  %10266 = bitcast <8 x i16> %10265 to <2 x i64>
  %10267 = shufflevector <2 x i64> %10266, <2 x i64> undef, <1 x i32> zeroinitializer
  %10268 = zext <8 x i8> %t2902 to <8 x i16>
  %10269 = bitcast <8 x i16> %10268 to <2 x i64>
  %10270 = shufflevector <2 x i64> %10269, <2 x i64> undef, <1 x i32> zeroinitializer
  %10271 = bitcast <1 x i64> %10270 to <4 x i16>
  %10272 = zext <8 x i8> %t2903 to <8 x i16>
  %10273 = bitcast <8 x i16> %10272 to <2 x i64>
  %10274 = shufflevector <2 x i64> %10273, <2 x i64> undef, <1 x i32> zeroinitializer
  %10275 = zext <8 x i8> %t2904 to <8 x i16>
  %10276 = bitcast <8 x i16> %10275 to <2 x i64>
  %10277 = shufflevector <2 x i64> %10276, <2 x i64> undef, <1 x i32> zeroinitializer
  %10278 = bitcast <1 x i64> %10277 to <4 x i16>
  %10279 = zext <8 x i8> %t2905 to <8 x i16>
  %10280 = bitcast <8 x i16> %10279 to <2 x i64>
  %10281 = shufflevector <2 x i64> %10280, <2 x i64> undef, <1 x i32> zeroinitializer
  %10282 = zext <8 x i8> %t2906 to <8 x i16>
  %10283 = bitcast <8 x i16> %10282 to <2 x i64>
  %10284 = shufflevector <2 x i64> %10283, <2 x i64> undef, <1 x i32> zeroinitializer
  %10285 = bitcast <1 x i64> %10284 to <4 x i16>
  %.cast2673 = bitcast <1 x i64> %10260 to <4 x i16>
  %10286 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2673, <4 x i16> %9992) #11
  %10287 = sext <4 x i16> %10257 to <4 x i32>
  %10288 = mul nsw <4 x i32> %10287, %10030
  %.cast2676 = bitcast <1 x i64> %10267 to <4 x i16>
  %10289 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2676, <4 x i16> %10001) #11
  %10290 = sext <4 x i16> %10264 to <4 x i32>
  %10291 = mul nsw <4 x i32> %10290, %10034
  %.cast2679 = bitcast <1 x i64> %10274 to <4 x i16>
  %10292 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2679, <4 x i16> %10010) #11
  %10293 = sext <4 x i16> %10271 to <4 x i32>
  %10294 = mul nsw <4 x i32> %10293, %10038
  %.cast2682 = bitcast <1 x i64> %10281 to <4 x i16>
  %10295 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2682, <4 x i16> %10019) #11
  %10296 = sext <4 x i16> %10278 to <4 x i32>
  %10297 = mul nsw <4 x i32> %10296, %10042
  %10298 = sext <4 x i16> %10285 to <4 x i32>
  %10299 = mul nsw <4 x i32> %10298, %10044
  %10300 = add <4 x i32> %10288, %9783
  %10301 = add <4 x i32> %10300, %10291
  %10302 = add <4 x i32> %10301, %10294
  %10303 = add <4 x i32> %10302, %10297
  %10304 = add <4 x i32> %10303, %10299
  %10305 = add <4 x i32> %10304, %10286
  %10306 = add <4 x i32> %10305, %10289
  %10307 = add <4 x i32> %10306, %10292
  %10308 = add <4 x i32> %10307, %10295
  %10309 = shufflevector <2 x i64> %10255, <2 x i64> undef, <1 x i32> <i32 1>
  %10310 = bitcast <1 x i64> %10309 to <4 x i16>
  %10311 = shufflevector <2 x i64> %10259, <2 x i64> undef, <1 x i32> <i32 1>
  %10312 = shufflevector <2 x i64> %10262, <2 x i64> undef, <1 x i32> <i32 1>
  %10313 = bitcast <1 x i64> %10312 to <4 x i16>
  %10314 = shufflevector <2 x i64> %10266, <2 x i64> undef, <1 x i32> <i32 1>
  %10315 = shufflevector <2 x i64> %10269, <2 x i64> undef, <1 x i32> <i32 1>
  %10316 = bitcast <1 x i64> %10315 to <4 x i16>
  %10317 = shufflevector <2 x i64> %10273, <2 x i64> undef, <1 x i32> <i32 1>
  %10318 = shufflevector <2 x i64> %10276, <2 x i64> undef, <1 x i32> <i32 1>
  %10319 = bitcast <1 x i64> %10318 to <4 x i16>
  %10320 = shufflevector <2 x i64> %10280, <2 x i64> undef, <1 x i32> <i32 1>
  %10321 = shufflevector <2 x i64> %10283, <2 x i64> undef, <1 x i32> <i32 1>
  %10322 = bitcast <1 x i64> %10321 to <4 x i16>
  %.cast2685 = bitcast <1 x i64> %10311 to <4 x i16>
  %10323 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2685, <4 x i16> %10059) #11
  %10324 = sext <4 x i16> %10310 to <4 x i32>
  %10325 = mul nsw <4 x i32> %10324, %10081
  %.cast2688 = bitcast <1 x i64> %10314 to <4 x i16>
  %10326 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2688, <4 x i16> %10064) #11
  %10327 = sext <4 x i16> %10313 to <4 x i32>
  %10328 = mul nsw <4 x i32> %10327, %10085
  %.cast2691 = bitcast <1 x i64> %10317 to <4 x i16>
  %10329 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2691, <4 x i16> %10069) #11
  %10330 = sext <4 x i16> %10316 to <4 x i32>
  %10331 = mul nsw <4 x i32> %10330, %10089
  %.cast2694 = bitcast <1 x i64> %10320 to <4 x i16>
  %10332 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2694, <4 x i16> %10074) #11
  %10333 = sext <4 x i16> %10319 to <4 x i32>
  %10334 = mul nsw <4 x i32> %10333, %10093
  %10335 = sext <4 x i16> %10322 to <4 x i32>
  %10336 = mul nsw <4 x i32> %10335, %10095
  %10337 = add <4 x i32> %10325, %9784
  %10338 = add <4 x i32> %10337, %10328
  %10339 = add <4 x i32> %10338, %10331
  %10340 = add <4 x i32> %10339, %10334
  %10341 = add <4 x i32> %10340, %10336
  %10342 = add <4 x i32> %10341, %10323
  %10343 = add <4 x i32> %10342, %10326
  %10344 = add <4 x i32> %10343, %10329
  %10345 = add <4 x i32> %10344, %10332
  %t2908 = add nsw i32 %t2851, %t2275374
  %10346 = sext i32 %t2908 to i64
  %10347 = shl nsw i64 %10346, 4
  %10348 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10347
  %10349 = bitcast i8* %10348 to <8 x i8>*
  %t2909 = load <8 x i8>, <8 x i8>* %10349, align 16, !tbaa !438
  %t2910 = add nsw i32 %t2851, %t2273373
  %10350 = sext i32 %t2910 to i64
  %10351 = shl nsw i64 %10350, 4
  %10352 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10351
  %10353 = bitcast i8* %10352 to <8 x i8>*
  %t2911 = load <8 x i8>, <8 x i8>* %10353, align 16, !tbaa !438
  %t2912 = add nsw i32 %t2851, %t2271364
  %10354 = sext i32 %t2912 to i64
  %10355 = shl nsw i64 %10354, 4
  %10356 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10355
  %10357 = bitcast i8* %10356 to <8 x i8>*
  %t2913 = load <8 x i8>, <8 x i8>* %10357, align 16, !tbaa !438
  %t2914 = add nsw i32 %t2851, %t2268372
  %10358 = sext i32 %t2914 to i64
  %10359 = shl nsw i64 %10358, 4
  %10360 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10359
  %10361 = bitcast i8* %10360 to <8 x i8>*
  %t2915 = load <8 x i8>, <8 x i8>* %10361, align 16, !tbaa !438
  %t2916 = add nsw i32 %t2851, %t2266371
  %10362 = sext i32 %t2916 to i64
  %10363 = shl nsw i64 %10362, 4
  %10364 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10363
  %10365 = bitcast i8* %10364 to <8 x i8>*
  %t2917 = load <8 x i8>, <8 x i8>* %10365, align 16, !tbaa !438
  %t2918 = add nsw i32 %t2851, %t2264363
  %10366 = sext i32 %t2918 to i64
  %10367 = shl nsw i64 %10366, 4
  %10368 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10367
  %10369 = bitcast i8* %10368 to <8 x i8>*
  %t2919 = load <8 x i8>, <8 x i8>* %10369, align 16, !tbaa !438
  %t2920 = add nsw i32 %t2851, %t2261368
  %10370 = sext i32 %t2920 to i64
  %10371 = shl nsw i64 %10370, 4
  %10372 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10371
  %10373 = bitcast i8* %10372 to <8 x i8>*
  %t2921 = load <8 x i8>, <8 x i8>* %10373, align 16, !tbaa !438
  %t2922 = add nsw i32 %t2851, %t2259367
  %10374 = sext i32 %t2922 to i64
  %10375 = shl nsw i64 %10374, 4
  %10376 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10375
  %10377 = bitcast i8* %10376 to <8 x i8>*
  %t2923 = load <8 x i8>, <8 x i8>* %10377, align 16, !tbaa !438
  %t2924 = add nsw i32 %t2851, %t2257361
  %10378 = sext i32 %t2924 to i64
  %10379 = shl nsw i64 %10378, 4
  %10380 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10379
  %10381 = bitcast i8* %10380 to <8 x i8>*
  %t2925 = load <8 x i8>, <8 x i8>* %10381, align 16, !tbaa !438
  %10382 = getelementptr inbounds i8, i8* %10348, i64 8
  %10383 = bitcast i8* %10382 to <8 x i8>*
  %t2926 = load <8 x i8>, <8 x i8>* %10383, align 8, !tbaa !438
  %10384 = getelementptr inbounds i8, i8* %10352, i64 8
  %10385 = bitcast i8* %10384 to <8 x i8>*
  %t2927 = load <8 x i8>, <8 x i8>* %10385, align 8, !tbaa !438
  %10386 = getelementptr inbounds i8, i8* %10356, i64 8
  %10387 = bitcast i8* %10386 to <8 x i8>*
  %t2928 = load <8 x i8>, <8 x i8>* %10387, align 8, !tbaa !438
  %10388 = getelementptr inbounds i8, i8* %10360, i64 8
  %10389 = bitcast i8* %10388 to <8 x i8>*
  %t2929 = load <8 x i8>, <8 x i8>* %10389, align 8, !tbaa !438
  %10390 = getelementptr inbounds i8, i8* %10364, i64 8
  %10391 = bitcast i8* %10390 to <8 x i8>*
  %t2930 = load <8 x i8>, <8 x i8>* %10391, align 8, !tbaa !438
  %10392 = getelementptr inbounds i8, i8* %10368, i64 8
  %10393 = bitcast i8* %10392 to <8 x i8>*
  %t2931 = load <8 x i8>, <8 x i8>* %10393, align 8, !tbaa !438
  %10394 = getelementptr inbounds i8, i8* %10372, i64 8
  %10395 = bitcast i8* %10394 to <8 x i8>*
  %t2932 = load <8 x i8>, <8 x i8>* %10395, align 8, !tbaa !438
  %10396 = getelementptr inbounds i8, i8* %10376, i64 8
  %10397 = bitcast i8* %10396 to <8 x i8>*
  %t2933 = load <8 x i8>, <8 x i8>* %10397, align 8, !tbaa !438
  %10398 = getelementptr inbounds i8, i8* %10380, i64 8
  %10399 = bitcast i8* %10398 to <8 x i8>*
  %t2934 = load <8 x i8>, <8 x i8>* %10399, align 8, !tbaa !438
  %10400 = zext <8 x i8> %t2909 to <8 x i16>
  %10401 = bitcast <8 x i16> %10400 to <2 x i64>
  %10402 = shufflevector <2 x i64> %10401, <2 x i64> undef, <1 x i32> zeroinitializer
  %10403 = bitcast <1 x i64> %10402 to <4 x i16>
  %10404 = zext <8 x i8> %t2911 to <8 x i16>
  %10405 = bitcast <8 x i16> %10404 to <2 x i64>
  %10406 = shufflevector <2 x i64> %10405, <2 x i64> undef, <1 x i32> zeroinitializer
  %10407 = zext <8 x i8> %t2913 to <8 x i16>
  %10408 = bitcast <8 x i16> %10407 to <2 x i64>
  %10409 = shufflevector <2 x i64> %10408, <2 x i64> undef, <1 x i32> zeroinitializer
  %10410 = bitcast <1 x i64> %10409 to <4 x i16>
  %10411 = zext <8 x i8> %t2915 to <8 x i16>
  %10412 = bitcast <8 x i16> %10411 to <2 x i64>
  %10413 = shufflevector <2 x i64> %10412, <2 x i64> undef, <1 x i32> zeroinitializer
  %10414 = zext <8 x i8> %t2917 to <8 x i16>
  %10415 = bitcast <8 x i16> %10414 to <2 x i64>
  %10416 = shufflevector <2 x i64> %10415, <2 x i64> undef, <1 x i32> zeroinitializer
  %10417 = bitcast <1 x i64> %10416 to <4 x i16>
  %10418 = zext <8 x i8> %t2919 to <8 x i16>
  %10419 = bitcast <8 x i16> %10418 to <2 x i64>
  %10420 = shufflevector <2 x i64> %10419, <2 x i64> undef, <1 x i32> zeroinitializer
  %10421 = zext <8 x i8> %t2921 to <8 x i16>
  %10422 = bitcast <8 x i16> %10421 to <2 x i64>
  %10423 = shufflevector <2 x i64> %10422, <2 x i64> undef, <1 x i32> zeroinitializer
  %10424 = bitcast <1 x i64> %10423 to <4 x i16>
  %10425 = zext <8 x i8> %t2923 to <8 x i16>
  %10426 = bitcast <8 x i16> %10425 to <2 x i64>
  %10427 = shufflevector <2 x i64> %10426, <2 x i64> undef, <1 x i32> zeroinitializer
  %10428 = zext <8 x i8> %t2925 to <8 x i16>
  %10429 = bitcast <8 x i16> %10428 to <2 x i64>
  %10430 = shufflevector <2 x i64> %10429, <2 x i64> undef, <1 x i32> zeroinitializer
  %10431 = bitcast <1 x i64> %10430 to <4 x i16>
  %.cast2697 = bitcast <1 x i64> %10406 to <4 x i16>
  %10432 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2697, <4 x i16> %9872) #11
  %10433 = sext <4 x i16> %10403 to <4 x i32>
  %10434 = mul nsw <4 x i32> %10433, %9910
  %.cast2700 = bitcast <1 x i64> %10413 to <4 x i16>
  %10435 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2700, <4 x i16> %9881) #11
  %10436 = sext <4 x i16> %10410 to <4 x i32>
  %10437 = mul nsw <4 x i32> %10436, %9914
  %.cast2703 = bitcast <1 x i64> %10420 to <4 x i16>
  %10438 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2703, <4 x i16> %9890) #11
  %10439 = sext <4 x i16> %10417 to <4 x i32>
  %10440 = mul nsw <4 x i32> %10439, %9918
  %.cast2706 = bitcast <1 x i64> %10427 to <4 x i16>
  %10441 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2706, <4 x i16> %9899) #11
  %10442 = sext <4 x i16> %10424 to <4 x i32>
  %10443 = mul nsw <4 x i32> %10442, %9922
  %10444 = sext <4 x i16> %10431 to <4 x i32>
  %10445 = mul nsw <4 x i32> %10444, %9924
  %10446 = add <4 x i32> %10434, %9781
  %10447 = add <4 x i32> %10446, %10437
  %10448 = add <4 x i32> %10447, %10440
  %10449 = add <4 x i32> %10448, %10443
  %10450 = add <4 x i32> %10449, %10445
  %10451 = add <4 x i32> %10450, %10432
  %10452 = add <4 x i32> %10451, %10435
  %10453 = add <4 x i32> %10452, %10438
  %10454 = add <4 x i32> %10453, %10441
  %10455 = shufflevector <2 x i64> %10401, <2 x i64> undef, <1 x i32> <i32 1>
  %10456 = bitcast <1 x i64> %10455 to <4 x i16>
  %10457 = shufflevector <2 x i64> %10405, <2 x i64> undef, <1 x i32> <i32 1>
  %10458 = shufflevector <2 x i64> %10408, <2 x i64> undef, <1 x i32> <i32 1>
  %10459 = bitcast <1 x i64> %10458 to <4 x i16>
  %10460 = shufflevector <2 x i64> %10412, <2 x i64> undef, <1 x i32> <i32 1>
  %10461 = shufflevector <2 x i64> %10415, <2 x i64> undef, <1 x i32> <i32 1>
  %10462 = bitcast <1 x i64> %10461 to <4 x i16>
  %10463 = shufflevector <2 x i64> %10419, <2 x i64> undef, <1 x i32> <i32 1>
  %10464 = shufflevector <2 x i64> %10422, <2 x i64> undef, <1 x i32> <i32 1>
  %10465 = bitcast <1 x i64> %10464 to <4 x i16>
  %10466 = shufflevector <2 x i64> %10426, <2 x i64> undef, <1 x i32> <i32 1>
  %10467 = shufflevector <2 x i64> %10429, <2 x i64> undef, <1 x i32> <i32 1>
  %10468 = bitcast <1 x i64> %10467 to <4 x i16>
  %.cast2709 = bitcast <1 x i64> %10457 to <4 x i16>
  %10469 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2709, <4 x i16> %9939) #11
  %10470 = sext <4 x i16> %10456 to <4 x i32>
  %10471 = mul nsw <4 x i32> %10470, %9961
  %.cast2712 = bitcast <1 x i64> %10460 to <4 x i16>
  %10472 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2712, <4 x i16> %9944) #11
  %10473 = sext <4 x i16> %10459 to <4 x i32>
  %10474 = mul nsw <4 x i32> %10473, %9965
  %.cast2715 = bitcast <1 x i64> %10463 to <4 x i16>
  %10475 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2715, <4 x i16> %9949) #11
  %10476 = sext <4 x i16> %10462 to <4 x i32>
  %10477 = mul nsw <4 x i32> %10476, %9969
  %.cast2718 = bitcast <1 x i64> %10466 to <4 x i16>
  %10478 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2718, <4 x i16> %9954) #11
  %10479 = sext <4 x i16> %10465 to <4 x i32>
  %10480 = mul nsw <4 x i32> %10479, %9973
  %10481 = sext <4 x i16> %10468 to <4 x i32>
  %10482 = mul nsw <4 x i32> %10481, %9975
  %10483 = add <4 x i32> %10471, %9782
  %10484 = add <4 x i32> %10483, %10474
  %10485 = add <4 x i32> %10484, %10477
  %10486 = add <4 x i32> %10485, %10480
  %10487 = add <4 x i32> %10486, %10482
  %10488 = add <4 x i32> %10487, %10469
  %10489 = add <4 x i32> %10488, %10472
  %10490 = add <4 x i32> %10489, %10475
  %10491 = add <4 x i32> %10490, %10478
  %10492 = zext <8 x i8> %t2926 to <8 x i16>
  %10493 = bitcast <8 x i16> %10492 to <2 x i64>
  %10494 = shufflevector <2 x i64> %10493, <2 x i64> undef, <1 x i32> zeroinitializer
  %10495 = bitcast <1 x i64> %10494 to <4 x i16>
  %10496 = zext <8 x i8> %t2927 to <8 x i16>
  %10497 = bitcast <8 x i16> %10496 to <2 x i64>
  %10498 = shufflevector <2 x i64> %10497, <2 x i64> undef, <1 x i32> zeroinitializer
  %10499 = zext <8 x i8> %t2928 to <8 x i16>
  %10500 = bitcast <8 x i16> %10499 to <2 x i64>
  %10501 = shufflevector <2 x i64> %10500, <2 x i64> undef, <1 x i32> zeroinitializer
  %10502 = bitcast <1 x i64> %10501 to <4 x i16>
  %10503 = zext <8 x i8> %t2929 to <8 x i16>
  %10504 = bitcast <8 x i16> %10503 to <2 x i64>
  %10505 = shufflevector <2 x i64> %10504, <2 x i64> undef, <1 x i32> zeroinitializer
  %10506 = zext <8 x i8> %t2930 to <8 x i16>
  %10507 = bitcast <8 x i16> %10506 to <2 x i64>
  %10508 = shufflevector <2 x i64> %10507, <2 x i64> undef, <1 x i32> zeroinitializer
  %10509 = bitcast <1 x i64> %10508 to <4 x i16>
  %10510 = zext <8 x i8> %t2931 to <8 x i16>
  %10511 = bitcast <8 x i16> %10510 to <2 x i64>
  %10512 = shufflevector <2 x i64> %10511, <2 x i64> undef, <1 x i32> zeroinitializer
  %10513 = zext <8 x i8> %t2932 to <8 x i16>
  %10514 = bitcast <8 x i16> %10513 to <2 x i64>
  %10515 = shufflevector <2 x i64> %10514, <2 x i64> undef, <1 x i32> zeroinitializer
  %10516 = bitcast <1 x i64> %10515 to <4 x i16>
  %10517 = zext <8 x i8> %t2933 to <8 x i16>
  %10518 = bitcast <8 x i16> %10517 to <2 x i64>
  %10519 = shufflevector <2 x i64> %10518, <2 x i64> undef, <1 x i32> zeroinitializer
  %10520 = zext <8 x i8> %t2934 to <8 x i16>
  %10521 = bitcast <8 x i16> %10520 to <2 x i64>
  %10522 = shufflevector <2 x i64> %10521, <2 x i64> undef, <1 x i32> zeroinitializer
  %10523 = bitcast <1 x i64> %10522 to <4 x i16>
  %.cast2721 = bitcast <1 x i64> %10498 to <4 x i16>
  %10524 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2721, <4 x i16> %9992) #11
  %10525 = sext <4 x i16> %10495 to <4 x i32>
  %10526 = mul nsw <4 x i32> %10525, %10030
  %.cast2724 = bitcast <1 x i64> %10505 to <4 x i16>
  %10527 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2724, <4 x i16> %10001) #11
  %10528 = sext <4 x i16> %10502 to <4 x i32>
  %10529 = mul nsw <4 x i32> %10528, %10034
  %.cast2727 = bitcast <1 x i64> %10512 to <4 x i16>
  %10530 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2727, <4 x i16> %10010) #11
  %10531 = sext <4 x i16> %10509 to <4 x i32>
  %10532 = mul nsw <4 x i32> %10531, %10038
  %.cast2730 = bitcast <1 x i64> %10519 to <4 x i16>
  %10533 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2730, <4 x i16> %10019) #11
  %10534 = sext <4 x i16> %10516 to <4 x i32>
  %10535 = mul nsw <4 x i32> %10534, %10042
  %10536 = sext <4 x i16> %10523 to <4 x i32>
  %10537 = mul nsw <4 x i32> %10536, %10044
  %10538 = add <4 x i32> %10526, %9783
  %10539 = add <4 x i32> %10538, %10529
  %10540 = add <4 x i32> %10539, %10532
  %10541 = add <4 x i32> %10540, %10535
  %10542 = add <4 x i32> %10541, %10537
  %10543 = add <4 x i32> %10542, %10524
  %10544 = add <4 x i32> %10543, %10527
  %10545 = add <4 x i32> %10544, %10530
  %10546 = add <4 x i32> %10545, %10533
  %10547 = shufflevector <2 x i64> %10493, <2 x i64> undef, <1 x i32> <i32 1>
  %10548 = bitcast <1 x i64> %10547 to <4 x i16>
  %10549 = shufflevector <2 x i64> %10497, <2 x i64> undef, <1 x i32> <i32 1>
  %10550 = shufflevector <2 x i64> %10500, <2 x i64> undef, <1 x i32> <i32 1>
  %10551 = bitcast <1 x i64> %10550 to <4 x i16>
  %10552 = shufflevector <2 x i64> %10504, <2 x i64> undef, <1 x i32> <i32 1>
  %10553 = shufflevector <2 x i64> %10507, <2 x i64> undef, <1 x i32> <i32 1>
  %10554 = bitcast <1 x i64> %10553 to <4 x i16>
  %10555 = shufflevector <2 x i64> %10511, <2 x i64> undef, <1 x i32> <i32 1>
  %10556 = shufflevector <2 x i64> %10514, <2 x i64> undef, <1 x i32> <i32 1>
  %10557 = bitcast <1 x i64> %10556 to <4 x i16>
  %10558 = shufflevector <2 x i64> %10518, <2 x i64> undef, <1 x i32> <i32 1>
  %10559 = shufflevector <2 x i64> %10521, <2 x i64> undef, <1 x i32> <i32 1>
  %10560 = bitcast <1 x i64> %10559 to <4 x i16>
  %.cast2733 = bitcast <1 x i64> %10549 to <4 x i16>
  %10561 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2733, <4 x i16> %10059) #11
  %10562 = sext <4 x i16> %10548 to <4 x i32>
  %10563 = mul nsw <4 x i32> %10562, %10081
  %.cast2736 = bitcast <1 x i64> %10552 to <4 x i16>
  %10564 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2736, <4 x i16> %10064) #11
  %10565 = sext <4 x i16> %10551 to <4 x i32>
  %10566 = mul nsw <4 x i32> %10565, %10085
  %.cast2739 = bitcast <1 x i64> %10555 to <4 x i16>
  %10567 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2739, <4 x i16> %10069) #11
  %10568 = sext <4 x i16> %10554 to <4 x i32>
  %10569 = mul nsw <4 x i32> %10568, %10089
  %.cast2742 = bitcast <1 x i64> %10558 to <4 x i16>
  %10570 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2742, <4 x i16> %10074) #11
  %10571 = sext <4 x i16> %10557 to <4 x i32>
  %10572 = mul nsw <4 x i32> %10571, %10093
  %10573 = sext <4 x i16> %10560 to <4 x i32>
  %10574 = mul nsw <4 x i32> %10573, %10095
  %10575 = add <4 x i32> %10563, %9784
  %10576 = add <4 x i32> %10575, %10566
  %10577 = add <4 x i32> %10576, %10569
  %10578 = add <4 x i32> %10577, %10572
  %10579 = add <4 x i32> %10578, %10574
  %10580 = add <4 x i32> %10579, %10561
  %10581 = add <4 x i32> %10580, %10564
  %10582 = add <4 x i32> %10581, %10567
  %10583 = add <4 x i32> %10582, %10570
  %t2936 = add nsw i32 %t2879, %t2275374
  %10584 = sext i32 %t2936 to i64
  %10585 = shl nsw i64 %10584, 4
  %10586 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10585
  %10587 = bitcast i8* %10586 to <8 x i8>*
  %t2937 = load <8 x i8>, <8 x i8>* %10587, align 16, !tbaa !438
  %t2938 = add nsw i32 %t2879, %t2273373
  %10588 = sext i32 %t2938 to i64
  %10589 = shl nsw i64 %10588, 4
  %10590 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10589
  %10591 = bitcast i8* %10590 to <8 x i8>*
  %t2939 = load <8 x i8>, <8 x i8>* %10591, align 16, !tbaa !438
  %t2940 = add nsw i32 %t2879, %t2271364
  %10592 = sext i32 %t2940 to i64
  %10593 = shl nsw i64 %10592, 4
  %10594 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10593
  %10595 = bitcast i8* %10594 to <8 x i8>*
  %t2941 = load <8 x i8>, <8 x i8>* %10595, align 16, !tbaa !438
  %t2942 = add nsw i32 %t2879, %t2268372
  %10596 = sext i32 %t2942 to i64
  %10597 = shl nsw i64 %10596, 4
  %10598 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10597
  %10599 = bitcast i8* %10598 to <8 x i8>*
  %t2943 = load <8 x i8>, <8 x i8>* %10599, align 16, !tbaa !438
  %t2944 = add nsw i32 %t2879, %t2266371
  %10600 = sext i32 %t2944 to i64
  %10601 = shl nsw i64 %10600, 4
  %10602 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10601
  %10603 = bitcast i8* %10602 to <8 x i8>*
  %t2945 = load <8 x i8>, <8 x i8>* %10603, align 16, !tbaa !438
  %t2946 = add nsw i32 %t2879, %t2264363
  %10604 = sext i32 %t2946 to i64
  %10605 = shl nsw i64 %10604, 4
  %10606 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10605
  %10607 = bitcast i8* %10606 to <8 x i8>*
  %t2947 = load <8 x i8>, <8 x i8>* %10607, align 16, !tbaa !438
  %t2948 = add nsw i32 %t2879, %t2261368
  %10608 = sext i32 %t2948 to i64
  %10609 = shl nsw i64 %10608, 4
  %10610 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10609
  %10611 = bitcast i8* %10610 to <8 x i8>*
  %t2949 = load <8 x i8>, <8 x i8>* %10611, align 16, !tbaa !438
  %t2950 = add nsw i32 %t2879, %t2259367
  %10612 = sext i32 %t2950 to i64
  %10613 = shl nsw i64 %10612, 4
  %10614 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10613
  %10615 = bitcast i8* %10614 to <8 x i8>*
  %t2951 = load <8 x i8>, <8 x i8>* %10615, align 16, !tbaa !438
  %t2952 = add nsw i32 %t2879, %t2257361
  %10616 = sext i32 %t2952 to i64
  %10617 = shl nsw i64 %10616, 4
  %10618 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10617
  %10619 = bitcast i8* %10618 to <8 x i8>*
  %t2953 = load <8 x i8>, <8 x i8>* %10619, align 16, !tbaa !438
  %10620 = getelementptr inbounds i8, i8* %10586, i64 8
  %10621 = bitcast i8* %10620 to <8 x i8>*
  %t2954 = load <8 x i8>, <8 x i8>* %10621, align 8, !tbaa !438
  %10622 = getelementptr inbounds i8, i8* %10590, i64 8
  %10623 = bitcast i8* %10622 to <8 x i8>*
  %t2955 = load <8 x i8>, <8 x i8>* %10623, align 8, !tbaa !438
  %10624 = getelementptr inbounds i8, i8* %10594, i64 8
  %10625 = bitcast i8* %10624 to <8 x i8>*
  %t2956 = load <8 x i8>, <8 x i8>* %10625, align 8, !tbaa !438
  %10626 = getelementptr inbounds i8, i8* %10598, i64 8
  %10627 = bitcast i8* %10626 to <8 x i8>*
  %t2957 = load <8 x i8>, <8 x i8>* %10627, align 8, !tbaa !438
  %10628 = getelementptr inbounds i8, i8* %10602, i64 8
  %10629 = bitcast i8* %10628 to <8 x i8>*
  %t2958 = load <8 x i8>, <8 x i8>* %10629, align 8, !tbaa !438
  %10630 = getelementptr inbounds i8, i8* %10606, i64 8
  %10631 = bitcast i8* %10630 to <8 x i8>*
  %t2959 = load <8 x i8>, <8 x i8>* %10631, align 8, !tbaa !438
  %10632 = getelementptr inbounds i8, i8* %10610, i64 8
  %10633 = bitcast i8* %10632 to <8 x i8>*
  %t2960 = load <8 x i8>, <8 x i8>* %10633, align 8, !tbaa !438
  %10634 = getelementptr inbounds i8, i8* %10614, i64 8
  %10635 = bitcast i8* %10634 to <8 x i8>*
  %t2961 = load <8 x i8>, <8 x i8>* %10635, align 8, !tbaa !438
  %10636 = getelementptr inbounds i8, i8* %10618, i64 8
  %10637 = bitcast i8* %10636 to <8 x i8>*
  %t2962 = load <8 x i8>, <8 x i8>* %10637, align 8, !tbaa !438
  %10638 = zext <8 x i8> %t2937 to <8 x i16>
  %10639 = bitcast <8 x i16> %10638 to <2 x i64>
  %10640 = shufflevector <2 x i64> %10639, <2 x i64> undef, <1 x i32> zeroinitializer
  %10641 = bitcast <1 x i64> %10640 to <4 x i16>
  %10642 = zext <8 x i8> %t2939 to <8 x i16>
  %10643 = bitcast <8 x i16> %10642 to <2 x i64>
  %10644 = shufflevector <2 x i64> %10643, <2 x i64> undef, <1 x i32> zeroinitializer
  %10645 = zext <8 x i8> %t2941 to <8 x i16>
  %10646 = bitcast <8 x i16> %10645 to <2 x i64>
  %10647 = shufflevector <2 x i64> %10646, <2 x i64> undef, <1 x i32> zeroinitializer
  %10648 = bitcast <1 x i64> %10647 to <4 x i16>
  %10649 = zext <8 x i8> %t2943 to <8 x i16>
  %10650 = bitcast <8 x i16> %10649 to <2 x i64>
  %10651 = shufflevector <2 x i64> %10650, <2 x i64> undef, <1 x i32> zeroinitializer
  %10652 = zext <8 x i8> %t2945 to <8 x i16>
  %10653 = bitcast <8 x i16> %10652 to <2 x i64>
  %10654 = shufflevector <2 x i64> %10653, <2 x i64> undef, <1 x i32> zeroinitializer
  %10655 = bitcast <1 x i64> %10654 to <4 x i16>
  %10656 = zext <8 x i8> %t2947 to <8 x i16>
  %10657 = bitcast <8 x i16> %10656 to <2 x i64>
  %10658 = shufflevector <2 x i64> %10657, <2 x i64> undef, <1 x i32> zeroinitializer
  %10659 = zext <8 x i8> %t2949 to <8 x i16>
  %10660 = bitcast <8 x i16> %10659 to <2 x i64>
  %10661 = shufflevector <2 x i64> %10660, <2 x i64> undef, <1 x i32> zeroinitializer
  %10662 = bitcast <1 x i64> %10661 to <4 x i16>
  %10663 = zext <8 x i8> %t2951 to <8 x i16>
  %10664 = bitcast <8 x i16> %10663 to <2 x i64>
  %10665 = shufflevector <2 x i64> %10664, <2 x i64> undef, <1 x i32> zeroinitializer
  %10666 = zext <8 x i8> %t2953 to <8 x i16>
  %10667 = bitcast <8 x i16> %10666 to <2 x i64>
  %10668 = shufflevector <2 x i64> %10667, <2 x i64> undef, <1 x i32> zeroinitializer
  %10669 = bitcast <1 x i64> %10668 to <4 x i16>
  %.cast2745 = bitcast <1 x i64> %10644 to <4 x i16>
  %10670 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2745, <4 x i16> %9872) #11
  %10671 = sext <4 x i16> %10641 to <4 x i32>
  %10672 = mul nsw <4 x i32> %10671, %9910
  %.cast2748 = bitcast <1 x i64> %10651 to <4 x i16>
  %10673 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2748, <4 x i16> %9881) #11
  %10674 = sext <4 x i16> %10648 to <4 x i32>
  %10675 = mul nsw <4 x i32> %10674, %9914
  %.cast2751 = bitcast <1 x i64> %10658 to <4 x i16>
  %10676 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2751, <4 x i16> %9890) #11
  %10677 = sext <4 x i16> %10655 to <4 x i32>
  %10678 = mul nsw <4 x i32> %10677, %9918
  %.cast2754 = bitcast <1 x i64> %10665 to <4 x i16>
  %10679 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2754, <4 x i16> %9899) #11
  %10680 = sext <4 x i16> %10662 to <4 x i32>
  %10681 = mul nsw <4 x i32> %10680, %9922
  %10682 = sext <4 x i16> %10669 to <4 x i32>
  %10683 = mul nsw <4 x i32> %10682, %9924
  %10684 = add <4 x i32> %10672, %9781
  %10685 = add <4 x i32> %10684, %10675
  %10686 = add <4 x i32> %10685, %10678
  %10687 = add <4 x i32> %10686, %10681
  %10688 = add <4 x i32> %10687, %10683
  %10689 = add <4 x i32> %10688, %10670
  %10690 = add <4 x i32> %10689, %10673
  %10691 = add <4 x i32> %10690, %10676
  %10692 = add <4 x i32> %10691, %10679
  %10693 = shufflevector <2 x i64> %10639, <2 x i64> undef, <1 x i32> <i32 1>
  %10694 = bitcast <1 x i64> %10693 to <4 x i16>
  %10695 = shufflevector <2 x i64> %10643, <2 x i64> undef, <1 x i32> <i32 1>
  %10696 = shufflevector <2 x i64> %10646, <2 x i64> undef, <1 x i32> <i32 1>
  %10697 = bitcast <1 x i64> %10696 to <4 x i16>
  %10698 = shufflevector <2 x i64> %10650, <2 x i64> undef, <1 x i32> <i32 1>
  %10699 = shufflevector <2 x i64> %10653, <2 x i64> undef, <1 x i32> <i32 1>
  %10700 = bitcast <1 x i64> %10699 to <4 x i16>
  %10701 = shufflevector <2 x i64> %10657, <2 x i64> undef, <1 x i32> <i32 1>
  %10702 = shufflevector <2 x i64> %10660, <2 x i64> undef, <1 x i32> <i32 1>
  %10703 = bitcast <1 x i64> %10702 to <4 x i16>
  %10704 = shufflevector <2 x i64> %10664, <2 x i64> undef, <1 x i32> <i32 1>
  %10705 = shufflevector <2 x i64> %10667, <2 x i64> undef, <1 x i32> <i32 1>
  %10706 = bitcast <1 x i64> %10705 to <4 x i16>
  %.cast2757 = bitcast <1 x i64> %10695 to <4 x i16>
  %10707 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2757, <4 x i16> %9939) #11
  %10708 = sext <4 x i16> %10694 to <4 x i32>
  %10709 = mul nsw <4 x i32> %10708, %9961
  %.cast2760 = bitcast <1 x i64> %10698 to <4 x i16>
  %10710 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2760, <4 x i16> %9944) #11
  %10711 = sext <4 x i16> %10697 to <4 x i32>
  %10712 = mul nsw <4 x i32> %10711, %9965
  %.cast2763 = bitcast <1 x i64> %10701 to <4 x i16>
  %10713 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2763, <4 x i16> %9949) #11
  %10714 = sext <4 x i16> %10700 to <4 x i32>
  %10715 = mul nsw <4 x i32> %10714, %9969
  %.cast2766 = bitcast <1 x i64> %10704 to <4 x i16>
  %10716 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2766, <4 x i16> %9954) #11
  %10717 = sext <4 x i16> %10703 to <4 x i32>
  %10718 = mul nsw <4 x i32> %10717, %9973
  %10719 = sext <4 x i16> %10706 to <4 x i32>
  %10720 = mul nsw <4 x i32> %10719, %9975
  %10721 = add <4 x i32> %10709, %9782
  %10722 = add <4 x i32> %10721, %10712
  %10723 = add <4 x i32> %10722, %10715
  %10724 = add <4 x i32> %10723, %10718
  %10725 = add <4 x i32> %10724, %10720
  %10726 = add <4 x i32> %10725, %10707
  %10727 = add <4 x i32> %10726, %10710
  %10728 = add <4 x i32> %10727, %10713
  %10729 = add <4 x i32> %10728, %10716
  %10730 = zext <8 x i8> %t2954 to <8 x i16>
  %10731 = bitcast <8 x i16> %10730 to <2 x i64>
  %10732 = shufflevector <2 x i64> %10731, <2 x i64> undef, <1 x i32> zeroinitializer
  %10733 = bitcast <1 x i64> %10732 to <4 x i16>
  %10734 = zext <8 x i8> %t2955 to <8 x i16>
  %10735 = bitcast <8 x i16> %10734 to <2 x i64>
  %10736 = shufflevector <2 x i64> %10735, <2 x i64> undef, <1 x i32> zeroinitializer
  %10737 = zext <8 x i8> %t2956 to <8 x i16>
  %10738 = bitcast <8 x i16> %10737 to <2 x i64>
  %10739 = shufflevector <2 x i64> %10738, <2 x i64> undef, <1 x i32> zeroinitializer
  %10740 = bitcast <1 x i64> %10739 to <4 x i16>
  %10741 = zext <8 x i8> %t2957 to <8 x i16>
  %10742 = bitcast <8 x i16> %10741 to <2 x i64>
  %10743 = shufflevector <2 x i64> %10742, <2 x i64> undef, <1 x i32> zeroinitializer
  %10744 = zext <8 x i8> %t2958 to <8 x i16>
  %10745 = bitcast <8 x i16> %10744 to <2 x i64>
  %10746 = shufflevector <2 x i64> %10745, <2 x i64> undef, <1 x i32> zeroinitializer
  %10747 = bitcast <1 x i64> %10746 to <4 x i16>
  %10748 = zext <8 x i8> %t2959 to <8 x i16>
  %10749 = bitcast <8 x i16> %10748 to <2 x i64>
  %10750 = shufflevector <2 x i64> %10749, <2 x i64> undef, <1 x i32> zeroinitializer
  %10751 = zext <8 x i8> %t2960 to <8 x i16>
  %10752 = bitcast <8 x i16> %10751 to <2 x i64>
  %10753 = shufflevector <2 x i64> %10752, <2 x i64> undef, <1 x i32> zeroinitializer
  %10754 = bitcast <1 x i64> %10753 to <4 x i16>
  %10755 = zext <8 x i8> %t2961 to <8 x i16>
  %10756 = bitcast <8 x i16> %10755 to <2 x i64>
  %10757 = shufflevector <2 x i64> %10756, <2 x i64> undef, <1 x i32> zeroinitializer
  %10758 = zext <8 x i8> %t2962 to <8 x i16>
  %10759 = bitcast <8 x i16> %10758 to <2 x i64>
  %10760 = shufflevector <2 x i64> %10759, <2 x i64> undef, <1 x i32> zeroinitializer
  %10761 = bitcast <1 x i64> %10760 to <4 x i16>
  %.cast2769 = bitcast <1 x i64> %10736 to <4 x i16>
  %10762 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2769, <4 x i16> %9992) #11
  %10763 = sext <4 x i16> %10733 to <4 x i32>
  %10764 = mul nsw <4 x i32> %10763, %10030
  %.cast2772 = bitcast <1 x i64> %10743 to <4 x i16>
  %10765 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2772, <4 x i16> %10001) #11
  %10766 = sext <4 x i16> %10740 to <4 x i32>
  %10767 = mul nsw <4 x i32> %10766, %10034
  %.cast2775 = bitcast <1 x i64> %10750 to <4 x i16>
  %10768 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2775, <4 x i16> %10010) #11
  %10769 = sext <4 x i16> %10747 to <4 x i32>
  %10770 = mul nsw <4 x i32> %10769, %10038
  %.cast2778 = bitcast <1 x i64> %10757 to <4 x i16>
  %10771 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2778, <4 x i16> %10019) #11
  %10772 = sext <4 x i16> %10754 to <4 x i32>
  %10773 = mul nsw <4 x i32> %10772, %10042
  %10774 = sext <4 x i16> %10761 to <4 x i32>
  %10775 = mul nsw <4 x i32> %10774, %10044
  %10776 = add <4 x i32> %10764, %9783
  %10777 = add <4 x i32> %10776, %10767
  %10778 = add <4 x i32> %10777, %10770
  %10779 = add <4 x i32> %10778, %10773
  %10780 = add <4 x i32> %10779, %10775
  %10781 = add <4 x i32> %10780, %10762
  %10782 = add <4 x i32> %10781, %10765
  %10783 = add <4 x i32> %10782, %10768
  %10784 = add <4 x i32> %10783, %10771
  %10785 = shufflevector <2 x i64> %10731, <2 x i64> undef, <1 x i32> <i32 1>
  %10786 = bitcast <1 x i64> %10785 to <4 x i16>
  %10787 = shufflevector <2 x i64> %10735, <2 x i64> undef, <1 x i32> <i32 1>
  %10788 = shufflevector <2 x i64> %10738, <2 x i64> undef, <1 x i32> <i32 1>
  %10789 = bitcast <1 x i64> %10788 to <4 x i16>
  %10790 = shufflevector <2 x i64> %10742, <2 x i64> undef, <1 x i32> <i32 1>
  %10791 = shufflevector <2 x i64> %10745, <2 x i64> undef, <1 x i32> <i32 1>
  %10792 = bitcast <1 x i64> %10791 to <4 x i16>
  %10793 = shufflevector <2 x i64> %10749, <2 x i64> undef, <1 x i32> <i32 1>
  %10794 = shufflevector <2 x i64> %10752, <2 x i64> undef, <1 x i32> <i32 1>
  %10795 = bitcast <1 x i64> %10794 to <4 x i16>
  %10796 = shufflevector <2 x i64> %10756, <2 x i64> undef, <1 x i32> <i32 1>
  %10797 = shufflevector <2 x i64> %10759, <2 x i64> undef, <1 x i32> <i32 1>
  %10798 = bitcast <1 x i64> %10797 to <4 x i16>
  %.cast2781 = bitcast <1 x i64> %10787 to <4 x i16>
  %10799 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2781, <4 x i16> %10059) #11
  %10800 = sext <4 x i16> %10786 to <4 x i32>
  %10801 = mul nsw <4 x i32> %10800, %10081
  %.cast2784 = bitcast <1 x i64> %10790 to <4 x i16>
  %10802 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2784, <4 x i16> %10064) #11
  %10803 = sext <4 x i16> %10789 to <4 x i32>
  %10804 = mul nsw <4 x i32> %10803, %10085
  %.cast2787 = bitcast <1 x i64> %10793 to <4 x i16>
  %10805 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2787, <4 x i16> %10069) #11
  %10806 = sext <4 x i16> %10792 to <4 x i32>
  %10807 = mul nsw <4 x i32> %10806, %10089
  %.cast2790 = bitcast <1 x i64> %10796 to <4 x i16>
  %10808 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2790, <4 x i16> %10074) #11
  %10809 = sext <4 x i16> %10795 to <4 x i32>
  %10810 = mul nsw <4 x i32> %10809, %10093
  %10811 = sext <4 x i16> %10798 to <4 x i32>
  %10812 = mul nsw <4 x i32> %10811, %10095
  %10813 = add <4 x i32> %10801, %9784
  %10814 = add <4 x i32> %10813, %10804
  %10815 = add <4 x i32> %10814, %10807
  %10816 = add <4 x i32> %10815, %10810
  %10817 = add <4 x i32> %10816, %10812
  %10818 = add <4 x i32> %10817, %10799
  %10819 = add <4 x i32> %10818, %10802
  %10820 = add <4 x i32> %10819, %10805
  %10821 = add <4 x i32> %10820, %10808
  br label %"consume convolved406"

next_bb388:                                       ; preds = %"for output.s0.x.xo381"
  br i1 %9377, label %"for convolved.s1.r19$y392.preheader", label %"consume convolved406", !prof !391

"for convolved.s1.r19$y392.preheader":            ; preds = %next_bb388
  %10822 = mul nsw i32 %9811, %stride_x
  %10823 = add nsw i32 %9811, 1
  %10824 = mul nsw i32 %10823, %stride_x
  %10825 = sub nsw i32 %10824, %t2178
  %10826 = sub nsw i32 %10822, %t2178
  br i1 %9375, label %"for convolved.s1.r19$y392.us", label %"consume convolved406", !prof !391

"for convolved.s1.r19$y392.us":                   ; preds = %"for convolved.s1.r19$y392.preheader", %"end for convolved.s1.r19$x404.loopexit.us"
  %indvars.iv6497 = phi i64 [ %indvars.iv.next6498, %"end for convolved.s1.r19$x404.loopexit.us" ], [ 0, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.930.8.us = phi <4 x i32> [ %10974, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9784, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.868.8.us = phi <4 x i32> [ %10969, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9783, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.806.8.us = phi <4 x i32> [ %10962, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9782, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.744.8.us = phi <4 x i32> [ %10957, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9781, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.682.8.us = phi <4 x i32> [ %10944, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9784, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.620.8.us = phi <4 x i32> [ %10939, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9783, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.558.8.us = phi <4 x i32> [ %10932, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9782, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.496.8.us = phi <4 x i32> [ %10927, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9781, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.434.8.us = phi <4 x i32> [ %10914, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9784, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.372.8.us = phi <4 x i32> [ %10909, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9783, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.310.8.us = phi <4 x i32> [ %10902, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9782, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.248.8.us = phi <4 x i32> [ %10897, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9781, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.186.8.us = phi <4 x i32> [ %10884, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9784, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.124.8.us = phi <4 x i32> [ %10875, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9783, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.62.8.us = phi <4 x i32> [ %10864, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9782, %"for convolved.s1.r19$y392.preheader" ]
  %convolved944.sroa.0.8.us = phi <4 x i32> [ %10855, %"end for convolved.s1.r19$x404.loopexit.us" ], [ %9781, %"for convolved.s1.r19$y392.preheader" ]
  %10827 = mul i64 %indvars.iv6497, %9475
  %10828 = mul nsw i64 %indvars.iv6497, %9465
  %10829 = trunc i64 %10827 to i32
  %10830 = add i32 %9805, %10829
  %10831 = mul i32 %10830, %9445
  %10832 = trunc i64 %10827 to i32
  %10833 = add i32 %9806, %10832
  %10834 = mul i32 %10833, %9445
  br label %"for convolved.s1.r19$x403.us"

"for convolved.s1.r19$x403.us":                   ; preds = %"for convolved.s1.r19$y392.us", %"for convolved.s1.r19$x403.us"
  %indvars.iv6494 = phi i64 [ 0, %"for convolved.s1.r19$y392.us" ], [ %indvars.iv.next6495, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.930.9.us = phi <4 x i32> [ %convolved944.sroa.930.8.us, %"for convolved.s1.r19$y392.us" ], [ %10974, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.868.9.us = phi <4 x i32> [ %convolved944.sroa.868.8.us, %"for convolved.s1.r19$y392.us" ], [ %10969, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.806.9.us = phi <4 x i32> [ %convolved944.sroa.806.8.us, %"for convolved.s1.r19$y392.us" ], [ %10962, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.744.9.us = phi <4 x i32> [ %convolved944.sroa.744.8.us, %"for convolved.s1.r19$y392.us" ], [ %10957, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.682.9.us = phi <4 x i32> [ %convolved944.sroa.682.8.us, %"for convolved.s1.r19$y392.us" ], [ %10944, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.620.9.us = phi <4 x i32> [ %convolved944.sroa.620.8.us, %"for convolved.s1.r19$y392.us" ], [ %10939, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.558.9.us = phi <4 x i32> [ %convolved944.sroa.558.8.us, %"for convolved.s1.r19$y392.us" ], [ %10932, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.496.9.us = phi <4 x i32> [ %convolved944.sroa.496.8.us, %"for convolved.s1.r19$y392.us" ], [ %10927, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.434.9.us = phi <4 x i32> [ %convolved944.sroa.434.8.us, %"for convolved.s1.r19$y392.us" ], [ %10914, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.372.9.us = phi <4 x i32> [ %convolved944.sroa.372.8.us, %"for convolved.s1.r19$y392.us" ], [ %10909, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.310.9.us = phi <4 x i32> [ %convolved944.sroa.310.8.us, %"for convolved.s1.r19$y392.us" ], [ %10902, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.248.9.us = phi <4 x i32> [ %convolved944.sroa.248.8.us, %"for convolved.s1.r19$y392.us" ], [ %10897, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.186.9.us = phi <4 x i32> [ %convolved944.sroa.186.8.us, %"for convolved.s1.r19$y392.us" ], [ %10884, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.124.9.us = phi <4 x i32> [ %convolved944.sroa.124.8.us, %"for convolved.s1.r19$y392.us" ], [ %10875, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.62.9.us = phi <4 x i32> [ %convolved944.sroa.62.8.us, %"for convolved.s1.r19$y392.us" ], [ %10864, %"for convolved.s1.r19$x403.us" ]
  %convolved944.sroa.0.9.us = phi <4 x i32> [ %convolved944.sroa.0.8.us, %"for convolved.s1.r19$y392.us" ], [ %10855, %"for convolved.s1.r19$x403.us" ]
  %10835 = add nsw i64 %indvars.iv6494, %10828
  %10836 = trunc i64 %indvars.iv6494 to i32
  %10837 = mul i32 %10836, %a614
  %t2298402.us = add i32 %10837, %10831
  %t2992.us = add i32 %t2298402.us, %10826
  %10838 = sext i32 %t2992.us to i64
  %10839 = shl nsw i64 %10838, 4
  %10840 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10839
  %10841 = bitcast i8* %10840 to <8 x i8>*
  %t2993.us = load <8 x i8>, <8 x i8>* %10841, align 16, !tbaa !438
  %10842 = getelementptr inbounds i8, i8* %10840, i64 8
  %10843 = bitcast i8* %10842 to <8 x i8>*
  %t2994.us = load <8 x i8>, <8 x i8>* %10843, align 8, !tbaa !438
  %10844 = shl nsw i64 %10835, 4
  %10845 = getelementptr inbounds i16, i16* %filter_zeroed272, i64 %10844
  %10846 = bitcast i16* %10845 to <4 x i16>*
  %10847 = load <4 x i16>, <4 x i16>* %10846, align 16, !tbaa !395
  %10848 = zext <8 x i8> %t2993.us to <8 x i16>
  %10849 = bitcast <8 x i16> %10848 to <2 x i64>
  %10850 = shufflevector <2 x i64> %10849, <2 x i64> undef, <1 x i32> zeroinitializer
  %10851 = bitcast <1 x i64> %10850 to <4 x i16>
  %10852 = sext <4 x i16> %10847 to <4 x i32>
  %10853 = sext <4 x i16> %10851 to <4 x i32>
  %10854 = mul nsw <4 x i32> %10853, %10852
  %10855 = add <4 x i32> %10854, %convolved944.sroa.0.9.us
  %10856 = getelementptr inbounds i16, i16* %10845, i64 4
  %10857 = bitcast i16* %10856 to <4 x i16>*
  %10858 = load <4 x i16>, <4 x i16>* %10857, align 8, !tbaa !395
  %10859 = shufflevector <2 x i64> %10849, <2 x i64> undef, <1 x i32> <i32 1>
  %10860 = bitcast <1 x i64> %10859 to <4 x i16>
  %10861 = sext <4 x i16> %10858 to <4 x i32>
  %10862 = sext <4 x i16> %10860 to <4 x i32>
  %10863 = mul nsw <4 x i32> %10862, %10861
  %10864 = add <4 x i32> %10863, %convolved944.sroa.62.9.us
  %10865 = getelementptr inbounds i16, i16* %10845, i64 8
  %10866 = bitcast i16* %10865 to <4 x i16>*
  %10867 = load <4 x i16>, <4 x i16>* %10866, align 16, !tbaa !395
  %10868 = zext <8 x i8> %t2994.us to <8 x i16>
  %10869 = bitcast <8 x i16> %10868 to <2 x i64>
  %10870 = shufflevector <2 x i64> %10869, <2 x i64> undef, <1 x i32> zeroinitializer
  %10871 = bitcast <1 x i64> %10870 to <4 x i16>
  %10872 = sext <4 x i16> %10867 to <4 x i32>
  %10873 = sext <4 x i16> %10871 to <4 x i32>
  %10874 = mul nsw <4 x i32> %10873, %10872
  %10875 = add <4 x i32> %10874, %convolved944.sroa.124.9.us
  %10876 = getelementptr inbounds i16, i16* %10845, i64 12
  %10877 = bitcast i16* %10876 to <4 x i16>*
  %10878 = load <4 x i16>, <4 x i16>* %10877, align 8, !tbaa !395
  %10879 = shufflevector <2 x i64> %10869, <2 x i64> undef, <1 x i32> <i32 1>
  %10880 = bitcast <1 x i64> %10879 to <4 x i16>
  %10881 = sext <4 x i16> %10878 to <4 x i32>
  %10882 = sext <4 x i16> %10880 to <4 x i32>
  %10883 = mul nsw <4 x i32> %10881, %10882
  %10884 = add <4 x i32> %10883, %convolved944.sroa.186.9.us
  %t2996.us = add i32 %t2298402.us, %10825
  %10885 = sext i32 %t2996.us to i64
  %10886 = shl nsw i64 %10885, 4
  %10887 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10886
  %10888 = bitcast i8* %10887 to <8 x i8>*
  %t2997.us = load <8 x i8>, <8 x i8>* %10888, align 16, !tbaa !438
  %10889 = getelementptr inbounds i8, i8* %10887, i64 8
  %10890 = bitcast i8* %10889 to <8 x i8>*
  %t2998.us = load <8 x i8>, <8 x i8>* %10890, align 8, !tbaa !438
  %10891 = zext <8 x i8> %t2997.us to <8 x i16>
  %10892 = bitcast <8 x i16> %10891 to <2 x i64>
  %10893 = shufflevector <2 x i64> %10892, <2 x i64> undef, <1 x i32> zeroinitializer
  %10894 = bitcast <1 x i64> %10893 to <4 x i16>
  %10895 = sext <4 x i16> %10894 to <4 x i32>
  %10896 = mul nsw <4 x i32> %10895, %10852
  %10897 = add <4 x i32> %10896, %convolved944.sroa.248.9.us
  %10898 = shufflevector <2 x i64> %10892, <2 x i64> undef, <1 x i32> <i32 1>
  %10899 = bitcast <1 x i64> %10898 to <4 x i16>
  %10900 = sext <4 x i16> %10899 to <4 x i32>
  %10901 = mul nsw <4 x i32> %10900, %10861
  %10902 = add <4 x i32> %10901, %convolved944.sroa.310.9.us
  %10903 = zext <8 x i8> %t2998.us to <8 x i16>
  %10904 = bitcast <8 x i16> %10903 to <2 x i64>
  %10905 = shufflevector <2 x i64> %10904, <2 x i64> undef, <1 x i32> zeroinitializer
  %10906 = bitcast <1 x i64> %10905 to <4 x i16>
  %10907 = sext <4 x i16> %10906 to <4 x i32>
  %10908 = mul nsw <4 x i32> %10907, %10872
  %10909 = add <4 x i32> %10908, %convolved944.sroa.372.9.us
  %10910 = shufflevector <2 x i64> %10904, <2 x i64> undef, <1 x i32> <i32 1>
  %10911 = bitcast <1 x i64> %10910 to <4 x i16>
  %10912 = sext <4 x i16> %10911 to <4 x i32>
  %10913 = mul nsw <4 x i32> %10912, %10881
  %10914 = add <4 x i32> %10913, %convolved944.sroa.434.9.us
  %t2300400.us = add i32 %10837, %10834
  %t3000.us = add i32 %t2300400.us, %10826
  %10915 = sext i32 %t3000.us to i64
  %10916 = shl nsw i64 %10915, 4
  %10917 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10916
  %10918 = bitcast i8* %10917 to <8 x i8>*
  %t3001.us = load <8 x i8>, <8 x i8>* %10918, align 16, !tbaa !438
  %10919 = getelementptr inbounds i8, i8* %10917, i64 8
  %10920 = bitcast i8* %10919 to <8 x i8>*
  %t3002.us = load <8 x i8>, <8 x i8>* %10920, align 8, !tbaa !438
  %10921 = zext <8 x i8> %t3001.us to <8 x i16>
  %10922 = bitcast <8 x i16> %10921 to <2 x i64>
  %10923 = shufflevector <2 x i64> %10922, <2 x i64> undef, <1 x i32> zeroinitializer
  %10924 = bitcast <1 x i64> %10923 to <4 x i16>
  %10925 = sext <4 x i16> %10924 to <4 x i32>
  %10926 = mul nsw <4 x i32> %10925, %10852
  %10927 = add <4 x i32> %10926, %convolved944.sroa.496.9.us
  %10928 = shufflevector <2 x i64> %10922, <2 x i64> undef, <1 x i32> <i32 1>
  %10929 = bitcast <1 x i64> %10928 to <4 x i16>
  %10930 = sext <4 x i16> %10929 to <4 x i32>
  %10931 = mul nsw <4 x i32> %10930, %10861
  %10932 = add <4 x i32> %10931, %convolved944.sroa.558.9.us
  %10933 = zext <8 x i8> %t3002.us to <8 x i16>
  %10934 = bitcast <8 x i16> %10933 to <2 x i64>
  %10935 = shufflevector <2 x i64> %10934, <2 x i64> undef, <1 x i32> zeroinitializer
  %10936 = bitcast <1 x i64> %10935 to <4 x i16>
  %10937 = sext <4 x i16> %10936 to <4 x i32>
  %10938 = mul nsw <4 x i32> %10937, %10872
  %10939 = add <4 x i32> %10938, %convolved944.sroa.620.9.us
  %10940 = shufflevector <2 x i64> %10934, <2 x i64> undef, <1 x i32> <i32 1>
  %10941 = bitcast <1 x i64> %10940 to <4 x i16>
  %10942 = sext <4 x i16> %10941 to <4 x i32>
  %10943 = mul nsw <4 x i32> %10942, %10881
  %10944 = add <4 x i32> %10943, %convolved944.sroa.682.9.us
  %t3004.us = add i32 %t2300400.us, %10825
  %10945 = sext i32 %t3004.us to i64
  %10946 = shl nsw i64 %10945, 4
  %10947 = getelementptr inbounds i8, i8* %resampled_input312, i64 %10946
  %10948 = bitcast i8* %10947 to <8 x i8>*
  %t3005.us = load <8 x i8>, <8 x i8>* %10948, align 16, !tbaa !438
  %10949 = getelementptr inbounds i8, i8* %10947, i64 8
  %10950 = bitcast i8* %10949 to <8 x i8>*
  %t3006.us = load <8 x i8>, <8 x i8>* %10950, align 8, !tbaa !438
  %10951 = zext <8 x i8> %t3005.us to <8 x i16>
  %10952 = bitcast <8 x i16> %10951 to <2 x i64>
  %10953 = shufflevector <2 x i64> %10952, <2 x i64> undef, <1 x i32> zeroinitializer
  %10954 = bitcast <1 x i64> %10953 to <4 x i16>
  %10955 = sext <4 x i16> %10954 to <4 x i32>
  %10956 = mul nsw <4 x i32> %10955, %10852
  %10957 = add <4 x i32> %10956, %convolved944.sroa.744.9.us
  %10958 = shufflevector <2 x i64> %10952, <2 x i64> undef, <1 x i32> <i32 1>
  %10959 = bitcast <1 x i64> %10958 to <4 x i16>
  %10960 = sext <4 x i16> %10959 to <4 x i32>
  %10961 = mul nsw <4 x i32> %10960, %10861
  %10962 = add <4 x i32> %10961, %convolved944.sroa.806.9.us
  %10963 = zext <8 x i8> %t3006.us to <8 x i16>
  %10964 = bitcast <8 x i16> %10963 to <2 x i64>
  %10965 = shufflevector <2 x i64> %10964, <2 x i64> undef, <1 x i32> zeroinitializer
  %10966 = bitcast <1 x i64> %10965 to <4 x i16>
  %10967 = sext <4 x i16> %10966 to <4 x i32>
  %10968 = mul nsw <4 x i32> %10967, %10872
  %10969 = add <4 x i32> %10968, %convolved944.sroa.868.9.us
  %10970 = shufflevector <2 x i64> %10964, <2 x i64> undef, <1 x i32> <i32 1>
  %10971 = bitcast <1 x i64> %10970 to <4 x i16>
  %10972 = sext <4 x i16> %10971 to <4 x i32>
  %10973 = mul nsw <4 x i32> %10972, %10881
  %10974 = add <4 x i32> %10973, %convolved944.sroa.930.9.us
  %indvars.iv.next6495 = add nuw nsw i64 %indvars.iv6494, 1
  %.not2598.us = icmp eq i64 %indvars.iv.next6495, %9464
  br i1 %.not2598.us, label %"end for convolved.s1.r19$x404.loopexit.us", label %"for convolved.s1.r19$x403.us"

"end for convolved.s1.r19$x404.loopexit.us":      ; preds = %"for convolved.s1.r19$x403.us"
  %indvars.iv.next6498 = add nuw nsw i64 %indvars.iv6497, 1
  %.not2597.us = icmp eq i64 %indvars.iv.next6498, %9467
  br i1 %.not2597.us, label %"consume convolved406", label %"for convolved.s1.r19$y392.us"

"consume convolved406":                           ; preds = %"end for convolved.s1.r19$x404.loopexit.us", %"for convolved.s1.r19$y392.preheader", %next_bb388, %then_bb387
  %convolved944.sroa.930.11 = phi <4 x i32> [ %10821, %then_bb387 ], [ %9784, %next_bb388 ], [ %9784, %"for convolved.s1.r19$y392.preheader" ], [ %10974, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.868.11 = phi <4 x i32> [ %10784, %then_bb387 ], [ %9783, %next_bb388 ], [ %9783, %"for convolved.s1.r19$y392.preheader" ], [ %10969, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.806.11 = phi <4 x i32> [ %10729, %then_bb387 ], [ %9782, %next_bb388 ], [ %9782, %"for convolved.s1.r19$y392.preheader" ], [ %10962, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.744.11 = phi <4 x i32> [ %10692, %then_bb387 ], [ %9781, %next_bb388 ], [ %9781, %"for convolved.s1.r19$y392.preheader" ], [ %10957, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.682.11 = phi <4 x i32> [ %10583, %then_bb387 ], [ %9784, %next_bb388 ], [ %9784, %"for convolved.s1.r19$y392.preheader" ], [ %10944, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.620.11 = phi <4 x i32> [ %10546, %then_bb387 ], [ %9783, %next_bb388 ], [ %9783, %"for convolved.s1.r19$y392.preheader" ], [ %10939, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.558.11 = phi <4 x i32> [ %10491, %then_bb387 ], [ %9782, %next_bb388 ], [ %9782, %"for convolved.s1.r19$y392.preheader" ], [ %10932, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.496.11 = phi <4 x i32> [ %10454, %then_bb387 ], [ %9781, %next_bb388 ], [ %9781, %"for convolved.s1.r19$y392.preheader" ], [ %10927, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.434.11 = phi <4 x i32> [ %10345, %then_bb387 ], [ %9784, %next_bb388 ], [ %9784, %"for convolved.s1.r19$y392.preheader" ], [ %10914, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.372.11 = phi <4 x i32> [ %10308, %then_bb387 ], [ %9783, %next_bb388 ], [ %9783, %"for convolved.s1.r19$y392.preheader" ], [ %10909, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.310.11 = phi <4 x i32> [ %10253, %then_bb387 ], [ %9782, %next_bb388 ], [ %9782, %"for convolved.s1.r19$y392.preheader" ], [ %10902, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.248.11 = phi <4 x i32> [ %10216, %then_bb387 ], [ %9781, %next_bb388 ], [ %9781, %"for convolved.s1.r19$y392.preheader" ], [ %10897, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.186.11 = phi <4 x i32> [ %10106, %then_bb387 ], [ %9784, %next_bb388 ], [ %9784, %"for convolved.s1.r19$y392.preheader" ], [ %10884, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.124.11 = phi <4 x i32> [ %10055, %then_bb387 ], [ %9783, %next_bb388 ], [ %9783, %"for convolved.s1.r19$y392.preheader" ], [ %10875, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.62.11 = phi <4 x i32> [ %9986, %then_bb387 ], [ %9782, %next_bb388 ], [ %9782, %"for convolved.s1.r19$y392.preheader" ], [ %10864, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %convolved944.sroa.0.11 = phi <4 x i32> [ %9935, %then_bb387 ], [ %9781, %next_bb388 ], [ %9781, %"for convolved.s1.r19$y392.preheader" ], [ %10855, %"end for convolved.s1.r19$x404.loopexit.us" ]
  %10975 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.0.11, <4 x i32> %9447) #11
  %10976 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %10975, <4 x i32> %9450) #11
  %10977 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %10976, <4 x i32> %9453) #11
  %10978 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %10977) #11
  %10979 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.62.11, <4 x i32> %9447) #11
  %10980 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %10979, <4 x i32> %9450) #11
  %10981 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %10980, <4 x i32> %9453) #11
  %10982 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %10981) #11
  %10983 = shufflevector <4 x i16> %10978, <4 x i16> %10982, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %10984 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %10983, <8 x i16> %9456) #11
  %10985 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %10984) #11
  %10986 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.124.11, <4 x i32> %9447) #11
  %10987 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %10986, <4 x i32> %9450) #11
  %10988 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %10987, <4 x i32> %9453) #11
  %10989 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %10988) #11
  %10990 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.186.11, <4 x i32> %9447) #11
  %10991 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %10990, <4 x i32> %9450) #11
  %10992 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %10991, <4 x i32> %9453) #11
  %10993 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %10992) #11
  %10994 = shufflevector <4 x i16> %10989, <4 x i16> %10993, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %10995 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %10994, <8 x i16> %9456) #11
  %10996 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %10995) #11
  %10997 = shufflevector <8 x i8> %10985, <8 x i8> %10996, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %10998 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9458, <16 x i8> %10997) #11
  %10999 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %10998, <16 x i8> %9460) #11
  %11000 = sext i32 %output.s0.x.x.base.s384 to i64
  %11001 = add nsw i64 %11000, %9461
  %11002 = mul nsw i64 %11001, %9462
  %11003 = add nsw i64 %11002, %9807
  %11004 = getelementptr inbounds i8, i8* %59, i64 %11003
  %11005 = bitcast i8* %11004 to <16 x i8>*
  store <16 x i8> %10999, <16 x i8>* %11005, align 1, !tbaa !515
  %11006 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.248.11, <4 x i32> %9447) #11
  %11007 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11006, <4 x i32> %9450) #11
  %11008 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11007, <4 x i32> %9453) #11
  %11009 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11008) #11
  %11010 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.310.11, <4 x i32> %9447) #11
  %11011 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11010, <4 x i32> %9450) #11
  %11012 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11011, <4 x i32> %9453) #11
  %11013 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11012) #11
  %11014 = shufflevector <4 x i16> %11009, <4 x i16> %11013, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11015 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %11014, <8 x i16> %9456) #11
  %11016 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %11015) #11
  %11017 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.372.11, <4 x i32> %9447) #11
  %11018 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11017, <4 x i32> %9450) #11
  %11019 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11018, <4 x i32> %9453) #11
  %11020 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11019) #11
  %11021 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.434.11, <4 x i32> %9447) #11
  %11022 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11021, <4 x i32> %9450) #11
  %11023 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11022, <4 x i32> %9453) #11
  %11024 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11023) #11
  %11025 = shufflevector <4 x i16> %11020, <4 x i16> %11024, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11026 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %11025, <8 x i16> %9456) #11
  %11027 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %11026) #11
  %11028 = shufflevector <8 x i8> %11016, <8 x i8> %11027, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %11029 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9458, <16 x i8> %11028) #11
  %11030 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %11029, <16 x i8> %9460) #11
  %11031 = add nsw i64 %11001, 1
  %11032 = mul nsw i64 %11031, %9462
  %11033 = add nsw i64 %11032, %9807
  %11034 = getelementptr inbounds i8, i8* %59, i64 %11033
  %11035 = bitcast i8* %11034 to <16 x i8>*
  store <16 x i8> %11030, <16 x i8>* %11035, align 1, !tbaa !515
  %11036 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.496.11, <4 x i32> %9447) #11
  %11037 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11036, <4 x i32> %9450) #11
  %11038 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11037, <4 x i32> %9453) #11
  %11039 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11038) #11
  %11040 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.558.11, <4 x i32> %9447) #11
  %11041 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11040, <4 x i32> %9450) #11
  %11042 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11041, <4 x i32> %9453) #11
  %11043 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11042) #11
  %11044 = shufflevector <4 x i16> %11039, <4 x i16> %11043, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11045 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %11044, <8 x i16> %9456) #11
  %11046 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %11045) #11
  %11047 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.620.11, <4 x i32> %9447) #11
  %11048 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11047, <4 x i32> %9450) #11
  %11049 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11048, <4 x i32> %9453) #11
  %11050 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11049) #11
  %11051 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.682.11, <4 x i32> %9447) #11
  %11052 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11051, <4 x i32> %9450) #11
  %11053 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11052, <4 x i32> %9453) #11
  %11054 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11053) #11
  %11055 = shufflevector <4 x i16> %11050, <4 x i16> %11054, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11056 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %11055, <8 x i16> %9456) #11
  %11057 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %11056) #11
  %11058 = shufflevector <8 x i8> %11046, <8 x i8> %11057, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %11059 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9458, <16 x i8> %11058) #11
  %11060 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %11059, <16 x i8> %9460) #11
  %11061 = add nsw i64 %11002, %9808
  %11062 = getelementptr inbounds i8, i8* %59, i64 %11061
  %11063 = bitcast i8* %11062 to <16 x i8>*
  store <16 x i8> %11060, <16 x i8>* %11063, align 1, !tbaa !515
  %11064 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.744.11, <4 x i32> %9447) #11
  %11065 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11064, <4 x i32> %9450) #11
  %11066 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11065, <4 x i32> %9453) #11
  %11067 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11066) #11
  %11068 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.806.11, <4 x i32> %9447) #11
  %11069 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11068, <4 x i32> %9450) #11
  %11070 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11069, <4 x i32> %9453) #11
  %11071 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11070) #11
  %11072 = shufflevector <4 x i16> %11067, <4 x i16> %11071, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11073 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %11072, <8 x i16> %9456) #11
  %11074 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %11073) #11
  %11075 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.868.11, <4 x i32> %9447) #11
  %11076 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11075, <4 x i32> %9450) #11
  %11077 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11076, <4 x i32> %9453) #11
  %11078 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11077) #11
  %11079 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.930.11, <4 x i32> %9447) #11
  %11080 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %11079, <4 x i32> %9450) #11
  %11081 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %11080, <4 x i32> %9453) #11
  %11082 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %11081) #11
  %11083 = shufflevector <4 x i16> %11078, <4 x i16> %11082, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11084 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %11083, <8 x i16> %9456) #11
  %11085 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %11084) #11
  %11086 = shufflevector <8 x i8> %11074, <8 x i8> %11085, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %11087 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %9458, <16 x i8> %11086) #11
  %11088 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %11087, <16 x i8> %9460) #11
  %11089 = add nsw i64 %11032, %9808
  %11090 = getelementptr inbounds i8, i8* %59, i64 %11089
  %11091 = bitcast i8* %11090 to <16 x i8>*
  store <16 x i8> %11088, <16 x i8>* %11091, align 1, !tbaa !515
  %11092 = add nuw nsw i32 %output.s0.x.xo383, 1
  %.not2596 = icmp eq i32 %11092, %t2197260
  br i1 %.not2596, label %"end for output.s0.x.xo382", label %"for output.s0.x.xo381"

then_bb407:                                       ; preds = %next_bb238
  %t2334409 = icmp slt i32 %a614, 0
  %11093 = add nsw i32 %46, -1
  %t2335410 = mul nsw i32 %11093, %a614
  %t2336411 = icmp slt i32 %stride_x, 0
  %11094 = add nsw i32 %62, 3
  %t2337412 = ashr i32 %11094, 2
  %t2338413 = and i32 %11094, -4
  %t2339414 = select i1 %t2334409, i32 %t2335410, i32 0
  %t2340415 = icmp slt i32 %a613, 0
  %11095 = add nsw i32 %48, -1
  %t2341416 = mul nsw i32 %11095, %a613
  %t2342417 = icmp slt i32 %stride_y, 0
  %11096 = add nsw i32 %65, 3
  %t2343418 = ashr i32 %11096, 2
  %t2344419 = and i32 %11096, -4
  %t2345420 = select i1 %t2340415, i32 %t2341416, i32 0
  %11097 = icmp eq i32 %46, 3
  %11098 = icmp eq i32 %48, 3
  %t2346421 = and i1 %11097, %11098
  %a614.op1799 = shl i32 %a614, 1
  %t2347422 = select i1 %t2334409, i32 %a614.op1799, i32 0
  %11099 = add nsw i32 %62, -1
  %t2348423 = and i32 %11099, -4
  %a613.op1800 = shl i32 %a613, 1
  %t2349424 = select i1 %t2340415, i32 %a613.op1800, i32 0
  %11100 = add nsw i32 %65, -1
  %t2350425 = and i32 %11100, -4
  %t2327426 = icmp eq i32 %depth_multiplier, 1
  %11101 = add nuw nsw i32 %45, 15
  %t2305427 = ashr i32 %11101, 4
  %11102 = icmp sgt i32 %a613, 0
  %11103 = select i1 %11102, i32 %a613, i32 0
  %t2317428 = shl nuw nsw i32 %11103, 1
  %11104 = icmp sgt i32 %a614, 0
  %11105 = select i1 %11104, i32 %a614, i32 0
  %t2324429 = shl nuw nsw i32 %11105, 1
  %11106 = select i1 %t2346421, i32 %t2349424, i32 %t2345420
  %11107 = select i1 %t2342417, i32 %t2350425, i32 -3
  %11108 = add i32 %64, 3
  %11109 = add i32 %11108, %11107
  %11110 = mul nsw i32 %11109, %stride_y
  %11111 = select i1 %t2346421, i32 %t2347422, i32 %t2339414
  %11112 = select i1 %t2336411, i32 %t2348423, i32 -3
  %11113 = add i32 %61, 3
  %11114 = add i32 %11113, %11112
  %11115 = mul nsw i32 %11114, %stride_x
  %11116 = select i1 %t2342417, i32 %t2344419, i32 1
  %11117 = add i32 %64, -1
  %11118 = add i32 %11117, %11116
  %11119 = mul nsw i32 %11118, %stride_y
  %11120 = select i1 %t2340415, i32 0, i32 %t2341416
  %11121 = add nsw i32 %t2344419, -1
  %11122 = select i1 %t2342417, i32 0, i32 %11121
  %11123 = add nsw i32 %11122, %64
  %11124 = mul nsw i32 %11123, %stride_y
  %11125 = select i1 %t2336411, i32 %t2338413, i32 1
  %11126 = add i32 %61, -1
  %11127 = add i32 %11126, %11125
  %11128 = mul nsw i32 %11127, %stride_x
  %t2311 = add nsw i32 %11128, %t2339414
  %11129 = select i1 %t2334409, i32 0, i32 %t2335410
  %11130 = add nsw i32 %t2338413, -1
  %11131 = select i1 %t2336411, i32 0, i32 %11130
  %11132 = add nsw i32 %11131, %61
  %11133 = mul nsw i32 %11132, %stride_x
  %11134 = mul nsw i32 %55, %54
  %11135 = mul nsw i32 %58, %56
  %11136 = mul nsw i32 %53, %52
  %11137 = add nsw i32 %11135, %11136
  %b294 = add nsw i32 %45, -16
  %11138 = icmp sgt i32 %46, 0
  %11139 = select i1 %11138, i32 %46, i32 0
  %t4606 = zext i32 %11139 to i64
  %11140 = icmp sgt i32 %48, 0
  %11141 = select i1 %11140, i32 %48, i32 0
  %t4607 = zext i32 %11141 to i64
  %11142 = shl nuw nsw i64 %t4606, 5
  %11143 = mul i64 %11142, %t4607
  %11144 = or i64 %11143, 6
  %11145 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8***
  %11146 = sext i32 %47 to i64
  %11147 = zext i8 %filter_zero to i16
  %11148 = insertelement <8 x i16> undef, i16 %11147, i32 0
  %11149 = shufflevector <8 x i16> %11148, <8 x i16> undef, <8 x i32> zeroinitializer
  %11150 = bitcast [64 x i32]* %sum_filter945 to <4 x i32>*
  %11151 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 4
  %11152 = bitcast i32* %11151 to <4 x i32>*
  %11153 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 8
  %11154 = bitcast i32* %11153 to <4 x i32>*
  %11155 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 12
  %11156 = bitcast i32* %11155 to <4 x i32>*
  %11157 = bitcast i8* %42 to i32*
  %11158 = zext i8 %input_zero to i32
  %11159 = insertelement <4 x i32> undef, i32 %11158, i32 0
  %11160 = shufflevector <4 x i32> %11159, <4 x i32> undef, <4 x i32> zeroinitializer
  %11161 = bitcast [16 x i32]* %offset_c947 to <4 x i32>*
  %11162 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 4
  %11163 = bitcast i32* %11162 to <4 x i32>*
  %11164 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 8
  %11165 = bitcast i32* %11164 to <4 x i32>*
  %11166 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c947, i64 0, i64 12
  %11167 = bitcast i32* %11166 to <4 x i32>*
  %b302 = add nsw i32 %11115, %11111
  %a301 = add nsw i32 %11119, %t2345420
  %b303 = add nsw i32 %11110, %11106
  %11168 = select i1 %t2346421, i32 %t2324429, i32 %11129
  %11169 = or i32 %11099, 3
  %11170 = select i1 %t2336411, i32 0, i32 %11169
  %11171 = add nsw i32 %11170, %61
  %11172 = mul nsw i32 %11171, %stride_x
  %a302 = add nsw i32 %11172, %11168
  %b304 = add nsw i32 %11133, %11129
  %11173 = icmp sgt i32 %a302, %b304
  %11174 = select i1 %11173, i32 %a302, i32 %b304
  %11175 = icmp slt i32 %b302, %t2311
  %11176 = select i1 %11175, i32 %b302, i32 %t2311
  %11177 = select i1 %t2346421, i32 %t2317428, i32 %11120
  %11178 = or i32 %11100, 3
  %11179 = select i1 %t2342417, i32 0, i32 %11178
  %11180 = add nsw i32 %11179, %64
  %11181 = mul nsw i32 %11180, %stride_y
  %a304 = add nsw i32 %11181, %11177
  %b306 = add nsw i32 %11124, %11120
  %11182 = icmp sgt i32 %a304, %b306
  %11183 = select i1 %11182, i32 %a304, i32 %b306
  %11184 = icmp slt i32 %b303, %a301
  %11185 = select i1 %11184, i32 %b303, i32 %a301
  %t2359465 = sub nsw i32 %b304, %t2311
  %t2358466 = sub nsw i32 %b306, %a301
  %11186 = icmp sgt i32 %57, 0
  %11187 = sub nsw i32 %11183, %11185
  %a314 = add nsw i32 %11187, 1
  %11188 = sub nsw i32 %11174, %11176
  %a313 = add nsw i32 %11188, 1
  %.inv1803 = icmp slt i32 %11188, 0
  %11189 = select i1 %.inv1803, i32 0, i32 %a313
  %t4608 = zext i32 %11189 to i64
  %.inv1804 = icmp slt i32 %11187, 0
  %11190 = select i1 %.inv1804, i32 0, i32 %a314
  %t4609 = zext i32 %11190 to i64
  %t4610 = shl nuw nsw i64 %t4608, 4
  %11191 = mul i64 %t4610, %t4609
  %11192 = or i64 %11191, 3
  %11193 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8***
  %11194 = icmp sgt i32 %t2358466, -1
  %11195 = sub i32 %a301, %11185
  %11196 = sub i32 %t2311, %11176
  %11197 = add i32 %11134, %11137
  %11198 = icmp sgt i32 %t2359465, -1
  %11199 = icmp eq i32 %depth_multiplier, 0
  %t4613 = select i1 %11199, <16 x i32> <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>, <16 x i32> zeroinitializer
  %depth_multiplier.lobit = ashr i32 %depth_multiplier, 31
  %11200 = insertelement <16 x i32> undef, i32 %depth_multiplier, i32 0
  %11201 = shufflevector <16 x i32> %11200, <16 x i32> undef, <16 x i32> zeroinitializer
  %11202 = sub nsw <16 x i32> %11201, %t4613
  %11203 = xor i32 %depth_multiplier.lobit, -1
  %11204 = sub nsw i32 %11203, %depth_multiplier.lobit
  %11205 = insertelement <16 x i32> undef, i32 %11204, i32 0
  %11206 = shufflevector <16 x i32> %11205, <16 x i32> undef, <16 x i32> zeroinitializer
  %11207 = xor <16 x i32> %t4613, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  %11208 = sext i32 %t2311 to i64
  %11209 = sext i32 %53 to i64
  %.neg5187 = mul i32 %66, %64
  %.neg5188 = mul i32 %63, %61
  %.neg5189 = mul i32 %67, %56
  %11210 = icmp sgt i32 %65, 0
  %11211 = sub nsw i32 %a613.op1800, %11185
  %11212 = sub nsw i32 %a613, %11185
  %11213 = sub nsw i32 %a614, %11176
  %11214 = sub nsw i32 %a614.op1799, %11176
  %reass.add5192 = add i32 %.neg5187, %.neg5188
  %reass.add5193 = add i32 %reass.add5192, %.neg5189
  %11215 = icmp sgt i32 %62, 0
  %b524 = add nsw i32 %11115, %t2339414
  %11216 = icmp slt i32 %11128, %11115
  %11217 = select i1 %11216, i32 %t2311, i32 %b524
  %b528 = add nsw i32 %11110, %t2345420
  %11218 = icmp slt i32 %11119, %11110
  %11219 = select i1 %11218, i32 %a301, i32 %b528
  %a527 = add nsw i32 %11172, %11129
  %11220 = icmp sgt i32 %11172, %11133
  %11221 = select i1 %11220, i32 %a527, i32 %b304
  %11222 = sub nsw i32 %11221, %11217
  %11223 = add nsw i32 %11222, 1
  %t3786 = shl nuw i32 1, %output_shift
  %11224 = insertelement <4 x i32> undef, i32 %output_multiplier, i32 0
  %11225 = shufflevector <4 x i32> %11224, <4 x i32> undef, <4 x i32> zeroinitializer
  %11226 = ashr i32 %t3786, 1
  %11227 = insertelement <4 x i32> undef, i32 %11226, i32 0
  %11228 = shufflevector <4 x i32> %11227, <4 x i32> undef, <4 x i32> zeroinitializer
  %11229 = sub i32 0, %output_shift
  %11230 = insertelement <4 x i32> undef, i32 %11229, i32 0
  %11231 = shufflevector <4 x i32> %11230, <4 x i32> undef, <4 x i32> zeroinitializer
  %11232 = zext i8 %output_zero to i16
  %11233 = insertelement <8 x i16> undef, i16 %11232, i32 0
  %11234 = shufflevector <8 x i16> %11233, <8 x i16> undef, <8 x i32> zeroinitializer
  %11235 = insertelement <16 x i8> undef, i8 %output_max, i32 0
  %11236 = shufflevector <16 x i8> %11235, <16 x i8> undef, <16 x i32> zeroinitializer
  %11237 = insertelement <16 x i8> undef, i8 %output_min, i32 0
  %11238 = shufflevector <16 x i8> %11237, <16 x i8> undef, <16 x i32> zeroinitializer
  %11239 = sext i32 %61 to i64
  %11240 = sext i32 %63 to i64
  %11241 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8*
  %11242 = zext i32 %46 to i64
  %11243 = sext i32 %46 to i64
  %11244 = sext i32 %49 to i64
  %11245 = zext i32 %48 to i64
  %11246 = zext i32 %t2359465 to i64
  %11247 = sext i32 %11195 to i64
  %11248 = sext i32 %a313 to i64
  %11249 = sext i32 %11196 to i64
  %11250 = sext i32 %a301 to i64
  %11251 = sext i32 %55 to i64
  %11252 = zext i32 %t2358466 to i64
  %11253 = zext i32 %a613 to i64
  %11254 = zext i32 %t2337412 to i64
  %11255 = zext i32 %t2343418 to i64
  %11256 = sext i32 %56 to i64
  %11257 = sext i32 %58 to i64
  %11258 = bitcast [64 x i32]* %sum_filter945 to i8*
  %11259 = bitcast [64 x i32]* %sum_filter945 to i8*
  %11260 = bitcast [64 x i32]* %sum_filter945 to i8*
  %zext6469 = zext i32 %57 to i64
  %11261 = or i32 %t2358466, %t2359465
  %11262 = icmp slt i32 %11261, 0
  br label %"for output.s0.c.co432"

next_bb408:                                       ; preds = %next_bb238
  %11263 = icmp sgt i32 %45, 0
  br i1 %11263, label %then_bb614, label %after_bb236

"for output.s0.c.co432":                          ; preds = %then_bb407, %"end for output.s0.b.rebased469"
  %.8 = phi i8* [ %.05065, %then_bb407 ], [ %.9, %"end for output.s0.b.rebased469" ]
  %output.s0.c.co434 = phi i32 [ 0, %then_bb407 ], [ %11430, %"end for output.s0.b.rebased469" ]
  %a292 = shl nsw i32 %output.s0.c.co434, 4
  %11264 = icmp slt i32 %a292, %b294
  %output.s0.c.c.base435 = select i1 %11264, i32 %a292, i32 %b294
  %11265 = load i64, i64* %.fca.1.gep, align 8, !tbaa !385
  %cmp.i3200 = icmp ult i64 %11265, %11144
  %11266 = load i8*, i8** %.fca.0.gep, align 8, !tbaa !387
  br i1 %cmp.i3200, label %if.then.i3203, label %pseudostack_alloc.exit3216, !prof !388

if.then.i3203:                                    ; preds = %"for output.s0.c.co432"
  %tobool1.not.i3202 = icmp ne i8* %11266, null
  %11267 = load i64, i64* %.fca.2.gep, align 8
  %cmp2.i3205 = icmp ugt i64 %11267, 16384
  %or.cond5132 = and i1 %tobool1.not.i3202, %cmp2.i3205
  br i1 %or.cond5132, label %if.then3.i3207, label %if.end.i3211

if.then3.i3207:                                   ; preds = %if.then.i3203
  call void @halide_free(i8* null, i8* nonnull %11266) #15
  %.pre6509 = load i64, i64* %.fca.2.gep, align 8, !tbaa !389
  br label %if.end.i3211

if.end.i3211:                                     ; preds = %if.then3.i3207, %if.then.i3203
  %11268 = phi i64 [ %.pre6509, %if.then3.i3207 ], [ %11267, %if.then.i3203 ]
  %add.i3209 = add i64 %11268, %11144
  store i64 %add.i3209, i64* %.fca.2.gep, align 8, !tbaa !389
  %cmp7.i3210 = icmp ugt i64 %add.i3209, 16384
  br i1 %cmp7.i3210, label %if.then8.i3213, label %if.end11.i3215

if.then8.i3213:                                   ; preds = %if.end.i3211
  %call.i3212 = call i8* @halide_malloc(i8* null, i64 %11144) #15
  br label %if.end11.i3215

if.end11.i3215:                                   ; preds = %if.then8.i3213, %if.end.i3211
  %storemerge.i3214 = phi i8* [ %call.i3212, %if.then8.i3213 ], [ null, %if.end.i3211 ]
  store i8* %storemerge.i3214, i8** %.fca.0.gep, align 8, !tbaa !387
  store i64 %11144, i64* %.fca.1.gep, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3216

pseudostack_alloc.exit3216:                       ; preds = %"for output.s0.c.co432", %if.end11.i3215
  %11269 = phi i8* [ %storemerge.i3214, %if.end11.i3215 ], [ %11266, %"for output.s0.c.co432" ]
  %11270 = bitcast i8* %11269 to i16*
  %.not1801 = icmp eq i8* %11269, null
  br i1 %.not1801, label %then_bb437, label %"produce filter_zeroed439", !prof !390

then_bb437:                                       ; preds = %pseudostack_alloc.exit3216
  %11271 = alloca i8*, i64 %11144, align 16
  %11272 = bitcast i8** %11271 to i16*
  store i8** %11271, i8*** %11145, align 8
  br label %"produce filter_zeroed439"

"produce filter_zeroed439":                       ; preds = %pseudostack_alloc.exit3216, %then_bb437
  %filter_zeroed438 = phi i16* [ %11272, %then_bb437 ], [ %11270, %pseudostack_alloc.exit3216 ]
  br i1 %11140, label %"for filter_zeroed.s0.y440.preheader", label %"consume sum_filter458.critedge", !prof !391

"for filter_zeroed.s0.y440.preheader":            ; preds = %"produce filter_zeroed439"
  br i1 %11138, label %"for filter_zeroed.s0.y440.us.preheader", label %"for sum_filter.s1.r19$y451.preheader.thread", !prof !391

"for sum_filter.s1.r19$y451.preheader.thread":    ; preds = %"for filter_zeroed.s0.y440.preheader"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %11259, i8 0, i64 64, i1 false)
  br label %"consume sum_filter458"

"for filter_zeroed.s0.y440.us.preheader":         ; preds = %"for filter_zeroed.s0.y440.preheader"
  %11273 = sext i32 %output.s0.c.c.base435 to i64
  br label %"for filter_zeroed.s0.y440.us"

"for filter_zeroed.s0.y440.us":                   ; preds = %"for filter_zeroed.s0.y440.us.preheader", %"end for filter_zeroed.s0.x446.loopexit.us"
  %indvars.iv6435 = phi i64 [ 0, %"for filter_zeroed.s0.y440.us.preheader" ], [ %indvars.iv.next6436, %"end for filter_zeroed.s0.x446.loopexit.us" ]
  %11274 = mul nsw i64 %indvars.iv6435, %11243
  %11275 = mul nsw i64 %indvars.iv6435, %11244
  %11276 = add nsw i64 %11275, %11273
  br label %"for filter_zeroed.s0.x445.us"

"for filter_zeroed.s0.x445.us":                   ; preds = %"for filter_zeroed.s0.y440.us", %"for filter_zeroed.s0.x445.us"
  %indvars.iv6433 = phi i64 [ 0, %"for filter_zeroed.s0.y440.us" ], [ %indvars.iv.next6434, %"for filter_zeroed.s0.x445.us" ]
  %11277 = mul nsw i64 %indvars.iv6433, %11146
  %11278 = add nsw i64 %11277, %11276
  %11279 = getelementptr inbounds i8, i8* %43, i64 %11278
  %11280 = bitcast i8* %11279 to <8 x i8>*
  %11281 = load <8 x i8>, <8 x i8>* %11280, align 1, !tbaa !392
  %11282 = zext <8 x i8> %11281 to <8 x i16>
  %11283 = sub nsw <8 x i16> %11282, %11149
  %11284 = add nsw i64 %indvars.iv6433, %11274
  %11285 = shl nsw i64 %11284, 4
  %11286 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 %11285
  %11287 = bitcast i16* %11286 to <8 x i16>*
  store <8 x i16> %11283, <8 x i16>* %11287, align 16, !tbaa !395
  %11288 = getelementptr inbounds i8, i8* %11279, i64 8
  %11289 = bitcast i8* %11288 to <8 x i8>*
  %11290 = load <8 x i8>, <8 x i8>* %11289, align 1, !tbaa !392
  %11291 = zext <8 x i8> %11290 to <8 x i16>
  %11292 = sub nsw <8 x i16> %11291, %11149
  %11293 = getelementptr inbounds i16, i16* %11286, i64 8
  %11294 = bitcast i16* %11293 to <8 x i16>*
  store <8 x i16> %11292, <8 x i16>* %11294, align 16, !tbaa !395
  %indvars.iv.next6434 = add nuw nsw i64 %indvars.iv6433, 1
  %.not2586.us = icmp eq i64 %indvars.iv.next6434, %11242
  br i1 %.not2586.us, label %"end for filter_zeroed.s0.x446.loopexit.us", label %"for filter_zeroed.s0.x445.us"

"end for filter_zeroed.s0.x446.loopexit.us":      ; preds = %"for filter_zeroed.s0.x445.us"
  %indvars.iv.next6436 = add nuw nsw i64 %indvars.iv6435, 1
  %.not2585.us = icmp eq i64 %indvars.iv.next6436, %11245
  br i1 %.not2585.us, label %"for sum_filter.s1.r19$y451.preheader", label %"for filter_zeroed.s0.y440.us"

"for sum_filter.s1.r19$y451.preheader":           ; preds = %"end for filter_zeroed.s0.x446.loopexit.us"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %11260, i8 0, i64 64, i1 false)
  br i1 %11138, label %"for sum_filter.s1.r19$y451.us", label %"consume sum_filter458", !prof !391

"for sum_filter.s1.r19$y451.us":                  ; preds = %"for sum_filter.s1.r19$y451.preheader", %"end for sum_filter.s1.r19$x456.loopexit.us"
  %indvars.iv6442 = phi i64 [ %indvars.iv.next6443, %"end for sum_filter.s1.r19$x456.loopexit.us" ], [ 0, %"for sum_filter.s1.r19$y451.preheader" ]
  %.lcssa5812.us5820 = phi <4 x i32> [ %11321, %"end for sum_filter.s1.r19$x456.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader" ]
  %.lcssa5810.us5818 = phi <4 x i32> [ %11316, %"end for sum_filter.s1.r19$x456.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader" ]
  %.lcssa5808.us5816 = phi <4 x i32> [ %11311, %"end for sum_filter.s1.r19$x456.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader" ]
  %.lcssa5806.us5814 = phi <4 x i32> [ %11306, %"end for sum_filter.s1.r19$x456.loopexit.us" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader" ]
  %11295 = mul nsw i64 %indvars.iv6442, %11243
  br label %"for sum_filter.s1.r19$x455.us"

"for sum_filter.s1.r19$x455.us":                  ; preds = %"for sum_filter.s1.r19$y451.us", %"for sum_filter.s1.r19$x455.us"
  %indvars.iv6439 = phi i64 [ 0, %"for sum_filter.s1.r19$y451.us" ], [ %indvars.iv.next6440, %"for sum_filter.s1.r19$x455.us" ]
  %11296 = phi <4 x i32> [ %.lcssa5812.us5820, %"for sum_filter.s1.r19$y451.us" ], [ %11321, %"for sum_filter.s1.r19$x455.us" ]
  %11297 = phi <4 x i32> [ %.lcssa5810.us5818, %"for sum_filter.s1.r19$y451.us" ], [ %11316, %"for sum_filter.s1.r19$x455.us" ]
  %11298 = phi <4 x i32> [ %.lcssa5808.us5816, %"for sum_filter.s1.r19$y451.us" ], [ %11311, %"for sum_filter.s1.r19$x455.us" ]
  %11299 = phi <4 x i32> [ %.lcssa5806.us5814, %"for sum_filter.s1.r19$y451.us" ], [ %11306, %"for sum_filter.s1.r19$x455.us" ]
  %11300 = add nsw i64 %indvars.iv6439, %11295
  %11301 = shl nsw i64 %11300, 4
  %11302 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 %11301
  %11303 = bitcast i16* %11302 to <4 x i16>*
  %11304 = load <4 x i16>, <4 x i16>* %11303, align 16, !tbaa !395
  %11305 = sext <4 x i16> %11304 to <4 x i32>
  %11306 = add <4 x i32> %11299, %11305
  %11307 = getelementptr inbounds i16, i16* %11302, i64 4
  %11308 = bitcast i16* %11307 to <4 x i16>*
  %11309 = load <4 x i16>, <4 x i16>* %11308, align 8, !tbaa !395
  %11310 = sext <4 x i16> %11309 to <4 x i32>
  %11311 = add <4 x i32> %11298, %11310
  %11312 = getelementptr inbounds i16, i16* %11302, i64 8
  %11313 = bitcast i16* %11312 to <4 x i16>*
  %11314 = load <4 x i16>, <4 x i16>* %11313, align 16, !tbaa !395
  %11315 = sext <4 x i16> %11314 to <4 x i32>
  %11316 = add <4 x i32> %11297, %11315
  %11317 = getelementptr inbounds i16, i16* %11302, i64 12
  %11318 = bitcast i16* %11317 to <4 x i16>*
  %11319 = load <4 x i16>, <4 x i16>* %11318, align 8, !tbaa !395
  %11320 = sext <4 x i16> %11319 to <4 x i32>
  %11321 = add <4 x i32> %11296, %11320
  %indvars.iv.next6440 = add nuw nsw i64 %indvars.iv6439, 1
  %.not2584.us = icmp eq i64 %indvars.iv.next6440, %11242
  br i1 %.not2584.us, label %"end for sum_filter.s1.r19$x456.loopexit.us", label %"for sum_filter.s1.r19$x455.us"

"end for sum_filter.s1.r19$x456.loopexit.us":     ; preds = %"for sum_filter.s1.r19$x455.us"
  %indvars.iv.next6443 = add nuw nsw i64 %indvars.iv6442, 1
  %.not2583.us = icmp eq i64 %indvars.iv.next6443, %11245
  br i1 %.not2583.us, label %"consume sum_filter458.loopexit.split.us", label %"for sum_filter.s1.r19$y451.us"

"consume sum_filter458.loopexit.split.us":        ; preds = %"end for sum_filter.s1.r19$x456.loopexit.us"
  store <4 x i32> %11306, <4 x i32>* %11150, align 16, !tbaa !441
  store <4 x i32> %11311, <4 x i32>* %11152, align 16, !tbaa !452
  store <4 x i32> %11316, <4 x i32>* %11154, align 16, !tbaa !454
  store <4 x i32> %11321, <4 x i32>* %11156, align 16, !tbaa !457
  br label %"consume sum_filter458"

"consume sum_filter458.critedge":                 ; preds = %"produce filter_zeroed439"
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 dereferenceable(64) %11258, i8 0, i64 64, i1 false)
  br label %"consume sum_filter458"

"consume sum_filter458":                          ; preds = %"for sum_filter.s1.r19$y451.preheader", %"for sum_filter.s1.r19$y451.preheader.thread", %"consume sum_filter458.loopexit.split.us", %"consume sum_filter458.critedge"
  %11322 = phi <4 x i32> [ %11321, %"consume sum_filter458.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter458.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader" ]
  %11323 = phi <4 x i32> [ %11316, %"consume sum_filter458.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter458.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader" ]
  %11324 = phi <4 x i32> [ %11311, %"consume sum_filter458.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter458.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader" ]
  %11325 = phi <4 x i32> [ %11306, %"consume sum_filter458.loopexit.split.us" ], [ zeroinitializer, %"consume sum_filter458.critedge" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader.thread" ], [ zeroinitializer, %"for sum_filter.s1.r19$y451.preheader" ]
  %11326 = sext i32 %output.s0.c.c.base435 to i64
  %11327 = getelementptr inbounds i32, i32* %11157, i64 %11326
  %11328 = bitcast i32* %11327 to <4 x i32>*
  %11329 = load <4 x i32>, <4 x i32>* %11328, align 4, !tbaa !415
  %11330 = mul <4 x i32> %11325, %11160
  %11331 = sub <4 x i32> %11329, %11330
  %11332 = getelementptr inbounds i32, i32* %11327, i64 4
  %11333 = bitcast i32* %11332 to <4 x i32>*
  %11334 = load <4 x i32>, <4 x i32>* %11333, align 4, !tbaa !415
  %11335 = mul <4 x i32> %11324, %11160
  %11336 = sub <4 x i32> %11334, %11335
  %11337 = getelementptr inbounds i32, i32* %11327, i64 8
  %11338 = bitcast i32* %11337 to <4 x i32>*
  %11339 = load <4 x i32>, <4 x i32>* %11338, align 4, !tbaa !415
  %11340 = mul <4 x i32> %11323, %11160
  %11341 = sub <4 x i32> %11339, %11340
  %11342 = getelementptr inbounds i32, i32* %11327, i64 12
  %11343 = bitcast i32* %11342 to <4 x i32>*
  %11344 = load <4 x i32>, <4 x i32>* %11343, align 4, !tbaa !415
  %11345 = mul <4 x i32> %11322, %11160
  %11346 = sub <4 x i32> %11344, %11345
  store <4 x i32> %11331, <4 x i32>* %11161, align 16, !tbaa !417
  store <4 x i32> %11336, <4 x i32>* %11163, align 16, !tbaa !428
  store <4 x i32> %11341, <4 x i32>* %11165, align 16, !tbaa !430
  store <4 x i32> %11346, <4 x i32>* %11167, align 16, !tbaa !433
  br i1 %11186, label %"for output.s0.b.rebased468.preheader", label %"end for output.s0.b.rebased469", !prof !391

"for output.s0.b.rebased468.preheader":           ; preds = %"consume sum_filter458"
  %11347 = insertelement <16 x i32> undef, i32 %output.s0.c.c.base435, i32 0
  %11348 = shufflevector <16 x i32> %11347, <16 x i32> undef, <16 x i32> zeroinitializer
  %11349 = add nsw <16 x i32> %11348, <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %.lobit = ashr <16 x i32> %11349, <i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31>
  %11350 = sub nsw <16 x i32> %11349, %.lobit
  %11351 = and <16 x i32> %.lobit, %11206
  %11352 = sub i32 %output.s0.c.c.base435, %11197
  %11353 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 96
  %11354 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 128
  %11355 = bitcast i16* %11354 to <4 x i16>*
  %11356 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 112
  %11357 = bitcast i16* %11356 to <4 x i16>*
  %11358 = bitcast i16* %11353 to <4 x i16>*
  %11359 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 48
  %11360 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 80
  %11361 = bitcast i16* %11360 to <4 x i16>*
  %11362 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 64
  %11363 = bitcast i16* %11362 to <4 x i16>*
  %11364 = bitcast i16* %11359 to <4 x i16>*
  %11365 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 32
  %11366 = bitcast i16* %11365 to <4 x i16>*
  %11367 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 16
  %11368 = bitcast i16* %11367 to <4 x i16>*
  %11369 = bitcast i16* %filter_zeroed438 to <4 x i16>*
  %11370 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 132
  %11371 = bitcast i16* %11370 to <4 x i16>*
  %11372 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 116
  %11373 = bitcast i16* %11372 to <4 x i16>*
  %11374 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 100
  %11375 = bitcast i16* %11374 to <4 x i16>*
  %11376 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 84
  %11377 = bitcast i16* %11376 to <4 x i16>*
  %11378 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 68
  %11379 = bitcast i16* %11378 to <4 x i16>*
  %11380 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 52
  %11381 = bitcast i16* %11380 to <4 x i16>*
  %11382 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 36
  %11383 = bitcast i16* %11382 to <4 x i16>*
  %11384 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 20
  %11385 = bitcast i16* %11384 to <4 x i16>*
  %11386 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 4
  %11387 = bitcast i16* %11386 to <4 x i16>*
  %11388 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 136
  %11389 = bitcast i16* %11388 to <4 x i16>*
  %11390 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 120
  %11391 = bitcast i16* %11390 to <4 x i16>*
  %11392 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 104
  %11393 = bitcast i16* %11392 to <4 x i16>*
  %11394 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 88
  %11395 = bitcast i16* %11394 to <4 x i16>*
  %11396 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 72
  %11397 = bitcast i16* %11396 to <4 x i16>*
  %11398 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 56
  %11399 = bitcast i16* %11398 to <4 x i16>*
  %11400 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 40
  %11401 = bitcast i16* %11400 to <4 x i16>*
  %11402 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 24
  %11403 = bitcast i16* %11402 to <4 x i16>*
  %11404 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 8
  %11405 = bitcast i16* %11404 to <4 x i16>*
  %11406 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 140
  %11407 = bitcast i16* %11406 to <4 x i16>*
  %11408 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 124
  %11409 = bitcast i16* %11408 to <4 x i16>*
  %11410 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 108
  %11411 = bitcast i16* %11410 to <4 x i16>*
  %11412 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 92
  %11413 = bitcast i16* %11412 to <4 x i16>*
  %11414 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 76
  %11415 = bitcast i16* %11414 to <4 x i16>*
  %11416 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 60
  %11417 = bitcast i16* %11416 to <4 x i16>*
  %11418 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 44
  %11419 = bitcast i16* %11418 to <4 x i16>*
  %11420 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 28
  %11421 = bitcast i16* %11420 to <4 x i16>*
  %11422 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 12
  %11423 = bitcast i16* %11422 to <4 x i16>*
  %11424 = sext i32 %11352 to i64
  br label %"for output.s0.b.rebased468"

"for output.s0.b.rebased468":                     ; preds = %"for output.s0.b.rebased468.preheader", %"end for output.s0.y.yo505"
  %indvars.iv6467 = phi i64 [ 0, %"for output.s0.b.rebased468.preheader" ], [ %indvars.iv.next6468, %"end for output.s0.y.yo505" ]
  %11425 = load i64, i64* %.fca.1.gep875, align 8, !tbaa !385
  %cmp.i3218 = icmp ult i64 %11425, %11192
  %11426 = load i8*, i8** %.fca.0.gep874, align 8, !tbaa !387
  br i1 %cmp.i3218, label %if.then.i3221, label %pseudostack_alloc.exit3234, !prof !388

if.then.i3221:                                    ; preds = %"for output.s0.b.rebased468"
  %tobool1.not.i3220 = icmp ne i8* %11426, null
  %11427 = load i64, i64* %.fca.2.gep876, align 8
  %cmp2.i3223 = icmp ugt i64 %11427, 16384
  %or.cond5133 = and i1 %tobool1.not.i3220, %cmp2.i3223
  br i1 %or.cond5133, label %if.then3.i3225, label %if.end.i3229

if.then3.i3225:                                   ; preds = %if.then.i3221
  call void @halide_free(i8* null, i8* nonnull %11426) #15
  %.pre6510 = load i64, i64* %.fca.2.gep876, align 8, !tbaa !389
  br label %if.end.i3229

if.end.i3229:                                     ; preds = %if.then3.i3225, %if.then.i3221
  %11428 = phi i64 [ %.pre6510, %if.then3.i3225 ], [ %11427, %if.then.i3221 ]
  %add.i3227 = add i64 %11428, %11192
  store i64 %add.i3227, i64* %.fca.2.gep876, align 8, !tbaa !389
  %cmp7.i3228 = icmp ugt i64 %add.i3227, 16384
  br i1 %cmp7.i3228, label %if.then8.i3231, label %if.end11.i3233

if.then8.i3231:                                   ; preds = %if.end.i3229
  %call.i3230 = call i8* @halide_malloc(i8* null, i64 %11192) #15
  br label %if.end11.i3233

if.end11.i3233:                                   ; preds = %if.then8.i3231, %if.end.i3229
  %storemerge.i3232 = phi i8* [ %call.i3230, %if.then8.i3231 ], [ null, %if.end.i3229 ]
  store i8* %storemerge.i3232, i8** %.fca.0.gep874, align 8, !tbaa !387
  store i64 %11192, i64* %.fca.1.gep875, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3234

pseudostack_alloc.exit3234:                       ; preds = %"for output.s0.b.rebased468", %if.end11.i3233
  %11429 = phi i8* [ %storemerge.i3232, %if.end11.i3233 ], [ %11426, %"for output.s0.b.rebased468" ]
  %.not1805 = icmp eq i8* %11429, null
  br i1 %.not1805, label %then_bb472, label %"produce resampled_input474", !prof !390

"end for output.s0.b.rebased469":                 ; preds = %"end for output.s0.y.yo505", %"consume sum_filter458"
  %.9 = phi i8* [ %.8, %"consume sum_filter458" ], [ %11241, %"end for output.s0.y.yo505" ]
  %11430 = add nuw nsw i32 %output.s0.c.co434, 1
  %.not1802 = icmp eq i32 %11430, %t2305427
  br i1 %.not1802, label %after_bb236.loopexit5212, label %"for output.s0.c.co432"

then_bb472:                                       ; preds = %pseudostack_alloc.exit3234
  %11431 = alloca i8*, i64 %11192, align 16
  %11432 = bitcast i8** %11431 to i8*
  store i8** %11431, i8*** %11193, align 8
  br label %"produce resampled_input474"

"produce resampled_input474":                     ; preds = %pseudostack_alloc.exit3234, %then_bb472
  %resampled_input473 = phi i8* [ %11432, %then_bb472 ], [ %11429, %pseudostack_alloc.exit3234 ]
  br i1 %t2327426, label %then_bb476, label %next_bb477

then_bb476:                                       ; preds = %"produce resampled_input474"
  br i1 %11194, label %"for resampled_input.s0.y.rebased481.preheader", label %"consume resampled_input500", !prof !391

"for resampled_input.s0.y.rebased481.preheader":  ; preds = %then_bb476
  %11433 = add nsw i64 %indvars.iv6467, %11256
  %11434 = mul nsw i64 %11433, %11257
  %11435 = add nsw i64 %11434, %11424
  br i1 %11198, label %"for resampled_input.s0.y.rebased481.us", label %"consume resampled_input500", !prof !391

"for resampled_input.s0.y.rebased481.us":         ; preds = %"for resampled_input.s0.y.rebased481.preheader", %"end for resampled_input.s0.x.rebased487.loopexit.us"
  %indvars.iv6453 = phi i64 [ %indvars.iv.next6454, %"end for resampled_input.s0.x.rebased487.loopexit.us" ], [ 0, %"for resampled_input.s0.y.rebased481.preheader" ]
  %11436 = add nsw i64 %indvars.iv6453, %11247
  %11437 = mul nsw i64 %11436, %11248
  %11438 = add nsw i64 %11437, %11249
  %11439 = add nsw i64 %indvars.iv6453, %11250
  %11440 = mul nsw i64 %11439, %11251
  %11441 = add nsw i64 %11435, %11440
  br label %"for resampled_input.s0.x.rebased486.us"

"for resampled_input.s0.x.rebased486.us":         ; preds = %"for resampled_input.s0.y.rebased481.us", %"for resampled_input.s0.x.rebased486.us"
  %indvars.iv6451 = phi i64 [ 0, %"for resampled_input.s0.y.rebased481.us" ], [ %indvars.iv.next6452, %"for resampled_input.s0.x.rebased486.us" ]
  %11442 = add nsw i64 %indvars.iv6451, %11208
  %11443 = mul nsw i64 %11442, %11209
  %11444 = add nsw i64 %11443, %11441
  %11445 = getelementptr inbounds i8, i8* %50, i64 %11444
  %11446 = bitcast i8* %11445 to <16 x i8>*
  %11447 = load <16 x i8>, <16 x i8>* %11446, align 1, !tbaa !436
  %11448 = add nsw i64 %indvars.iv6451, %11438
  %11449 = shl nsw i64 %11448, 4
  %11450 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11449
  %11451 = bitcast i8* %11450 to <16 x i8>*
  store <16 x i8> %11447, <16 x i8>* %11451, align 16, !tbaa !438
  %indvars.iv.next6452 = add nuw nsw i64 %indvars.iv6451, 1
  %.not2582.us = icmp eq i64 %indvars.iv6451, %11246
  br i1 %.not2582.us, label %"end for resampled_input.s0.x.rebased487.loopexit.us", label %"for resampled_input.s0.x.rebased486.us"

"end for resampled_input.s0.x.rebased487.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased486.us"
  %indvars.iv.next6454 = add nuw nsw i64 %indvars.iv6453, 1
  %.not2581.us = icmp eq i64 %indvars.iv6453, %11252
  br i1 %.not2581.us, label %"consume resampled_input500", label %"for resampled_input.s0.y.rebased481.us"

next_bb477:                                       ; preds = %"produce resampled_input474"
  br i1 %11262, label %"consume resampled_input500", label %"for resampled_input.s0.y.rebased492.preheader.split.us", !prof !440

"for resampled_input.s0.y.rebased492.preheader.split.us": ; preds = %next_bb477
  %11452 = trunc i64 %indvars.iv6467 to i32
  %11453 = add i32 %56, %11452
  %11454 = mul i32 %11453, %58
  %11455 = sub i32 %11454, %11197
  %11456 = sdiv <16 x i32> %11350, %11202
  %11457 = add nsw <16 x i32> %11456, %11351
  %11458 = and <16 x i32> %11457, %11207
  br label %"for resampled_input.s0.y.rebased492.us"

"for resampled_input.s0.y.rebased492.us":         ; preds = %"end for resampled_input.s0.x.rebased498.loopexit.us", %"for resampled_input.s0.y.rebased492.preheader.split.us"
  %indvars.iv6447 = phi i64 [ %indvars.iv.next6448, %"end for resampled_input.s0.x.rebased498.loopexit.us" ], [ 0, %"for resampled_input.s0.y.rebased492.preheader.split.us" ]
  %11459 = add nsw i64 %indvars.iv6447, %11247
  %11460 = mul nsw i64 %11459, %11248
  %11461 = add nsw i64 %11460, %11249
  %11462 = trunc i64 %indvars.iv6447 to i32
  %11463 = add i32 %a301, %11462
  %11464 = mul i32 %11463, %55
  %11465 = add i32 %11464, %11455
  br label %"for resampled_input.s0.x.rebased497.us"

"for resampled_input.s0.x.rebased497.us":         ; preds = %"for resampled_input.s0.y.rebased492.us", %"for resampled_input.s0.x.rebased497.us"
  %indvars.iv6445 = phi i64 [ 0, %"for resampled_input.s0.y.rebased492.us" ], [ %indvars.iv.next6446, %"for resampled_input.s0.x.rebased497.us" ]
  %11466 = trunc i64 %indvars.iv6445 to i32
  %11467 = add nsw i32 %t2311, %11466
  %11468 = mul nsw i32 %11467, %53
  %11469 = add nsw i32 %11465, %11468
  %11470 = insertelement <16 x i32> undef, i32 %11469, i32 0
  %11471 = shufflevector <16 x i32> %11470, <16 x i32> undef, <16 x i32> zeroinitializer
  %11472 = add nsw <16 x i32> %11471, %11458
  %11473 = extractelement <16 x i32> %11472, i32 0
  %11474 = sext i32 %11473 to i64
  %11475 = getelementptr inbounds i8, i8* %50, i64 %11474
  %11476 = load i8, i8* %11475, align 1, !tbaa !436
  %11477 = insertelement <16 x i8> undef, i8 %11476, i32 0
  %11478 = extractelement <16 x i32> %11472, i32 1
  %11479 = sext i32 %11478 to i64
  %11480 = getelementptr inbounds i8, i8* %50, i64 %11479
  %11481 = load i8, i8* %11480, align 1, !tbaa !436
  %11482 = insertelement <16 x i8> %11477, i8 %11481, i32 1
  %11483 = extractelement <16 x i32> %11472, i32 2
  %11484 = sext i32 %11483 to i64
  %11485 = getelementptr inbounds i8, i8* %50, i64 %11484
  %11486 = load i8, i8* %11485, align 1, !tbaa !436
  %11487 = insertelement <16 x i8> %11482, i8 %11486, i32 2
  %11488 = extractelement <16 x i32> %11472, i32 3
  %11489 = sext i32 %11488 to i64
  %11490 = getelementptr inbounds i8, i8* %50, i64 %11489
  %11491 = load i8, i8* %11490, align 1, !tbaa !436
  %11492 = insertelement <16 x i8> %11487, i8 %11491, i32 3
  %11493 = extractelement <16 x i32> %11472, i32 4
  %11494 = sext i32 %11493 to i64
  %11495 = getelementptr inbounds i8, i8* %50, i64 %11494
  %11496 = load i8, i8* %11495, align 1, !tbaa !436
  %11497 = insertelement <16 x i8> %11492, i8 %11496, i32 4
  %11498 = extractelement <16 x i32> %11472, i32 5
  %11499 = sext i32 %11498 to i64
  %11500 = getelementptr inbounds i8, i8* %50, i64 %11499
  %11501 = load i8, i8* %11500, align 1, !tbaa !436
  %11502 = insertelement <16 x i8> %11497, i8 %11501, i32 5
  %11503 = extractelement <16 x i32> %11472, i32 6
  %11504 = sext i32 %11503 to i64
  %11505 = getelementptr inbounds i8, i8* %50, i64 %11504
  %11506 = load i8, i8* %11505, align 1, !tbaa !436
  %11507 = insertelement <16 x i8> %11502, i8 %11506, i32 6
  %11508 = extractelement <16 x i32> %11472, i32 7
  %11509 = sext i32 %11508 to i64
  %11510 = getelementptr inbounds i8, i8* %50, i64 %11509
  %11511 = load i8, i8* %11510, align 1, !tbaa !436
  %11512 = insertelement <16 x i8> %11507, i8 %11511, i32 7
  %11513 = extractelement <16 x i32> %11472, i32 8
  %11514 = sext i32 %11513 to i64
  %11515 = getelementptr inbounds i8, i8* %50, i64 %11514
  %11516 = load i8, i8* %11515, align 1, !tbaa !436
  %11517 = insertelement <16 x i8> %11512, i8 %11516, i32 8
  %11518 = extractelement <16 x i32> %11472, i32 9
  %11519 = sext i32 %11518 to i64
  %11520 = getelementptr inbounds i8, i8* %50, i64 %11519
  %11521 = load i8, i8* %11520, align 1, !tbaa !436
  %11522 = insertelement <16 x i8> %11517, i8 %11521, i32 9
  %11523 = extractelement <16 x i32> %11472, i32 10
  %11524 = sext i32 %11523 to i64
  %11525 = getelementptr inbounds i8, i8* %50, i64 %11524
  %11526 = load i8, i8* %11525, align 1, !tbaa !436
  %11527 = insertelement <16 x i8> %11522, i8 %11526, i32 10
  %11528 = extractelement <16 x i32> %11472, i32 11
  %11529 = sext i32 %11528 to i64
  %11530 = getelementptr inbounds i8, i8* %50, i64 %11529
  %11531 = load i8, i8* %11530, align 1, !tbaa !436
  %11532 = insertelement <16 x i8> %11527, i8 %11531, i32 11
  %11533 = extractelement <16 x i32> %11472, i32 12
  %11534 = sext i32 %11533 to i64
  %11535 = getelementptr inbounds i8, i8* %50, i64 %11534
  %11536 = load i8, i8* %11535, align 1, !tbaa !436
  %11537 = insertelement <16 x i8> %11532, i8 %11536, i32 12
  %11538 = extractelement <16 x i32> %11472, i32 13
  %11539 = sext i32 %11538 to i64
  %11540 = getelementptr inbounds i8, i8* %50, i64 %11539
  %11541 = load i8, i8* %11540, align 1, !tbaa !436
  %11542 = insertelement <16 x i8> %11537, i8 %11541, i32 13
  %11543 = extractelement <16 x i32> %11472, i32 14
  %11544 = sext i32 %11543 to i64
  %11545 = getelementptr inbounds i8, i8* %50, i64 %11544
  %11546 = load i8, i8* %11545, align 1, !tbaa !436
  %11547 = insertelement <16 x i8> %11542, i8 %11546, i32 14
  %11548 = extractelement <16 x i32> %11472, i32 15
  %11549 = sext i32 %11548 to i64
  %11550 = getelementptr inbounds i8, i8* %50, i64 %11549
  %11551 = load i8, i8* %11550, align 1, !tbaa !436
  %11552 = insertelement <16 x i8> %11547, i8 %11551, i32 15
  %11553 = add nsw i64 %indvars.iv6445, %11461
  %11554 = shl nsw i64 %11553, 4
  %11555 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11554
  %11556 = bitcast i8* %11555 to <16 x i8>*
  store <16 x i8> %11552, <16 x i8>* %11556, align 16, !tbaa !438
  %indvars.iv.next6446 = add nuw nsw i64 %indvars.iv6445, 1
  %.not2580.us = icmp eq i64 %indvars.iv6445, %11246
  br i1 %.not2580.us, label %"end for resampled_input.s0.x.rebased498.loopexit.us", label %"for resampled_input.s0.x.rebased497.us"

"end for resampled_input.s0.x.rebased498.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased497.us"
  %indvars.iv.next6448 = add nuw nsw i64 %indvars.iv6447, 1
  %.not2579.us = icmp eq i64 %indvars.iv6447, %11252
  br i1 %.not2579.us, label %"consume resampled_input500", label %"for resampled_input.s0.y.rebased492.us"

"consume resampled_input500":                     ; preds = %"end for resampled_input.s0.x.rebased498.loopexit.us", %"end for resampled_input.s0.x.rebased487.loopexit.us", %next_bb477, %"for resampled_input.s0.y.rebased481.preheader", %then_bb476
  br i1 %11210, label %"for output.s0.y.yo504.preheader", label %"end for output.s0.y.yo505", !prof !391

"for output.s0.y.yo504.preheader":                ; preds = %"consume resampled_input500"
  %11557 = trunc i64 %indvars.iv6467 to i32
  %11558 = add i32 %56, %11557
  %11559 = mul i32 %11558, %67
  %11560 = add i32 %11559, %output.s0.c.c.base435
  %11561 = sub i32 %11560, %reass.add5193
  %11562 = load <4 x i32>, <4 x i32>* %11161, align 16
  %11563 = load <4 x i32>, <4 x i32>* %11163, align 16
  %11564 = load <4 x i32>, <4 x i32>* %11165, align 16
  %11565 = load <4 x i32>, <4 x i32>* %11167, align 16
  br label %"for output.s0.y.yo504"

"for output.s0.y.yo504":                          ; preds = %"for output.s0.y.yo504.preheader", %"end for output.s0.x.xo574"
  %indvars.iv6465 = phi i64 [ 0, %"for output.s0.y.yo504.preheader" ], [ %indvars.iv.next6466, %"end for output.s0.x.xo574" ]
  %11566 = trunc i64 %indvars.iv6465 to i32
  %11567 = shl nsw i32 %11566, 2
  %11568 = add nsw i32 %11567, %64
  %11569 = add nsw i32 %11568, 1
  %11570 = mul nsw i32 %11569, %stride_y
  %11571 = add nsw i32 %11570, %11211
  %11572 = add nsw i32 %11570, %11212
  %11573 = add nsw i32 %11568, 2
  %11574 = mul nsw i32 %11573, %stride_y
  %11575 = add nsw i32 %11574, %11211
  %11576 = add nsw i32 %11574, %11212
  %11577 = add nsw i32 %11568, 3
  %11578 = mul nsw i32 %11577, %stride_y
  %11579 = add nsw i32 %11578, %11211
  %11580 = add nsw i32 %11578, %11212
  %11581 = mul nsw i32 %11568, %stride_y
  %11582 = add nsw i32 %11581, %11211
  %11583 = add nsw i32 %11581, %11212
  %11584 = sub nsw i32 %11570, %11185
  %11585 = sub nsw i32 %11574, %11185
  %11586 = sub nsw i32 %11578, %11185
  %11587 = sub nsw i32 %11581, %11185
  %11588 = mul nsw i32 %11587, %a313
  %t2383533 = sub nsw i32 %11588, %11176
  %11589 = mul nsw i32 %11586, %a313
  %t2386534 = sub nsw i32 %11589, %11176
  %11590 = mul nsw i32 %11585, %a313
  %t2385535 = sub nsw i32 %11590, %11176
  %11591 = mul nsw i32 %11584, %a313
  %t2384536 = sub nsw i32 %11591, %11176
  %11592 = mul nsw i32 %11583, %a313
  %t2396537 = sub nsw i32 %11592, %11176
  %11593 = mul nsw i32 %11582, %a313
  %t2409538 = sub nsw i32 %11593, %11176
  %11594 = mul nsw i32 %11580, %a313
  %t2399539 = sub nsw i32 %11594, %11176
  %11595 = mul nsw i32 %11579, %a313
  %t2412540 = sub nsw i32 %11595, %11176
  %11596 = mul nsw i32 %11576, %a313
  %t2398541 = sub nsw i32 %11596, %11176
  %11597 = mul nsw i32 %11575, %a313
  %t2411542 = sub nsw i32 %11597, %11176
  %11598 = mul nsw i32 %11572, %a313
  %t2397543 = sub nsw i32 %11598, %11176
  %11599 = mul nsw i32 %11571, %a313
  %t2410544 = sub nsw i32 %11599, %11176
  %t2387545 = add nsw i32 %11588, %11213
  %t2391546 = add nsw i32 %11588, %11214
  %t2390547 = add nsw i32 %11589, %11213
  %t2394548 = add nsw i32 %11589, %11214
  %t2389549 = add nsw i32 %11590, %11213
  %t2393550 = add nsw i32 %11590, %11214
  %t2388551 = add nsw i32 %11591, %11213
  %t2392552 = add nsw i32 %11591, %11214
  %t2400553 = add nsw i32 %11592, %11213
  %t2404554 = add nsw i32 %11592, %11214
  %t2413555 = add nsw i32 %11593, %11213
  %t2417556 = add nsw i32 %11593, %11214
  %t2403557 = add nsw i32 %11594, %11213
  %t2407558 = add nsw i32 %11594, %11214
  %t2416559 = add nsw i32 %11595, %11213
  %t2420560 = add nsw i32 %11595, %11214
  %t2402561 = add nsw i32 %11596, %11213
  %t2406562 = add nsw i32 %11596, %11214
  %t2415563 = add nsw i32 %11597, %11213
  %t2419564 = add nsw i32 %11597, %11214
  %t2401565 = add nsw i32 %11598, %11213
  %t2405566 = add nsw i32 %11598, %11214
  %t2414567 = add nsw i32 %11599, %11213
  %t2418568 = add nsw i32 %11599, %11214
  br i1 %11215, label %"for output.s0.x.xo573.preheader", label %"end for output.s0.x.xo574", !prof !391

"for output.s0.x.xo573.preheader":                ; preds = %"for output.s0.y.yo504"
  %11600 = mul nsw i32 %11569, %66
  %t2426572 = add nsw i32 %11600, %11561
  %11601 = mul nsw i32 %11573, %66
  %t2427571 = add nsw i32 %11601, %11561
  %11602 = mul nsw i32 %11577, %66
  %t2428570 = add nsw i32 %11602, %11561
  %11603 = mul nsw i32 %11568, %66
  %t2425569 = add nsw i32 %11603, %11561
  %11604 = sub i32 %11570, %11219
  %11605 = sub i32 %11574, %11219
  %11606 = sub i32 %11578, %11219
  %11607 = sub i32 %11581, %11219
  %11608 = sext i32 %t2425569 to i64
  %11609 = sext i32 %t2426572 to i64
  %11610 = sext i32 %t2427571 to i64
  %11611 = sext i32 %t2428570 to i64
  br label %"for output.s0.x.xo573"

"end for output.s0.y.yo505":                      ; preds = %"end for output.s0.x.xo574", %"consume resampled_input500"
  %indvars.iv.next6468 = add nuw nsw i64 %indvars.iv6467, 1
  %11612 = icmp eq i64 %indvars.iv.next6468, %zext6469
  br i1 %11612, label %"end for output.s0.b.rebased469", label %"for output.s0.b.rebased468"

"for output.s0.x.xo573":                          ; preds = %"for output.s0.x.xo573.preheader", %"consume convolved613"
  %indvars.iv6463 = phi i64 [ 0, %"for output.s0.x.xo573.preheader" ], [ %indvars.iv.next6464, %"consume convolved613" ]
  %11613 = trunc i64 %indvars.iv6463 to i32
  %11614 = shl nsw i32 %11613, 2
  %11615 = add nsw i32 %11614, %61
  br i1 %t2346421, label %then_bb578, label %next_bb579

"end for output.s0.x.xo574":                      ; preds = %"consume convolved613", %"for output.s0.y.yo504"
  %indvars.iv.next6466 = add nuw nsw i64 %indvars.iv6465, 1
  %.not1807 = icmp eq i64 %indvars.iv.next6466, %11255
  br i1 %.not1807, label %"end for output.s0.y.yo505", label %"for output.s0.y.yo504"

then_bb578:                                       ; preds = %"for output.s0.x.xo573"
  %t3202 = mul nsw i32 %11615, %stride_x
  %t3203 = add nsw i32 %t3202, %t2417556
  %11616 = sext i32 %t3203 to i64
  %11617 = shl nsw i64 %11616, 4
  %11618 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11617
  %11619 = bitcast i8* %11618 to <8 x i8>*
  %t3204 = load <8 x i8>, <8 x i8>* %11619, align 16, !tbaa !438
  %t3205 = add nsw i32 %t3202, %t2413555
  %11620 = sext i32 %t3205 to i64
  %11621 = shl nsw i64 %11620, 4
  %11622 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11621
  %11623 = bitcast i8* %11622 to <8 x i8>*
  %t3206 = load <8 x i8>, <8 x i8>* %11623, align 16, !tbaa !438
  %t3207 = add nsw i32 %t3202, %t2409538
  %11624 = sext i32 %t3207 to i64
  %11625 = shl nsw i64 %11624, 4
  %11626 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11625
  %11627 = bitcast i8* %11626 to <8 x i8>*
  %t3208 = load <8 x i8>, <8 x i8>* %11627, align 16, !tbaa !438
  %t3209 = add nsw i32 %t3202, %t2404554
  %11628 = sext i32 %t3209 to i64
  %11629 = shl nsw i64 %11628, 4
  %11630 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11629
  %11631 = bitcast i8* %11630 to <8 x i8>*
  %t3210 = load <8 x i8>, <8 x i8>* %11631, align 16, !tbaa !438
  %t3211 = add nsw i32 %t3202, %t2400553
  %11632 = sext i32 %t3211 to i64
  %11633 = shl nsw i64 %11632, 4
  %11634 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11633
  %11635 = bitcast i8* %11634 to <8 x i8>*
  %t3212 = load <8 x i8>, <8 x i8>* %11635, align 16, !tbaa !438
  %t3213 = add nsw i32 %t3202, %t2396537
  %11636 = sext i32 %t3213 to i64
  %11637 = shl nsw i64 %11636, 4
  %11638 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11637
  %11639 = bitcast i8* %11638 to <8 x i8>*
  %t3214 = load <8 x i8>, <8 x i8>* %11639, align 16, !tbaa !438
  %t3215 = add nsw i32 %t3202, %t2391546
  %11640 = sext i32 %t3215 to i64
  %11641 = shl nsw i64 %11640, 4
  %11642 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11641
  %11643 = bitcast i8* %11642 to <8 x i8>*
  %t3216 = load <8 x i8>, <8 x i8>* %11643, align 16, !tbaa !438
  %t3217 = add nsw i32 %t3202, %t2387545
  %11644 = sext i32 %t3217 to i64
  %11645 = shl nsw i64 %11644, 4
  %11646 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11645
  %11647 = bitcast i8* %11646 to <8 x i8>*
  %t3218 = load <8 x i8>, <8 x i8>* %11647, align 16, !tbaa !438
  %t3219 = add nsw i32 %t3202, %t2383533
  %11648 = sext i32 %t3219 to i64
  %11649 = shl nsw i64 %11648, 4
  %11650 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11649
  %11651 = bitcast i8* %11650 to <8 x i8>*
  %t3220 = load <8 x i8>, <8 x i8>* %11651, align 16, !tbaa !438
  %11652 = getelementptr inbounds i8, i8* %11618, i64 8
  %11653 = bitcast i8* %11652 to <8 x i8>*
  %t3221 = load <8 x i8>, <8 x i8>* %11653, align 8, !tbaa !438
  %11654 = getelementptr inbounds i8, i8* %11622, i64 8
  %11655 = bitcast i8* %11654 to <8 x i8>*
  %t3222 = load <8 x i8>, <8 x i8>* %11655, align 8, !tbaa !438
  %11656 = getelementptr inbounds i8, i8* %11626, i64 8
  %11657 = bitcast i8* %11656 to <8 x i8>*
  %t3223 = load <8 x i8>, <8 x i8>* %11657, align 8, !tbaa !438
  %11658 = getelementptr inbounds i8, i8* %11630, i64 8
  %11659 = bitcast i8* %11658 to <8 x i8>*
  %t3224 = load <8 x i8>, <8 x i8>* %11659, align 8, !tbaa !438
  %11660 = getelementptr inbounds i8, i8* %11634, i64 8
  %11661 = bitcast i8* %11660 to <8 x i8>*
  %t3225 = load <8 x i8>, <8 x i8>* %11661, align 8, !tbaa !438
  %11662 = getelementptr inbounds i8, i8* %11638, i64 8
  %11663 = bitcast i8* %11662 to <8 x i8>*
  %t3226 = load <8 x i8>, <8 x i8>* %11663, align 8, !tbaa !438
  %11664 = getelementptr inbounds i8, i8* %11642, i64 8
  %11665 = bitcast i8* %11664 to <8 x i8>*
  %t3227 = load <8 x i8>, <8 x i8>* %11665, align 8, !tbaa !438
  %11666 = getelementptr inbounds i8, i8* %11646, i64 8
  %11667 = bitcast i8* %11666 to <8 x i8>*
  %t3228 = load <8 x i8>, <8 x i8>* %11667, align 8, !tbaa !438
  %11668 = getelementptr inbounds i8, i8* %11650, i64 8
  %11669 = bitcast i8* %11668 to <8 x i8>*
  %t3229 = load <8 x i8>, <8 x i8>* %11669, align 8, !tbaa !438
  %11670 = load <4 x i16>, <4 x i16>* %11355, align 16, !tbaa !395
  %11671 = zext <8 x i8> %t3204 to <8 x i16>
  %11672 = bitcast <8 x i16> %11671 to <2 x i64>
  %11673 = shufflevector <2 x i64> %11672, <2 x i64> undef, <1 x i32> zeroinitializer
  %11674 = bitcast <1 x i64> %11673 to <4 x i16>
  %11675 = load <4 x i16>, <4 x i16>* %11357, align 16, !tbaa !395
  %11676 = zext <8 x i8> %t3206 to <8 x i16>
  %11677 = bitcast <8 x i16> %11676 to <2 x i64>
  %11678 = shufflevector <2 x i64> %11677, <2 x i64> undef, <1 x i32> zeroinitializer
  %11679 = load <4 x i16>, <4 x i16>* %11358, align 16, !tbaa !395
  %11680 = zext <8 x i8> %t3208 to <8 x i16>
  %11681 = bitcast <8 x i16> %11680 to <2 x i64>
  %11682 = shufflevector <2 x i64> %11681, <2 x i64> undef, <1 x i32> zeroinitializer
  %11683 = bitcast <1 x i64> %11682 to <4 x i16>
  %11684 = load <4 x i16>, <4 x i16>* %11361, align 16, !tbaa !395
  %11685 = zext <8 x i8> %t3210 to <8 x i16>
  %11686 = bitcast <8 x i16> %11685 to <2 x i64>
  %11687 = shufflevector <2 x i64> %11686, <2 x i64> undef, <1 x i32> zeroinitializer
  %11688 = load <4 x i16>, <4 x i16>* %11363, align 16, !tbaa !395
  %11689 = zext <8 x i8> %t3212 to <8 x i16>
  %11690 = bitcast <8 x i16> %11689 to <2 x i64>
  %11691 = shufflevector <2 x i64> %11690, <2 x i64> undef, <1 x i32> zeroinitializer
  %11692 = bitcast <1 x i64> %11691 to <4 x i16>
  %11693 = load <4 x i16>, <4 x i16>* %11364, align 16, !tbaa !395
  %11694 = zext <8 x i8> %t3214 to <8 x i16>
  %11695 = bitcast <8 x i16> %11694 to <2 x i64>
  %11696 = shufflevector <2 x i64> %11695, <2 x i64> undef, <1 x i32> zeroinitializer
  %11697 = load <4 x i16>, <4 x i16>* %11366, align 16, !tbaa !534
  %11698 = zext <8 x i8> %t3216 to <8 x i16>
  %11699 = bitcast <8 x i16> %11698 to <2 x i64>
  %11700 = shufflevector <2 x i64> %11699, <2 x i64> undef, <1 x i32> zeroinitializer
  %11701 = bitcast <1 x i64> %11700 to <4 x i16>
  %11702 = load <4 x i16>, <4 x i16>* %11368, align 16, !tbaa !536
  %11703 = zext <8 x i8> %t3218 to <8 x i16>
  %11704 = bitcast <8 x i16> %11703 to <2 x i64>
  %11705 = shufflevector <2 x i64> %11704, <2 x i64> undef, <1 x i32> zeroinitializer
  %11706 = load <4 x i16>, <4 x i16>* %11369, align 16, !tbaa !538
  %11707 = zext <8 x i8> %t3220 to <8 x i16>
  %11708 = bitcast <8 x i16> %11707 to <2 x i64>
  %11709 = shufflevector <2 x i64> %11708, <2 x i64> undef, <1 x i32> zeroinitializer
  %11710 = bitcast <1 x i64> %11709 to <4 x i16>
  %.cast1813 = bitcast <1 x i64> %11678 to <4 x i16>
  %11711 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1813, <4 x i16> %11675) #11
  %11712 = sext <4 x i16> %11674 to <4 x i32>
  %11713 = sext <4 x i16> %11670 to <4 x i32>
  %11714 = mul nsw <4 x i32> %11713, %11712
  %.cast1816 = bitcast <1 x i64> %11687 to <4 x i16>
  %11715 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1816, <4 x i16> %11684) #11
  %11716 = sext <4 x i16> %11683 to <4 x i32>
  %11717 = sext <4 x i16> %11679 to <4 x i32>
  %11718 = mul nsw <4 x i32> %11717, %11716
  %.cast1819 = bitcast <1 x i64> %11696 to <4 x i16>
  %11719 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1819, <4 x i16> %11693) #11
  %11720 = sext <4 x i16> %11692 to <4 x i32>
  %11721 = sext <4 x i16> %11688 to <4 x i32>
  %11722 = mul nsw <4 x i32> %11721, %11720
  %.cast1822 = bitcast <1 x i64> %11705 to <4 x i16>
  %11723 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1822, <4 x i16> %11702) #11
  %11724 = sext <4 x i16> %11701 to <4 x i32>
  %11725 = sext <4 x i16> %11697 to <4 x i32>
  %11726 = mul nsw <4 x i32> %11725, %11724
  %11727 = sext <4 x i16> %11706 to <4 x i32>
  %11728 = sext <4 x i16> %11710 to <4 x i32>
  %11729 = mul nsw <4 x i32> %11727, %11728
  %11730 = add <4 x i32> %11714, %11562
  %11731 = add <4 x i32> %11730, %11718
  %11732 = add <4 x i32> %11731, %11722
  %11733 = add <4 x i32> %11732, %11726
  %11734 = add <4 x i32> %11733, %11711
  %11735 = add <4 x i32> %11734, %11715
  %11736 = add <4 x i32> %11735, %11729
  %11737 = add <4 x i32> %11736, %11719
  %11738 = add <4 x i32> %11737, %11723
  %11739 = load <4 x i16>, <4 x i16>* %11371, align 8, !tbaa !395
  %11740 = shufflevector <2 x i64> %11672, <2 x i64> undef, <1 x i32> <i32 1>
  %11741 = bitcast <1 x i64> %11740 to <4 x i16>
  %11742 = load <4 x i16>, <4 x i16>* %11373, align 8, !tbaa !395
  %11743 = shufflevector <2 x i64> %11677, <2 x i64> undef, <1 x i32> <i32 1>
  %11744 = load <4 x i16>, <4 x i16>* %11375, align 8, !tbaa !395
  %11745 = shufflevector <2 x i64> %11681, <2 x i64> undef, <1 x i32> <i32 1>
  %11746 = bitcast <1 x i64> %11745 to <4 x i16>
  %11747 = load <4 x i16>, <4 x i16>* %11377, align 8, !tbaa !395
  %11748 = shufflevector <2 x i64> %11686, <2 x i64> undef, <1 x i32> <i32 1>
  %11749 = load <4 x i16>, <4 x i16>* %11379, align 8, !tbaa !395
  %11750 = shufflevector <2 x i64> %11690, <2 x i64> undef, <1 x i32> <i32 1>
  %11751 = bitcast <1 x i64> %11750 to <4 x i16>
  %11752 = load <4 x i16>, <4 x i16>* %11381, align 8, !tbaa !395
  %11753 = shufflevector <2 x i64> %11695, <2 x i64> undef, <1 x i32> <i32 1>
  %11754 = load <4 x i16>, <4 x i16>* %11383, align 16, !tbaa !540
  %11755 = shufflevector <2 x i64> %11699, <2 x i64> undef, <1 x i32> <i32 1>
  %11756 = bitcast <1 x i64> %11755 to <4 x i16>
  %11757 = load <4 x i16>, <4 x i16>* %11385, align 16, !tbaa !542
  %11758 = shufflevector <2 x i64> %11704, <2 x i64> undef, <1 x i32> <i32 1>
  %11759 = load <4 x i16>, <4 x i16>* %11387, align 16, !tbaa !544
  %11760 = shufflevector <2 x i64> %11708, <2 x i64> undef, <1 x i32> <i32 1>
  %11761 = bitcast <1 x i64> %11760 to <4 x i16>
  %.cast1825 = bitcast <1 x i64> %11743 to <4 x i16>
  %11762 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1825, <4 x i16> %11742) #11
  %11763 = sext <4 x i16> %11741 to <4 x i32>
  %11764 = sext <4 x i16> %11739 to <4 x i32>
  %11765 = mul nsw <4 x i32> %11764, %11763
  %.cast1828 = bitcast <1 x i64> %11748 to <4 x i16>
  %11766 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1828, <4 x i16> %11747) #11
  %11767 = sext <4 x i16> %11746 to <4 x i32>
  %11768 = sext <4 x i16> %11744 to <4 x i32>
  %11769 = mul nsw <4 x i32> %11768, %11767
  %.cast1831 = bitcast <1 x i64> %11753 to <4 x i16>
  %11770 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1831, <4 x i16> %11752) #11
  %11771 = sext <4 x i16> %11751 to <4 x i32>
  %11772 = sext <4 x i16> %11749 to <4 x i32>
  %11773 = mul nsw <4 x i32> %11772, %11771
  %.cast1834 = bitcast <1 x i64> %11758 to <4 x i16>
  %11774 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1834, <4 x i16> %11757) #11
  %11775 = sext <4 x i16> %11756 to <4 x i32>
  %11776 = sext <4 x i16> %11754 to <4 x i32>
  %11777 = mul nsw <4 x i32> %11776, %11775
  %11778 = sext <4 x i16> %11759 to <4 x i32>
  %11779 = sext <4 x i16> %11761 to <4 x i32>
  %11780 = mul nsw <4 x i32> %11778, %11779
  %11781 = add <4 x i32> %11765, %11563
  %11782 = add <4 x i32> %11781, %11769
  %11783 = add <4 x i32> %11782, %11773
  %11784 = add <4 x i32> %11783, %11777
  %11785 = add <4 x i32> %11784, %11762
  %11786 = add <4 x i32> %11785, %11766
  %11787 = add <4 x i32> %11786, %11780
  %11788 = add <4 x i32> %11787, %11770
  %11789 = add <4 x i32> %11788, %11774
  %11790 = load <4 x i16>, <4 x i16>* %11389, align 16, !tbaa !395
  %11791 = zext <8 x i8> %t3221 to <8 x i16>
  %11792 = bitcast <8 x i16> %11791 to <2 x i64>
  %11793 = shufflevector <2 x i64> %11792, <2 x i64> undef, <1 x i32> zeroinitializer
  %11794 = bitcast <1 x i64> %11793 to <4 x i16>
  %11795 = load <4 x i16>, <4 x i16>* %11391, align 16, !tbaa !395
  %11796 = zext <8 x i8> %t3222 to <8 x i16>
  %11797 = bitcast <8 x i16> %11796 to <2 x i64>
  %11798 = shufflevector <2 x i64> %11797, <2 x i64> undef, <1 x i32> zeroinitializer
  %11799 = load <4 x i16>, <4 x i16>* %11393, align 16, !tbaa !395
  %11800 = zext <8 x i8> %t3223 to <8 x i16>
  %11801 = bitcast <8 x i16> %11800 to <2 x i64>
  %11802 = shufflevector <2 x i64> %11801, <2 x i64> undef, <1 x i32> zeroinitializer
  %11803 = bitcast <1 x i64> %11802 to <4 x i16>
  %11804 = load <4 x i16>, <4 x i16>* %11395, align 16, !tbaa !395
  %11805 = zext <8 x i8> %t3224 to <8 x i16>
  %11806 = bitcast <8 x i16> %11805 to <2 x i64>
  %11807 = shufflevector <2 x i64> %11806, <2 x i64> undef, <1 x i32> zeroinitializer
  %11808 = load <4 x i16>, <4 x i16>* %11397, align 16, !tbaa !395
  %11809 = zext <8 x i8> %t3225 to <8 x i16>
  %11810 = bitcast <8 x i16> %11809 to <2 x i64>
  %11811 = shufflevector <2 x i64> %11810, <2 x i64> undef, <1 x i32> zeroinitializer
  %11812 = bitcast <1 x i64> %11811 to <4 x i16>
  %11813 = load <4 x i16>, <4 x i16>* %11399, align 16, !tbaa !395
  %11814 = zext <8 x i8> %t3226 to <8 x i16>
  %11815 = bitcast <8 x i16> %11814 to <2 x i64>
  %11816 = shufflevector <2 x i64> %11815, <2 x i64> undef, <1 x i32> zeroinitializer
  %11817 = load <4 x i16>, <4 x i16>* %11401, align 16, !tbaa !546
  %11818 = zext <8 x i8> %t3227 to <8 x i16>
  %11819 = bitcast <8 x i16> %11818 to <2 x i64>
  %11820 = shufflevector <2 x i64> %11819, <2 x i64> undef, <1 x i32> zeroinitializer
  %11821 = bitcast <1 x i64> %11820 to <4 x i16>
  %11822 = load <4 x i16>, <4 x i16>* %11403, align 16, !tbaa !548
  %11823 = zext <8 x i8> %t3228 to <8 x i16>
  %11824 = bitcast <8 x i16> %11823 to <2 x i64>
  %11825 = shufflevector <2 x i64> %11824, <2 x i64> undef, <1 x i32> zeroinitializer
  %11826 = load <4 x i16>, <4 x i16>* %11405, align 16, !tbaa !550
  %11827 = zext <8 x i8> %t3229 to <8 x i16>
  %11828 = bitcast <8 x i16> %11827 to <2 x i64>
  %11829 = shufflevector <2 x i64> %11828, <2 x i64> undef, <1 x i32> zeroinitializer
  %11830 = bitcast <1 x i64> %11829 to <4 x i16>
  %.cast1837 = bitcast <1 x i64> %11798 to <4 x i16>
  %11831 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1837, <4 x i16> %11795) #11
  %11832 = sext <4 x i16> %11794 to <4 x i32>
  %11833 = sext <4 x i16> %11790 to <4 x i32>
  %11834 = mul nsw <4 x i32> %11833, %11832
  %.cast1840 = bitcast <1 x i64> %11807 to <4 x i16>
  %11835 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1840, <4 x i16> %11804) #11
  %11836 = sext <4 x i16> %11803 to <4 x i32>
  %11837 = sext <4 x i16> %11799 to <4 x i32>
  %11838 = mul nsw <4 x i32> %11837, %11836
  %.cast1843 = bitcast <1 x i64> %11816 to <4 x i16>
  %11839 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1843, <4 x i16> %11813) #11
  %11840 = sext <4 x i16> %11812 to <4 x i32>
  %11841 = sext <4 x i16> %11808 to <4 x i32>
  %11842 = mul nsw <4 x i32> %11841, %11840
  %.cast1846 = bitcast <1 x i64> %11825 to <4 x i16>
  %11843 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1846, <4 x i16> %11822) #11
  %11844 = sext <4 x i16> %11821 to <4 x i32>
  %11845 = sext <4 x i16> %11817 to <4 x i32>
  %11846 = mul nsw <4 x i32> %11845, %11844
  %11847 = sext <4 x i16> %11826 to <4 x i32>
  %11848 = sext <4 x i16> %11830 to <4 x i32>
  %11849 = mul nsw <4 x i32> %11847, %11848
  %11850 = add <4 x i32> %11834, %11564
  %11851 = add <4 x i32> %11850, %11838
  %11852 = add <4 x i32> %11851, %11842
  %11853 = add <4 x i32> %11852, %11846
  %11854 = add <4 x i32> %11853, %11831
  %11855 = add <4 x i32> %11854, %11835
  %11856 = add <4 x i32> %11855, %11849
  %11857 = add <4 x i32> %11856, %11839
  %11858 = add <4 x i32> %11857, %11843
  %11859 = load <4 x i16>, <4 x i16>* %11407, align 8, !tbaa !395
  %11860 = shufflevector <2 x i64> %11792, <2 x i64> undef, <1 x i32> <i32 1>
  %11861 = bitcast <1 x i64> %11860 to <4 x i16>
  %11862 = load <4 x i16>, <4 x i16>* %11409, align 8, !tbaa !395
  %11863 = shufflevector <2 x i64> %11797, <2 x i64> undef, <1 x i32> <i32 1>
  %11864 = load <4 x i16>, <4 x i16>* %11411, align 8, !tbaa !395
  %11865 = shufflevector <2 x i64> %11801, <2 x i64> undef, <1 x i32> <i32 1>
  %11866 = bitcast <1 x i64> %11865 to <4 x i16>
  %11867 = load <4 x i16>, <4 x i16>* %11413, align 8, !tbaa !395
  %11868 = shufflevector <2 x i64> %11806, <2 x i64> undef, <1 x i32> <i32 1>
  %11869 = load <4 x i16>, <4 x i16>* %11415, align 8, !tbaa !395
  %11870 = shufflevector <2 x i64> %11810, <2 x i64> undef, <1 x i32> <i32 1>
  %11871 = bitcast <1 x i64> %11870 to <4 x i16>
  %11872 = load <4 x i16>, <4 x i16>* %11417, align 8, !tbaa !395
  %11873 = shufflevector <2 x i64> %11815, <2 x i64> undef, <1 x i32> <i32 1>
  %11874 = load <4 x i16>, <4 x i16>* %11419, align 16, !tbaa !552
  %11875 = shufflevector <2 x i64> %11819, <2 x i64> undef, <1 x i32> <i32 1>
  %11876 = bitcast <1 x i64> %11875 to <4 x i16>
  %11877 = load <4 x i16>, <4 x i16>* %11421, align 16, !tbaa !554
  %11878 = shufflevector <2 x i64> %11824, <2 x i64> undef, <1 x i32> <i32 1>
  %11879 = load <4 x i16>, <4 x i16>* %11423, align 16, !tbaa !556
  %11880 = shufflevector <2 x i64> %11828, <2 x i64> undef, <1 x i32> <i32 1>
  %11881 = bitcast <1 x i64> %11880 to <4 x i16>
  %.cast1849 = bitcast <1 x i64> %11863 to <4 x i16>
  %11882 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1849, <4 x i16> %11862) #11
  %11883 = sext <4 x i16> %11861 to <4 x i32>
  %11884 = sext <4 x i16> %11859 to <4 x i32>
  %11885 = mul nsw <4 x i32> %11884, %11883
  %.cast1852 = bitcast <1 x i64> %11868 to <4 x i16>
  %11886 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1852, <4 x i16> %11867) #11
  %11887 = sext <4 x i16> %11866 to <4 x i32>
  %11888 = sext <4 x i16> %11864 to <4 x i32>
  %11889 = mul nsw <4 x i32> %11888, %11887
  %.cast1855 = bitcast <1 x i64> %11873 to <4 x i16>
  %11890 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1855, <4 x i16> %11872) #11
  %11891 = sext <4 x i16> %11871 to <4 x i32>
  %11892 = sext <4 x i16> %11869 to <4 x i32>
  %11893 = mul nsw <4 x i32> %11892, %11891
  %.cast1858 = bitcast <1 x i64> %11878 to <4 x i16>
  %11894 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1858, <4 x i16> %11877) #11
  %11895 = sext <4 x i16> %11876 to <4 x i32>
  %11896 = sext <4 x i16> %11874 to <4 x i32>
  %11897 = mul nsw <4 x i32> %11896, %11895
  %11898 = sext <4 x i16> %11879 to <4 x i32>
  %11899 = sext <4 x i16> %11881 to <4 x i32>
  %11900 = mul nsw <4 x i32> %11898, %11899
  %11901 = add <4 x i32> %11885, %11565
  %11902 = add <4 x i32> %11901, %11889
  %11903 = add <4 x i32> %11902, %11893
  %11904 = add <4 x i32> %11903, %11897
  %11905 = add <4 x i32> %11904, %11882
  %11906 = add <4 x i32> %11905, %11886
  %11907 = add <4 x i32> %11906, %11900
  %11908 = add <4 x i32> %11907, %11890
  %11909 = add <4 x i32> %11908, %11894
  %11910 = add nsw i32 %11615, 1
  %t3230 = mul nsw i32 %11910, %stride_x
  %t3231 = add nsw i32 %t3230, %t2417556
  %11911 = sext i32 %t3231 to i64
  %11912 = shl nsw i64 %11911, 4
  %11913 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11912
  %11914 = bitcast i8* %11913 to <8 x i8>*
  %t3232 = load <8 x i8>, <8 x i8>* %11914, align 16, !tbaa !438
  %t3233 = add nsw i32 %t3230, %t2413555
  %11915 = sext i32 %t3233 to i64
  %11916 = shl nsw i64 %11915, 4
  %11917 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11916
  %11918 = bitcast i8* %11917 to <8 x i8>*
  %t3234 = load <8 x i8>, <8 x i8>* %11918, align 16, !tbaa !438
  %t3235 = add nsw i32 %t3230, %t2409538
  %11919 = sext i32 %t3235 to i64
  %11920 = shl nsw i64 %11919, 4
  %11921 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11920
  %11922 = bitcast i8* %11921 to <8 x i8>*
  %t3236 = load <8 x i8>, <8 x i8>* %11922, align 16, !tbaa !438
  %t3237 = add nsw i32 %t3230, %t2404554
  %11923 = sext i32 %t3237 to i64
  %11924 = shl nsw i64 %11923, 4
  %11925 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11924
  %11926 = bitcast i8* %11925 to <8 x i8>*
  %t3238 = load <8 x i8>, <8 x i8>* %11926, align 16, !tbaa !438
  %t3239 = add nsw i32 %t3230, %t2400553
  %11927 = sext i32 %t3239 to i64
  %11928 = shl nsw i64 %11927, 4
  %11929 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11928
  %11930 = bitcast i8* %11929 to <8 x i8>*
  %t3240 = load <8 x i8>, <8 x i8>* %11930, align 16, !tbaa !438
  %t3241 = add nsw i32 %t3230, %t2396537
  %11931 = sext i32 %t3241 to i64
  %11932 = shl nsw i64 %11931, 4
  %11933 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11932
  %11934 = bitcast i8* %11933 to <8 x i8>*
  %t3242 = load <8 x i8>, <8 x i8>* %11934, align 16, !tbaa !438
  %t3243 = add nsw i32 %t3230, %t2391546
  %11935 = sext i32 %t3243 to i64
  %11936 = shl nsw i64 %11935, 4
  %11937 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11936
  %11938 = bitcast i8* %11937 to <8 x i8>*
  %t3244 = load <8 x i8>, <8 x i8>* %11938, align 16, !tbaa !438
  %t3245 = add nsw i32 %t3230, %t2387545
  %11939 = sext i32 %t3245 to i64
  %11940 = shl nsw i64 %11939, 4
  %11941 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11940
  %11942 = bitcast i8* %11941 to <8 x i8>*
  %t3246 = load <8 x i8>, <8 x i8>* %11942, align 16, !tbaa !438
  %t3247 = add nsw i32 %t3230, %t2383533
  %11943 = sext i32 %t3247 to i64
  %11944 = shl nsw i64 %11943, 4
  %11945 = getelementptr inbounds i8, i8* %resampled_input473, i64 %11944
  %11946 = bitcast i8* %11945 to <8 x i8>*
  %t3248 = load <8 x i8>, <8 x i8>* %11946, align 16, !tbaa !438
  %11947 = getelementptr inbounds i8, i8* %11913, i64 8
  %11948 = bitcast i8* %11947 to <8 x i8>*
  %t3249 = load <8 x i8>, <8 x i8>* %11948, align 8, !tbaa !438
  %11949 = getelementptr inbounds i8, i8* %11917, i64 8
  %11950 = bitcast i8* %11949 to <8 x i8>*
  %t3250 = load <8 x i8>, <8 x i8>* %11950, align 8, !tbaa !438
  %11951 = getelementptr inbounds i8, i8* %11921, i64 8
  %11952 = bitcast i8* %11951 to <8 x i8>*
  %t3251 = load <8 x i8>, <8 x i8>* %11952, align 8, !tbaa !438
  %11953 = getelementptr inbounds i8, i8* %11925, i64 8
  %11954 = bitcast i8* %11953 to <8 x i8>*
  %t3252 = load <8 x i8>, <8 x i8>* %11954, align 8, !tbaa !438
  %11955 = getelementptr inbounds i8, i8* %11929, i64 8
  %11956 = bitcast i8* %11955 to <8 x i8>*
  %t3253 = load <8 x i8>, <8 x i8>* %11956, align 8, !tbaa !438
  %11957 = getelementptr inbounds i8, i8* %11933, i64 8
  %11958 = bitcast i8* %11957 to <8 x i8>*
  %t3254 = load <8 x i8>, <8 x i8>* %11958, align 8, !tbaa !438
  %11959 = getelementptr inbounds i8, i8* %11937, i64 8
  %11960 = bitcast i8* %11959 to <8 x i8>*
  %t3255 = load <8 x i8>, <8 x i8>* %11960, align 8, !tbaa !438
  %11961 = getelementptr inbounds i8, i8* %11941, i64 8
  %11962 = bitcast i8* %11961 to <8 x i8>*
  %t3256 = load <8 x i8>, <8 x i8>* %11962, align 8, !tbaa !438
  %11963 = getelementptr inbounds i8, i8* %11945, i64 8
  %11964 = bitcast i8* %11963 to <8 x i8>*
  %t3257 = load <8 x i8>, <8 x i8>* %11964, align 8, !tbaa !438
  %11965 = zext <8 x i8> %t3232 to <8 x i16>
  %11966 = bitcast <8 x i16> %11965 to <2 x i64>
  %11967 = shufflevector <2 x i64> %11966, <2 x i64> undef, <1 x i32> zeroinitializer
  %11968 = bitcast <1 x i64> %11967 to <4 x i16>
  %11969 = zext <8 x i8> %t3234 to <8 x i16>
  %11970 = bitcast <8 x i16> %11969 to <2 x i64>
  %11971 = shufflevector <2 x i64> %11970, <2 x i64> undef, <1 x i32> zeroinitializer
  %11972 = zext <8 x i8> %t3236 to <8 x i16>
  %11973 = bitcast <8 x i16> %11972 to <2 x i64>
  %11974 = shufflevector <2 x i64> %11973, <2 x i64> undef, <1 x i32> zeroinitializer
  %11975 = bitcast <1 x i64> %11974 to <4 x i16>
  %11976 = zext <8 x i8> %t3238 to <8 x i16>
  %11977 = bitcast <8 x i16> %11976 to <2 x i64>
  %11978 = shufflevector <2 x i64> %11977, <2 x i64> undef, <1 x i32> zeroinitializer
  %11979 = zext <8 x i8> %t3240 to <8 x i16>
  %11980 = bitcast <8 x i16> %11979 to <2 x i64>
  %11981 = shufflevector <2 x i64> %11980, <2 x i64> undef, <1 x i32> zeroinitializer
  %11982 = bitcast <1 x i64> %11981 to <4 x i16>
  %11983 = zext <8 x i8> %t3242 to <8 x i16>
  %11984 = bitcast <8 x i16> %11983 to <2 x i64>
  %11985 = shufflevector <2 x i64> %11984, <2 x i64> undef, <1 x i32> zeroinitializer
  %11986 = zext <8 x i8> %t3244 to <8 x i16>
  %11987 = bitcast <8 x i16> %11986 to <2 x i64>
  %11988 = shufflevector <2 x i64> %11987, <2 x i64> undef, <1 x i32> zeroinitializer
  %11989 = bitcast <1 x i64> %11988 to <4 x i16>
  %11990 = zext <8 x i8> %t3246 to <8 x i16>
  %11991 = bitcast <8 x i16> %11990 to <2 x i64>
  %11992 = shufflevector <2 x i64> %11991, <2 x i64> undef, <1 x i32> zeroinitializer
  %11993 = zext <8 x i8> %t3248 to <8 x i16>
  %11994 = bitcast <8 x i16> %11993 to <2 x i64>
  %11995 = shufflevector <2 x i64> %11994, <2 x i64> undef, <1 x i32> zeroinitializer
  %11996 = bitcast <1 x i64> %11995 to <4 x i16>
  %.cast1861 = bitcast <1 x i64> %11971 to <4 x i16>
  %11997 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1861, <4 x i16> %11675) #11
  %11998 = sext <4 x i16> %11968 to <4 x i32>
  %11999 = mul nsw <4 x i32> %11998, %11713
  %.cast1864 = bitcast <1 x i64> %11978 to <4 x i16>
  %12000 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1864, <4 x i16> %11684) #11
  %12001 = sext <4 x i16> %11975 to <4 x i32>
  %12002 = mul nsw <4 x i32> %12001, %11717
  %.cast1867 = bitcast <1 x i64> %11985 to <4 x i16>
  %12003 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1867, <4 x i16> %11693) #11
  %12004 = sext <4 x i16> %11982 to <4 x i32>
  %12005 = mul nsw <4 x i32> %12004, %11721
  %.cast1870 = bitcast <1 x i64> %11992 to <4 x i16>
  %12006 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1870, <4 x i16> %11702) #11
  %12007 = sext <4 x i16> %11989 to <4 x i32>
  %12008 = mul nsw <4 x i32> %12007, %11725
  %12009 = sext <4 x i16> %11996 to <4 x i32>
  %12010 = mul nsw <4 x i32> %12009, %11727
  %12011 = add <4 x i32> %11999, %11562
  %12012 = add <4 x i32> %12011, %12002
  %12013 = add <4 x i32> %12012, %12005
  %12014 = add <4 x i32> %12013, %12008
  %12015 = add <4 x i32> %12014, %12010
  %12016 = add <4 x i32> %12015, %11997
  %12017 = add <4 x i32> %12016, %12000
  %12018 = add <4 x i32> %12017, %12003
  %12019 = add <4 x i32> %12018, %12006
  %12020 = shufflevector <2 x i64> %11966, <2 x i64> undef, <1 x i32> <i32 1>
  %12021 = bitcast <1 x i64> %12020 to <4 x i16>
  %12022 = shufflevector <2 x i64> %11970, <2 x i64> undef, <1 x i32> <i32 1>
  %12023 = shufflevector <2 x i64> %11973, <2 x i64> undef, <1 x i32> <i32 1>
  %12024 = bitcast <1 x i64> %12023 to <4 x i16>
  %12025 = shufflevector <2 x i64> %11977, <2 x i64> undef, <1 x i32> <i32 1>
  %12026 = shufflevector <2 x i64> %11980, <2 x i64> undef, <1 x i32> <i32 1>
  %12027 = bitcast <1 x i64> %12026 to <4 x i16>
  %12028 = shufflevector <2 x i64> %11984, <2 x i64> undef, <1 x i32> <i32 1>
  %12029 = shufflevector <2 x i64> %11987, <2 x i64> undef, <1 x i32> <i32 1>
  %12030 = bitcast <1 x i64> %12029 to <4 x i16>
  %12031 = shufflevector <2 x i64> %11991, <2 x i64> undef, <1 x i32> <i32 1>
  %12032 = shufflevector <2 x i64> %11994, <2 x i64> undef, <1 x i32> <i32 1>
  %12033 = bitcast <1 x i64> %12032 to <4 x i16>
  %.cast1873 = bitcast <1 x i64> %12022 to <4 x i16>
  %12034 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1873, <4 x i16> %11742) #11
  %12035 = sext <4 x i16> %12021 to <4 x i32>
  %12036 = mul nsw <4 x i32> %12035, %11764
  %.cast1876 = bitcast <1 x i64> %12025 to <4 x i16>
  %12037 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1876, <4 x i16> %11747) #11
  %12038 = sext <4 x i16> %12024 to <4 x i32>
  %12039 = mul nsw <4 x i32> %12038, %11768
  %.cast1879 = bitcast <1 x i64> %12028 to <4 x i16>
  %12040 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1879, <4 x i16> %11752) #11
  %12041 = sext <4 x i16> %12027 to <4 x i32>
  %12042 = mul nsw <4 x i32> %12041, %11772
  %.cast1882 = bitcast <1 x i64> %12031 to <4 x i16>
  %12043 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1882, <4 x i16> %11757) #11
  %12044 = sext <4 x i16> %12030 to <4 x i32>
  %12045 = mul nsw <4 x i32> %12044, %11776
  %12046 = sext <4 x i16> %12033 to <4 x i32>
  %12047 = mul nsw <4 x i32> %12046, %11778
  %12048 = add <4 x i32> %12036, %11563
  %12049 = add <4 x i32> %12048, %12039
  %12050 = add <4 x i32> %12049, %12042
  %12051 = add <4 x i32> %12050, %12045
  %12052 = add <4 x i32> %12051, %12047
  %12053 = add <4 x i32> %12052, %12034
  %12054 = add <4 x i32> %12053, %12037
  %12055 = add <4 x i32> %12054, %12040
  %12056 = add <4 x i32> %12055, %12043
  %12057 = zext <8 x i8> %t3249 to <8 x i16>
  %12058 = bitcast <8 x i16> %12057 to <2 x i64>
  %12059 = shufflevector <2 x i64> %12058, <2 x i64> undef, <1 x i32> zeroinitializer
  %12060 = bitcast <1 x i64> %12059 to <4 x i16>
  %12061 = zext <8 x i8> %t3250 to <8 x i16>
  %12062 = bitcast <8 x i16> %12061 to <2 x i64>
  %12063 = shufflevector <2 x i64> %12062, <2 x i64> undef, <1 x i32> zeroinitializer
  %12064 = zext <8 x i8> %t3251 to <8 x i16>
  %12065 = bitcast <8 x i16> %12064 to <2 x i64>
  %12066 = shufflevector <2 x i64> %12065, <2 x i64> undef, <1 x i32> zeroinitializer
  %12067 = bitcast <1 x i64> %12066 to <4 x i16>
  %12068 = zext <8 x i8> %t3252 to <8 x i16>
  %12069 = bitcast <8 x i16> %12068 to <2 x i64>
  %12070 = shufflevector <2 x i64> %12069, <2 x i64> undef, <1 x i32> zeroinitializer
  %12071 = zext <8 x i8> %t3253 to <8 x i16>
  %12072 = bitcast <8 x i16> %12071 to <2 x i64>
  %12073 = shufflevector <2 x i64> %12072, <2 x i64> undef, <1 x i32> zeroinitializer
  %12074 = bitcast <1 x i64> %12073 to <4 x i16>
  %12075 = zext <8 x i8> %t3254 to <8 x i16>
  %12076 = bitcast <8 x i16> %12075 to <2 x i64>
  %12077 = shufflevector <2 x i64> %12076, <2 x i64> undef, <1 x i32> zeroinitializer
  %12078 = zext <8 x i8> %t3255 to <8 x i16>
  %12079 = bitcast <8 x i16> %12078 to <2 x i64>
  %12080 = shufflevector <2 x i64> %12079, <2 x i64> undef, <1 x i32> zeroinitializer
  %12081 = bitcast <1 x i64> %12080 to <4 x i16>
  %12082 = zext <8 x i8> %t3256 to <8 x i16>
  %12083 = bitcast <8 x i16> %12082 to <2 x i64>
  %12084 = shufflevector <2 x i64> %12083, <2 x i64> undef, <1 x i32> zeroinitializer
  %12085 = zext <8 x i8> %t3257 to <8 x i16>
  %12086 = bitcast <8 x i16> %12085 to <2 x i64>
  %12087 = shufflevector <2 x i64> %12086, <2 x i64> undef, <1 x i32> zeroinitializer
  %12088 = bitcast <1 x i64> %12087 to <4 x i16>
  %.cast1885 = bitcast <1 x i64> %12063 to <4 x i16>
  %12089 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1885, <4 x i16> %11795) #11
  %12090 = sext <4 x i16> %12060 to <4 x i32>
  %12091 = mul nsw <4 x i32> %12090, %11833
  %.cast1888 = bitcast <1 x i64> %12070 to <4 x i16>
  %12092 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1888, <4 x i16> %11804) #11
  %12093 = sext <4 x i16> %12067 to <4 x i32>
  %12094 = mul nsw <4 x i32> %12093, %11837
  %.cast1891 = bitcast <1 x i64> %12077 to <4 x i16>
  %12095 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1891, <4 x i16> %11813) #11
  %12096 = sext <4 x i16> %12074 to <4 x i32>
  %12097 = mul nsw <4 x i32> %12096, %11841
  %.cast1894 = bitcast <1 x i64> %12084 to <4 x i16>
  %12098 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1894, <4 x i16> %11822) #11
  %12099 = sext <4 x i16> %12081 to <4 x i32>
  %12100 = mul nsw <4 x i32> %12099, %11845
  %12101 = sext <4 x i16> %12088 to <4 x i32>
  %12102 = mul nsw <4 x i32> %12101, %11847
  %12103 = add <4 x i32> %12091, %11564
  %12104 = add <4 x i32> %12103, %12094
  %12105 = add <4 x i32> %12104, %12097
  %12106 = add <4 x i32> %12105, %12100
  %12107 = add <4 x i32> %12106, %12102
  %12108 = add <4 x i32> %12107, %12089
  %12109 = add <4 x i32> %12108, %12092
  %12110 = add <4 x i32> %12109, %12095
  %12111 = add <4 x i32> %12110, %12098
  %12112 = shufflevector <2 x i64> %12058, <2 x i64> undef, <1 x i32> <i32 1>
  %12113 = bitcast <1 x i64> %12112 to <4 x i16>
  %12114 = shufflevector <2 x i64> %12062, <2 x i64> undef, <1 x i32> <i32 1>
  %12115 = shufflevector <2 x i64> %12065, <2 x i64> undef, <1 x i32> <i32 1>
  %12116 = bitcast <1 x i64> %12115 to <4 x i16>
  %12117 = shufflevector <2 x i64> %12069, <2 x i64> undef, <1 x i32> <i32 1>
  %12118 = shufflevector <2 x i64> %12072, <2 x i64> undef, <1 x i32> <i32 1>
  %12119 = bitcast <1 x i64> %12118 to <4 x i16>
  %12120 = shufflevector <2 x i64> %12076, <2 x i64> undef, <1 x i32> <i32 1>
  %12121 = shufflevector <2 x i64> %12079, <2 x i64> undef, <1 x i32> <i32 1>
  %12122 = bitcast <1 x i64> %12121 to <4 x i16>
  %12123 = shufflevector <2 x i64> %12083, <2 x i64> undef, <1 x i32> <i32 1>
  %12124 = shufflevector <2 x i64> %12086, <2 x i64> undef, <1 x i32> <i32 1>
  %12125 = bitcast <1 x i64> %12124 to <4 x i16>
  %.cast1897 = bitcast <1 x i64> %12114 to <4 x i16>
  %12126 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1897, <4 x i16> %11862) #11
  %12127 = sext <4 x i16> %12113 to <4 x i32>
  %12128 = mul nsw <4 x i32> %12127, %11884
  %.cast1900 = bitcast <1 x i64> %12117 to <4 x i16>
  %12129 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1900, <4 x i16> %11867) #11
  %12130 = sext <4 x i16> %12116 to <4 x i32>
  %12131 = mul nsw <4 x i32> %12130, %11888
  %.cast1903 = bitcast <1 x i64> %12120 to <4 x i16>
  %12132 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1903, <4 x i16> %11872) #11
  %12133 = sext <4 x i16> %12119 to <4 x i32>
  %12134 = mul nsw <4 x i32> %12133, %11892
  %.cast1906 = bitcast <1 x i64> %12123 to <4 x i16>
  %12135 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1906, <4 x i16> %11877) #11
  %12136 = sext <4 x i16> %12122 to <4 x i32>
  %12137 = mul nsw <4 x i32> %12136, %11896
  %12138 = sext <4 x i16> %12125 to <4 x i32>
  %12139 = mul nsw <4 x i32> %12138, %11898
  %12140 = add <4 x i32> %12128, %11565
  %12141 = add <4 x i32> %12140, %12131
  %12142 = add <4 x i32> %12141, %12134
  %12143 = add <4 x i32> %12142, %12137
  %12144 = add <4 x i32> %12143, %12139
  %12145 = add <4 x i32> %12144, %12126
  %12146 = add <4 x i32> %12145, %12129
  %12147 = add <4 x i32> %12146, %12132
  %12148 = add <4 x i32> %12147, %12135
  %12149 = add nsw i32 %11615, 2
  %t3258 = mul nsw i32 %12149, %stride_x
  %t3259 = add nsw i32 %t3258, %t2417556
  %12150 = sext i32 %t3259 to i64
  %12151 = shl nsw i64 %12150, 4
  %12152 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12151
  %12153 = bitcast i8* %12152 to <8 x i8>*
  %t3260 = load <8 x i8>, <8 x i8>* %12153, align 16, !tbaa !438
  %t3261 = add nsw i32 %t3258, %t2413555
  %12154 = sext i32 %t3261 to i64
  %12155 = shl nsw i64 %12154, 4
  %12156 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12155
  %12157 = bitcast i8* %12156 to <8 x i8>*
  %t3262 = load <8 x i8>, <8 x i8>* %12157, align 16, !tbaa !438
  %t3263 = add nsw i32 %t3258, %t2409538
  %12158 = sext i32 %t3263 to i64
  %12159 = shl nsw i64 %12158, 4
  %12160 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12159
  %12161 = bitcast i8* %12160 to <8 x i8>*
  %t3264 = load <8 x i8>, <8 x i8>* %12161, align 16, !tbaa !438
  %t3265 = add nsw i32 %t3258, %t2404554
  %12162 = sext i32 %t3265 to i64
  %12163 = shl nsw i64 %12162, 4
  %12164 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12163
  %12165 = bitcast i8* %12164 to <8 x i8>*
  %t3266 = load <8 x i8>, <8 x i8>* %12165, align 16, !tbaa !438
  %t3267 = add nsw i32 %t3258, %t2400553
  %12166 = sext i32 %t3267 to i64
  %12167 = shl nsw i64 %12166, 4
  %12168 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12167
  %12169 = bitcast i8* %12168 to <8 x i8>*
  %t3268 = load <8 x i8>, <8 x i8>* %12169, align 16, !tbaa !438
  %t3269 = add nsw i32 %t3258, %t2396537
  %12170 = sext i32 %t3269 to i64
  %12171 = shl nsw i64 %12170, 4
  %12172 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12171
  %12173 = bitcast i8* %12172 to <8 x i8>*
  %t3270 = load <8 x i8>, <8 x i8>* %12173, align 16, !tbaa !438
  %t3271 = add nsw i32 %t3258, %t2391546
  %12174 = sext i32 %t3271 to i64
  %12175 = shl nsw i64 %12174, 4
  %12176 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12175
  %12177 = bitcast i8* %12176 to <8 x i8>*
  %t3272 = load <8 x i8>, <8 x i8>* %12177, align 16, !tbaa !438
  %t3273 = add nsw i32 %t3258, %t2387545
  %12178 = sext i32 %t3273 to i64
  %12179 = shl nsw i64 %12178, 4
  %12180 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12179
  %12181 = bitcast i8* %12180 to <8 x i8>*
  %t3274 = load <8 x i8>, <8 x i8>* %12181, align 16, !tbaa !438
  %t3275 = add nsw i32 %t3258, %t2383533
  %12182 = sext i32 %t3275 to i64
  %12183 = shl nsw i64 %12182, 4
  %12184 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12183
  %12185 = bitcast i8* %12184 to <8 x i8>*
  %t3276 = load <8 x i8>, <8 x i8>* %12185, align 16, !tbaa !438
  %12186 = getelementptr inbounds i8, i8* %12152, i64 8
  %12187 = bitcast i8* %12186 to <8 x i8>*
  %t3277 = load <8 x i8>, <8 x i8>* %12187, align 8, !tbaa !438
  %12188 = getelementptr inbounds i8, i8* %12156, i64 8
  %12189 = bitcast i8* %12188 to <8 x i8>*
  %t3278 = load <8 x i8>, <8 x i8>* %12189, align 8, !tbaa !438
  %12190 = getelementptr inbounds i8, i8* %12160, i64 8
  %12191 = bitcast i8* %12190 to <8 x i8>*
  %t3279 = load <8 x i8>, <8 x i8>* %12191, align 8, !tbaa !438
  %12192 = getelementptr inbounds i8, i8* %12164, i64 8
  %12193 = bitcast i8* %12192 to <8 x i8>*
  %t3280 = load <8 x i8>, <8 x i8>* %12193, align 8, !tbaa !438
  %12194 = getelementptr inbounds i8, i8* %12168, i64 8
  %12195 = bitcast i8* %12194 to <8 x i8>*
  %t3281 = load <8 x i8>, <8 x i8>* %12195, align 8, !tbaa !438
  %12196 = getelementptr inbounds i8, i8* %12172, i64 8
  %12197 = bitcast i8* %12196 to <8 x i8>*
  %t3282 = load <8 x i8>, <8 x i8>* %12197, align 8, !tbaa !438
  %12198 = getelementptr inbounds i8, i8* %12176, i64 8
  %12199 = bitcast i8* %12198 to <8 x i8>*
  %t3283 = load <8 x i8>, <8 x i8>* %12199, align 8, !tbaa !438
  %12200 = getelementptr inbounds i8, i8* %12180, i64 8
  %12201 = bitcast i8* %12200 to <8 x i8>*
  %t3284 = load <8 x i8>, <8 x i8>* %12201, align 8, !tbaa !438
  %12202 = getelementptr inbounds i8, i8* %12184, i64 8
  %12203 = bitcast i8* %12202 to <8 x i8>*
  %t3285 = load <8 x i8>, <8 x i8>* %12203, align 8, !tbaa !438
  %12204 = zext <8 x i8> %t3260 to <8 x i16>
  %12205 = bitcast <8 x i16> %12204 to <2 x i64>
  %12206 = shufflevector <2 x i64> %12205, <2 x i64> undef, <1 x i32> zeroinitializer
  %12207 = bitcast <1 x i64> %12206 to <4 x i16>
  %12208 = zext <8 x i8> %t3262 to <8 x i16>
  %12209 = bitcast <8 x i16> %12208 to <2 x i64>
  %12210 = shufflevector <2 x i64> %12209, <2 x i64> undef, <1 x i32> zeroinitializer
  %12211 = zext <8 x i8> %t3264 to <8 x i16>
  %12212 = bitcast <8 x i16> %12211 to <2 x i64>
  %12213 = shufflevector <2 x i64> %12212, <2 x i64> undef, <1 x i32> zeroinitializer
  %12214 = bitcast <1 x i64> %12213 to <4 x i16>
  %12215 = zext <8 x i8> %t3266 to <8 x i16>
  %12216 = bitcast <8 x i16> %12215 to <2 x i64>
  %12217 = shufflevector <2 x i64> %12216, <2 x i64> undef, <1 x i32> zeroinitializer
  %12218 = zext <8 x i8> %t3268 to <8 x i16>
  %12219 = bitcast <8 x i16> %12218 to <2 x i64>
  %12220 = shufflevector <2 x i64> %12219, <2 x i64> undef, <1 x i32> zeroinitializer
  %12221 = bitcast <1 x i64> %12220 to <4 x i16>
  %12222 = zext <8 x i8> %t3270 to <8 x i16>
  %12223 = bitcast <8 x i16> %12222 to <2 x i64>
  %12224 = shufflevector <2 x i64> %12223, <2 x i64> undef, <1 x i32> zeroinitializer
  %12225 = zext <8 x i8> %t3272 to <8 x i16>
  %12226 = bitcast <8 x i16> %12225 to <2 x i64>
  %12227 = shufflevector <2 x i64> %12226, <2 x i64> undef, <1 x i32> zeroinitializer
  %12228 = bitcast <1 x i64> %12227 to <4 x i16>
  %12229 = zext <8 x i8> %t3274 to <8 x i16>
  %12230 = bitcast <8 x i16> %12229 to <2 x i64>
  %12231 = shufflevector <2 x i64> %12230, <2 x i64> undef, <1 x i32> zeroinitializer
  %12232 = zext <8 x i8> %t3276 to <8 x i16>
  %12233 = bitcast <8 x i16> %12232 to <2 x i64>
  %12234 = shufflevector <2 x i64> %12233, <2 x i64> undef, <1 x i32> zeroinitializer
  %12235 = bitcast <1 x i64> %12234 to <4 x i16>
  %.cast1909 = bitcast <1 x i64> %12210 to <4 x i16>
  %12236 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1909, <4 x i16> %11675) #11
  %12237 = sext <4 x i16> %12207 to <4 x i32>
  %12238 = mul nsw <4 x i32> %12237, %11713
  %.cast1912 = bitcast <1 x i64> %12217 to <4 x i16>
  %12239 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1912, <4 x i16> %11684) #11
  %12240 = sext <4 x i16> %12214 to <4 x i32>
  %12241 = mul nsw <4 x i32> %12240, %11717
  %.cast1915 = bitcast <1 x i64> %12224 to <4 x i16>
  %12242 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1915, <4 x i16> %11693) #11
  %12243 = sext <4 x i16> %12221 to <4 x i32>
  %12244 = mul nsw <4 x i32> %12243, %11721
  %.cast1918 = bitcast <1 x i64> %12231 to <4 x i16>
  %12245 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1918, <4 x i16> %11702) #11
  %12246 = sext <4 x i16> %12228 to <4 x i32>
  %12247 = mul nsw <4 x i32> %12246, %11725
  %12248 = sext <4 x i16> %12235 to <4 x i32>
  %12249 = mul nsw <4 x i32> %12248, %11727
  %12250 = add <4 x i32> %12238, %11562
  %12251 = add <4 x i32> %12250, %12241
  %12252 = add <4 x i32> %12251, %12244
  %12253 = add <4 x i32> %12252, %12247
  %12254 = add <4 x i32> %12253, %12249
  %12255 = add <4 x i32> %12254, %12236
  %12256 = add <4 x i32> %12255, %12239
  %12257 = add <4 x i32> %12256, %12242
  %12258 = add <4 x i32> %12257, %12245
  %12259 = shufflevector <2 x i64> %12205, <2 x i64> undef, <1 x i32> <i32 1>
  %12260 = bitcast <1 x i64> %12259 to <4 x i16>
  %12261 = shufflevector <2 x i64> %12209, <2 x i64> undef, <1 x i32> <i32 1>
  %12262 = shufflevector <2 x i64> %12212, <2 x i64> undef, <1 x i32> <i32 1>
  %12263 = bitcast <1 x i64> %12262 to <4 x i16>
  %12264 = shufflevector <2 x i64> %12216, <2 x i64> undef, <1 x i32> <i32 1>
  %12265 = shufflevector <2 x i64> %12219, <2 x i64> undef, <1 x i32> <i32 1>
  %12266 = bitcast <1 x i64> %12265 to <4 x i16>
  %12267 = shufflevector <2 x i64> %12223, <2 x i64> undef, <1 x i32> <i32 1>
  %12268 = shufflevector <2 x i64> %12226, <2 x i64> undef, <1 x i32> <i32 1>
  %12269 = bitcast <1 x i64> %12268 to <4 x i16>
  %12270 = shufflevector <2 x i64> %12230, <2 x i64> undef, <1 x i32> <i32 1>
  %12271 = shufflevector <2 x i64> %12233, <2 x i64> undef, <1 x i32> <i32 1>
  %12272 = bitcast <1 x i64> %12271 to <4 x i16>
  %.cast1921 = bitcast <1 x i64> %12261 to <4 x i16>
  %12273 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1921, <4 x i16> %11742) #11
  %12274 = sext <4 x i16> %12260 to <4 x i32>
  %12275 = mul nsw <4 x i32> %12274, %11764
  %.cast1924 = bitcast <1 x i64> %12264 to <4 x i16>
  %12276 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1924, <4 x i16> %11747) #11
  %12277 = sext <4 x i16> %12263 to <4 x i32>
  %12278 = mul nsw <4 x i32> %12277, %11768
  %.cast1927 = bitcast <1 x i64> %12267 to <4 x i16>
  %12279 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1927, <4 x i16> %11752) #11
  %12280 = sext <4 x i16> %12266 to <4 x i32>
  %12281 = mul nsw <4 x i32> %12280, %11772
  %.cast1930 = bitcast <1 x i64> %12270 to <4 x i16>
  %12282 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1930, <4 x i16> %11757) #11
  %12283 = sext <4 x i16> %12269 to <4 x i32>
  %12284 = mul nsw <4 x i32> %12283, %11776
  %12285 = sext <4 x i16> %12272 to <4 x i32>
  %12286 = mul nsw <4 x i32> %12285, %11778
  %12287 = add <4 x i32> %12275, %11563
  %12288 = add <4 x i32> %12287, %12278
  %12289 = add <4 x i32> %12288, %12281
  %12290 = add <4 x i32> %12289, %12284
  %12291 = add <4 x i32> %12290, %12286
  %12292 = add <4 x i32> %12291, %12273
  %12293 = add <4 x i32> %12292, %12276
  %12294 = add <4 x i32> %12293, %12279
  %12295 = add <4 x i32> %12294, %12282
  %12296 = zext <8 x i8> %t3277 to <8 x i16>
  %12297 = bitcast <8 x i16> %12296 to <2 x i64>
  %12298 = shufflevector <2 x i64> %12297, <2 x i64> undef, <1 x i32> zeroinitializer
  %12299 = bitcast <1 x i64> %12298 to <4 x i16>
  %12300 = zext <8 x i8> %t3278 to <8 x i16>
  %12301 = bitcast <8 x i16> %12300 to <2 x i64>
  %12302 = shufflevector <2 x i64> %12301, <2 x i64> undef, <1 x i32> zeroinitializer
  %12303 = zext <8 x i8> %t3279 to <8 x i16>
  %12304 = bitcast <8 x i16> %12303 to <2 x i64>
  %12305 = shufflevector <2 x i64> %12304, <2 x i64> undef, <1 x i32> zeroinitializer
  %12306 = bitcast <1 x i64> %12305 to <4 x i16>
  %12307 = zext <8 x i8> %t3280 to <8 x i16>
  %12308 = bitcast <8 x i16> %12307 to <2 x i64>
  %12309 = shufflevector <2 x i64> %12308, <2 x i64> undef, <1 x i32> zeroinitializer
  %12310 = zext <8 x i8> %t3281 to <8 x i16>
  %12311 = bitcast <8 x i16> %12310 to <2 x i64>
  %12312 = shufflevector <2 x i64> %12311, <2 x i64> undef, <1 x i32> zeroinitializer
  %12313 = bitcast <1 x i64> %12312 to <4 x i16>
  %12314 = zext <8 x i8> %t3282 to <8 x i16>
  %12315 = bitcast <8 x i16> %12314 to <2 x i64>
  %12316 = shufflevector <2 x i64> %12315, <2 x i64> undef, <1 x i32> zeroinitializer
  %12317 = zext <8 x i8> %t3283 to <8 x i16>
  %12318 = bitcast <8 x i16> %12317 to <2 x i64>
  %12319 = shufflevector <2 x i64> %12318, <2 x i64> undef, <1 x i32> zeroinitializer
  %12320 = bitcast <1 x i64> %12319 to <4 x i16>
  %12321 = zext <8 x i8> %t3284 to <8 x i16>
  %12322 = bitcast <8 x i16> %12321 to <2 x i64>
  %12323 = shufflevector <2 x i64> %12322, <2 x i64> undef, <1 x i32> zeroinitializer
  %12324 = zext <8 x i8> %t3285 to <8 x i16>
  %12325 = bitcast <8 x i16> %12324 to <2 x i64>
  %12326 = shufflevector <2 x i64> %12325, <2 x i64> undef, <1 x i32> zeroinitializer
  %12327 = bitcast <1 x i64> %12326 to <4 x i16>
  %.cast1933 = bitcast <1 x i64> %12302 to <4 x i16>
  %12328 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1933, <4 x i16> %11795) #11
  %12329 = sext <4 x i16> %12299 to <4 x i32>
  %12330 = mul nsw <4 x i32> %12329, %11833
  %.cast1936 = bitcast <1 x i64> %12309 to <4 x i16>
  %12331 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1936, <4 x i16> %11804) #11
  %12332 = sext <4 x i16> %12306 to <4 x i32>
  %12333 = mul nsw <4 x i32> %12332, %11837
  %.cast1939 = bitcast <1 x i64> %12316 to <4 x i16>
  %12334 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1939, <4 x i16> %11813) #11
  %12335 = sext <4 x i16> %12313 to <4 x i32>
  %12336 = mul nsw <4 x i32> %12335, %11841
  %.cast1942 = bitcast <1 x i64> %12323 to <4 x i16>
  %12337 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1942, <4 x i16> %11822) #11
  %12338 = sext <4 x i16> %12320 to <4 x i32>
  %12339 = mul nsw <4 x i32> %12338, %11845
  %12340 = sext <4 x i16> %12327 to <4 x i32>
  %12341 = mul nsw <4 x i32> %12340, %11847
  %12342 = add <4 x i32> %12330, %11564
  %12343 = add <4 x i32> %12342, %12333
  %12344 = add <4 x i32> %12343, %12336
  %12345 = add <4 x i32> %12344, %12339
  %12346 = add <4 x i32> %12345, %12341
  %12347 = add <4 x i32> %12346, %12328
  %12348 = add <4 x i32> %12347, %12331
  %12349 = add <4 x i32> %12348, %12334
  %12350 = add <4 x i32> %12349, %12337
  %12351 = shufflevector <2 x i64> %12297, <2 x i64> undef, <1 x i32> <i32 1>
  %12352 = bitcast <1 x i64> %12351 to <4 x i16>
  %12353 = shufflevector <2 x i64> %12301, <2 x i64> undef, <1 x i32> <i32 1>
  %12354 = shufflevector <2 x i64> %12304, <2 x i64> undef, <1 x i32> <i32 1>
  %12355 = bitcast <1 x i64> %12354 to <4 x i16>
  %12356 = shufflevector <2 x i64> %12308, <2 x i64> undef, <1 x i32> <i32 1>
  %12357 = shufflevector <2 x i64> %12311, <2 x i64> undef, <1 x i32> <i32 1>
  %12358 = bitcast <1 x i64> %12357 to <4 x i16>
  %12359 = shufflevector <2 x i64> %12315, <2 x i64> undef, <1 x i32> <i32 1>
  %12360 = shufflevector <2 x i64> %12318, <2 x i64> undef, <1 x i32> <i32 1>
  %12361 = bitcast <1 x i64> %12360 to <4 x i16>
  %12362 = shufflevector <2 x i64> %12322, <2 x i64> undef, <1 x i32> <i32 1>
  %12363 = shufflevector <2 x i64> %12325, <2 x i64> undef, <1 x i32> <i32 1>
  %12364 = bitcast <1 x i64> %12363 to <4 x i16>
  %.cast1945 = bitcast <1 x i64> %12353 to <4 x i16>
  %12365 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1945, <4 x i16> %11862) #11
  %12366 = sext <4 x i16> %12352 to <4 x i32>
  %12367 = mul nsw <4 x i32> %12366, %11884
  %.cast1948 = bitcast <1 x i64> %12356 to <4 x i16>
  %12368 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1948, <4 x i16> %11867) #11
  %12369 = sext <4 x i16> %12355 to <4 x i32>
  %12370 = mul nsw <4 x i32> %12369, %11888
  %.cast1951 = bitcast <1 x i64> %12359 to <4 x i16>
  %12371 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1951, <4 x i16> %11872) #11
  %12372 = sext <4 x i16> %12358 to <4 x i32>
  %12373 = mul nsw <4 x i32> %12372, %11892
  %.cast1954 = bitcast <1 x i64> %12362 to <4 x i16>
  %12374 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1954, <4 x i16> %11877) #11
  %12375 = sext <4 x i16> %12361 to <4 x i32>
  %12376 = mul nsw <4 x i32> %12375, %11896
  %12377 = sext <4 x i16> %12364 to <4 x i32>
  %12378 = mul nsw <4 x i32> %12377, %11898
  %12379 = add <4 x i32> %12367, %11565
  %12380 = add <4 x i32> %12379, %12370
  %12381 = add <4 x i32> %12380, %12373
  %12382 = add <4 x i32> %12381, %12376
  %12383 = add <4 x i32> %12382, %12378
  %12384 = add <4 x i32> %12383, %12365
  %12385 = add <4 x i32> %12384, %12368
  %12386 = add <4 x i32> %12385, %12371
  %12387 = add <4 x i32> %12386, %12374
  %12388 = add nsw i32 %11615, 3
  %t3286 = mul nsw i32 %12388, %stride_x
  %t3287 = add nsw i32 %t3286, %t2417556
  %12389 = sext i32 %t3287 to i64
  %12390 = shl nsw i64 %12389, 4
  %12391 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12390
  %12392 = bitcast i8* %12391 to <8 x i8>*
  %t3288 = load <8 x i8>, <8 x i8>* %12392, align 16, !tbaa !438
  %t3289 = add nsw i32 %t3286, %t2413555
  %12393 = sext i32 %t3289 to i64
  %12394 = shl nsw i64 %12393, 4
  %12395 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12394
  %12396 = bitcast i8* %12395 to <8 x i8>*
  %t3290 = load <8 x i8>, <8 x i8>* %12396, align 16, !tbaa !438
  %t3291 = add nsw i32 %t3286, %t2409538
  %12397 = sext i32 %t3291 to i64
  %12398 = shl nsw i64 %12397, 4
  %12399 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12398
  %12400 = bitcast i8* %12399 to <8 x i8>*
  %t3292 = load <8 x i8>, <8 x i8>* %12400, align 16, !tbaa !438
  %t3293 = add nsw i32 %t3286, %t2404554
  %12401 = sext i32 %t3293 to i64
  %12402 = shl nsw i64 %12401, 4
  %12403 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12402
  %12404 = bitcast i8* %12403 to <8 x i8>*
  %t3294 = load <8 x i8>, <8 x i8>* %12404, align 16, !tbaa !438
  %t3295 = add nsw i32 %t3286, %t2400553
  %12405 = sext i32 %t3295 to i64
  %12406 = shl nsw i64 %12405, 4
  %12407 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12406
  %12408 = bitcast i8* %12407 to <8 x i8>*
  %t3296 = load <8 x i8>, <8 x i8>* %12408, align 16, !tbaa !438
  %t3297 = add nsw i32 %t3286, %t2396537
  %12409 = sext i32 %t3297 to i64
  %12410 = shl nsw i64 %12409, 4
  %12411 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12410
  %12412 = bitcast i8* %12411 to <8 x i8>*
  %t3298 = load <8 x i8>, <8 x i8>* %12412, align 16, !tbaa !438
  %t3299 = add nsw i32 %t3286, %t2391546
  %12413 = sext i32 %t3299 to i64
  %12414 = shl nsw i64 %12413, 4
  %12415 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12414
  %12416 = bitcast i8* %12415 to <8 x i8>*
  %t3300 = load <8 x i8>, <8 x i8>* %12416, align 16, !tbaa !438
  %t3301 = add nsw i32 %t3286, %t2387545
  %12417 = sext i32 %t3301 to i64
  %12418 = shl nsw i64 %12417, 4
  %12419 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12418
  %12420 = bitcast i8* %12419 to <8 x i8>*
  %t3302 = load <8 x i8>, <8 x i8>* %12420, align 16, !tbaa !438
  %t3303 = add nsw i32 %t3286, %t2383533
  %12421 = sext i32 %t3303 to i64
  %12422 = shl nsw i64 %12421, 4
  %12423 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12422
  %12424 = bitcast i8* %12423 to <8 x i8>*
  %t3304 = load <8 x i8>, <8 x i8>* %12424, align 16, !tbaa !438
  %12425 = getelementptr inbounds i8, i8* %12391, i64 8
  %12426 = bitcast i8* %12425 to <8 x i8>*
  %t3305 = load <8 x i8>, <8 x i8>* %12426, align 8, !tbaa !438
  %12427 = getelementptr inbounds i8, i8* %12395, i64 8
  %12428 = bitcast i8* %12427 to <8 x i8>*
  %t3306 = load <8 x i8>, <8 x i8>* %12428, align 8, !tbaa !438
  %12429 = getelementptr inbounds i8, i8* %12399, i64 8
  %12430 = bitcast i8* %12429 to <8 x i8>*
  %t3307 = load <8 x i8>, <8 x i8>* %12430, align 8, !tbaa !438
  %12431 = getelementptr inbounds i8, i8* %12403, i64 8
  %12432 = bitcast i8* %12431 to <8 x i8>*
  %t3308 = load <8 x i8>, <8 x i8>* %12432, align 8, !tbaa !438
  %12433 = getelementptr inbounds i8, i8* %12407, i64 8
  %12434 = bitcast i8* %12433 to <8 x i8>*
  %t3309 = load <8 x i8>, <8 x i8>* %12434, align 8, !tbaa !438
  %12435 = getelementptr inbounds i8, i8* %12411, i64 8
  %12436 = bitcast i8* %12435 to <8 x i8>*
  %t3310 = load <8 x i8>, <8 x i8>* %12436, align 8, !tbaa !438
  %12437 = getelementptr inbounds i8, i8* %12415, i64 8
  %12438 = bitcast i8* %12437 to <8 x i8>*
  %t3311 = load <8 x i8>, <8 x i8>* %12438, align 8, !tbaa !438
  %12439 = getelementptr inbounds i8, i8* %12419, i64 8
  %12440 = bitcast i8* %12439 to <8 x i8>*
  %t3312 = load <8 x i8>, <8 x i8>* %12440, align 8, !tbaa !438
  %12441 = getelementptr inbounds i8, i8* %12423, i64 8
  %12442 = bitcast i8* %12441 to <8 x i8>*
  %t3313 = load <8 x i8>, <8 x i8>* %12442, align 8, !tbaa !438
  %12443 = zext <8 x i8> %t3288 to <8 x i16>
  %12444 = bitcast <8 x i16> %12443 to <2 x i64>
  %12445 = shufflevector <2 x i64> %12444, <2 x i64> undef, <1 x i32> zeroinitializer
  %12446 = bitcast <1 x i64> %12445 to <4 x i16>
  %12447 = zext <8 x i8> %t3290 to <8 x i16>
  %12448 = bitcast <8 x i16> %12447 to <2 x i64>
  %12449 = shufflevector <2 x i64> %12448, <2 x i64> undef, <1 x i32> zeroinitializer
  %12450 = zext <8 x i8> %t3292 to <8 x i16>
  %12451 = bitcast <8 x i16> %12450 to <2 x i64>
  %12452 = shufflevector <2 x i64> %12451, <2 x i64> undef, <1 x i32> zeroinitializer
  %12453 = bitcast <1 x i64> %12452 to <4 x i16>
  %12454 = zext <8 x i8> %t3294 to <8 x i16>
  %12455 = bitcast <8 x i16> %12454 to <2 x i64>
  %12456 = shufflevector <2 x i64> %12455, <2 x i64> undef, <1 x i32> zeroinitializer
  %12457 = zext <8 x i8> %t3296 to <8 x i16>
  %12458 = bitcast <8 x i16> %12457 to <2 x i64>
  %12459 = shufflevector <2 x i64> %12458, <2 x i64> undef, <1 x i32> zeroinitializer
  %12460 = bitcast <1 x i64> %12459 to <4 x i16>
  %12461 = zext <8 x i8> %t3298 to <8 x i16>
  %12462 = bitcast <8 x i16> %12461 to <2 x i64>
  %12463 = shufflevector <2 x i64> %12462, <2 x i64> undef, <1 x i32> zeroinitializer
  %12464 = zext <8 x i8> %t3300 to <8 x i16>
  %12465 = bitcast <8 x i16> %12464 to <2 x i64>
  %12466 = shufflevector <2 x i64> %12465, <2 x i64> undef, <1 x i32> zeroinitializer
  %12467 = bitcast <1 x i64> %12466 to <4 x i16>
  %12468 = zext <8 x i8> %t3302 to <8 x i16>
  %12469 = bitcast <8 x i16> %12468 to <2 x i64>
  %12470 = shufflevector <2 x i64> %12469, <2 x i64> undef, <1 x i32> zeroinitializer
  %12471 = zext <8 x i8> %t3304 to <8 x i16>
  %12472 = bitcast <8 x i16> %12471 to <2 x i64>
  %12473 = shufflevector <2 x i64> %12472, <2 x i64> undef, <1 x i32> zeroinitializer
  %12474 = bitcast <1 x i64> %12473 to <4 x i16>
  %.cast1957 = bitcast <1 x i64> %12449 to <4 x i16>
  %12475 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1957, <4 x i16> %11675) #11
  %12476 = sext <4 x i16> %12446 to <4 x i32>
  %12477 = mul nsw <4 x i32> %12476, %11713
  %.cast1960 = bitcast <1 x i64> %12456 to <4 x i16>
  %12478 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1960, <4 x i16> %11684) #11
  %12479 = sext <4 x i16> %12453 to <4 x i32>
  %12480 = mul nsw <4 x i32> %12479, %11717
  %.cast1963 = bitcast <1 x i64> %12463 to <4 x i16>
  %12481 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1963, <4 x i16> %11693) #11
  %12482 = sext <4 x i16> %12460 to <4 x i32>
  %12483 = mul nsw <4 x i32> %12482, %11721
  %.cast1966 = bitcast <1 x i64> %12470 to <4 x i16>
  %12484 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1966, <4 x i16> %11702) #11
  %12485 = sext <4 x i16> %12467 to <4 x i32>
  %12486 = mul nsw <4 x i32> %12485, %11725
  %12487 = sext <4 x i16> %12474 to <4 x i32>
  %12488 = mul nsw <4 x i32> %12487, %11727
  %12489 = add <4 x i32> %12477, %11562
  %12490 = add <4 x i32> %12489, %12480
  %12491 = add <4 x i32> %12490, %12483
  %12492 = add <4 x i32> %12491, %12486
  %12493 = add <4 x i32> %12492, %12488
  %12494 = add <4 x i32> %12493, %12475
  %12495 = add <4 x i32> %12494, %12478
  %12496 = add <4 x i32> %12495, %12481
  %12497 = add <4 x i32> %12496, %12484
  %12498 = shufflevector <2 x i64> %12444, <2 x i64> undef, <1 x i32> <i32 1>
  %12499 = bitcast <1 x i64> %12498 to <4 x i16>
  %12500 = shufflevector <2 x i64> %12448, <2 x i64> undef, <1 x i32> <i32 1>
  %12501 = shufflevector <2 x i64> %12451, <2 x i64> undef, <1 x i32> <i32 1>
  %12502 = bitcast <1 x i64> %12501 to <4 x i16>
  %12503 = shufflevector <2 x i64> %12455, <2 x i64> undef, <1 x i32> <i32 1>
  %12504 = shufflevector <2 x i64> %12458, <2 x i64> undef, <1 x i32> <i32 1>
  %12505 = bitcast <1 x i64> %12504 to <4 x i16>
  %12506 = shufflevector <2 x i64> %12462, <2 x i64> undef, <1 x i32> <i32 1>
  %12507 = shufflevector <2 x i64> %12465, <2 x i64> undef, <1 x i32> <i32 1>
  %12508 = bitcast <1 x i64> %12507 to <4 x i16>
  %12509 = shufflevector <2 x i64> %12469, <2 x i64> undef, <1 x i32> <i32 1>
  %12510 = shufflevector <2 x i64> %12472, <2 x i64> undef, <1 x i32> <i32 1>
  %12511 = bitcast <1 x i64> %12510 to <4 x i16>
  %.cast1969 = bitcast <1 x i64> %12500 to <4 x i16>
  %12512 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1969, <4 x i16> %11742) #11
  %12513 = sext <4 x i16> %12499 to <4 x i32>
  %12514 = mul nsw <4 x i32> %12513, %11764
  %.cast1972 = bitcast <1 x i64> %12503 to <4 x i16>
  %12515 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1972, <4 x i16> %11747) #11
  %12516 = sext <4 x i16> %12502 to <4 x i32>
  %12517 = mul nsw <4 x i32> %12516, %11768
  %.cast1975 = bitcast <1 x i64> %12506 to <4 x i16>
  %12518 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1975, <4 x i16> %11752) #11
  %12519 = sext <4 x i16> %12505 to <4 x i32>
  %12520 = mul nsw <4 x i32> %12519, %11772
  %.cast1978 = bitcast <1 x i64> %12509 to <4 x i16>
  %12521 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1978, <4 x i16> %11757) #11
  %12522 = sext <4 x i16> %12508 to <4 x i32>
  %12523 = mul nsw <4 x i32> %12522, %11776
  %12524 = sext <4 x i16> %12511 to <4 x i32>
  %12525 = mul nsw <4 x i32> %12524, %11778
  %12526 = add <4 x i32> %12514, %11563
  %12527 = add <4 x i32> %12526, %12517
  %12528 = add <4 x i32> %12527, %12520
  %12529 = add <4 x i32> %12528, %12523
  %12530 = add <4 x i32> %12529, %12525
  %12531 = add <4 x i32> %12530, %12512
  %12532 = add <4 x i32> %12531, %12515
  %12533 = add <4 x i32> %12532, %12518
  %12534 = add <4 x i32> %12533, %12521
  %12535 = zext <8 x i8> %t3305 to <8 x i16>
  %12536 = bitcast <8 x i16> %12535 to <2 x i64>
  %12537 = shufflevector <2 x i64> %12536, <2 x i64> undef, <1 x i32> zeroinitializer
  %12538 = bitcast <1 x i64> %12537 to <4 x i16>
  %12539 = zext <8 x i8> %t3306 to <8 x i16>
  %12540 = bitcast <8 x i16> %12539 to <2 x i64>
  %12541 = shufflevector <2 x i64> %12540, <2 x i64> undef, <1 x i32> zeroinitializer
  %12542 = zext <8 x i8> %t3307 to <8 x i16>
  %12543 = bitcast <8 x i16> %12542 to <2 x i64>
  %12544 = shufflevector <2 x i64> %12543, <2 x i64> undef, <1 x i32> zeroinitializer
  %12545 = bitcast <1 x i64> %12544 to <4 x i16>
  %12546 = zext <8 x i8> %t3308 to <8 x i16>
  %12547 = bitcast <8 x i16> %12546 to <2 x i64>
  %12548 = shufflevector <2 x i64> %12547, <2 x i64> undef, <1 x i32> zeroinitializer
  %12549 = zext <8 x i8> %t3309 to <8 x i16>
  %12550 = bitcast <8 x i16> %12549 to <2 x i64>
  %12551 = shufflevector <2 x i64> %12550, <2 x i64> undef, <1 x i32> zeroinitializer
  %12552 = bitcast <1 x i64> %12551 to <4 x i16>
  %12553 = zext <8 x i8> %t3310 to <8 x i16>
  %12554 = bitcast <8 x i16> %12553 to <2 x i64>
  %12555 = shufflevector <2 x i64> %12554, <2 x i64> undef, <1 x i32> zeroinitializer
  %12556 = zext <8 x i8> %t3311 to <8 x i16>
  %12557 = bitcast <8 x i16> %12556 to <2 x i64>
  %12558 = shufflevector <2 x i64> %12557, <2 x i64> undef, <1 x i32> zeroinitializer
  %12559 = bitcast <1 x i64> %12558 to <4 x i16>
  %12560 = zext <8 x i8> %t3312 to <8 x i16>
  %12561 = bitcast <8 x i16> %12560 to <2 x i64>
  %12562 = shufflevector <2 x i64> %12561, <2 x i64> undef, <1 x i32> zeroinitializer
  %12563 = zext <8 x i8> %t3313 to <8 x i16>
  %12564 = bitcast <8 x i16> %12563 to <2 x i64>
  %12565 = shufflevector <2 x i64> %12564, <2 x i64> undef, <1 x i32> zeroinitializer
  %12566 = bitcast <1 x i64> %12565 to <4 x i16>
  %.cast1981 = bitcast <1 x i64> %12541 to <4 x i16>
  %12567 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1981, <4 x i16> %11795) #11
  %12568 = sext <4 x i16> %12538 to <4 x i32>
  %12569 = mul nsw <4 x i32> %12568, %11833
  %.cast1984 = bitcast <1 x i64> %12548 to <4 x i16>
  %12570 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1984, <4 x i16> %11804) #11
  %12571 = sext <4 x i16> %12545 to <4 x i32>
  %12572 = mul nsw <4 x i32> %12571, %11837
  %.cast1987 = bitcast <1 x i64> %12555 to <4 x i16>
  %12573 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1987, <4 x i16> %11813) #11
  %12574 = sext <4 x i16> %12552 to <4 x i32>
  %12575 = mul nsw <4 x i32> %12574, %11841
  %.cast1990 = bitcast <1 x i64> %12562 to <4 x i16>
  %12576 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1990, <4 x i16> %11822) #11
  %12577 = sext <4 x i16> %12559 to <4 x i32>
  %12578 = mul nsw <4 x i32> %12577, %11845
  %12579 = sext <4 x i16> %12566 to <4 x i32>
  %12580 = mul nsw <4 x i32> %12579, %11847
  %12581 = add <4 x i32> %12569, %11564
  %12582 = add <4 x i32> %12581, %12572
  %12583 = add <4 x i32> %12582, %12575
  %12584 = add <4 x i32> %12583, %12578
  %12585 = add <4 x i32> %12584, %12580
  %12586 = add <4 x i32> %12585, %12567
  %12587 = add <4 x i32> %12586, %12570
  %12588 = add <4 x i32> %12587, %12573
  %12589 = add <4 x i32> %12588, %12576
  %12590 = shufflevector <2 x i64> %12536, <2 x i64> undef, <1 x i32> <i32 1>
  %12591 = bitcast <1 x i64> %12590 to <4 x i16>
  %12592 = shufflevector <2 x i64> %12540, <2 x i64> undef, <1 x i32> <i32 1>
  %12593 = shufflevector <2 x i64> %12543, <2 x i64> undef, <1 x i32> <i32 1>
  %12594 = bitcast <1 x i64> %12593 to <4 x i16>
  %12595 = shufflevector <2 x i64> %12547, <2 x i64> undef, <1 x i32> <i32 1>
  %12596 = shufflevector <2 x i64> %12550, <2 x i64> undef, <1 x i32> <i32 1>
  %12597 = bitcast <1 x i64> %12596 to <4 x i16>
  %12598 = shufflevector <2 x i64> %12554, <2 x i64> undef, <1 x i32> <i32 1>
  %12599 = shufflevector <2 x i64> %12557, <2 x i64> undef, <1 x i32> <i32 1>
  %12600 = bitcast <1 x i64> %12599 to <4 x i16>
  %12601 = shufflevector <2 x i64> %12561, <2 x i64> undef, <1 x i32> <i32 1>
  %12602 = shufflevector <2 x i64> %12564, <2 x i64> undef, <1 x i32> <i32 1>
  %12603 = bitcast <1 x i64> %12602 to <4 x i16>
  %.cast1993 = bitcast <1 x i64> %12592 to <4 x i16>
  %12604 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1993, <4 x i16> %11862) #11
  %12605 = sext <4 x i16> %12591 to <4 x i32>
  %12606 = mul nsw <4 x i32> %12605, %11884
  %.cast1996 = bitcast <1 x i64> %12595 to <4 x i16>
  %12607 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1996, <4 x i16> %11867) #11
  %12608 = sext <4 x i16> %12594 to <4 x i32>
  %12609 = mul nsw <4 x i32> %12608, %11888
  %.cast1999 = bitcast <1 x i64> %12598 to <4 x i16>
  %12610 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1999, <4 x i16> %11872) #11
  %12611 = sext <4 x i16> %12597 to <4 x i32>
  %12612 = mul nsw <4 x i32> %12611, %11892
  %.cast2002 = bitcast <1 x i64> %12601 to <4 x i16>
  %12613 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2002, <4 x i16> %11877) #11
  %12614 = sext <4 x i16> %12600 to <4 x i32>
  %12615 = mul nsw <4 x i32> %12614, %11896
  %12616 = sext <4 x i16> %12603 to <4 x i32>
  %12617 = mul nsw <4 x i32> %12616, %11898
  %12618 = add <4 x i32> %12606, %11565
  %12619 = add <4 x i32> %12618, %12609
  %12620 = add <4 x i32> %12619, %12612
  %12621 = add <4 x i32> %12620, %12615
  %12622 = add <4 x i32> %12621, %12617
  %12623 = add <4 x i32> %12622, %12604
  %12624 = add <4 x i32> %12623, %12607
  %12625 = add <4 x i32> %12624, %12610
  %12626 = add <4 x i32> %12625, %12613
  %t3315 = add nsw i32 %t3202, %t2418568
  %12627 = sext i32 %t3315 to i64
  %12628 = shl nsw i64 %12627, 4
  %12629 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12628
  %12630 = bitcast i8* %12629 to <8 x i8>*
  %t3316 = load <8 x i8>, <8 x i8>* %12630, align 16, !tbaa !438
  %t3317 = add nsw i32 %t3202, %t2414567
  %12631 = sext i32 %t3317 to i64
  %12632 = shl nsw i64 %12631, 4
  %12633 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12632
  %12634 = bitcast i8* %12633 to <8 x i8>*
  %t3318 = load <8 x i8>, <8 x i8>* %12634, align 16, !tbaa !438
  %t3319 = add nsw i32 %t3202, %t2410544
  %12635 = sext i32 %t3319 to i64
  %12636 = shl nsw i64 %12635, 4
  %12637 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12636
  %12638 = bitcast i8* %12637 to <8 x i8>*
  %t3320 = load <8 x i8>, <8 x i8>* %12638, align 16, !tbaa !438
  %t3321 = add nsw i32 %t3202, %t2405566
  %12639 = sext i32 %t3321 to i64
  %12640 = shl nsw i64 %12639, 4
  %12641 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12640
  %12642 = bitcast i8* %12641 to <8 x i8>*
  %t3322 = load <8 x i8>, <8 x i8>* %12642, align 16, !tbaa !438
  %t3323 = add nsw i32 %t3202, %t2401565
  %12643 = sext i32 %t3323 to i64
  %12644 = shl nsw i64 %12643, 4
  %12645 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12644
  %12646 = bitcast i8* %12645 to <8 x i8>*
  %t3324 = load <8 x i8>, <8 x i8>* %12646, align 16, !tbaa !438
  %t3325 = add nsw i32 %t3202, %t2397543
  %12647 = sext i32 %t3325 to i64
  %12648 = shl nsw i64 %12647, 4
  %12649 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12648
  %12650 = bitcast i8* %12649 to <8 x i8>*
  %t3326 = load <8 x i8>, <8 x i8>* %12650, align 16, !tbaa !438
  %t3327 = add nsw i32 %t3202, %t2392552
  %12651 = sext i32 %t3327 to i64
  %12652 = shl nsw i64 %12651, 4
  %12653 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12652
  %12654 = bitcast i8* %12653 to <8 x i8>*
  %t3328 = load <8 x i8>, <8 x i8>* %12654, align 16, !tbaa !438
  %t3329 = add nsw i32 %t3202, %t2388551
  %12655 = sext i32 %t3329 to i64
  %12656 = shl nsw i64 %12655, 4
  %12657 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12656
  %12658 = bitcast i8* %12657 to <8 x i8>*
  %t3330 = load <8 x i8>, <8 x i8>* %12658, align 16, !tbaa !438
  %t3331 = add nsw i32 %t3202, %t2384536
  %12659 = sext i32 %t3331 to i64
  %12660 = shl nsw i64 %12659, 4
  %12661 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12660
  %12662 = bitcast i8* %12661 to <8 x i8>*
  %t3332 = load <8 x i8>, <8 x i8>* %12662, align 16, !tbaa !438
  %12663 = getelementptr inbounds i8, i8* %12629, i64 8
  %12664 = bitcast i8* %12663 to <8 x i8>*
  %t3333 = load <8 x i8>, <8 x i8>* %12664, align 8, !tbaa !438
  %12665 = getelementptr inbounds i8, i8* %12633, i64 8
  %12666 = bitcast i8* %12665 to <8 x i8>*
  %t3334 = load <8 x i8>, <8 x i8>* %12666, align 8, !tbaa !438
  %12667 = getelementptr inbounds i8, i8* %12637, i64 8
  %12668 = bitcast i8* %12667 to <8 x i8>*
  %t3335 = load <8 x i8>, <8 x i8>* %12668, align 8, !tbaa !438
  %12669 = getelementptr inbounds i8, i8* %12641, i64 8
  %12670 = bitcast i8* %12669 to <8 x i8>*
  %t3336 = load <8 x i8>, <8 x i8>* %12670, align 8, !tbaa !438
  %12671 = getelementptr inbounds i8, i8* %12645, i64 8
  %12672 = bitcast i8* %12671 to <8 x i8>*
  %t3337 = load <8 x i8>, <8 x i8>* %12672, align 8, !tbaa !438
  %12673 = getelementptr inbounds i8, i8* %12649, i64 8
  %12674 = bitcast i8* %12673 to <8 x i8>*
  %t3338 = load <8 x i8>, <8 x i8>* %12674, align 8, !tbaa !438
  %12675 = getelementptr inbounds i8, i8* %12653, i64 8
  %12676 = bitcast i8* %12675 to <8 x i8>*
  %t3339 = load <8 x i8>, <8 x i8>* %12676, align 8, !tbaa !438
  %12677 = getelementptr inbounds i8, i8* %12657, i64 8
  %12678 = bitcast i8* %12677 to <8 x i8>*
  %t3340 = load <8 x i8>, <8 x i8>* %12678, align 8, !tbaa !438
  %12679 = getelementptr inbounds i8, i8* %12661, i64 8
  %12680 = bitcast i8* %12679 to <8 x i8>*
  %t3341 = load <8 x i8>, <8 x i8>* %12680, align 8, !tbaa !438
  %12681 = zext <8 x i8> %t3316 to <8 x i16>
  %12682 = bitcast <8 x i16> %12681 to <2 x i64>
  %12683 = shufflevector <2 x i64> %12682, <2 x i64> undef, <1 x i32> zeroinitializer
  %12684 = bitcast <1 x i64> %12683 to <4 x i16>
  %12685 = zext <8 x i8> %t3318 to <8 x i16>
  %12686 = bitcast <8 x i16> %12685 to <2 x i64>
  %12687 = shufflevector <2 x i64> %12686, <2 x i64> undef, <1 x i32> zeroinitializer
  %12688 = zext <8 x i8> %t3320 to <8 x i16>
  %12689 = bitcast <8 x i16> %12688 to <2 x i64>
  %12690 = shufflevector <2 x i64> %12689, <2 x i64> undef, <1 x i32> zeroinitializer
  %12691 = bitcast <1 x i64> %12690 to <4 x i16>
  %12692 = zext <8 x i8> %t3322 to <8 x i16>
  %12693 = bitcast <8 x i16> %12692 to <2 x i64>
  %12694 = shufflevector <2 x i64> %12693, <2 x i64> undef, <1 x i32> zeroinitializer
  %12695 = zext <8 x i8> %t3324 to <8 x i16>
  %12696 = bitcast <8 x i16> %12695 to <2 x i64>
  %12697 = shufflevector <2 x i64> %12696, <2 x i64> undef, <1 x i32> zeroinitializer
  %12698 = bitcast <1 x i64> %12697 to <4 x i16>
  %12699 = zext <8 x i8> %t3326 to <8 x i16>
  %12700 = bitcast <8 x i16> %12699 to <2 x i64>
  %12701 = shufflevector <2 x i64> %12700, <2 x i64> undef, <1 x i32> zeroinitializer
  %12702 = zext <8 x i8> %t3328 to <8 x i16>
  %12703 = bitcast <8 x i16> %12702 to <2 x i64>
  %12704 = shufflevector <2 x i64> %12703, <2 x i64> undef, <1 x i32> zeroinitializer
  %12705 = bitcast <1 x i64> %12704 to <4 x i16>
  %12706 = zext <8 x i8> %t3330 to <8 x i16>
  %12707 = bitcast <8 x i16> %12706 to <2 x i64>
  %12708 = shufflevector <2 x i64> %12707, <2 x i64> undef, <1 x i32> zeroinitializer
  %12709 = zext <8 x i8> %t3332 to <8 x i16>
  %12710 = bitcast <8 x i16> %12709 to <2 x i64>
  %12711 = shufflevector <2 x i64> %12710, <2 x i64> undef, <1 x i32> zeroinitializer
  %12712 = bitcast <1 x i64> %12711 to <4 x i16>
  %.cast2005 = bitcast <1 x i64> %12687 to <4 x i16>
  %12713 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2005, <4 x i16> %11675) #11
  %12714 = sext <4 x i16> %12684 to <4 x i32>
  %12715 = mul nsw <4 x i32> %12714, %11713
  %.cast2008 = bitcast <1 x i64> %12694 to <4 x i16>
  %12716 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2008, <4 x i16> %11684) #11
  %12717 = sext <4 x i16> %12691 to <4 x i32>
  %12718 = mul nsw <4 x i32> %12717, %11717
  %.cast2011 = bitcast <1 x i64> %12701 to <4 x i16>
  %12719 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2011, <4 x i16> %11693) #11
  %12720 = sext <4 x i16> %12698 to <4 x i32>
  %12721 = mul nsw <4 x i32> %12720, %11721
  %.cast2014 = bitcast <1 x i64> %12708 to <4 x i16>
  %12722 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2014, <4 x i16> %11702) #11
  %12723 = sext <4 x i16> %12705 to <4 x i32>
  %12724 = mul nsw <4 x i32> %12723, %11725
  %12725 = sext <4 x i16> %12712 to <4 x i32>
  %12726 = mul nsw <4 x i32> %12725, %11727
  %12727 = add <4 x i32> %12715, %11562
  %12728 = add <4 x i32> %12727, %12718
  %12729 = add <4 x i32> %12728, %12721
  %12730 = add <4 x i32> %12729, %12724
  %12731 = add <4 x i32> %12730, %12726
  %12732 = add <4 x i32> %12731, %12713
  %12733 = add <4 x i32> %12732, %12716
  %12734 = add <4 x i32> %12733, %12719
  %12735 = add <4 x i32> %12734, %12722
  %12736 = shufflevector <2 x i64> %12682, <2 x i64> undef, <1 x i32> <i32 1>
  %12737 = bitcast <1 x i64> %12736 to <4 x i16>
  %12738 = shufflevector <2 x i64> %12686, <2 x i64> undef, <1 x i32> <i32 1>
  %12739 = shufflevector <2 x i64> %12689, <2 x i64> undef, <1 x i32> <i32 1>
  %12740 = bitcast <1 x i64> %12739 to <4 x i16>
  %12741 = shufflevector <2 x i64> %12693, <2 x i64> undef, <1 x i32> <i32 1>
  %12742 = shufflevector <2 x i64> %12696, <2 x i64> undef, <1 x i32> <i32 1>
  %12743 = bitcast <1 x i64> %12742 to <4 x i16>
  %12744 = shufflevector <2 x i64> %12700, <2 x i64> undef, <1 x i32> <i32 1>
  %12745 = shufflevector <2 x i64> %12703, <2 x i64> undef, <1 x i32> <i32 1>
  %12746 = bitcast <1 x i64> %12745 to <4 x i16>
  %12747 = shufflevector <2 x i64> %12707, <2 x i64> undef, <1 x i32> <i32 1>
  %12748 = shufflevector <2 x i64> %12710, <2 x i64> undef, <1 x i32> <i32 1>
  %12749 = bitcast <1 x i64> %12748 to <4 x i16>
  %.cast2017 = bitcast <1 x i64> %12738 to <4 x i16>
  %12750 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2017, <4 x i16> %11742) #11
  %12751 = sext <4 x i16> %12737 to <4 x i32>
  %12752 = mul nsw <4 x i32> %12751, %11764
  %.cast2020 = bitcast <1 x i64> %12741 to <4 x i16>
  %12753 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2020, <4 x i16> %11747) #11
  %12754 = sext <4 x i16> %12740 to <4 x i32>
  %12755 = mul nsw <4 x i32> %12754, %11768
  %.cast2023 = bitcast <1 x i64> %12744 to <4 x i16>
  %12756 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2023, <4 x i16> %11752) #11
  %12757 = sext <4 x i16> %12743 to <4 x i32>
  %12758 = mul nsw <4 x i32> %12757, %11772
  %.cast2026 = bitcast <1 x i64> %12747 to <4 x i16>
  %12759 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2026, <4 x i16> %11757) #11
  %12760 = sext <4 x i16> %12746 to <4 x i32>
  %12761 = mul nsw <4 x i32> %12760, %11776
  %12762 = sext <4 x i16> %12749 to <4 x i32>
  %12763 = mul nsw <4 x i32> %12762, %11778
  %12764 = add <4 x i32> %12752, %11563
  %12765 = add <4 x i32> %12764, %12755
  %12766 = add <4 x i32> %12765, %12758
  %12767 = add <4 x i32> %12766, %12761
  %12768 = add <4 x i32> %12767, %12763
  %12769 = add <4 x i32> %12768, %12750
  %12770 = add <4 x i32> %12769, %12753
  %12771 = add <4 x i32> %12770, %12756
  %12772 = add <4 x i32> %12771, %12759
  %12773 = zext <8 x i8> %t3333 to <8 x i16>
  %12774 = bitcast <8 x i16> %12773 to <2 x i64>
  %12775 = shufflevector <2 x i64> %12774, <2 x i64> undef, <1 x i32> zeroinitializer
  %12776 = bitcast <1 x i64> %12775 to <4 x i16>
  %12777 = zext <8 x i8> %t3334 to <8 x i16>
  %12778 = bitcast <8 x i16> %12777 to <2 x i64>
  %12779 = shufflevector <2 x i64> %12778, <2 x i64> undef, <1 x i32> zeroinitializer
  %12780 = zext <8 x i8> %t3335 to <8 x i16>
  %12781 = bitcast <8 x i16> %12780 to <2 x i64>
  %12782 = shufflevector <2 x i64> %12781, <2 x i64> undef, <1 x i32> zeroinitializer
  %12783 = bitcast <1 x i64> %12782 to <4 x i16>
  %12784 = zext <8 x i8> %t3336 to <8 x i16>
  %12785 = bitcast <8 x i16> %12784 to <2 x i64>
  %12786 = shufflevector <2 x i64> %12785, <2 x i64> undef, <1 x i32> zeroinitializer
  %12787 = zext <8 x i8> %t3337 to <8 x i16>
  %12788 = bitcast <8 x i16> %12787 to <2 x i64>
  %12789 = shufflevector <2 x i64> %12788, <2 x i64> undef, <1 x i32> zeroinitializer
  %12790 = bitcast <1 x i64> %12789 to <4 x i16>
  %12791 = zext <8 x i8> %t3338 to <8 x i16>
  %12792 = bitcast <8 x i16> %12791 to <2 x i64>
  %12793 = shufflevector <2 x i64> %12792, <2 x i64> undef, <1 x i32> zeroinitializer
  %12794 = zext <8 x i8> %t3339 to <8 x i16>
  %12795 = bitcast <8 x i16> %12794 to <2 x i64>
  %12796 = shufflevector <2 x i64> %12795, <2 x i64> undef, <1 x i32> zeroinitializer
  %12797 = bitcast <1 x i64> %12796 to <4 x i16>
  %12798 = zext <8 x i8> %t3340 to <8 x i16>
  %12799 = bitcast <8 x i16> %12798 to <2 x i64>
  %12800 = shufflevector <2 x i64> %12799, <2 x i64> undef, <1 x i32> zeroinitializer
  %12801 = zext <8 x i8> %t3341 to <8 x i16>
  %12802 = bitcast <8 x i16> %12801 to <2 x i64>
  %12803 = shufflevector <2 x i64> %12802, <2 x i64> undef, <1 x i32> zeroinitializer
  %12804 = bitcast <1 x i64> %12803 to <4 x i16>
  %.cast2029 = bitcast <1 x i64> %12779 to <4 x i16>
  %12805 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2029, <4 x i16> %11795) #11
  %12806 = sext <4 x i16> %12776 to <4 x i32>
  %12807 = mul nsw <4 x i32> %12806, %11833
  %.cast2032 = bitcast <1 x i64> %12786 to <4 x i16>
  %12808 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2032, <4 x i16> %11804) #11
  %12809 = sext <4 x i16> %12783 to <4 x i32>
  %12810 = mul nsw <4 x i32> %12809, %11837
  %.cast2035 = bitcast <1 x i64> %12793 to <4 x i16>
  %12811 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2035, <4 x i16> %11813) #11
  %12812 = sext <4 x i16> %12790 to <4 x i32>
  %12813 = mul nsw <4 x i32> %12812, %11841
  %.cast2038 = bitcast <1 x i64> %12800 to <4 x i16>
  %12814 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2038, <4 x i16> %11822) #11
  %12815 = sext <4 x i16> %12797 to <4 x i32>
  %12816 = mul nsw <4 x i32> %12815, %11845
  %12817 = sext <4 x i16> %12804 to <4 x i32>
  %12818 = mul nsw <4 x i32> %12817, %11847
  %12819 = add <4 x i32> %12807, %11564
  %12820 = add <4 x i32> %12819, %12810
  %12821 = add <4 x i32> %12820, %12813
  %12822 = add <4 x i32> %12821, %12816
  %12823 = add <4 x i32> %12822, %12818
  %12824 = add <4 x i32> %12823, %12805
  %12825 = add <4 x i32> %12824, %12808
  %12826 = add <4 x i32> %12825, %12811
  %12827 = add <4 x i32> %12826, %12814
  %12828 = shufflevector <2 x i64> %12774, <2 x i64> undef, <1 x i32> <i32 1>
  %12829 = bitcast <1 x i64> %12828 to <4 x i16>
  %12830 = shufflevector <2 x i64> %12778, <2 x i64> undef, <1 x i32> <i32 1>
  %12831 = shufflevector <2 x i64> %12781, <2 x i64> undef, <1 x i32> <i32 1>
  %12832 = bitcast <1 x i64> %12831 to <4 x i16>
  %12833 = shufflevector <2 x i64> %12785, <2 x i64> undef, <1 x i32> <i32 1>
  %12834 = shufflevector <2 x i64> %12788, <2 x i64> undef, <1 x i32> <i32 1>
  %12835 = bitcast <1 x i64> %12834 to <4 x i16>
  %12836 = shufflevector <2 x i64> %12792, <2 x i64> undef, <1 x i32> <i32 1>
  %12837 = shufflevector <2 x i64> %12795, <2 x i64> undef, <1 x i32> <i32 1>
  %12838 = bitcast <1 x i64> %12837 to <4 x i16>
  %12839 = shufflevector <2 x i64> %12799, <2 x i64> undef, <1 x i32> <i32 1>
  %12840 = shufflevector <2 x i64> %12802, <2 x i64> undef, <1 x i32> <i32 1>
  %12841 = bitcast <1 x i64> %12840 to <4 x i16>
  %.cast2041 = bitcast <1 x i64> %12830 to <4 x i16>
  %12842 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2041, <4 x i16> %11862) #11
  %12843 = sext <4 x i16> %12829 to <4 x i32>
  %12844 = mul nsw <4 x i32> %12843, %11884
  %.cast2044 = bitcast <1 x i64> %12833 to <4 x i16>
  %12845 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2044, <4 x i16> %11867) #11
  %12846 = sext <4 x i16> %12832 to <4 x i32>
  %12847 = mul nsw <4 x i32> %12846, %11888
  %.cast2047 = bitcast <1 x i64> %12836 to <4 x i16>
  %12848 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2047, <4 x i16> %11872) #11
  %12849 = sext <4 x i16> %12835 to <4 x i32>
  %12850 = mul nsw <4 x i32> %12849, %11892
  %.cast2050 = bitcast <1 x i64> %12839 to <4 x i16>
  %12851 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2050, <4 x i16> %11877) #11
  %12852 = sext <4 x i16> %12838 to <4 x i32>
  %12853 = mul nsw <4 x i32> %12852, %11896
  %12854 = sext <4 x i16> %12841 to <4 x i32>
  %12855 = mul nsw <4 x i32> %12854, %11898
  %12856 = add <4 x i32> %12844, %11565
  %12857 = add <4 x i32> %12856, %12847
  %12858 = add <4 x i32> %12857, %12850
  %12859 = add <4 x i32> %12858, %12853
  %12860 = add <4 x i32> %12859, %12855
  %12861 = add <4 x i32> %12860, %12842
  %12862 = add <4 x i32> %12861, %12845
  %12863 = add <4 x i32> %12862, %12848
  %12864 = add <4 x i32> %12863, %12851
  %t3343 = add nsw i32 %t3230, %t2418568
  %12865 = sext i32 %t3343 to i64
  %12866 = shl nsw i64 %12865, 4
  %12867 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12866
  %12868 = bitcast i8* %12867 to <8 x i8>*
  %t3344 = load <8 x i8>, <8 x i8>* %12868, align 16, !tbaa !438
  %t3345 = add nsw i32 %t3230, %t2414567
  %12869 = sext i32 %t3345 to i64
  %12870 = shl nsw i64 %12869, 4
  %12871 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12870
  %12872 = bitcast i8* %12871 to <8 x i8>*
  %t3346 = load <8 x i8>, <8 x i8>* %12872, align 16, !tbaa !438
  %t3347 = add nsw i32 %t3230, %t2410544
  %12873 = sext i32 %t3347 to i64
  %12874 = shl nsw i64 %12873, 4
  %12875 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12874
  %12876 = bitcast i8* %12875 to <8 x i8>*
  %t3348 = load <8 x i8>, <8 x i8>* %12876, align 16, !tbaa !438
  %t3349 = add nsw i32 %t3230, %t2405566
  %12877 = sext i32 %t3349 to i64
  %12878 = shl nsw i64 %12877, 4
  %12879 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12878
  %12880 = bitcast i8* %12879 to <8 x i8>*
  %t3350 = load <8 x i8>, <8 x i8>* %12880, align 16, !tbaa !438
  %t3351 = add nsw i32 %t3230, %t2401565
  %12881 = sext i32 %t3351 to i64
  %12882 = shl nsw i64 %12881, 4
  %12883 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12882
  %12884 = bitcast i8* %12883 to <8 x i8>*
  %t3352 = load <8 x i8>, <8 x i8>* %12884, align 16, !tbaa !438
  %t3353 = add nsw i32 %t3230, %t2397543
  %12885 = sext i32 %t3353 to i64
  %12886 = shl nsw i64 %12885, 4
  %12887 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12886
  %12888 = bitcast i8* %12887 to <8 x i8>*
  %t3354 = load <8 x i8>, <8 x i8>* %12888, align 16, !tbaa !438
  %t3355 = add nsw i32 %t3230, %t2392552
  %12889 = sext i32 %t3355 to i64
  %12890 = shl nsw i64 %12889, 4
  %12891 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12890
  %12892 = bitcast i8* %12891 to <8 x i8>*
  %t3356 = load <8 x i8>, <8 x i8>* %12892, align 16, !tbaa !438
  %t3357 = add nsw i32 %t3230, %t2388551
  %12893 = sext i32 %t3357 to i64
  %12894 = shl nsw i64 %12893, 4
  %12895 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12894
  %12896 = bitcast i8* %12895 to <8 x i8>*
  %t3358 = load <8 x i8>, <8 x i8>* %12896, align 16, !tbaa !438
  %t3359 = add nsw i32 %t3230, %t2384536
  %12897 = sext i32 %t3359 to i64
  %12898 = shl nsw i64 %12897, 4
  %12899 = getelementptr inbounds i8, i8* %resampled_input473, i64 %12898
  %12900 = bitcast i8* %12899 to <8 x i8>*
  %t3360 = load <8 x i8>, <8 x i8>* %12900, align 16, !tbaa !438
  %12901 = getelementptr inbounds i8, i8* %12867, i64 8
  %12902 = bitcast i8* %12901 to <8 x i8>*
  %t3361 = load <8 x i8>, <8 x i8>* %12902, align 8, !tbaa !438
  %12903 = getelementptr inbounds i8, i8* %12871, i64 8
  %12904 = bitcast i8* %12903 to <8 x i8>*
  %t3362 = load <8 x i8>, <8 x i8>* %12904, align 8, !tbaa !438
  %12905 = getelementptr inbounds i8, i8* %12875, i64 8
  %12906 = bitcast i8* %12905 to <8 x i8>*
  %t3363 = load <8 x i8>, <8 x i8>* %12906, align 8, !tbaa !438
  %12907 = getelementptr inbounds i8, i8* %12879, i64 8
  %12908 = bitcast i8* %12907 to <8 x i8>*
  %t3364 = load <8 x i8>, <8 x i8>* %12908, align 8, !tbaa !438
  %12909 = getelementptr inbounds i8, i8* %12883, i64 8
  %12910 = bitcast i8* %12909 to <8 x i8>*
  %t3365 = load <8 x i8>, <8 x i8>* %12910, align 8, !tbaa !438
  %12911 = getelementptr inbounds i8, i8* %12887, i64 8
  %12912 = bitcast i8* %12911 to <8 x i8>*
  %t3366 = load <8 x i8>, <8 x i8>* %12912, align 8, !tbaa !438
  %12913 = getelementptr inbounds i8, i8* %12891, i64 8
  %12914 = bitcast i8* %12913 to <8 x i8>*
  %t3367 = load <8 x i8>, <8 x i8>* %12914, align 8, !tbaa !438
  %12915 = getelementptr inbounds i8, i8* %12895, i64 8
  %12916 = bitcast i8* %12915 to <8 x i8>*
  %t3368 = load <8 x i8>, <8 x i8>* %12916, align 8, !tbaa !438
  %12917 = getelementptr inbounds i8, i8* %12899, i64 8
  %12918 = bitcast i8* %12917 to <8 x i8>*
  %t3369 = load <8 x i8>, <8 x i8>* %12918, align 8, !tbaa !438
  %12919 = zext <8 x i8> %t3344 to <8 x i16>
  %12920 = bitcast <8 x i16> %12919 to <2 x i64>
  %12921 = shufflevector <2 x i64> %12920, <2 x i64> undef, <1 x i32> zeroinitializer
  %12922 = bitcast <1 x i64> %12921 to <4 x i16>
  %12923 = zext <8 x i8> %t3346 to <8 x i16>
  %12924 = bitcast <8 x i16> %12923 to <2 x i64>
  %12925 = shufflevector <2 x i64> %12924, <2 x i64> undef, <1 x i32> zeroinitializer
  %12926 = zext <8 x i8> %t3348 to <8 x i16>
  %12927 = bitcast <8 x i16> %12926 to <2 x i64>
  %12928 = shufflevector <2 x i64> %12927, <2 x i64> undef, <1 x i32> zeroinitializer
  %12929 = bitcast <1 x i64> %12928 to <4 x i16>
  %12930 = zext <8 x i8> %t3350 to <8 x i16>
  %12931 = bitcast <8 x i16> %12930 to <2 x i64>
  %12932 = shufflevector <2 x i64> %12931, <2 x i64> undef, <1 x i32> zeroinitializer
  %12933 = zext <8 x i8> %t3352 to <8 x i16>
  %12934 = bitcast <8 x i16> %12933 to <2 x i64>
  %12935 = shufflevector <2 x i64> %12934, <2 x i64> undef, <1 x i32> zeroinitializer
  %12936 = bitcast <1 x i64> %12935 to <4 x i16>
  %12937 = zext <8 x i8> %t3354 to <8 x i16>
  %12938 = bitcast <8 x i16> %12937 to <2 x i64>
  %12939 = shufflevector <2 x i64> %12938, <2 x i64> undef, <1 x i32> zeroinitializer
  %12940 = zext <8 x i8> %t3356 to <8 x i16>
  %12941 = bitcast <8 x i16> %12940 to <2 x i64>
  %12942 = shufflevector <2 x i64> %12941, <2 x i64> undef, <1 x i32> zeroinitializer
  %12943 = bitcast <1 x i64> %12942 to <4 x i16>
  %12944 = zext <8 x i8> %t3358 to <8 x i16>
  %12945 = bitcast <8 x i16> %12944 to <2 x i64>
  %12946 = shufflevector <2 x i64> %12945, <2 x i64> undef, <1 x i32> zeroinitializer
  %12947 = zext <8 x i8> %t3360 to <8 x i16>
  %12948 = bitcast <8 x i16> %12947 to <2 x i64>
  %12949 = shufflevector <2 x i64> %12948, <2 x i64> undef, <1 x i32> zeroinitializer
  %12950 = bitcast <1 x i64> %12949 to <4 x i16>
  %.cast2053 = bitcast <1 x i64> %12925 to <4 x i16>
  %12951 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2053, <4 x i16> %11675) #11
  %12952 = sext <4 x i16> %12922 to <4 x i32>
  %12953 = mul nsw <4 x i32> %12952, %11713
  %.cast2056 = bitcast <1 x i64> %12932 to <4 x i16>
  %12954 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2056, <4 x i16> %11684) #11
  %12955 = sext <4 x i16> %12929 to <4 x i32>
  %12956 = mul nsw <4 x i32> %12955, %11717
  %.cast2059 = bitcast <1 x i64> %12939 to <4 x i16>
  %12957 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2059, <4 x i16> %11693) #11
  %12958 = sext <4 x i16> %12936 to <4 x i32>
  %12959 = mul nsw <4 x i32> %12958, %11721
  %.cast2062 = bitcast <1 x i64> %12946 to <4 x i16>
  %12960 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2062, <4 x i16> %11702) #11
  %12961 = sext <4 x i16> %12943 to <4 x i32>
  %12962 = mul nsw <4 x i32> %12961, %11725
  %12963 = sext <4 x i16> %12950 to <4 x i32>
  %12964 = mul nsw <4 x i32> %12963, %11727
  %12965 = add <4 x i32> %12953, %11562
  %12966 = add <4 x i32> %12965, %12956
  %12967 = add <4 x i32> %12966, %12959
  %12968 = add <4 x i32> %12967, %12962
  %12969 = add <4 x i32> %12968, %12964
  %12970 = add <4 x i32> %12969, %12951
  %12971 = add <4 x i32> %12970, %12954
  %12972 = add <4 x i32> %12971, %12957
  %12973 = add <4 x i32> %12972, %12960
  %12974 = shufflevector <2 x i64> %12920, <2 x i64> undef, <1 x i32> <i32 1>
  %12975 = bitcast <1 x i64> %12974 to <4 x i16>
  %12976 = shufflevector <2 x i64> %12924, <2 x i64> undef, <1 x i32> <i32 1>
  %12977 = shufflevector <2 x i64> %12927, <2 x i64> undef, <1 x i32> <i32 1>
  %12978 = bitcast <1 x i64> %12977 to <4 x i16>
  %12979 = shufflevector <2 x i64> %12931, <2 x i64> undef, <1 x i32> <i32 1>
  %12980 = shufflevector <2 x i64> %12934, <2 x i64> undef, <1 x i32> <i32 1>
  %12981 = bitcast <1 x i64> %12980 to <4 x i16>
  %12982 = shufflevector <2 x i64> %12938, <2 x i64> undef, <1 x i32> <i32 1>
  %12983 = shufflevector <2 x i64> %12941, <2 x i64> undef, <1 x i32> <i32 1>
  %12984 = bitcast <1 x i64> %12983 to <4 x i16>
  %12985 = shufflevector <2 x i64> %12945, <2 x i64> undef, <1 x i32> <i32 1>
  %12986 = shufflevector <2 x i64> %12948, <2 x i64> undef, <1 x i32> <i32 1>
  %12987 = bitcast <1 x i64> %12986 to <4 x i16>
  %.cast2065 = bitcast <1 x i64> %12976 to <4 x i16>
  %12988 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2065, <4 x i16> %11742) #11
  %12989 = sext <4 x i16> %12975 to <4 x i32>
  %12990 = mul nsw <4 x i32> %12989, %11764
  %.cast2068 = bitcast <1 x i64> %12979 to <4 x i16>
  %12991 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2068, <4 x i16> %11747) #11
  %12992 = sext <4 x i16> %12978 to <4 x i32>
  %12993 = mul nsw <4 x i32> %12992, %11768
  %.cast2071 = bitcast <1 x i64> %12982 to <4 x i16>
  %12994 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2071, <4 x i16> %11752) #11
  %12995 = sext <4 x i16> %12981 to <4 x i32>
  %12996 = mul nsw <4 x i32> %12995, %11772
  %.cast2074 = bitcast <1 x i64> %12985 to <4 x i16>
  %12997 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2074, <4 x i16> %11757) #11
  %12998 = sext <4 x i16> %12984 to <4 x i32>
  %12999 = mul nsw <4 x i32> %12998, %11776
  %13000 = sext <4 x i16> %12987 to <4 x i32>
  %13001 = mul nsw <4 x i32> %13000, %11778
  %13002 = add <4 x i32> %12990, %11563
  %13003 = add <4 x i32> %13002, %12993
  %13004 = add <4 x i32> %13003, %12996
  %13005 = add <4 x i32> %13004, %12999
  %13006 = add <4 x i32> %13005, %13001
  %13007 = add <4 x i32> %13006, %12988
  %13008 = add <4 x i32> %13007, %12991
  %13009 = add <4 x i32> %13008, %12994
  %13010 = add <4 x i32> %13009, %12997
  %13011 = zext <8 x i8> %t3361 to <8 x i16>
  %13012 = bitcast <8 x i16> %13011 to <2 x i64>
  %13013 = shufflevector <2 x i64> %13012, <2 x i64> undef, <1 x i32> zeroinitializer
  %13014 = bitcast <1 x i64> %13013 to <4 x i16>
  %13015 = zext <8 x i8> %t3362 to <8 x i16>
  %13016 = bitcast <8 x i16> %13015 to <2 x i64>
  %13017 = shufflevector <2 x i64> %13016, <2 x i64> undef, <1 x i32> zeroinitializer
  %13018 = zext <8 x i8> %t3363 to <8 x i16>
  %13019 = bitcast <8 x i16> %13018 to <2 x i64>
  %13020 = shufflevector <2 x i64> %13019, <2 x i64> undef, <1 x i32> zeroinitializer
  %13021 = bitcast <1 x i64> %13020 to <4 x i16>
  %13022 = zext <8 x i8> %t3364 to <8 x i16>
  %13023 = bitcast <8 x i16> %13022 to <2 x i64>
  %13024 = shufflevector <2 x i64> %13023, <2 x i64> undef, <1 x i32> zeroinitializer
  %13025 = zext <8 x i8> %t3365 to <8 x i16>
  %13026 = bitcast <8 x i16> %13025 to <2 x i64>
  %13027 = shufflevector <2 x i64> %13026, <2 x i64> undef, <1 x i32> zeroinitializer
  %13028 = bitcast <1 x i64> %13027 to <4 x i16>
  %13029 = zext <8 x i8> %t3366 to <8 x i16>
  %13030 = bitcast <8 x i16> %13029 to <2 x i64>
  %13031 = shufflevector <2 x i64> %13030, <2 x i64> undef, <1 x i32> zeroinitializer
  %13032 = zext <8 x i8> %t3367 to <8 x i16>
  %13033 = bitcast <8 x i16> %13032 to <2 x i64>
  %13034 = shufflevector <2 x i64> %13033, <2 x i64> undef, <1 x i32> zeroinitializer
  %13035 = bitcast <1 x i64> %13034 to <4 x i16>
  %13036 = zext <8 x i8> %t3368 to <8 x i16>
  %13037 = bitcast <8 x i16> %13036 to <2 x i64>
  %13038 = shufflevector <2 x i64> %13037, <2 x i64> undef, <1 x i32> zeroinitializer
  %13039 = zext <8 x i8> %t3369 to <8 x i16>
  %13040 = bitcast <8 x i16> %13039 to <2 x i64>
  %13041 = shufflevector <2 x i64> %13040, <2 x i64> undef, <1 x i32> zeroinitializer
  %13042 = bitcast <1 x i64> %13041 to <4 x i16>
  %.cast2077 = bitcast <1 x i64> %13017 to <4 x i16>
  %13043 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2077, <4 x i16> %11795) #11
  %13044 = sext <4 x i16> %13014 to <4 x i32>
  %13045 = mul nsw <4 x i32> %13044, %11833
  %.cast2080 = bitcast <1 x i64> %13024 to <4 x i16>
  %13046 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2080, <4 x i16> %11804) #11
  %13047 = sext <4 x i16> %13021 to <4 x i32>
  %13048 = mul nsw <4 x i32> %13047, %11837
  %.cast2083 = bitcast <1 x i64> %13031 to <4 x i16>
  %13049 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2083, <4 x i16> %11813) #11
  %13050 = sext <4 x i16> %13028 to <4 x i32>
  %13051 = mul nsw <4 x i32> %13050, %11841
  %.cast2086 = bitcast <1 x i64> %13038 to <4 x i16>
  %13052 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2086, <4 x i16> %11822) #11
  %13053 = sext <4 x i16> %13035 to <4 x i32>
  %13054 = mul nsw <4 x i32> %13053, %11845
  %13055 = sext <4 x i16> %13042 to <4 x i32>
  %13056 = mul nsw <4 x i32> %13055, %11847
  %13057 = add <4 x i32> %13045, %11564
  %13058 = add <4 x i32> %13057, %13048
  %13059 = add <4 x i32> %13058, %13051
  %13060 = add <4 x i32> %13059, %13054
  %13061 = add <4 x i32> %13060, %13056
  %13062 = add <4 x i32> %13061, %13043
  %13063 = add <4 x i32> %13062, %13046
  %13064 = add <4 x i32> %13063, %13049
  %13065 = add <4 x i32> %13064, %13052
  %13066 = shufflevector <2 x i64> %13012, <2 x i64> undef, <1 x i32> <i32 1>
  %13067 = bitcast <1 x i64> %13066 to <4 x i16>
  %13068 = shufflevector <2 x i64> %13016, <2 x i64> undef, <1 x i32> <i32 1>
  %13069 = shufflevector <2 x i64> %13019, <2 x i64> undef, <1 x i32> <i32 1>
  %13070 = bitcast <1 x i64> %13069 to <4 x i16>
  %13071 = shufflevector <2 x i64> %13023, <2 x i64> undef, <1 x i32> <i32 1>
  %13072 = shufflevector <2 x i64> %13026, <2 x i64> undef, <1 x i32> <i32 1>
  %13073 = bitcast <1 x i64> %13072 to <4 x i16>
  %13074 = shufflevector <2 x i64> %13030, <2 x i64> undef, <1 x i32> <i32 1>
  %13075 = shufflevector <2 x i64> %13033, <2 x i64> undef, <1 x i32> <i32 1>
  %13076 = bitcast <1 x i64> %13075 to <4 x i16>
  %13077 = shufflevector <2 x i64> %13037, <2 x i64> undef, <1 x i32> <i32 1>
  %13078 = shufflevector <2 x i64> %13040, <2 x i64> undef, <1 x i32> <i32 1>
  %13079 = bitcast <1 x i64> %13078 to <4 x i16>
  %.cast2089 = bitcast <1 x i64> %13068 to <4 x i16>
  %13080 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2089, <4 x i16> %11862) #11
  %13081 = sext <4 x i16> %13067 to <4 x i32>
  %13082 = mul nsw <4 x i32> %13081, %11884
  %.cast2092 = bitcast <1 x i64> %13071 to <4 x i16>
  %13083 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2092, <4 x i16> %11867) #11
  %13084 = sext <4 x i16> %13070 to <4 x i32>
  %13085 = mul nsw <4 x i32> %13084, %11888
  %.cast2095 = bitcast <1 x i64> %13074 to <4 x i16>
  %13086 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2095, <4 x i16> %11872) #11
  %13087 = sext <4 x i16> %13073 to <4 x i32>
  %13088 = mul nsw <4 x i32> %13087, %11892
  %.cast2098 = bitcast <1 x i64> %13077 to <4 x i16>
  %13089 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2098, <4 x i16> %11877) #11
  %13090 = sext <4 x i16> %13076 to <4 x i32>
  %13091 = mul nsw <4 x i32> %13090, %11896
  %13092 = sext <4 x i16> %13079 to <4 x i32>
  %13093 = mul nsw <4 x i32> %13092, %11898
  %13094 = add <4 x i32> %13082, %11565
  %13095 = add <4 x i32> %13094, %13085
  %13096 = add <4 x i32> %13095, %13088
  %13097 = add <4 x i32> %13096, %13091
  %13098 = add <4 x i32> %13097, %13093
  %13099 = add <4 x i32> %13098, %13080
  %13100 = add <4 x i32> %13099, %13083
  %13101 = add <4 x i32> %13100, %13086
  %13102 = add <4 x i32> %13101, %13089
  %t3371 = add nsw i32 %t3258, %t2418568
  %13103 = sext i32 %t3371 to i64
  %13104 = shl nsw i64 %13103, 4
  %13105 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13104
  %13106 = bitcast i8* %13105 to <8 x i8>*
  %t3372 = load <8 x i8>, <8 x i8>* %13106, align 16, !tbaa !438
  %t3373 = add nsw i32 %t3258, %t2414567
  %13107 = sext i32 %t3373 to i64
  %13108 = shl nsw i64 %13107, 4
  %13109 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13108
  %13110 = bitcast i8* %13109 to <8 x i8>*
  %t3374 = load <8 x i8>, <8 x i8>* %13110, align 16, !tbaa !438
  %t3375 = add nsw i32 %t3258, %t2410544
  %13111 = sext i32 %t3375 to i64
  %13112 = shl nsw i64 %13111, 4
  %13113 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13112
  %13114 = bitcast i8* %13113 to <8 x i8>*
  %t3376 = load <8 x i8>, <8 x i8>* %13114, align 16, !tbaa !438
  %t3377 = add nsw i32 %t3258, %t2405566
  %13115 = sext i32 %t3377 to i64
  %13116 = shl nsw i64 %13115, 4
  %13117 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13116
  %13118 = bitcast i8* %13117 to <8 x i8>*
  %t3378 = load <8 x i8>, <8 x i8>* %13118, align 16, !tbaa !438
  %t3379 = add nsw i32 %t3258, %t2401565
  %13119 = sext i32 %t3379 to i64
  %13120 = shl nsw i64 %13119, 4
  %13121 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13120
  %13122 = bitcast i8* %13121 to <8 x i8>*
  %t3380 = load <8 x i8>, <8 x i8>* %13122, align 16, !tbaa !438
  %t3381 = add nsw i32 %t3258, %t2397543
  %13123 = sext i32 %t3381 to i64
  %13124 = shl nsw i64 %13123, 4
  %13125 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13124
  %13126 = bitcast i8* %13125 to <8 x i8>*
  %t3382 = load <8 x i8>, <8 x i8>* %13126, align 16, !tbaa !438
  %t3383 = add nsw i32 %t3258, %t2392552
  %13127 = sext i32 %t3383 to i64
  %13128 = shl nsw i64 %13127, 4
  %13129 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13128
  %13130 = bitcast i8* %13129 to <8 x i8>*
  %t3384 = load <8 x i8>, <8 x i8>* %13130, align 16, !tbaa !438
  %t3385 = add nsw i32 %t3258, %t2388551
  %13131 = sext i32 %t3385 to i64
  %13132 = shl nsw i64 %13131, 4
  %13133 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13132
  %13134 = bitcast i8* %13133 to <8 x i8>*
  %t3386 = load <8 x i8>, <8 x i8>* %13134, align 16, !tbaa !438
  %t3387 = add nsw i32 %t3258, %t2384536
  %13135 = sext i32 %t3387 to i64
  %13136 = shl nsw i64 %13135, 4
  %13137 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13136
  %13138 = bitcast i8* %13137 to <8 x i8>*
  %t3388 = load <8 x i8>, <8 x i8>* %13138, align 16, !tbaa !438
  %13139 = getelementptr inbounds i8, i8* %13105, i64 8
  %13140 = bitcast i8* %13139 to <8 x i8>*
  %t3389 = load <8 x i8>, <8 x i8>* %13140, align 8, !tbaa !438
  %13141 = getelementptr inbounds i8, i8* %13109, i64 8
  %13142 = bitcast i8* %13141 to <8 x i8>*
  %t3390 = load <8 x i8>, <8 x i8>* %13142, align 8, !tbaa !438
  %13143 = getelementptr inbounds i8, i8* %13113, i64 8
  %13144 = bitcast i8* %13143 to <8 x i8>*
  %t3391 = load <8 x i8>, <8 x i8>* %13144, align 8, !tbaa !438
  %13145 = getelementptr inbounds i8, i8* %13117, i64 8
  %13146 = bitcast i8* %13145 to <8 x i8>*
  %t3392 = load <8 x i8>, <8 x i8>* %13146, align 8, !tbaa !438
  %13147 = getelementptr inbounds i8, i8* %13121, i64 8
  %13148 = bitcast i8* %13147 to <8 x i8>*
  %t3393 = load <8 x i8>, <8 x i8>* %13148, align 8, !tbaa !438
  %13149 = getelementptr inbounds i8, i8* %13125, i64 8
  %13150 = bitcast i8* %13149 to <8 x i8>*
  %t3394 = load <8 x i8>, <8 x i8>* %13150, align 8, !tbaa !438
  %13151 = getelementptr inbounds i8, i8* %13129, i64 8
  %13152 = bitcast i8* %13151 to <8 x i8>*
  %t3395 = load <8 x i8>, <8 x i8>* %13152, align 8, !tbaa !438
  %13153 = getelementptr inbounds i8, i8* %13133, i64 8
  %13154 = bitcast i8* %13153 to <8 x i8>*
  %t3396 = load <8 x i8>, <8 x i8>* %13154, align 8, !tbaa !438
  %13155 = getelementptr inbounds i8, i8* %13137, i64 8
  %13156 = bitcast i8* %13155 to <8 x i8>*
  %t3397 = load <8 x i8>, <8 x i8>* %13156, align 8, !tbaa !438
  %13157 = zext <8 x i8> %t3372 to <8 x i16>
  %13158 = bitcast <8 x i16> %13157 to <2 x i64>
  %13159 = shufflevector <2 x i64> %13158, <2 x i64> undef, <1 x i32> zeroinitializer
  %13160 = bitcast <1 x i64> %13159 to <4 x i16>
  %13161 = zext <8 x i8> %t3374 to <8 x i16>
  %13162 = bitcast <8 x i16> %13161 to <2 x i64>
  %13163 = shufflevector <2 x i64> %13162, <2 x i64> undef, <1 x i32> zeroinitializer
  %13164 = zext <8 x i8> %t3376 to <8 x i16>
  %13165 = bitcast <8 x i16> %13164 to <2 x i64>
  %13166 = shufflevector <2 x i64> %13165, <2 x i64> undef, <1 x i32> zeroinitializer
  %13167 = bitcast <1 x i64> %13166 to <4 x i16>
  %13168 = zext <8 x i8> %t3378 to <8 x i16>
  %13169 = bitcast <8 x i16> %13168 to <2 x i64>
  %13170 = shufflevector <2 x i64> %13169, <2 x i64> undef, <1 x i32> zeroinitializer
  %13171 = zext <8 x i8> %t3380 to <8 x i16>
  %13172 = bitcast <8 x i16> %13171 to <2 x i64>
  %13173 = shufflevector <2 x i64> %13172, <2 x i64> undef, <1 x i32> zeroinitializer
  %13174 = bitcast <1 x i64> %13173 to <4 x i16>
  %13175 = zext <8 x i8> %t3382 to <8 x i16>
  %13176 = bitcast <8 x i16> %13175 to <2 x i64>
  %13177 = shufflevector <2 x i64> %13176, <2 x i64> undef, <1 x i32> zeroinitializer
  %13178 = zext <8 x i8> %t3384 to <8 x i16>
  %13179 = bitcast <8 x i16> %13178 to <2 x i64>
  %13180 = shufflevector <2 x i64> %13179, <2 x i64> undef, <1 x i32> zeroinitializer
  %13181 = bitcast <1 x i64> %13180 to <4 x i16>
  %13182 = zext <8 x i8> %t3386 to <8 x i16>
  %13183 = bitcast <8 x i16> %13182 to <2 x i64>
  %13184 = shufflevector <2 x i64> %13183, <2 x i64> undef, <1 x i32> zeroinitializer
  %13185 = zext <8 x i8> %t3388 to <8 x i16>
  %13186 = bitcast <8 x i16> %13185 to <2 x i64>
  %13187 = shufflevector <2 x i64> %13186, <2 x i64> undef, <1 x i32> zeroinitializer
  %13188 = bitcast <1 x i64> %13187 to <4 x i16>
  %.cast2101 = bitcast <1 x i64> %13163 to <4 x i16>
  %13189 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2101, <4 x i16> %11675) #11
  %13190 = sext <4 x i16> %13160 to <4 x i32>
  %13191 = mul nsw <4 x i32> %13190, %11713
  %.cast2104 = bitcast <1 x i64> %13170 to <4 x i16>
  %13192 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2104, <4 x i16> %11684) #11
  %13193 = sext <4 x i16> %13167 to <4 x i32>
  %13194 = mul nsw <4 x i32> %13193, %11717
  %.cast2107 = bitcast <1 x i64> %13177 to <4 x i16>
  %13195 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2107, <4 x i16> %11693) #11
  %13196 = sext <4 x i16> %13174 to <4 x i32>
  %13197 = mul nsw <4 x i32> %13196, %11721
  %.cast2110 = bitcast <1 x i64> %13184 to <4 x i16>
  %13198 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2110, <4 x i16> %11702) #11
  %13199 = sext <4 x i16> %13181 to <4 x i32>
  %13200 = mul nsw <4 x i32> %13199, %11725
  %13201 = sext <4 x i16> %13188 to <4 x i32>
  %13202 = mul nsw <4 x i32> %13201, %11727
  %13203 = add <4 x i32> %13191, %11562
  %13204 = add <4 x i32> %13203, %13194
  %13205 = add <4 x i32> %13204, %13197
  %13206 = add <4 x i32> %13205, %13200
  %13207 = add <4 x i32> %13206, %13202
  %13208 = add <4 x i32> %13207, %13189
  %13209 = add <4 x i32> %13208, %13192
  %13210 = add <4 x i32> %13209, %13195
  %13211 = add <4 x i32> %13210, %13198
  %13212 = shufflevector <2 x i64> %13158, <2 x i64> undef, <1 x i32> <i32 1>
  %13213 = bitcast <1 x i64> %13212 to <4 x i16>
  %13214 = shufflevector <2 x i64> %13162, <2 x i64> undef, <1 x i32> <i32 1>
  %13215 = shufflevector <2 x i64> %13165, <2 x i64> undef, <1 x i32> <i32 1>
  %13216 = bitcast <1 x i64> %13215 to <4 x i16>
  %13217 = shufflevector <2 x i64> %13169, <2 x i64> undef, <1 x i32> <i32 1>
  %13218 = shufflevector <2 x i64> %13172, <2 x i64> undef, <1 x i32> <i32 1>
  %13219 = bitcast <1 x i64> %13218 to <4 x i16>
  %13220 = shufflevector <2 x i64> %13176, <2 x i64> undef, <1 x i32> <i32 1>
  %13221 = shufflevector <2 x i64> %13179, <2 x i64> undef, <1 x i32> <i32 1>
  %13222 = bitcast <1 x i64> %13221 to <4 x i16>
  %13223 = shufflevector <2 x i64> %13183, <2 x i64> undef, <1 x i32> <i32 1>
  %13224 = shufflevector <2 x i64> %13186, <2 x i64> undef, <1 x i32> <i32 1>
  %13225 = bitcast <1 x i64> %13224 to <4 x i16>
  %.cast2113 = bitcast <1 x i64> %13214 to <4 x i16>
  %13226 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2113, <4 x i16> %11742) #11
  %13227 = sext <4 x i16> %13213 to <4 x i32>
  %13228 = mul nsw <4 x i32> %13227, %11764
  %.cast2116 = bitcast <1 x i64> %13217 to <4 x i16>
  %13229 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2116, <4 x i16> %11747) #11
  %13230 = sext <4 x i16> %13216 to <4 x i32>
  %13231 = mul nsw <4 x i32> %13230, %11768
  %.cast2119 = bitcast <1 x i64> %13220 to <4 x i16>
  %13232 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2119, <4 x i16> %11752) #11
  %13233 = sext <4 x i16> %13219 to <4 x i32>
  %13234 = mul nsw <4 x i32> %13233, %11772
  %.cast2122 = bitcast <1 x i64> %13223 to <4 x i16>
  %13235 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2122, <4 x i16> %11757) #11
  %13236 = sext <4 x i16> %13222 to <4 x i32>
  %13237 = mul nsw <4 x i32> %13236, %11776
  %13238 = sext <4 x i16> %13225 to <4 x i32>
  %13239 = mul nsw <4 x i32> %13238, %11778
  %13240 = add <4 x i32> %13228, %11563
  %13241 = add <4 x i32> %13240, %13231
  %13242 = add <4 x i32> %13241, %13234
  %13243 = add <4 x i32> %13242, %13237
  %13244 = add <4 x i32> %13243, %13239
  %13245 = add <4 x i32> %13244, %13226
  %13246 = add <4 x i32> %13245, %13229
  %13247 = add <4 x i32> %13246, %13232
  %13248 = add <4 x i32> %13247, %13235
  %13249 = zext <8 x i8> %t3389 to <8 x i16>
  %13250 = bitcast <8 x i16> %13249 to <2 x i64>
  %13251 = shufflevector <2 x i64> %13250, <2 x i64> undef, <1 x i32> zeroinitializer
  %13252 = bitcast <1 x i64> %13251 to <4 x i16>
  %13253 = zext <8 x i8> %t3390 to <8 x i16>
  %13254 = bitcast <8 x i16> %13253 to <2 x i64>
  %13255 = shufflevector <2 x i64> %13254, <2 x i64> undef, <1 x i32> zeroinitializer
  %13256 = zext <8 x i8> %t3391 to <8 x i16>
  %13257 = bitcast <8 x i16> %13256 to <2 x i64>
  %13258 = shufflevector <2 x i64> %13257, <2 x i64> undef, <1 x i32> zeroinitializer
  %13259 = bitcast <1 x i64> %13258 to <4 x i16>
  %13260 = zext <8 x i8> %t3392 to <8 x i16>
  %13261 = bitcast <8 x i16> %13260 to <2 x i64>
  %13262 = shufflevector <2 x i64> %13261, <2 x i64> undef, <1 x i32> zeroinitializer
  %13263 = zext <8 x i8> %t3393 to <8 x i16>
  %13264 = bitcast <8 x i16> %13263 to <2 x i64>
  %13265 = shufflevector <2 x i64> %13264, <2 x i64> undef, <1 x i32> zeroinitializer
  %13266 = bitcast <1 x i64> %13265 to <4 x i16>
  %13267 = zext <8 x i8> %t3394 to <8 x i16>
  %13268 = bitcast <8 x i16> %13267 to <2 x i64>
  %13269 = shufflevector <2 x i64> %13268, <2 x i64> undef, <1 x i32> zeroinitializer
  %13270 = zext <8 x i8> %t3395 to <8 x i16>
  %13271 = bitcast <8 x i16> %13270 to <2 x i64>
  %13272 = shufflevector <2 x i64> %13271, <2 x i64> undef, <1 x i32> zeroinitializer
  %13273 = bitcast <1 x i64> %13272 to <4 x i16>
  %13274 = zext <8 x i8> %t3396 to <8 x i16>
  %13275 = bitcast <8 x i16> %13274 to <2 x i64>
  %13276 = shufflevector <2 x i64> %13275, <2 x i64> undef, <1 x i32> zeroinitializer
  %13277 = zext <8 x i8> %t3397 to <8 x i16>
  %13278 = bitcast <8 x i16> %13277 to <2 x i64>
  %13279 = shufflevector <2 x i64> %13278, <2 x i64> undef, <1 x i32> zeroinitializer
  %13280 = bitcast <1 x i64> %13279 to <4 x i16>
  %.cast2125 = bitcast <1 x i64> %13255 to <4 x i16>
  %13281 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2125, <4 x i16> %11795) #11
  %13282 = sext <4 x i16> %13252 to <4 x i32>
  %13283 = mul nsw <4 x i32> %13282, %11833
  %.cast2128 = bitcast <1 x i64> %13262 to <4 x i16>
  %13284 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2128, <4 x i16> %11804) #11
  %13285 = sext <4 x i16> %13259 to <4 x i32>
  %13286 = mul nsw <4 x i32> %13285, %11837
  %.cast2131 = bitcast <1 x i64> %13269 to <4 x i16>
  %13287 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2131, <4 x i16> %11813) #11
  %13288 = sext <4 x i16> %13266 to <4 x i32>
  %13289 = mul nsw <4 x i32> %13288, %11841
  %.cast2134 = bitcast <1 x i64> %13276 to <4 x i16>
  %13290 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2134, <4 x i16> %11822) #11
  %13291 = sext <4 x i16> %13273 to <4 x i32>
  %13292 = mul nsw <4 x i32> %13291, %11845
  %13293 = sext <4 x i16> %13280 to <4 x i32>
  %13294 = mul nsw <4 x i32> %13293, %11847
  %13295 = add <4 x i32> %13283, %11564
  %13296 = add <4 x i32> %13295, %13286
  %13297 = add <4 x i32> %13296, %13289
  %13298 = add <4 x i32> %13297, %13292
  %13299 = add <4 x i32> %13298, %13294
  %13300 = add <4 x i32> %13299, %13281
  %13301 = add <4 x i32> %13300, %13284
  %13302 = add <4 x i32> %13301, %13287
  %13303 = add <4 x i32> %13302, %13290
  %13304 = shufflevector <2 x i64> %13250, <2 x i64> undef, <1 x i32> <i32 1>
  %13305 = bitcast <1 x i64> %13304 to <4 x i16>
  %13306 = shufflevector <2 x i64> %13254, <2 x i64> undef, <1 x i32> <i32 1>
  %13307 = shufflevector <2 x i64> %13257, <2 x i64> undef, <1 x i32> <i32 1>
  %13308 = bitcast <1 x i64> %13307 to <4 x i16>
  %13309 = shufflevector <2 x i64> %13261, <2 x i64> undef, <1 x i32> <i32 1>
  %13310 = shufflevector <2 x i64> %13264, <2 x i64> undef, <1 x i32> <i32 1>
  %13311 = bitcast <1 x i64> %13310 to <4 x i16>
  %13312 = shufflevector <2 x i64> %13268, <2 x i64> undef, <1 x i32> <i32 1>
  %13313 = shufflevector <2 x i64> %13271, <2 x i64> undef, <1 x i32> <i32 1>
  %13314 = bitcast <1 x i64> %13313 to <4 x i16>
  %13315 = shufflevector <2 x i64> %13275, <2 x i64> undef, <1 x i32> <i32 1>
  %13316 = shufflevector <2 x i64> %13278, <2 x i64> undef, <1 x i32> <i32 1>
  %13317 = bitcast <1 x i64> %13316 to <4 x i16>
  %.cast2137 = bitcast <1 x i64> %13306 to <4 x i16>
  %13318 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2137, <4 x i16> %11862) #11
  %13319 = sext <4 x i16> %13305 to <4 x i32>
  %13320 = mul nsw <4 x i32> %13319, %11884
  %.cast2140 = bitcast <1 x i64> %13309 to <4 x i16>
  %13321 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2140, <4 x i16> %11867) #11
  %13322 = sext <4 x i16> %13308 to <4 x i32>
  %13323 = mul nsw <4 x i32> %13322, %11888
  %.cast2143 = bitcast <1 x i64> %13312 to <4 x i16>
  %13324 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2143, <4 x i16> %11872) #11
  %13325 = sext <4 x i16> %13311 to <4 x i32>
  %13326 = mul nsw <4 x i32> %13325, %11892
  %.cast2146 = bitcast <1 x i64> %13315 to <4 x i16>
  %13327 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2146, <4 x i16> %11877) #11
  %13328 = sext <4 x i16> %13314 to <4 x i32>
  %13329 = mul nsw <4 x i32> %13328, %11896
  %13330 = sext <4 x i16> %13317 to <4 x i32>
  %13331 = mul nsw <4 x i32> %13330, %11898
  %13332 = add <4 x i32> %13320, %11565
  %13333 = add <4 x i32> %13332, %13323
  %13334 = add <4 x i32> %13333, %13326
  %13335 = add <4 x i32> %13334, %13329
  %13336 = add <4 x i32> %13335, %13331
  %13337 = add <4 x i32> %13336, %13318
  %13338 = add <4 x i32> %13337, %13321
  %13339 = add <4 x i32> %13338, %13324
  %13340 = add <4 x i32> %13339, %13327
  %t3399 = add nsw i32 %t3286, %t2418568
  %13341 = sext i32 %t3399 to i64
  %13342 = shl nsw i64 %13341, 4
  %13343 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13342
  %13344 = bitcast i8* %13343 to <8 x i8>*
  %t3400 = load <8 x i8>, <8 x i8>* %13344, align 16, !tbaa !438
  %t3401 = add nsw i32 %t3286, %t2414567
  %13345 = sext i32 %t3401 to i64
  %13346 = shl nsw i64 %13345, 4
  %13347 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13346
  %13348 = bitcast i8* %13347 to <8 x i8>*
  %t3402 = load <8 x i8>, <8 x i8>* %13348, align 16, !tbaa !438
  %t3403 = add nsw i32 %t3286, %t2410544
  %13349 = sext i32 %t3403 to i64
  %13350 = shl nsw i64 %13349, 4
  %13351 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13350
  %13352 = bitcast i8* %13351 to <8 x i8>*
  %t3404 = load <8 x i8>, <8 x i8>* %13352, align 16, !tbaa !438
  %t3405 = add nsw i32 %t3286, %t2405566
  %13353 = sext i32 %t3405 to i64
  %13354 = shl nsw i64 %13353, 4
  %13355 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13354
  %13356 = bitcast i8* %13355 to <8 x i8>*
  %t3406 = load <8 x i8>, <8 x i8>* %13356, align 16, !tbaa !438
  %t3407 = add nsw i32 %t3286, %t2401565
  %13357 = sext i32 %t3407 to i64
  %13358 = shl nsw i64 %13357, 4
  %13359 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13358
  %13360 = bitcast i8* %13359 to <8 x i8>*
  %t3408 = load <8 x i8>, <8 x i8>* %13360, align 16, !tbaa !438
  %t3409 = add nsw i32 %t3286, %t2397543
  %13361 = sext i32 %t3409 to i64
  %13362 = shl nsw i64 %13361, 4
  %13363 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13362
  %13364 = bitcast i8* %13363 to <8 x i8>*
  %t3410 = load <8 x i8>, <8 x i8>* %13364, align 16, !tbaa !438
  %t3411 = add nsw i32 %t3286, %t2392552
  %13365 = sext i32 %t3411 to i64
  %13366 = shl nsw i64 %13365, 4
  %13367 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13366
  %13368 = bitcast i8* %13367 to <8 x i8>*
  %t3412 = load <8 x i8>, <8 x i8>* %13368, align 16, !tbaa !438
  %t3413 = add nsw i32 %t3286, %t2388551
  %13369 = sext i32 %t3413 to i64
  %13370 = shl nsw i64 %13369, 4
  %13371 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13370
  %13372 = bitcast i8* %13371 to <8 x i8>*
  %t3414 = load <8 x i8>, <8 x i8>* %13372, align 16, !tbaa !438
  %t3415 = add nsw i32 %t3286, %t2384536
  %13373 = sext i32 %t3415 to i64
  %13374 = shl nsw i64 %13373, 4
  %13375 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13374
  %13376 = bitcast i8* %13375 to <8 x i8>*
  %t3416 = load <8 x i8>, <8 x i8>* %13376, align 16, !tbaa !438
  %13377 = getelementptr inbounds i8, i8* %13343, i64 8
  %13378 = bitcast i8* %13377 to <8 x i8>*
  %t3417 = load <8 x i8>, <8 x i8>* %13378, align 8, !tbaa !438
  %13379 = getelementptr inbounds i8, i8* %13347, i64 8
  %13380 = bitcast i8* %13379 to <8 x i8>*
  %t3418 = load <8 x i8>, <8 x i8>* %13380, align 8, !tbaa !438
  %13381 = getelementptr inbounds i8, i8* %13351, i64 8
  %13382 = bitcast i8* %13381 to <8 x i8>*
  %t3419 = load <8 x i8>, <8 x i8>* %13382, align 8, !tbaa !438
  %13383 = getelementptr inbounds i8, i8* %13355, i64 8
  %13384 = bitcast i8* %13383 to <8 x i8>*
  %t3420 = load <8 x i8>, <8 x i8>* %13384, align 8, !tbaa !438
  %13385 = getelementptr inbounds i8, i8* %13359, i64 8
  %13386 = bitcast i8* %13385 to <8 x i8>*
  %t3421 = load <8 x i8>, <8 x i8>* %13386, align 8, !tbaa !438
  %13387 = getelementptr inbounds i8, i8* %13363, i64 8
  %13388 = bitcast i8* %13387 to <8 x i8>*
  %t3422 = load <8 x i8>, <8 x i8>* %13388, align 8, !tbaa !438
  %13389 = getelementptr inbounds i8, i8* %13367, i64 8
  %13390 = bitcast i8* %13389 to <8 x i8>*
  %t3423 = load <8 x i8>, <8 x i8>* %13390, align 8, !tbaa !438
  %13391 = getelementptr inbounds i8, i8* %13371, i64 8
  %13392 = bitcast i8* %13391 to <8 x i8>*
  %t3424 = load <8 x i8>, <8 x i8>* %13392, align 8, !tbaa !438
  %13393 = getelementptr inbounds i8, i8* %13375, i64 8
  %13394 = bitcast i8* %13393 to <8 x i8>*
  %t3425 = load <8 x i8>, <8 x i8>* %13394, align 8, !tbaa !438
  %13395 = zext <8 x i8> %t3400 to <8 x i16>
  %13396 = bitcast <8 x i16> %13395 to <2 x i64>
  %13397 = shufflevector <2 x i64> %13396, <2 x i64> undef, <1 x i32> zeroinitializer
  %13398 = bitcast <1 x i64> %13397 to <4 x i16>
  %13399 = zext <8 x i8> %t3402 to <8 x i16>
  %13400 = bitcast <8 x i16> %13399 to <2 x i64>
  %13401 = shufflevector <2 x i64> %13400, <2 x i64> undef, <1 x i32> zeroinitializer
  %13402 = zext <8 x i8> %t3404 to <8 x i16>
  %13403 = bitcast <8 x i16> %13402 to <2 x i64>
  %13404 = shufflevector <2 x i64> %13403, <2 x i64> undef, <1 x i32> zeroinitializer
  %13405 = bitcast <1 x i64> %13404 to <4 x i16>
  %13406 = zext <8 x i8> %t3406 to <8 x i16>
  %13407 = bitcast <8 x i16> %13406 to <2 x i64>
  %13408 = shufflevector <2 x i64> %13407, <2 x i64> undef, <1 x i32> zeroinitializer
  %13409 = zext <8 x i8> %t3408 to <8 x i16>
  %13410 = bitcast <8 x i16> %13409 to <2 x i64>
  %13411 = shufflevector <2 x i64> %13410, <2 x i64> undef, <1 x i32> zeroinitializer
  %13412 = bitcast <1 x i64> %13411 to <4 x i16>
  %13413 = zext <8 x i8> %t3410 to <8 x i16>
  %13414 = bitcast <8 x i16> %13413 to <2 x i64>
  %13415 = shufflevector <2 x i64> %13414, <2 x i64> undef, <1 x i32> zeroinitializer
  %13416 = zext <8 x i8> %t3412 to <8 x i16>
  %13417 = bitcast <8 x i16> %13416 to <2 x i64>
  %13418 = shufflevector <2 x i64> %13417, <2 x i64> undef, <1 x i32> zeroinitializer
  %13419 = bitcast <1 x i64> %13418 to <4 x i16>
  %13420 = zext <8 x i8> %t3414 to <8 x i16>
  %13421 = bitcast <8 x i16> %13420 to <2 x i64>
  %13422 = shufflevector <2 x i64> %13421, <2 x i64> undef, <1 x i32> zeroinitializer
  %13423 = zext <8 x i8> %t3416 to <8 x i16>
  %13424 = bitcast <8 x i16> %13423 to <2 x i64>
  %13425 = shufflevector <2 x i64> %13424, <2 x i64> undef, <1 x i32> zeroinitializer
  %13426 = bitcast <1 x i64> %13425 to <4 x i16>
  %.cast2149 = bitcast <1 x i64> %13401 to <4 x i16>
  %13427 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2149, <4 x i16> %11675) #11
  %13428 = sext <4 x i16> %13398 to <4 x i32>
  %13429 = mul nsw <4 x i32> %13428, %11713
  %.cast2152 = bitcast <1 x i64> %13408 to <4 x i16>
  %13430 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2152, <4 x i16> %11684) #11
  %13431 = sext <4 x i16> %13405 to <4 x i32>
  %13432 = mul nsw <4 x i32> %13431, %11717
  %.cast2155 = bitcast <1 x i64> %13415 to <4 x i16>
  %13433 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2155, <4 x i16> %11693) #11
  %13434 = sext <4 x i16> %13412 to <4 x i32>
  %13435 = mul nsw <4 x i32> %13434, %11721
  %.cast2158 = bitcast <1 x i64> %13422 to <4 x i16>
  %13436 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2158, <4 x i16> %11702) #11
  %13437 = sext <4 x i16> %13419 to <4 x i32>
  %13438 = mul nsw <4 x i32> %13437, %11725
  %13439 = sext <4 x i16> %13426 to <4 x i32>
  %13440 = mul nsw <4 x i32> %13439, %11727
  %13441 = add <4 x i32> %13429, %11562
  %13442 = add <4 x i32> %13441, %13432
  %13443 = add <4 x i32> %13442, %13435
  %13444 = add <4 x i32> %13443, %13438
  %13445 = add <4 x i32> %13444, %13440
  %13446 = add <4 x i32> %13445, %13427
  %13447 = add <4 x i32> %13446, %13430
  %13448 = add <4 x i32> %13447, %13433
  %13449 = add <4 x i32> %13448, %13436
  %13450 = shufflevector <2 x i64> %13396, <2 x i64> undef, <1 x i32> <i32 1>
  %13451 = bitcast <1 x i64> %13450 to <4 x i16>
  %13452 = shufflevector <2 x i64> %13400, <2 x i64> undef, <1 x i32> <i32 1>
  %13453 = shufflevector <2 x i64> %13403, <2 x i64> undef, <1 x i32> <i32 1>
  %13454 = bitcast <1 x i64> %13453 to <4 x i16>
  %13455 = shufflevector <2 x i64> %13407, <2 x i64> undef, <1 x i32> <i32 1>
  %13456 = shufflevector <2 x i64> %13410, <2 x i64> undef, <1 x i32> <i32 1>
  %13457 = bitcast <1 x i64> %13456 to <4 x i16>
  %13458 = shufflevector <2 x i64> %13414, <2 x i64> undef, <1 x i32> <i32 1>
  %13459 = shufflevector <2 x i64> %13417, <2 x i64> undef, <1 x i32> <i32 1>
  %13460 = bitcast <1 x i64> %13459 to <4 x i16>
  %13461 = shufflevector <2 x i64> %13421, <2 x i64> undef, <1 x i32> <i32 1>
  %13462 = shufflevector <2 x i64> %13424, <2 x i64> undef, <1 x i32> <i32 1>
  %13463 = bitcast <1 x i64> %13462 to <4 x i16>
  %.cast2161 = bitcast <1 x i64> %13452 to <4 x i16>
  %13464 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2161, <4 x i16> %11742) #11
  %13465 = sext <4 x i16> %13451 to <4 x i32>
  %13466 = mul nsw <4 x i32> %13465, %11764
  %.cast2164 = bitcast <1 x i64> %13455 to <4 x i16>
  %13467 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2164, <4 x i16> %11747) #11
  %13468 = sext <4 x i16> %13454 to <4 x i32>
  %13469 = mul nsw <4 x i32> %13468, %11768
  %.cast2167 = bitcast <1 x i64> %13458 to <4 x i16>
  %13470 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2167, <4 x i16> %11752) #11
  %13471 = sext <4 x i16> %13457 to <4 x i32>
  %13472 = mul nsw <4 x i32> %13471, %11772
  %.cast2170 = bitcast <1 x i64> %13461 to <4 x i16>
  %13473 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2170, <4 x i16> %11757) #11
  %13474 = sext <4 x i16> %13460 to <4 x i32>
  %13475 = mul nsw <4 x i32> %13474, %11776
  %13476 = sext <4 x i16> %13463 to <4 x i32>
  %13477 = mul nsw <4 x i32> %13476, %11778
  %13478 = add <4 x i32> %13466, %11563
  %13479 = add <4 x i32> %13478, %13469
  %13480 = add <4 x i32> %13479, %13472
  %13481 = add <4 x i32> %13480, %13475
  %13482 = add <4 x i32> %13481, %13477
  %13483 = add <4 x i32> %13482, %13464
  %13484 = add <4 x i32> %13483, %13467
  %13485 = add <4 x i32> %13484, %13470
  %13486 = add <4 x i32> %13485, %13473
  %13487 = zext <8 x i8> %t3417 to <8 x i16>
  %13488 = bitcast <8 x i16> %13487 to <2 x i64>
  %13489 = shufflevector <2 x i64> %13488, <2 x i64> undef, <1 x i32> zeroinitializer
  %13490 = bitcast <1 x i64> %13489 to <4 x i16>
  %13491 = zext <8 x i8> %t3418 to <8 x i16>
  %13492 = bitcast <8 x i16> %13491 to <2 x i64>
  %13493 = shufflevector <2 x i64> %13492, <2 x i64> undef, <1 x i32> zeroinitializer
  %13494 = zext <8 x i8> %t3419 to <8 x i16>
  %13495 = bitcast <8 x i16> %13494 to <2 x i64>
  %13496 = shufflevector <2 x i64> %13495, <2 x i64> undef, <1 x i32> zeroinitializer
  %13497 = bitcast <1 x i64> %13496 to <4 x i16>
  %13498 = zext <8 x i8> %t3420 to <8 x i16>
  %13499 = bitcast <8 x i16> %13498 to <2 x i64>
  %13500 = shufflevector <2 x i64> %13499, <2 x i64> undef, <1 x i32> zeroinitializer
  %13501 = zext <8 x i8> %t3421 to <8 x i16>
  %13502 = bitcast <8 x i16> %13501 to <2 x i64>
  %13503 = shufflevector <2 x i64> %13502, <2 x i64> undef, <1 x i32> zeroinitializer
  %13504 = bitcast <1 x i64> %13503 to <4 x i16>
  %13505 = zext <8 x i8> %t3422 to <8 x i16>
  %13506 = bitcast <8 x i16> %13505 to <2 x i64>
  %13507 = shufflevector <2 x i64> %13506, <2 x i64> undef, <1 x i32> zeroinitializer
  %13508 = zext <8 x i8> %t3423 to <8 x i16>
  %13509 = bitcast <8 x i16> %13508 to <2 x i64>
  %13510 = shufflevector <2 x i64> %13509, <2 x i64> undef, <1 x i32> zeroinitializer
  %13511 = bitcast <1 x i64> %13510 to <4 x i16>
  %13512 = zext <8 x i8> %t3424 to <8 x i16>
  %13513 = bitcast <8 x i16> %13512 to <2 x i64>
  %13514 = shufflevector <2 x i64> %13513, <2 x i64> undef, <1 x i32> zeroinitializer
  %13515 = zext <8 x i8> %t3425 to <8 x i16>
  %13516 = bitcast <8 x i16> %13515 to <2 x i64>
  %13517 = shufflevector <2 x i64> %13516, <2 x i64> undef, <1 x i32> zeroinitializer
  %13518 = bitcast <1 x i64> %13517 to <4 x i16>
  %.cast2173 = bitcast <1 x i64> %13493 to <4 x i16>
  %13519 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2173, <4 x i16> %11795) #11
  %13520 = sext <4 x i16> %13490 to <4 x i32>
  %13521 = mul nsw <4 x i32> %13520, %11833
  %.cast2176 = bitcast <1 x i64> %13500 to <4 x i16>
  %13522 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2176, <4 x i16> %11804) #11
  %13523 = sext <4 x i16> %13497 to <4 x i32>
  %13524 = mul nsw <4 x i32> %13523, %11837
  %.cast2179 = bitcast <1 x i64> %13507 to <4 x i16>
  %13525 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2179, <4 x i16> %11813) #11
  %13526 = sext <4 x i16> %13504 to <4 x i32>
  %13527 = mul nsw <4 x i32> %13526, %11841
  %.cast2182 = bitcast <1 x i64> %13514 to <4 x i16>
  %13528 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2182, <4 x i16> %11822) #11
  %13529 = sext <4 x i16> %13511 to <4 x i32>
  %13530 = mul nsw <4 x i32> %13529, %11845
  %13531 = sext <4 x i16> %13518 to <4 x i32>
  %13532 = mul nsw <4 x i32> %13531, %11847
  %13533 = add <4 x i32> %13521, %11564
  %13534 = add <4 x i32> %13533, %13524
  %13535 = add <4 x i32> %13534, %13527
  %13536 = add <4 x i32> %13535, %13530
  %13537 = add <4 x i32> %13536, %13532
  %13538 = add <4 x i32> %13537, %13519
  %13539 = add <4 x i32> %13538, %13522
  %13540 = add <4 x i32> %13539, %13525
  %13541 = add <4 x i32> %13540, %13528
  %13542 = shufflevector <2 x i64> %13488, <2 x i64> undef, <1 x i32> <i32 1>
  %13543 = bitcast <1 x i64> %13542 to <4 x i16>
  %13544 = shufflevector <2 x i64> %13492, <2 x i64> undef, <1 x i32> <i32 1>
  %13545 = shufflevector <2 x i64> %13495, <2 x i64> undef, <1 x i32> <i32 1>
  %13546 = bitcast <1 x i64> %13545 to <4 x i16>
  %13547 = shufflevector <2 x i64> %13499, <2 x i64> undef, <1 x i32> <i32 1>
  %13548 = shufflevector <2 x i64> %13502, <2 x i64> undef, <1 x i32> <i32 1>
  %13549 = bitcast <1 x i64> %13548 to <4 x i16>
  %13550 = shufflevector <2 x i64> %13506, <2 x i64> undef, <1 x i32> <i32 1>
  %13551 = shufflevector <2 x i64> %13509, <2 x i64> undef, <1 x i32> <i32 1>
  %13552 = bitcast <1 x i64> %13551 to <4 x i16>
  %13553 = shufflevector <2 x i64> %13513, <2 x i64> undef, <1 x i32> <i32 1>
  %13554 = shufflevector <2 x i64> %13516, <2 x i64> undef, <1 x i32> <i32 1>
  %13555 = bitcast <1 x i64> %13554 to <4 x i16>
  %.cast2185 = bitcast <1 x i64> %13544 to <4 x i16>
  %13556 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2185, <4 x i16> %11862) #11
  %13557 = sext <4 x i16> %13543 to <4 x i32>
  %13558 = mul nsw <4 x i32> %13557, %11884
  %.cast2188 = bitcast <1 x i64> %13547 to <4 x i16>
  %13559 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2188, <4 x i16> %11867) #11
  %13560 = sext <4 x i16> %13546 to <4 x i32>
  %13561 = mul nsw <4 x i32> %13560, %11888
  %.cast2191 = bitcast <1 x i64> %13550 to <4 x i16>
  %13562 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2191, <4 x i16> %11872) #11
  %13563 = sext <4 x i16> %13549 to <4 x i32>
  %13564 = mul nsw <4 x i32> %13563, %11892
  %.cast2194 = bitcast <1 x i64> %13553 to <4 x i16>
  %13565 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2194, <4 x i16> %11877) #11
  %13566 = sext <4 x i16> %13552 to <4 x i32>
  %13567 = mul nsw <4 x i32> %13566, %11896
  %13568 = sext <4 x i16> %13555 to <4 x i32>
  %13569 = mul nsw <4 x i32> %13568, %11898
  %13570 = add <4 x i32> %13558, %11565
  %13571 = add <4 x i32> %13570, %13561
  %13572 = add <4 x i32> %13571, %13564
  %13573 = add <4 x i32> %13572, %13567
  %13574 = add <4 x i32> %13573, %13569
  %13575 = add <4 x i32> %13574, %13556
  %13576 = add <4 x i32> %13575, %13559
  %13577 = add <4 x i32> %13576, %13562
  %13578 = add <4 x i32> %13577, %13565
  %t3427 = add nsw i32 %t3202, %t2419564
  %13579 = sext i32 %t3427 to i64
  %13580 = shl nsw i64 %13579, 4
  %13581 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13580
  %13582 = bitcast i8* %13581 to <8 x i8>*
  %t3428 = load <8 x i8>, <8 x i8>* %13582, align 16, !tbaa !438
  %t3429 = add nsw i32 %t3202, %t2415563
  %13583 = sext i32 %t3429 to i64
  %13584 = shl nsw i64 %13583, 4
  %13585 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13584
  %13586 = bitcast i8* %13585 to <8 x i8>*
  %t3430 = load <8 x i8>, <8 x i8>* %13586, align 16, !tbaa !438
  %t3431 = add nsw i32 %t3202, %t2411542
  %13587 = sext i32 %t3431 to i64
  %13588 = shl nsw i64 %13587, 4
  %13589 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13588
  %13590 = bitcast i8* %13589 to <8 x i8>*
  %t3432 = load <8 x i8>, <8 x i8>* %13590, align 16, !tbaa !438
  %t3433 = add nsw i32 %t3202, %t2406562
  %13591 = sext i32 %t3433 to i64
  %13592 = shl nsw i64 %13591, 4
  %13593 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13592
  %13594 = bitcast i8* %13593 to <8 x i8>*
  %t3434 = load <8 x i8>, <8 x i8>* %13594, align 16, !tbaa !438
  %t3435 = add nsw i32 %t3202, %t2402561
  %13595 = sext i32 %t3435 to i64
  %13596 = shl nsw i64 %13595, 4
  %13597 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13596
  %13598 = bitcast i8* %13597 to <8 x i8>*
  %t3436 = load <8 x i8>, <8 x i8>* %13598, align 16, !tbaa !438
  %t3437 = add nsw i32 %t3202, %t2398541
  %13599 = sext i32 %t3437 to i64
  %13600 = shl nsw i64 %13599, 4
  %13601 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13600
  %13602 = bitcast i8* %13601 to <8 x i8>*
  %t3438 = load <8 x i8>, <8 x i8>* %13602, align 16, !tbaa !438
  %t3439 = add nsw i32 %t3202, %t2393550
  %13603 = sext i32 %t3439 to i64
  %13604 = shl nsw i64 %13603, 4
  %13605 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13604
  %13606 = bitcast i8* %13605 to <8 x i8>*
  %t3440 = load <8 x i8>, <8 x i8>* %13606, align 16, !tbaa !438
  %t3441 = add nsw i32 %t3202, %t2389549
  %13607 = sext i32 %t3441 to i64
  %13608 = shl nsw i64 %13607, 4
  %13609 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13608
  %13610 = bitcast i8* %13609 to <8 x i8>*
  %t3442 = load <8 x i8>, <8 x i8>* %13610, align 16, !tbaa !438
  %t3443 = add nsw i32 %t3202, %t2385535
  %13611 = sext i32 %t3443 to i64
  %13612 = shl nsw i64 %13611, 4
  %13613 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13612
  %13614 = bitcast i8* %13613 to <8 x i8>*
  %t3444 = load <8 x i8>, <8 x i8>* %13614, align 16, !tbaa !438
  %13615 = getelementptr inbounds i8, i8* %13581, i64 8
  %13616 = bitcast i8* %13615 to <8 x i8>*
  %t3445 = load <8 x i8>, <8 x i8>* %13616, align 8, !tbaa !438
  %13617 = getelementptr inbounds i8, i8* %13585, i64 8
  %13618 = bitcast i8* %13617 to <8 x i8>*
  %t3446 = load <8 x i8>, <8 x i8>* %13618, align 8, !tbaa !438
  %13619 = getelementptr inbounds i8, i8* %13589, i64 8
  %13620 = bitcast i8* %13619 to <8 x i8>*
  %t3447 = load <8 x i8>, <8 x i8>* %13620, align 8, !tbaa !438
  %13621 = getelementptr inbounds i8, i8* %13593, i64 8
  %13622 = bitcast i8* %13621 to <8 x i8>*
  %t3448 = load <8 x i8>, <8 x i8>* %13622, align 8, !tbaa !438
  %13623 = getelementptr inbounds i8, i8* %13597, i64 8
  %13624 = bitcast i8* %13623 to <8 x i8>*
  %t3449 = load <8 x i8>, <8 x i8>* %13624, align 8, !tbaa !438
  %13625 = getelementptr inbounds i8, i8* %13601, i64 8
  %13626 = bitcast i8* %13625 to <8 x i8>*
  %t3450 = load <8 x i8>, <8 x i8>* %13626, align 8, !tbaa !438
  %13627 = getelementptr inbounds i8, i8* %13605, i64 8
  %13628 = bitcast i8* %13627 to <8 x i8>*
  %t3451 = load <8 x i8>, <8 x i8>* %13628, align 8, !tbaa !438
  %13629 = getelementptr inbounds i8, i8* %13609, i64 8
  %13630 = bitcast i8* %13629 to <8 x i8>*
  %t3452 = load <8 x i8>, <8 x i8>* %13630, align 8, !tbaa !438
  %13631 = getelementptr inbounds i8, i8* %13613, i64 8
  %13632 = bitcast i8* %13631 to <8 x i8>*
  %t3453 = load <8 x i8>, <8 x i8>* %13632, align 8, !tbaa !438
  %13633 = zext <8 x i8> %t3428 to <8 x i16>
  %13634 = bitcast <8 x i16> %13633 to <2 x i64>
  %13635 = shufflevector <2 x i64> %13634, <2 x i64> undef, <1 x i32> zeroinitializer
  %13636 = bitcast <1 x i64> %13635 to <4 x i16>
  %13637 = zext <8 x i8> %t3430 to <8 x i16>
  %13638 = bitcast <8 x i16> %13637 to <2 x i64>
  %13639 = shufflevector <2 x i64> %13638, <2 x i64> undef, <1 x i32> zeroinitializer
  %13640 = zext <8 x i8> %t3432 to <8 x i16>
  %13641 = bitcast <8 x i16> %13640 to <2 x i64>
  %13642 = shufflevector <2 x i64> %13641, <2 x i64> undef, <1 x i32> zeroinitializer
  %13643 = bitcast <1 x i64> %13642 to <4 x i16>
  %13644 = zext <8 x i8> %t3434 to <8 x i16>
  %13645 = bitcast <8 x i16> %13644 to <2 x i64>
  %13646 = shufflevector <2 x i64> %13645, <2 x i64> undef, <1 x i32> zeroinitializer
  %13647 = zext <8 x i8> %t3436 to <8 x i16>
  %13648 = bitcast <8 x i16> %13647 to <2 x i64>
  %13649 = shufflevector <2 x i64> %13648, <2 x i64> undef, <1 x i32> zeroinitializer
  %13650 = bitcast <1 x i64> %13649 to <4 x i16>
  %13651 = zext <8 x i8> %t3438 to <8 x i16>
  %13652 = bitcast <8 x i16> %13651 to <2 x i64>
  %13653 = shufflevector <2 x i64> %13652, <2 x i64> undef, <1 x i32> zeroinitializer
  %13654 = zext <8 x i8> %t3440 to <8 x i16>
  %13655 = bitcast <8 x i16> %13654 to <2 x i64>
  %13656 = shufflevector <2 x i64> %13655, <2 x i64> undef, <1 x i32> zeroinitializer
  %13657 = bitcast <1 x i64> %13656 to <4 x i16>
  %13658 = zext <8 x i8> %t3442 to <8 x i16>
  %13659 = bitcast <8 x i16> %13658 to <2 x i64>
  %13660 = shufflevector <2 x i64> %13659, <2 x i64> undef, <1 x i32> zeroinitializer
  %13661 = zext <8 x i8> %t3444 to <8 x i16>
  %13662 = bitcast <8 x i16> %13661 to <2 x i64>
  %13663 = shufflevector <2 x i64> %13662, <2 x i64> undef, <1 x i32> zeroinitializer
  %13664 = bitcast <1 x i64> %13663 to <4 x i16>
  %.cast2197 = bitcast <1 x i64> %13639 to <4 x i16>
  %13665 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2197, <4 x i16> %11675) #11
  %13666 = sext <4 x i16> %13636 to <4 x i32>
  %13667 = mul nsw <4 x i32> %13666, %11713
  %.cast2200 = bitcast <1 x i64> %13646 to <4 x i16>
  %13668 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2200, <4 x i16> %11684) #11
  %13669 = sext <4 x i16> %13643 to <4 x i32>
  %13670 = mul nsw <4 x i32> %13669, %11717
  %.cast2203 = bitcast <1 x i64> %13653 to <4 x i16>
  %13671 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2203, <4 x i16> %11693) #11
  %13672 = sext <4 x i16> %13650 to <4 x i32>
  %13673 = mul nsw <4 x i32> %13672, %11721
  %.cast2206 = bitcast <1 x i64> %13660 to <4 x i16>
  %13674 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2206, <4 x i16> %11702) #11
  %13675 = sext <4 x i16> %13657 to <4 x i32>
  %13676 = mul nsw <4 x i32> %13675, %11725
  %13677 = sext <4 x i16> %13664 to <4 x i32>
  %13678 = mul nsw <4 x i32> %13677, %11727
  %13679 = add <4 x i32> %13667, %11562
  %13680 = add <4 x i32> %13679, %13670
  %13681 = add <4 x i32> %13680, %13673
  %13682 = add <4 x i32> %13681, %13676
  %13683 = add <4 x i32> %13682, %13678
  %13684 = add <4 x i32> %13683, %13665
  %13685 = add <4 x i32> %13684, %13668
  %13686 = add <4 x i32> %13685, %13671
  %13687 = add <4 x i32> %13686, %13674
  %13688 = shufflevector <2 x i64> %13634, <2 x i64> undef, <1 x i32> <i32 1>
  %13689 = bitcast <1 x i64> %13688 to <4 x i16>
  %13690 = shufflevector <2 x i64> %13638, <2 x i64> undef, <1 x i32> <i32 1>
  %13691 = shufflevector <2 x i64> %13641, <2 x i64> undef, <1 x i32> <i32 1>
  %13692 = bitcast <1 x i64> %13691 to <4 x i16>
  %13693 = shufflevector <2 x i64> %13645, <2 x i64> undef, <1 x i32> <i32 1>
  %13694 = shufflevector <2 x i64> %13648, <2 x i64> undef, <1 x i32> <i32 1>
  %13695 = bitcast <1 x i64> %13694 to <4 x i16>
  %13696 = shufflevector <2 x i64> %13652, <2 x i64> undef, <1 x i32> <i32 1>
  %13697 = shufflevector <2 x i64> %13655, <2 x i64> undef, <1 x i32> <i32 1>
  %13698 = bitcast <1 x i64> %13697 to <4 x i16>
  %13699 = shufflevector <2 x i64> %13659, <2 x i64> undef, <1 x i32> <i32 1>
  %13700 = shufflevector <2 x i64> %13662, <2 x i64> undef, <1 x i32> <i32 1>
  %13701 = bitcast <1 x i64> %13700 to <4 x i16>
  %.cast2209 = bitcast <1 x i64> %13690 to <4 x i16>
  %13702 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2209, <4 x i16> %11742) #11
  %13703 = sext <4 x i16> %13689 to <4 x i32>
  %13704 = mul nsw <4 x i32> %13703, %11764
  %.cast2212 = bitcast <1 x i64> %13693 to <4 x i16>
  %13705 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2212, <4 x i16> %11747) #11
  %13706 = sext <4 x i16> %13692 to <4 x i32>
  %13707 = mul nsw <4 x i32> %13706, %11768
  %.cast2215 = bitcast <1 x i64> %13696 to <4 x i16>
  %13708 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2215, <4 x i16> %11752) #11
  %13709 = sext <4 x i16> %13695 to <4 x i32>
  %13710 = mul nsw <4 x i32> %13709, %11772
  %.cast2218 = bitcast <1 x i64> %13699 to <4 x i16>
  %13711 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2218, <4 x i16> %11757) #11
  %13712 = sext <4 x i16> %13698 to <4 x i32>
  %13713 = mul nsw <4 x i32> %13712, %11776
  %13714 = sext <4 x i16> %13701 to <4 x i32>
  %13715 = mul nsw <4 x i32> %13714, %11778
  %13716 = add <4 x i32> %13704, %11563
  %13717 = add <4 x i32> %13716, %13707
  %13718 = add <4 x i32> %13717, %13710
  %13719 = add <4 x i32> %13718, %13713
  %13720 = add <4 x i32> %13719, %13715
  %13721 = add <4 x i32> %13720, %13702
  %13722 = add <4 x i32> %13721, %13705
  %13723 = add <4 x i32> %13722, %13708
  %13724 = add <4 x i32> %13723, %13711
  %13725 = zext <8 x i8> %t3445 to <8 x i16>
  %13726 = bitcast <8 x i16> %13725 to <2 x i64>
  %13727 = shufflevector <2 x i64> %13726, <2 x i64> undef, <1 x i32> zeroinitializer
  %13728 = bitcast <1 x i64> %13727 to <4 x i16>
  %13729 = zext <8 x i8> %t3446 to <8 x i16>
  %13730 = bitcast <8 x i16> %13729 to <2 x i64>
  %13731 = shufflevector <2 x i64> %13730, <2 x i64> undef, <1 x i32> zeroinitializer
  %13732 = zext <8 x i8> %t3447 to <8 x i16>
  %13733 = bitcast <8 x i16> %13732 to <2 x i64>
  %13734 = shufflevector <2 x i64> %13733, <2 x i64> undef, <1 x i32> zeroinitializer
  %13735 = bitcast <1 x i64> %13734 to <4 x i16>
  %13736 = zext <8 x i8> %t3448 to <8 x i16>
  %13737 = bitcast <8 x i16> %13736 to <2 x i64>
  %13738 = shufflevector <2 x i64> %13737, <2 x i64> undef, <1 x i32> zeroinitializer
  %13739 = zext <8 x i8> %t3449 to <8 x i16>
  %13740 = bitcast <8 x i16> %13739 to <2 x i64>
  %13741 = shufflevector <2 x i64> %13740, <2 x i64> undef, <1 x i32> zeroinitializer
  %13742 = bitcast <1 x i64> %13741 to <4 x i16>
  %13743 = zext <8 x i8> %t3450 to <8 x i16>
  %13744 = bitcast <8 x i16> %13743 to <2 x i64>
  %13745 = shufflevector <2 x i64> %13744, <2 x i64> undef, <1 x i32> zeroinitializer
  %13746 = zext <8 x i8> %t3451 to <8 x i16>
  %13747 = bitcast <8 x i16> %13746 to <2 x i64>
  %13748 = shufflevector <2 x i64> %13747, <2 x i64> undef, <1 x i32> zeroinitializer
  %13749 = bitcast <1 x i64> %13748 to <4 x i16>
  %13750 = zext <8 x i8> %t3452 to <8 x i16>
  %13751 = bitcast <8 x i16> %13750 to <2 x i64>
  %13752 = shufflevector <2 x i64> %13751, <2 x i64> undef, <1 x i32> zeroinitializer
  %13753 = zext <8 x i8> %t3453 to <8 x i16>
  %13754 = bitcast <8 x i16> %13753 to <2 x i64>
  %13755 = shufflevector <2 x i64> %13754, <2 x i64> undef, <1 x i32> zeroinitializer
  %13756 = bitcast <1 x i64> %13755 to <4 x i16>
  %.cast2221 = bitcast <1 x i64> %13731 to <4 x i16>
  %13757 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2221, <4 x i16> %11795) #11
  %13758 = sext <4 x i16> %13728 to <4 x i32>
  %13759 = mul nsw <4 x i32> %13758, %11833
  %.cast2224 = bitcast <1 x i64> %13738 to <4 x i16>
  %13760 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2224, <4 x i16> %11804) #11
  %13761 = sext <4 x i16> %13735 to <4 x i32>
  %13762 = mul nsw <4 x i32> %13761, %11837
  %.cast2227 = bitcast <1 x i64> %13745 to <4 x i16>
  %13763 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2227, <4 x i16> %11813) #11
  %13764 = sext <4 x i16> %13742 to <4 x i32>
  %13765 = mul nsw <4 x i32> %13764, %11841
  %.cast2230 = bitcast <1 x i64> %13752 to <4 x i16>
  %13766 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2230, <4 x i16> %11822) #11
  %13767 = sext <4 x i16> %13749 to <4 x i32>
  %13768 = mul nsw <4 x i32> %13767, %11845
  %13769 = sext <4 x i16> %13756 to <4 x i32>
  %13770 = mul nsw <4 x i32> %13769, %11847
  %13771 = add <4 x i32> %13759, %11564
  %13772 = add <4 x i32> %13771, %13762
  %13773 = add <4 x i32> %13772, %13765
  %13774 = add <4 x i32> %13773, %13768
  %13775 = add <4 x i32> %13774, %13770
  %13776 = add <4 x i32> %13775, %13757
  %13777 = add <4 x i32> %13776, %13760
  %13778 = add <4 x i32> %13777, %13763
  %13779 = add <4 x i32> %13778, %13766
  %13780 = shufflevector <2 x i64> %13726, <2 x i64> undef, <1 x i32> <i32 1>
  %13781 = bitcast <1 x i64> %13780 to <4 x i16>
  %13782 = shufflevector <2 x i64> %13730, <2 x i64> undef, <1 x i32> <i32 1>
  %13783 = shufflevector <2 x i64> %13733, <2 x i64> undef, <1 x i32> <i32 1>
  %13784 = bitcast <1 x i64> %13783 to <4 x i16>
  %13785 = shufflevector <2 x i64> %13737, <2 x i64> undef, <1 x i32> <i32 1>
  %13786 = shufflevector <2 x i64> %13740, <2 x i64> undef, <1 x i32> <i32 1>
  %13787 = bitcast <1 x i64> %13786 to <4 x i16>
  %13788 = shufflevector <2 x i64> %13744, <2 x i64> undef, <1 x i32> <i32 1>
  %13789 = shufflevector <2 x i64> %13747, <2 x i64> undef, <1 x i32> <i32 1>
  %13790 = bitcast <1 x i64> %13789 to <4 x i16>
  %13791 = shufflevector <2 x i64> %13751, <2 x i64> undef, <1 x i32> <i32 1>
  %13792 = shufflevector <2 x i64> %13754, <2 x i64> undef, <1 x i32> <i32 1>
  %13793 = bitcast <1 x i64> %13792 to <4 x i16>
  %.cast2233 = bitcast <1 x i64> %13782 to <4 x i16>
  %13794 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2233, <4 x i16> %11862) #11
  %13795 = sext <4 x i16> %13781 to <4 x i32>
  %13796 = mul nsw <4 x i32> %13795, %11884
  %.cast2236 = bitcast <1 x i64> %13785 to <4 x i16>
  %13797 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2236, <4 x i16> %11867) #11
  %13798 = sext <4 x i16> %13784 to <4 x i32>
  %13799 = mul nsw <4 x i32> %13798, %11888
  %.cast2239 = bitcast <1 x i64> %13788 to <4 x i16>
  %13800 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2239, <4 x i16> %11872) #11
  %13801 = sext <4 x i16> %13787 to <4 x i32>
  %13802 = mul nsw <4 x i32> %13801, %11892
  %.cast2242 = bitcast <1 x i64> %13791 to <4 x i16>
  %13803 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2242, <4 x i16> %11877) #11
  %13804 = sext <4 x i16> %13790 to <4 x i32>
  %13805 = mul nsw <4 x i32> %13804, %11896
  %13806 = sext <4 x i16> %13793 to <4 x i32>
  %13807 = mul nsw <4 x i32> %13806, %11898
  %13808 = add <4 x i32> %13796, %11565
  %13809 = add <4 x i32> %13808, %13799
  %13810 = add <4 x i32> %13809, %13802
  %13811 = add <4 x i32> %13810, %13805
  %13812 = add <4 x i32> %13811, %13807
  %13813 = add <4 x i32> %13812, %13794
  %13814 = add <4 x i32> %13813, %13797
  %13815 = add <4 x i32> %13814, %13800
  %13816 = add <4 x i32> %13815, %13803
  %t3455 = add nsw i32 %t3230, %t2419564
  %13817 = sext i32 %t3455 to i64
  %13818 = shl nsw i64 %13817, 4
  %13819 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13818
  %13820 = bitcast i8* %13819 to <8 x i8>*
  %t3456 = load <8 x i8>, <8 x i8>* %13820, align 16, !tbaa !438
  %t3457 = add nsw i32 %t3230, %t2415563
  %13821 = sext i32 %t3457 to i64
  %13822 = shl nsw i64 %13821, 4
  %13823 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13822
  %13824 = bitcast i8* %13823 to <8 x i8>*
  %t3458 = load <8 x i8>, <8 x i8>* %13824, align 16, !tbaa !438
  %t3459 = add nsw i32 %t3230, %t2411542
  %13825 = sext i32 %t3459 to i64
  %13826 = shl nsw i64 %13825, 4
  %13827 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13826
  %13828 = bitcast i8* %13827 to <8 x i8>*
  %t3460 = load <8 x i8>, <8 x i8>* %13828, align 16, !tbaa !438
  %t3461 = add nsw i32 %t3230, %t2406562
  %13829 = sext i32 %t3461 to i64
  %13830 = shl nsw i64 %13829, 4
  %13831 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13830
  %13832 = bitcast i8* %13831 to <8 x i8>*
  %t3462 = load <8 x i8>, <8 x i8>* %13832, align 16, !tbaa !438
  %t3463 = add nsw i32 %t3230, %t2402561
  %13833 = sext i32 %t3463 to i64
  %13834 = shl nsw i64 %13833, 4
  %13835 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13834
  %13836 = bitcast i8* %13835 to <8 x i8>*
  %t3464 = load <8 x i8>, <8 x i8>* %13836, align 16, !tbaa !438
  %t3465 = add nsw i32 %t3230, %t2398541
  %13837 = sext i32 %t3465 to i64
  %13838 = shl nsw i64 %13837, 4
  %13839 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13838
  %13840 = bitcast i8* %13839 to <8 x i8>*
  %t3466 = load <8 x i8>, <8 x i8>* %13840, align 16, !tbaa !438
  %t3467 = add nsw i32 %t3230, %t2393550
  %13841 = sext i32 %t3467 to i64
  %13842 = shl nsw i64 %13841, 4
  %13843 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13842
  %13844 = bitcast i8* %13843 to <8 x i8>*
  %t3468 = load <8 x i8>, <8 x i8>* %13844, align 16, !tbaa !438
  %t3469 = add nsw i32 %t3230, %t2389549
  %13845 = sext i32 %t3469 to i64
  %13846 = shl nsw i64 %13845, 4
  %13847 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13846
  %13848 = bitcast i8* %13847 to <8 x i8>*
  %t3470 = load <8 x i8>, <8 x i8>* %13848, align 16, !tbaa !438
  %t3471 = add nsw i32 %t3230, %t2385535
  %13849 = sext i32 %t3471 to i64
  %13850 = shl nsw i64 %13849, 4
  %13851 = getelementptr inbounds i8, i8* %resampled_input473, i64 %13850
  %13852 = bitcast i8* %13851 to <8 x i8>*
  %t3472 = load <8 x i8>, <8 x i8>* %13852, align 16, !tbaa !438
  %13853 = getelementptr inbounds i8, i8* %13819, i64 8
  %13854 = bitcast i8* %13853 to <8 x i8>*
  %t3473 = load <8 x i8>, <8 x i8>* %13854, align 8, !tbaa !438
  %13855 = getelementptr inbounds i8, i8* %13823, i64 8
  %13856 = bitcast i8* %13855 to <8 x i8>*
  %t3474 = load <8 x i8>, <8 x i8>* %13856, align 8, !tbaa !438
  %13857 = getelementptr inbounds i8, i8* %13827, i64 8
  %13858 = bitcast i8* %13857 to <8 x i8>*
  %t3475 = load <8 x i8>, <8 x i8>* %13858, align 8, !tbaa !438
  %13859 = getelementptr inbounds i8, i8* %13831, i64 8
  %13860 = bitcast i8* %13859 to <8 x i8>*
  %t3476 = load <8 x i8>, <8 x i8>* %13860, align 8, !tbaa !438
  %13861 = getelementptr inbounds i8, i8* %13835, i64 8
  %13862 = bitcast i8* %13861 to <8 x i8>*
  %t3477 = load <8 x i8>, <8 x i8>* %13862, align 8, !tbaa !438
  %13863 = getelementptr inbounds i8, i8* %13839, i64 8
  %13864 = bitcast i8* %13863 to <8 x i8>*
  %t3478 = load <8 x i8>, <8 x i8>* %13864, align 8, !tbaa !438
  %13865 = getelementptr inbounds i8, i8* %13843, i64 8
  %13866 = bitcast i8* %13865 to <8 x i8>*
  %t3479 = load <8 x i8>, <8 x i8>* %13866, align 8, !tbaa !438
  %13867 = getelementptr inbounds i8, i8* %13847, i64 8
  %13868 = bitcast i8* %13867 to <8 x i8>*
  %t3480 = load <8 x i8>, <8 x i8>* %13868, align 8, !tbaa !438
  %13869 = getelementptr inbounds i8, i8* %13851, i64 8
  %13870 = bitcast i8* %13869 to <8 x i8>*
  %t3481 = load <8 x i8>, <8 x i8>* %13870, align 8, !tbaa !438
  %13871 = zext <8 x i8> %t3456 to <8 x i16>
  %13872 = bitcast <8 x i16> %13871 to <2 x i64>
  %13873 = shufflevector <2 x i64> %13872, <2 x i64> undef, <1 x i32> zeroinitializer
  %13874 = bitcast <1 x i64> %13873 to <4 x i16>
  %13875 = zext <8 x i8> %t3458 to <8 x i16>
  %13876 = bitcast <8 x i16> %13875 to <2 x i64>
  %13877 = shufflevector <2 x i64> %13876, <2 x i64> undef, <1 x i32> zeroinitializer
  %13878 = zext <8 x i8> %t3460 to <8 x i16>
  %13879 = bitcast <8 x i16> %13878 to <2 x i64>
  %13880 = shufflevector <2 x i64> %13879, <2 x i64> undef, <1 x i32> zeroinitializer
  %13881 = bitcast <1 x i64> %13880 to <4 x i16>
  %13882 = zext <8 x i8> %t3462 to <8 x i16>
  %13883 = bitcast <8 x i16> %13882 to <2 x i64>
  %13884 = shufflevector <2 x i64> %13883, <2 x i64> undef, <1 x i32> zeroinitializer
  %13885 = zext <8 x i8> %t3464 to <8 x i16>
  %13886 = bitcast <8 x i16> %13885 to <2 x i64>
  %13887 = shufflevector <2 x i64> %13886, <2 x i64> undef, <1 x i32> zeroinitializer
  %13888 = bitcast <1 x i64> %13887 to <4 x i16>
  %13889 = zext <8 x i8> %t3466 to <8 x i16>
  %13890 = bitcast <8 x i16> %13889 to <2 x i64>
  %13891 = shufflevector <2 x i64> %13890, <2 x i64> undef, <1 x i32> zeroinitializer
  %13892 = zext <8 x i8> %t3468 to <8 x i16>
  %13893 = bitcast <8 x i16> %13892 to <2 x i64>
  %13894 = shufflevector <2 x i64> %13893, <2 x i64> undef, <1 x i32> zeroinitializer
  %13895 = bitcast <1 x i64> %13894 to <4 x i16>
  %13896 = zext <8 x i8> %t3470 to <8 x i16>
  %13897 = bitcast <8 x i16> %13896 to <2 x i64>
  %13898 = shufflevector <2 x i64> %13897, <2 x i64> undef, <1 x i32> zeroinitializer
  %13899 = zext <8 x i8> %t3472 to <8 x i16>
  %13900 = bitcast <8 x i16> %13899 to <2 x i64>
  %13901 = shufflevector <2 x i64> %13900, <2 x i64> undef, <1 x i32> zeroinitializer
  %13902 = bitcast <1 x i64> %13901 to <4 x i16>
  %.cast2245 = bitcast <1 x i64> %13877 to <4 x i16>
  %13903 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2245, <4 x i16> %11675) #11
  %13904 = sext <4 x i16> %13874 to <4 x i32>
  %13905 = mul nsw <4 x i32> %13904, %11713
  %.cast2248 = bitcast <1 x i64> %13884 to <4 x i16>
  %13906 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2248, <4 x i16> %11684) #11
  %13907 = sext <4 x i16> %13881 to <4 x i32>
  %13908 = mul nsw <4 x i32> %13907, %11717
  %.cast2251 = bitcast <1 x i64> %13891 to <4 x i16>
  %13909 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2251, <4 x i16> %11693) #11
  %13910 = sext <4 x i16> %13888 to <4 x i32>
  %13911 = mul nsw <4 x i32> %13910, %11721
  %.cast2254 = bitcast <1 x i64> %13898 to <4 x i16>
  %13912 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2254, <4 x i16> %11702) #11
  %13913 = sext <4 x i16> %13895 to <4 x i32>
  %13914 = mul nsw <4 x i32> %13913, %11725
  %13915 = sext <4 x i16> %13902 to <4 x i32>
  %13916 = mul nsw <4 x i32> %13915, %11727
  %13917 = add <4 x i32> %13905, %11562
  %13918 = add <4 x i32> %13917, %13908
  %13919 = add <4 x i32> %13918, %13911
  %13920 = add <4 x i32> %13919, %13914
  %13921 = add <4 x i32> %13920, %13916
  %13922 = add <4 x i32> %13921, %13903
  %13923 = add <4 x i32> %13922, %13906
  %13924 = add <4 x i32> %13923, %13909
  %13925 = add <4 x i32> %13924, %13912
  %13926 = shufflevector <2 x i64> %13872, <2 x i64> undef, <1 x i32> <i32 1>
  %13927 = bitcast <1 x i64> %13926 to <4 x i16>
  %13928 = shufflevector <2 x i64> %13876, <2 x i64> undef, <1 x i32> <i32 1>
  %13929 = shufflevector <2 x i64> %13879, <2 x i64> undef, <1 x i32> <i32 1>
  %13930 = bitcast <1 x i64> %13929 to <4 x i16>
  %13931 = shufflevector <2 x i64> %13883, <2 x i64> undef, <1 x i32> <i32 1>
  %13932 = shufflevector <2 x i64> %13886, <2 x i64> undef, <1 x i32> <i32 1>
  %13933 = bitcast <1 x i64> %13932 to <4 x i16>
  %13934 = shufflevector <2 x i64> %13890, <2 x i64> undef, <1 x i32> <i32 1>
  %13935 = shufflevector <2 x i64> %13893, <2 x i64> undef, <1 x i32> <i32 1>
  %13936 = bitcast <1 x i64> %13935 to <4 x i16>
  %13937 = shufflevector <2 x i64> %13897, <2 x i64> undef, <1 x i32> <i32 1>
  %13938 = shufflevector <2 x i64> %13900, <2 x i64> undef, <1 x i32> <i32 1>
  %13939 = bitcast <1 x i64> %13938 to <4 x i16>
  %.cast2257 = bitcast <1 x i64> %13928 to <4 x i16>
  %13940 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2257, <4 x i16> %11742) #11
  %13941 = sext <4 x i16> %13927 to <4 x i32>
  %13942 = mul nsw <4 x i32> %13941, %11764
  %.cast2260 = bitcast <1 x i64> %13931 to <4 x i16>
  %13943 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2260, <4 x i16> %11747) #11
  %13944 = sext <4 x i16> %13930 to <4 x i32>
  %13945 = mul nsw <4 x i32> %13944, %11768
  %.cast2263 = bitcast <1 x i64> %13934 to <4 x i16>
  %13946 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2263, <4 x i16> %11752) #11
  %13947 = sext <4 x i16> %13933 to <4 x i32>
  %13948 = mul nsw <4 x i32> %13947, %11772
  %.cast2266 = bitcast <1 x i64> %13937 to <4 x i16>
  %13949 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2266, <4 x i16> %11757) #11
  %13950 = sext <4 x i16> %13936 to <4 x i32>
  %13951 = mul nsw <4 x i32> %13950, %11776
  %13952 = sext <4 x i16> %13939 to <4 x i32>
  %13953 = mul nsw <4 x i32> %13952, %11778
  %13954 = add <4 x i32> %13942, %11563
  %13955 = add <4 x i32> %13954, %13945
  %13956 = add <4 x i32> %13955, %13948
  %13957 = add <4 x i32> %13956, %13951
  %13958 = add <4 x i32> %13957, %13953
  %13959 = add <4 x i32> %13958, %13940
  %13960 = add <4 x i32> %13959, %13943
  %13961 = add <4 x i32> %13960, %13946
  %13962 = add <4 x i32> %13961, %13949
  %13963 = zext <8 x i8> %t3473 to <8 x i16>
  %13964 = bitcast <8 x i16> %13963 to <2 x i64>
  %13965 = shufflevector <2 x i64> %13964, <2 x i64> undef, <1 x i32> zeroinitializer
  %13966 = bitcast <1 x i64> %13965 to <4 x i16>
  %13967 = zext <8 x i8> %t3474 to <8 x i16>
  %13968 = bitcast <8 x i16> %13967 to <2 x i64>
  %13969 = shufflevector <2 x i64> %13968, <2 x i64> undef, <1 x i32> zeroinitializer
  %13970 = zext <8 x i8> %t3475 to <8 x i16>
  %13971 = bitcast <8 x i16> %13970 to <2 x i64>
  %13972 = shufflevector <2 x i64> %13971, <2 x i64> undef, <1 x i32> zeroinitializer
  %13973 = bitcast <1 x i64> %13972 to <4 x i16>
  %13974 = zext <8 x i8> %t3476 to <8 x i16>
  %13975 = bitcast <8 x i16> %13974 to <2 x i64>
  %13976 = shufflevector <2 x i64> %13975, <2 x i64> undef, <1 x i32> zeroinitializer
  %13977 = zext <8 x i8> %t3477 to <8 x i16>
  %13978 = bitcast <8 x i16> %13977 to <2 x i64>
  %13979 = shufflevector <2 x i64> %13978, <2 x i64> undef, <1 x i32> zeroinitializer
  %13980 = bitcast <1 x i64> %13979 to <4 x i16>
  %13981 = zext <8 x i8> %t3478 to <8 x i16>
  %13982 = bitcast <8 x i16> %13981 to <2 x i64>
  %13983 = shufflevector <2 x i64> %13982, <2 x i64> undef, <1 x i32> zeroinitializer
  %13984 = zext <8 x i8> %t3479 to <8 x i16>
  %13985 = bitcast <8 x i16> %13984 to <2 x i64>
  %13986 = shufflevector <2 x i64> %13985, <2 x i64> undef, <1 x i32> zeroinitializer
  %13987 = bitcast <1 x i64> %13986 to <4 x i16>
  %13988 = zext <8 x i8> %t3480 to <8 x i16>
  %13989 = bitcast <8 x i16> %13988 to <2 x i64>
  %13990 = shufflevector <2 x i64> %13989, <2 x i64> undef, <1 x i32> zeroinitializer
  %13991 = zext <8 x i8> %t3481 to <8 x i16>
  %13992 = bitcast <8 x i16> %13991 to <2 x i64>
  %13993 = shufflevector <2 x i64> %13992, <2 x i64> undef, <1 x i32> zeroinitializer
  %13994 = bitcast <1 x i64> %13993 to <4 x i16>
  %.cast2269 = bitcast <1 x i64> %13969 to <4 x i16>
  %13995 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2269, <4 x i16> %11795) #11
  %13996 = sext <4 x i16> %13966 to <4 x i32>
  %13997 = mul nsw <4 x i32> %13996, %11833
  %.cast2272 = bitcast <1 x i64> %13976 to <4 x i16>
  %13998 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2272, <4 x i16> %11804) #11
  %13999 = sext <4 x i16> %13973 to <4 x i32>
  %14000 = mul nsw <4 x i32> %13999, %11837
  %.cast2275 = bitcast <1 x i64> %13983 to <4 x i16>
  %14001 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2275, <4 x i16> %11813) #11
  %14002 = sext <4 x i16> %13980 to <4 x i32>
  %14003 = mul nsw <4 x i32> %14002, %11841
  %.cast2278 = bitcast <1 x i64> %13990 to <4 x i16>
  %14004 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2278, <4 x i16> %11822) #11
  %14005 = sext <4 x i16> %13987 to <4 x i32>
  %14006 = mul nsw <4 x i32> %14005, %11845
  %14007 = sext <4 x i16> %13994 to <4 x i32>
  %14008 = mul nsw <4 x i32> %14007, %11847
  %14009 = add <4 x i32> %13997, %11564
  %14010 = add <4 x i32> %14009, %14000
  %14011 = add <4 x i32> %14010, %14003
  %14012 = add <4 x i32> %14011, %14006
  %14013 = add <4 x i32> %14012, %14008
  %14014 = add <4 x i32> %14013, %13995
  %14015 = add <4 x i32> %14014, %13998
  %14016 = add <4 x i32> %14015, %14001
  %14017 = add <4 x i32> %14016, %14004
  %14018 = shufflevector <2 x i64> %13964, <2 x i64> undef, <1 x i32> <i32 1>
  %14019 = bitcast <1 x i64> %14018 to <4 x i16>
  %14020 = shufflevector <2 x i64> %13968, <2 x i64> undef, <1 x i32> <i32 1>
  %14021 = shufflevector <2 x i64> %13971, <2 x i64> undef, <1 x i32> <i32 1>
  %14022 = bitcast <1 x i64> %14021 to <4 x i16>
  %14023 = shufflevector <2 x i64> %13975, <2 x i64> undef, <1 x i32> <i32 1>
  %14024 = shufflevector <2 x i64> %13978, <2 x i64> undef, <1 x i32> <i32 1>
  %14025 = bitcast <1 x i64> %14024 to <4 x i16>
  %14026 = shufflevector <2 x i64> %13982, <2 x i64> undef, <1 x i32> <i32 1>
  %14027 = shufflevector <2 x i64> %13985, <2 x i64> undef, <1 x i32> <i32 1>
  %14028 = bitcast <1 x i64> %14027 to <4 x i16>
  %14029 = shufflevector <2 x i64> %13989, <2 x i64> undef, <1 x i32> <i32 1>
  %14030 = shufflevector <2 x i64> %13992, <2 x i64> undef, <1 x i32> <i32 1>
  %14031 = bitcast <1 x i64> %14030 to <4 x i16>
  %.cast2281 = bitcast <1 x i64> %14020 to <4 x i16>
  %14032 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2281, <4 x i16> %11862) #11
  %14033 = sext <4 x i16> %14019 to <4 x i32>
  %14034 = mul nsw <4 x i32> %14033, %11884
  %.cast2284 = bitcast <1 x i64> %14023 to <4 x i16>
  %14035 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2284, <4 x i16> %11867) #11
  %14036 = sext <4 x i16> %14022 to <4 x i32>
  %14037 = mul nsw <4 x i32> %14036, %11888
  %.cast2287 = bitcast <1 x i64> %14026 to <4 x i16>
  %14038 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2287, <4 x i16> %11872) #11
  %14039 = sext <4 x i16> %14025 to <4 x i32>
  %14040 = mul nsw <4 x i32> %14039, %11892
  %.cast2290 = bitcast <1 x i64> %14029 to <4 x i16>
  %14041 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2290, <4 x i16> %11877) #11
  %14042 = sext <4 x i16> %14028 to <4 x i32>
  %14043 = mul nsw <4 x i32> %14042, %11896
  %14044 = sext <4 x i16> %14031 to <4 x i32>
  %14045 = mul nsw <4 x i32> %14044, %11898
  %14046 = add <4 x i32> %14034, %11565
  %14047 = add <4 x i32> %14046, %14037
  %14048 = add <4 x i32> %14047, %14040
  %14049 = add <4 x i32> %14048, %14043
  %14050 = add <4 x i32> %14049, %14045
  %14051 = add <4 x i32> %14050, %14032
  %14052 = add <4 x i32> %14051, %14035
  %14053 = add <4 x i32> %14052, %14038
  %14054 = add <4 x i32> %14053, %14041
  %t3483 = add nsw i32 %t3258, %t2419564
  %14055 = sext i32 %t3483 to i64
  %14056 = shl nsw i64 %14055, 4
  %14057 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14056
  %14058 = bitcast i8* %14057 to <8 x i8>*
  %t3484 = load <8 x i8>, <8 x i8>* %14058, align 16, !tbaa !438
  %t3485 = add nsw i32 %t3258, %t2415563
  %14059 = sext i32 %t3485 to i64
  %14060 = shl nsw i64 %14059, 4
  %14061 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14060
  %14062 = bitcast i8* %14061 to <8 x i8>*
  %t3486 = load <8 x i8>, <8 x i8>* %14062, align 16, !tbaa !438
  %t3487 = add nsw i32 %t3258, %t2411542
  %14063 = sext i32 %t3487 to i64
  %14064 = shl nsw i64 %14063, 4
  %14065 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14064
  %14066 = bitcast i8* %14065 to <8 x i8>*
  %t3488 = load <8 x i8>, <8 x i8>* %14066, align 16, !tbaa !438
  %t3489 = add nsw i32 %t3258, %t2406562
  %14067 = sext i32 %t3489 to i64
  %14068 = shl nsw i64 %14067, 4
  %14069 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14068
  %14070 = bitcast i8* %14069 to <8 x i8>*
  %t3490 = load <8 x i8>, <8 x i8>* %14070, align 16, !tbaa !438
  %t3491 = add nsw i32 %t3258, %t2402561
  %14071 = sext i32 %t3491 to i64
  %14072 = shl nsw i64 %14071, 4
  %14073 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14072
  %14074 = bitcast i8* %14073 to <8 x i8>*
  %t3492 = load <8 x i8>, <8 x i8>* %14074, align 16, !tbaa !438
  %t3493 = add nsw i32 %t3258, %t2398541
  %14075 = sext i32 %t3493 to i64
  %14076 = shl nsw i64 %14075, 4
  %14077 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14076
  %14078 = bitcast i8* %14077 to <8 x i8>*
  %t3494 = load <8 x i8>, <8 x i8>* %14078, align 16, !tbaa !438
  %t3495 = add nsw i32 %t3258, %t2393550
  %14079 = sext i32 %t3495 to i64
  %14080 = shl nsw i64 %14079, 4
  %14081 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14080
  %14082 = bitcast i8* %14081 to <8 x i8>*
  %t3496 = load <8 x i8>, <8 x i8>* %14082, align 16, !tbaa !438
  %t3497 = add nsw i32 %t3258, %t2389549
  %14083 = sext i32 %t3497 to i64
  %14084 = shl nsw i64 %14083, 4
  %14085 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14084
  %14086 = bitcast i8* %14085 to <8 x i8>*
  %t3498 = load <8 x i8>, <8 x i8>* %14086, align 16, !tbaa !438
  %t3499 = add nsw i32 %t3258, %t2385535
  %14087 = sext i32 %t3499 to i64
  %14088 = shl nsw i64 %14087, 4
  %14089 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14088
  %14090 = bitcast i8* %14089 to <8 x i8>*
  %t3500 = load <8 x i8>, <8 x i8>* %14090, align 16, !tbaa !438
  %14091 = getelementptr inbounds i8, i8* %14057, i64 8
  %14092 = bitcast i8* %14091 to <8 x i8>*
  %t3501 = load <8 x i8>, <8 x i8>* %14092, align 8, !tbaa !438
  %14093 = getelementptr inbounds i8, i8* %14061, i64 8
  %14094 = bitcast i8* %14093 to <8 x i8>*
  %t3502 = load <8 x i8>, <8 x i8>* %14094, align 8, !tbaa !438
  %14095 = getelementptr inbounds i8, i8* %14065, i64 8
  %14096 = bitcast i8* %14095 to <8 x i8>*
  %t3503 = load <8 x i8>, <8 x i8>* %14096, align 8, !tbaa !438
  %14097 = getelementptr inbounds i8, i8* %14069, i64 8
  %14098 = bitcast i8* %14097 to <8 x i8>*
  %t3504 = load <8 x i8>, <8 x i8>* %14098, align 8, !tbaa !438
  %14099 = getelementptr inbounds i8, i8* %14073, i64 8
  %14100 = bitcast i8* %14099 to <8 x i8>*
  %t3505 = load <8 x i8>, <8 x i8>* %14100, align 8, !tbaa !438
  %14101 = getelementptr inbounds i8, i8* %14077, i64 8
  %14102 = bitcast i8* %14101 to <8 x i8>*
  %t3506 = load <8 x i8>, <8 x i8>* %14102, align 8, !tbaa !438
  %14103 = getelementptr inbounds i8, i8* %14081, i64 8
  %14104 = bitcast i8* %14103 to <8 x i8>*
  %t3507 = load <8 x i8>, <8 x i8>* %14104, align 8, !tbaa !438
  %14105 = getelementptr inbounds i8, i8* %14085, i64 8
  %14106 = bitcast i8* %14105 to <8 x i8>*
  %t3508 = load <8 x i8>, <8 x i8>* %14106, align 8, !tbaa !438
  %14107 = getelementptr inbounds i8, i8* %14089, i64 8
  %14108 = bitcast i8* %14107 to <8 x i8>*
  %t3509 = load <8 x i8>, <8 x i8>* %14108, align 8, !tbaa !438
  %14109 = zext <8 x i8> %t3484 to <8 x i16>
  %14110 = bitcast <8 x i16> %14109 to <2 x i64>
  %14111 = shufflevector <2 x i64> %14110, <2 x i64> undef, <1 x i32> zeroinitializer
  %14112 = bitcast <1 x i64> %14111 to <4 x i16>
  %14113 = zext <8 x i8> %t3486 to <8 x i16>
  %14114 = bitcast <8 x i16> %14113 to <2 x i64>
  %14115 = shufflevector <2 x i64> %14114, <2 x i64> undef, <1 x i32> zeroinitializer
  %14116 = zext <8 x i8> %t3488 to <8 x i16>
  %14117 = bitcast <8 x i16> %14116 to <2 x i64>
  %14118 = shufflevector <2 x i64> %14117, <2 x i64> undef, <1 x i32> zeroinitializer
  %14119 = bitcast <1 x i64> %14118 to <4 x i16>
  %14120 = zext <8 x i8> %t3490 to <8 x i16>
  %14121 = bitcast <8 x i16> %14120 to <2 x i64>
  %14122 = shufflevector <2 x i64> %14121, <2 x i64> undef, <1 x i32> zeroinitializer
  %14123 = zext <8 x i8> %t3492 to <8 x i16>
  %14124 = bitcast <8 x i16> %14123 to <2 x i64>
  %14125 = shufflevector <2 x i64> %14124, <2 x i64> undef, <1 x i32> zeroinitializer
  %14126 = bitcast <1 x i64> %14125 to <4 x i16>
  %14127 = zext <8 x i8> %t3494 to <8 x i16>
  %14128 = bitcast <8 x i16> %14127 to <2 x i64>
  %14129 = shufflevector <2 x i64> %14128, <2 x i64> undef, <1 x i32> zeroinitializer
  %14130 = zext <8 x i8> %t3496 to <8 x i16>
  %14131 = bitcast <8 x i16> %14130 to <2 x i64>
  %14132 = shufflevector <2 x i64> %14131, <2 x i64> undef, <1 x i32> zeroinitializer
  %14133 = bitcast <1 x i64> %14132 to <4 x i16>
  %14134 = zext <8 x i8> %t3498 to <8 x i16>
  %14135 = bitcast <8 x i16> %14134 to <2 x i64>
  %14136 = shufflevector <2 x i64> %14135, <2 x i64> undef, <1 x i32> zeroinitializer
  %14137 = zext <8 x i8> %t3500 to <8 x i16>
  %14138 = bitcast <8 x i16> %14137 to <2 x i64>
  %14139 = shufflevector <2 x i64> %14138, <2 x i64> undef, <1 x i32> zeroinitializer
  %14140 = bitcast <1 x i64> %14139 to <4 x i16>
  %.cast2293 = bitcast <1 x i64> %14115 to <4 x i16>
  %14141 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2293, <4 x i16> %11675) #11
  %14142 = sext <4 x i16> %14112 to <4 x i32>
  %14143 = mul nsw <4 x i32> %14142, %11713
  %.cast2296 = bitcast <1 x i64> %14122 to <4 x i16>
  %14144 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2296, <4 x i16> %11684) #11
  %14145 = sext <4 x i16> %14119 to <4 x i32>
  %14146 = mul nsw <4 x i32> %14145, %11717
  %.cast2299 = bitcast <1 x i64> %14129 to <4 x i16>
  %14147 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2299, <4 x i16> %11693) #11
  %14148 = sext <4 x i16> %14126 to <4 x i32>
  %14149 = mul nsw <4 x i32> %14148, %11721
  %.cast2302 = bitcast <1 x i64> %14136 to <4 x i16>
  %14150 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2302, <4 x i16> %11702) #11
  %14151 = sext <4 x i16> %14133 to <4 x i32>
  %14152 = mul nsw <4 x i32> %14151, %11725
  %14153 = sext <4 x i16> %14140 to <4 x i32>
  %14154 = mul nsw <4 x i32> %14153, %11727
  %14155 = add <4 x i32> %14143, %11562
  %14156 = add <4 x i32> %14155, %14146
  %14157 = add <4 x i32> %14156, %14149
  %14158 = add <4 x i32> %14157, %14152
  %14159 = add <4 x i32> %14158, %14154
  %14160 = add <4 x i32> %14159, %14141
  %14161 = add <4 x i32> %14160, %14144
  %14162 = add <4 x i32> %14161, %14147
  %14163 = add <4 x i32> %14162, %14150
  %14164 = shufflevector <2 x i64> %14110, <2 x i64> undef, <1 x i32> <i32 1>
  %14165 = bitcast <1 x i64> %14164 to <4 x i16>
  %14166 = shufflevector <2 x i64> %14114, <2 x i64> undef, <1 x i32> <i32 1>
  %14167 = shufflevector <2 x i64> %14117, <2 x i64> undef, <1 x i32> <i32 1>
  %14168 = bitcast <1 x i64> %14167 to <4 x i16>
  %14169 = shufflevector <2 x i64> %14121, <2 x i64> undef, <1 x i32> <i32 1>
  %14170 = shufflevector <2 x i64> %14124, <2 x i64> undef, <1 x i32> <i32 1>
  %14171 = bitcast <1 x i64> %14170 to <4 x i16>
  %14172 = shufflevector <2 x i64> %14128, <2 x i64> undef, <1 x i32> <i32 1>
  %14173 = shufflevector <2 x i64> %14131, <2 x i64> undef, <1 x i32> <i32 1>
  %14174 = bitcast <1 x i64> %14173 to <4 x i16>
  %14175 = shufflevector <2 x i64> %14135, <2 x i64> undef, <1 x i32> <i32 1>
  %14176 = shufflevector <2 x i64> %14138, <2 x i64> undef, <1 x i32> <i32 1>
  %14177 = bitcast <1 x i64> %14176 to <4 x i16>
  %.cast2305 = bitcast <1 x i64> %14166 to <4 x i16>
  %14178 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2305, <4 x i16> %11742) #11
  %14179 = sext <4 x i16> %14165 to <4 x i32>
  %14180 = mul nsw <4 x i32> %14179, %11764
  %.cast2308 = bitcast <1 x i64> %14169 to <4 x i16>
  %14181 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2308, <4 x i16> %11747) #11
  %14182 = sext <4 x i16> %14168 to <4 x i32>
  %14183 = mul nsw <4 x i32> %14182, %11768
  %.cast2311 = bitcast <1 x i64> %14172 to <4 x i16>
  %14184 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2311, <4 x i16> %11752) #11
  %14185 = sext <4 x i16> %14171 to <4 x i32>
  %14186 = mul nsw <4 x i32> %14185, %11772
  %.cast2314 = bitcast <1 x i64> %14175 to <4 x i16>
  %14187 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2314, <4 x i16> %11757) #11
  %14188 = sext <4 x i16> %14174 to <4 x i32>
  %14189 = mul nsw <4 x i32> %14188, %11776
  %14190 = sext <4 x i16> %14177 to <4 x i32>
  %14191 = mul nsw <4 x i32> %14190, %11778
  %14192 = add <4 x i32> %14180, %11563
  %14193 = add <4 x i32> %14192, %14183
  %14194 = add <4 x i32> %14193, %14186
  %14195 = add <4 x i32> %14194, %14189
  %14196 = add <4 x i32> %14195, %14191
  %14197 = add <4 x i32> %14196, %14178
  %14198 = add <4 x i32> %14197, %14181
  %14199 = add <4 x i32> %14198, %14184
  %14200 = add <4 x i32> %14199, %14187
  %14201 = zext <8 x i8> %t3501 to <8 x i16>
  %14202 = bitcast <8 x i16> %14201 to <2 x i64>
  %14203 = shufflevector <2 x i64> %14202, <2 x i64> undef, <1 x i32> zeroinitializer
  %14204 = bitcast <1 x i64> %14203 to <4 x i16>
  %14205 = zext <8 x i8> %t3502 to <8 x i16>
  %14206 = bitcast <8 x i16> %14205 to <2 x i64>
  %14207 = shufflevector <2 x i64> %14206, <2 x i64> undef, <1 x i32> zeroinitializer
  %14208 = zext <8 x i8> %t3503 to <8 x i16>
  %14209 = bitcast <8 x i16> %14208 to <2 x i64>
  %14210 = shufflevector <2 x i64> %14209, <2 x i64> undef, <1 x i32> zeroinitializer
  %14211 = bitcast <1 x i64> %14210 to <4 x i16>
  %14212 = zext <8 x i8> %t3504 to <8 x i16>
  %14213 = bitcast <8 x i16> %14212 to <2 x i64>
  %14214 = shufflevector <2 x i64> %14213, <2 x i64> undef, <1 x i32> zeroinitializer
  %14215 = zext <8 x i8> %t3505 to <8 x i16>
  %14216 = bitcast <8 x i16> %14215 to <2 x i64>
  %14217 = shufflevector <2 x i64> %14216, <2 x i64> undef, <1 x i32> zeroinitializer
  %14218 = bitcast <1 x i64> %14217 to <4 x i16>
  %14219 = zext <8 x i8> %t3506 to <8 x i16>
  %14220 = bitcast <8 x i16> %14219 to <2 x i64>
  %14221 = shufflevector <2 x i64> %14220, <2 x i64> undef, <1 x i32> zeroinitializer
  %14222 = zext <8 x i8> %t3507 to <8 x i16>
  %14223 = bitcast <8 x i16> %14222 to <2 x i64>
  %14224 = shufflevector <2 x i64> %14223, <2 x i64> undef, <1 x i32> zeroinitializer
  %14225 = bitcast <1 x i64> %14224 to <4 x i16>
  %14226 = zext <8 x i8> %t3508 to <8 x i16>
  %14227 = bitcast <8 x i16> %14226 to <2 x i64>
  %14228 = shufflevector <2 x i64> %14227, <2 x i64> undef, <1 x i32> zeroinitializer
  %14229 = zext <8 x i8> %t3509 to <8 x i16>
  %14230 = bitcast <8 x i16> %14229 to <2 x i64>
  %14231 = shufflevector <2 x i64> %14230, <2 x i64> undef, <1 x i32> zeroinitializer
  %14232 = bitcast <1 x i64> %14231 to <4 x i16>
  %.cast2317 = bitcast <1 x i64> %14207 to <4 x i16>
  %14233 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2317, <4 x i16> %11795) #11
  %14234 = sext <4 x i16> %14204 to <4 x i32>
  %14235 = mul nsw <4 x i32> %14234, %11833
  %.cast2320 = bitcast <1 x i64> %14214 to <4 x i16>
  %14236 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2320, <4 x i16> %11804) #11
  %14237 = sext <4 x i16> %14211 to <4 x i32>
  %14238 = mul nsw <4 x i32> %14237, %11837
  %.cast2323 = bitcast <1 x i64> %14221 to <4 x i16>
  %14239 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2323, <4 x i16> %11813) #11
  %14240 = sext <4 x i16> %14218 to <4 x i32>
  %14241 = mul nsw <4 x i32> %14240, %11841
  %.cast2326 = bitcast <1 x i64> %14228 to <4 x i16>
  %14242 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2326, <4 x i16> %11822) #11
  %14243 = sext <4 x i16> %14225 to <4 x i32>
  %14244 = mul nsw <4 x i32> %14243, %11845
  %14245 = sext <4 x i16> %14232 to <4 x i32>
  %14246 = mul nsw <4 x i32> %14245, %11847
  %14247 = add <4 x i32> %14235, %11564
  %14248 = add <4 x i32> %14247, %14238
  %14249 = add <4 x i32> %14248, %14241
  %14250 = add <4 x i32> %14249, %14244
  %14251 = add <4 x i32> %14250, %14246
  %14252 = add <4 x i32> %14251, %14233
  %14253 = add <4 x i32> %14252, %14236
  %14254 = add <4 x i32> %14253, %14239
  %14255 = add <4 x i32> %14254, %14242
  %14256 = shufflevector <2 x i64> %14202, <2 x i64> undef, <1 x i32> <i32 1>
  %14257 = bitcast <1 x i64> %14256 to <4 x i16>
  %14258 = shufflevector <2 x i64> %14206, <2 x i64> undef, <1 x i32> <i32 1>
  %14259 = shufflevector <2 x i64> %14209, <2 x i64> undef, <1 x i32> <i32 1>
  %14260 = bitcast <1 x i64> %14259 to <4 x i16>
  %14261 = shufflevector <2 x i64> %14213, <2 x i64> undef, <1 x i32> <i32 1>
  %14262 = shufflevector <2 x i64> %14216, <2 x i64> undef, <1 x i32> <i32 1>
  %14263 = bitcast <1 x i64> %14262 to <4 x i16>
  %14264 = shufflevector <2 x i64> %14220, <2 x i64> undef, <1 x i32> <i32 1>
  %14265 = shufflevector <2 x i64> %14223, <2 x i64> undef, <1 x i32> <i32 1>
  %14266 = bitcast <1 x i64> %14265 to <4 x i16>
  %14267 = shufflevector <2 x i64> %14227, <2 x i64> undef, <1 x i32> <i32 1>
  %14268 = shufflevector <2 x i64> %14230, <2 x i64> undef, <1 x i32> <i32 1>
  %14269 = bitcast <1 x i64> %14268 to <4 x i16>
  %.cast2329 = bitcast <1 x i64> %14258 to <4 x i16>
  %14270 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2329, <4 x i16> %11862) #11
  %14271 = sext <4 x i16> %14257 to <4 x i32>
  %14272 = mul nsw <4 x i32> %14271, %11884
  %.cast2332 = bitcast <1 x i64> %14261 to <4 x i16>
  %14273 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2332, <4 x i16> %11867) #11
  %14274 = sext <4 x i16> %14260 to <4 x i32>
  %14275 = mul nsw <4 x i32> %14274, %11888
  %.cast2335 = bitcast <1 x i64> %14264 to <4 x i16>
  %14276 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2335, <4 x i16> %11872) #11
  %14277 = sext <4 x i16> %14263 to <4 x i32>
  %14278 = mul nsw <4 x i32> %14277, %11892
  %.cast2338 = bitcast <1 x i64> %14267 to <4 x i16>
  %14279 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2338, <4 x i16> %11877) #11
  %14280 = sext <4 x i16> %14266 to <4 x i32>
  %14281 = mul nsw <4 x i32> %14280, %11896
  %14282 = sext <4 x i16> %14269 to <4 x i32>
  %14283 = mul nsw <4 x i32> %14282, %11898
  %14284 = add <4 x i32> %14272, %11565
  %14285 = add <4 x i32> %14284, %14275
  %14286 = add <4 x i32> %14285, %14278
  %14287 = add <4 x i32> %14286, %14281
  %14288 = add <4 x i32> %14287, %14283
  %14289 = add <4 x i32> %14288, %14270
  %14290 = add <4 x i32> %14289, %14273
  %14291 = add <4 x i32> %14290, %14276
  %14292 = add <4 x i32> %14291, %14279
  %t3511 = add nsw i32 %t3286, %t2419564
  %14293 = sext i32 %t3511 to i64
  %14294 = shl nsw i64 %14293, 4
  %14295 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14294
  %14296 = bitcast i8* %14295 to <8 x i8>*
  %t3512 = load <8 x i8>, <8 x i8>* %14296, align 16, !tbaa !438
  %t3513 = add nsw i32 %t3286, %t2415563
  %14297 = sext i32 %t3513 to i64
  %14298 = shl nsw i64 %14297, 4
  %14299 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14298
  %14300 = bitcast i8* %14299 to <8 x i8>*
  %t3514 = load <8 x i8>, <8 x i8>* %14300, align 16, !tbaa !438
  %t3515 = add nsw i32 %t3286, %t2411542
  %14301 = sext i32 %t3515 to i64
  %14302 = shl nsw i64 %14301, 4
  %14303 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14302
  %14304 = bitcast i8* %14303 to <8 x i8>*
  %t3516 = load <8 x i8>, <8 x i8>* %14304, align 16, !tbaa !438
  %t3517 = add nsw i32 %t3286, %t2406562
  %14305 = sext i32 %t3517 to i64
  %14306 = shl nsw i64 %14305, 4
  %14307 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14306
  %14308 = bitcast i8* %14307 to <8 x i8>*
  %t3518 = load <8 x i8>, <8 x i8>* %14308, align 16, !tbaa !438
  %t3519 = add nsw i32 %t3286, %t2402561
  %14309 = sext i32 %t3519 to i64
  %14310 = shl nsw i64 %14309, 4
  %14311 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14310
  %14312 = bitcast i8* %14311 to <8 x i8>*
  %t3520 = load <8 x i8>, <8 x i8>* %14312, align 16, !tbaa !438
  %t3521 = add nsw i32 %t3286, %t2398541
  %14313 = sext i32 %t3521 to i64
  %14314 = shl nsw i64 %14313, 4
  %14315 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14314
  %14316 = bitcast i8* %14315 to <8 x i8>*
  %t3522 = load <8 x i8>, <8 x i8>* %14316, align 16, !tbaa !438
  %t3523 = add nsw i32 %t3286, %t2393550
  %14317 = sext i32 %t3523 to i64
  %14318 = shl nsw i64 %14317, 4
  %14319 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14318
  %14320 = bitcast i8* %14319 to <8 x i8>*
  %t3524 = load <8 x i8>, <8 x i8>* %14320, align 16, !tbaa !438
  %t3525 = add nsw i32 %t3286, %t2389549
  %14321 = sext i32 %t3525 to i64
  %14322 = shl nsw i64 %14321, 4
  %14323 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14322
  %14324 = bitcast i8* %14323 to <8 x i8>*
  %t3526 = load <8 x i8>, <8 x i8>* %14324, align 16, !tbaa !438
  %t3527 = add nsw i32 %t3286, %t2385535
  %14325 = sext i32 %t3527 to i64
  %14326 = shl nsw i64 %14325, 4
  %14327 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14326
  %14328 = bitcast i8* %14327 to <8 x i8>*
  %t3528 = load <8 x i8>, <8 x i8>* %14328, align 16, !tbaa !438
  %14329 = getelementptr inbounds i8, i8* %14295, i64 8
  %14330 = bitcast i8* %14329 to <8 x i8>*
  %t3529 = load <8 x i8>, <8 x i8>* %14330, align 8, !tbaa !438
  %14331 = getelementptr inbounds i8, i8* %14299, i64 8
  %14332 = bitcast i8* %14331 to <8 x i8>*
  %t3530 = load <8 x i8>, <8 x i8>* %14332, align 8, !tbaa !438
  %14333 = getelementptr inbounds i8, i8* %14303, i64 8
  %14334 = bitcast i8* %14333 to <8 x i8>*
  %t3531 = load <8 x i8>, <8 x i8>* %14334, align 8, !tbaa !438
  %14335 = getelementptr inbounds i8, i8* %14307, i64 8
  %14336 = bitcast i8* %14335 to <8 x i8>*
  %t3532 = load <8 x i8>, <8 x i8>* %14336, align 8, !tbaa !438
  %14337 = getelementptr inbounds i8, i8* %14311, i64 8
  %14338 = bitcast i8* %14337 to <8 x i8>*
  %t3533 = load <8 x i8>, <8 x i8>* %14338, align 8, !tbaa !438
  %14339 = getelementptr inbounds i8, i8* %14315, i64 8
  %14340 = bitcast i8* %14339 to <8 x i8>*
  %t3534 = load <8 x i8>, <8 x i8>* %14340, align 8, !tbaa !438
  %14341 = getelementptr inbounds i8, i8* %14319, i64 8
  %14342 = bitcast i8* %14341 to <8 x i8>*
  %t3535 = load <8 x i8>, <8 x i8>* %14342, align 8, !tbaa !438
  %14343 = getelementptr inbounds i8, i8* %14323, i64 8
  %14344 = bitcast i8* %14343 to <8 x i8>*
  %t3536 = load <8 x i8>, <8 x i8>* %14344, align 8, !tbaa !438
  %14345 = getelementptr inbounds i8, i8* %14327, i64 8
  %14346 = bitcast i8* %14345 to <8 x i8>*
  %t3537 = load <8 x i8>, <8 x i8>* %14346, align 8, !tbaa !438
  %14347 = zext <8 x i8> %t3512 to <8 x i16>
  %14348 = bitcast <8 x i16> %14347 to <2 x i64>
  %14349 = shufflevector <2 x i64> %14348, <2 x i64> undef, <1 x i32> zeroinitializer
  %14350 = bitcast <1 x i64> %14349 to <4 x i16>
  %14351 = zext <8 x i8> %t3514 to <8 x i16>
  %14352 = bitcast <8 x i16> %14351 to <2 x i64>
  %14353 = shufflevector <2 x i64> %14352, <2 x i64> undef, <1 x i32> zeroinitializer
  %14354 = zext <8 x i8> %t3516 to <8 x i16>
  %14355 = bitcast <8 x i16> %14354 to <2 x i64>
  %14356 = shufflevector <2 x i64> %14355, <2 x i64> undef, <1 x i32> zeroinitializer
  %14357 = bitcast <1 x i64> %14356 to <4 x i16>
  %14358 = zext <8 x i8> %t3518 to <8 x i16>
  %14359 = bitcast <8 x i16> %14358 to <2 x i64>
  %14360 = shufflevector <2 x i64> %14359, <2 x i64> undef, <1 x i32> zeroinitializer
  %14361 = zext <8 x i8> %t3520 to <8 x i16>
  %14362 = bitcast <8 x i16> %14361 to <2 x i64>
  %14363 = shufflevector <2 x i64> %14362, <2 x i64> undef, <1 x i32> zeroinitializer
  %14364 = bitcast <1 x i64> %14363 to <4 x i16>
  %14365 = zext <8 x i8> %t3522 to <8 x i16>
  %14366 = bitcast <8 x i16> %14365 to <2 x i64>
  %14367 = shufflevector <2 x i64> %14366, <2 x i64> undef, <1 x i32> zeroinitializer
  %14368 = zext <8 x i8> %t3524 to <8 x i16>
  %14369 = bitcast <8 x i16> %14368 to <2 x i64>
  %14370 = shufflevector <2 x i64> %14369, <2 x i64> undef, <1 x i32> zeroinitializer
  %14371 = bitcast <1 x i64> %14370 to <4 x i16>
  %14372 = zext <8 x i8> %t3526 to <8 x i16>
  %14373 = bitcast <8 x i16> %14372 to <2 x i64>
  %14374 = shufflevector <2 x i64> %14373, <2 x i64> undef, <1 x i32> zeroinitializer
  %14375 = zext <8 x i8> %t3528 to <8 x i16>
  %14376 = bitcast <8 x i16> %14375 to <2 x i64>
  %14377 = shufflevector <2 x i64> %14376, <2 x i64> undef, <1 x i32> zeroinitializer
  %14378 = bitcast <1 x i64> %14377 to <4 x i16>
  %.cast2341 = bitcast <1 x i64> %14353 to <4 x i16>
  %14379 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2341, <4 x i16> %11675) #11
  %14380 = sext <4 x i16> %14350 to <4 x i32>
  %14381 = mul nsw <4 x i32> %14380, %11713
  %.cast2344 = bitcast <1 x i64> %14360 to <4 x i16>
  %14382 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2344, <4 x i16> %11684) #11
  %14383 = sext <4 x i16> %14357 to <4 x i32>
  %14384 = mul nsw <4 x i32> %14383, %11717
  %.cast2347 = bitcast <1 x i64> %14367 to <4 x i16>
  %14385 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2347, <4 x i16> %11693) #11
  %14386 = sext <4 x i16> %14364 to <4 x i32>
  %14387 = mul nsw <4 x i32> %14386, %11721
  %.cast2350 = bitcast <1 x i64> %14374 to <4 x i16>
  %14388 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2350, <4 x i16> %11702) #11
  %14389 = sext <4 x i16> %14371 to <4 x i32>
  %14390 = mul nsw <4 x i32> %14389, %11725
  %14391 = sext <4 x i16> %14378 to <4 x i32>
  %14392 = mul nsw <4 x i32> %14391, %11727
  %14393 = add <4 x i32> %14381, %11562
  %14394 = add <4 x i32> %14393, %14384
  %14395 = add <4 x i32> %14394, %14387
  %14396 = add <4 x i32> %14395, %14390
  %14397 = add <4 x i32> %14396, %14392
  %14398 = add <4 x i32> %14397, %14379
  %14399 = add <4 x i32> %14398, %14382
  %14400 = add <4 x i32> %14399, %14385
  %14401 = add <4 x i32> %14400, %14388
  %14402 = shufflevector <2 x i64> %14348, <2 x i64> undef, <1 x i32> <i32 1>
  %14403 = bitcast <1 x i64> %14402 to <4 x i16>
  %14404 = shufflevector <2 x i64> %14352, <2 x i64> undef, <1 x i32> <i32 1>
  %14405 = shufflevector <2 x i64> %14355, <2 x i64> undef, <1 x i32> <i32 1>
  %14406 = bitcast <1 x i64> %14405 to <4 x i16>
  %14407 = shufflevector <2 x i64> %14359, <2 x i64> undef, <1 x i32> <i32 1>
  %14408 = shufflevector <2 x i64> %14362, <2 x i64> undef, <1 x i32> <i32 1>
  %14409 = bitcast <1 x i64> %14408 to <4 x i16>
  %14410 = shufflevector <2 x i64> %14366, <2 x i64> undef, <1 x i32> <i32 1>
  %14411 = shufflevector <2 x i64> %14369, <2 x i64> undef, <1 x i32> <i32 1>
  %14412 = bitcast <1 x i64> %14411 to <4 x i16>
  %14413 = shufflevector <2 x i64> %14373, <2 x i64> undef, <1 x i32> <i32 1>
  %14414 = shufflevector <2 x i64> %14376, <2 x i64> undef, <1 x i32> <i32 1>
  %14415 = bitcast <1 x i64> %14414 to <4 x i16>
  %.cast2353 = bitcast <1 x i64> %14404 to <4 x i16>
  %14416 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2353, <4 x i16> %11742) #11
  %14417 = sext <4 x i16> %14403 to <4 x i32>
  %14418 = mul nsw <4 x i32> %14417, %11764
  %.cast2356 = bitcast <1 x i64> %14407 to <4 x i16>
  %14419 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2356, <4 x i16> %11747) #11
  %14420 = sext <4 x i16> %14406 to <4 x i32>
  %14421 = mul nsw <4 x i32> %14420, %11768
  %.cast2359 = bitcast <1 x i64> %14410 to <4 x i16>
  %14422 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2359, <4 x i16> %11752) #11
  %14423 = sext <4 x i16> %14409 to <4 x i32>
  %14424 = mul nsw <4 x i32> %14423, %11772
  %.cast2362 = bitcast <1 x i64> %14413 to <4 x i16>
  %14425 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2362, <4 x i16> %11757) #11
  %14426 = sext <4 x i16> %14412 to <4 x i32>
  %14427 = mul nsw <4 x i32> %14426, %11776
  %14428 = sext <4 x i16> %14415 to <4 x i32>
  %14429 = mul nsw <4 x i32> %14428, %11778
  %14430 = add <4 x i32> %14418, %11563
  %14431 = add <4 x i32> %14430, %14421
  %14432 = add <4 x i32> %14431, %14424
  %14433 = add <4 x i32> %14432, %14427
  %14434 = add <4 x i32> %14433, %14429
  %14435 = add <4 x i32> %14434, %14416
  %14436 = add <4 x i32> %14435, %14419
  %14437 = add <4 x i32> %14436, %14422
  %14438 = add <4 x i32> %14437, %14425
  %14439 = zext <8 x i8> %t3529 to <8 x i16>
  %14440 = bitcast <8 x i16> %14439 to <2 x i64>
  %14441 = shufflevector <2 x i64> %14440, <2 x i64> undef, <1 x i32> zeroinitializer
  %14442 = bitcast <1 x i64> %14441 to <4 x i16>
  %14443 = zext <8 x i8> %t3530 to <8 x i16>
  %14444 = bitcast <8 x i16> %14443 to <2 x i64>
  %14445 = shufflevector <2 x i64> %14444, <2 x i64> undef, <1 x i32> zeroinitializer
  %14446 = zext <8 x i8> %t3531 to <8 x i16>
  %14447 = bitcast <8 x i16> %14446 to <2 x i64>
  %14448 = shufflevector <2 x i64> %14447, <2 x i64> undef, <1 x i32> zeroinitializer
  %14449 = bitcast <1 x i64> %14448 to <4 x i16>
  %14450 = zext <8 x i8> %t3532 to <8 x i16>
  %14451 = bitcast <8 x i16> %14450 to <2 x i64>
  %14452 = shufflevector <2 x i64> %14451, <2 x i64> undef, <1 x i32> zeroinitializer
  %14453 = zext <8 x i8> %t3533 to <8 x i16>
  %14454 = bitcast <8 x i16> %14453 to <2 x i64>
  %14455 = shufflevector <2 x i64> %14454, <2 x i64> undef, <1 x i32> zeroinitializer
  %14456 = bitcast <1 x i64> %14455 to <4 x i16>
  %14457 = zext <8 x i8> %t3534 to <8 x i16>
  %14458 = bitcast <8 x i16> %14457 to <2 x i64>
  %14459 = shufflevector <2 x i64> %14458, <2 x i64> undef, <1 x i32> zeroinitializer
  %14460 = zext <8 x i8> %t3535 to <8 x i16>
  %14461 = bitcast <8 x i16> %14460 to <2 x i64>
  %14462 = shufflevector <2 x i64> %14461, <2 x i64> undef, <1 x i32> zeroinitializer
  %14463 = bitcast <1 x i64> %14462 to <4 x i16>
  %14464 = zext <8 x i8> %t3536 to <8 x i16>
  %14465 = bitcast <8 x i16> %14464 to <2 x i64>
  %14466 = shufflevector <2 x i64> %14465, <2 x i64> undef, <1 x i32> zeroinitializer
  %14467 = zext <8 x i8> %t3537 to <8 x i16>
  %14468 = bitcast <8 x i16> %14467 to <2 x i64>
  %14469 = shufflevector <2 x i64> %14468, <2 x i64> undef, <1 x i32> zeroinitializer
  %14470 = bitcast <1 x i64> %14469 to <4 x i16>
  %.cast2365 = bitcast <1 x i64> %14445 to <4 x i16>
  %14471 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2365, <4 x i16> %11795) #11
  %14472 = sext <4 x i16> %14442 to <4 x i32>
  %14473 = mul nsw <4 x i32> %14472, %11833
  %.cast2368 = bitcast <1 x i64> %14452 to <4 x i16>
  %14474 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2368, <4 x i16> %11804) #11
  %14475 = sext <4 x i16> %14449 to <4 x i32>
  %14476 = mul nsw <4 x i32> %14475, %11837
  %.cast2371 = bitcast <1 x i64> %14459 to <4 x i16>
  %14477 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2371, <4 x i16> %11813) #11
  %14478 = sext <4 x i16> %14456 to <4 x i32>
  %14479 = mul nsw <4 x i32> %14478, %11841
  %.cast2374 = bitcast <1 x i64> %14466 to <4 x i16>
  %14480 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2374, <4 x i16> %11822) #11
  %14481 = sext <4 x i16> %14463 to <4 x i32>
  %14482 = mul nsw <4 x i32> %14481, %11845
  %14483 = sext <4 x i16> %14470 to <4 x i32>
  %14484 = mul nsw <4 x i32> %14483, %11847
  %14485 = add <4 x i32> %14473, %11564
  %14486 = add <4 x i32> %14485, %14476
  %14487 = add <4 x i32> %14486, %14479
  %14488 = add <4 x i32> %14487, %14482
  %14489 = add <4 x i32> %14488, %14484
  %14490 = add <4 x i32> %14489, %14471
  %14491 = add <4 x i32> %14490, %14474
  %14492 = add <4 x i32> %14491, %14477
  %14493 = add <4 x i32> %14492, %14480
  %14494 = shufflevector <2 x i64> %14440, <2 x i64> undef, <1 x i32> <i32 1>
  %14495 = bitcast <1 x i64> %14494 to <4 x i16>
  %14496 = shufflevector <2 x i64> %14444, <2 x i64> undef, <1 x i32> <i32 1>
  %14497 = shufflevector <2 x i64> %14447, <2 x i64> undef, <1 x i32> <i32 1>
  %14498 = bitcast <1 x i64> %14497 to <4 x i16>
  %14499 = shufflevector <2 x i64> %14451, <2 x i64> undef, <1 x i32> <i32 1>
  %14500 = shufflevector <2 x i64> %14454, <2 x i64> undef, <1 x i32> <i32 1>
  %14501 = bitcast <1 x i64> %14500 to <4 x i16>
  %14502 = shufflevector <2 x i64> %14458, <2 x i64> undef, <1 x i32> <i32 1>
  %14503 = shufflevector <2 x i64> %14461, <2 x i64> undef, <1 x i32> <i32 1>
  %14504 = bitcast <1 x i64> %14503 to <4 x i16>
  %14505 = shufflevector <2 x i64> %14465, <2 x i64> undef, <1 x i32> <i32 1>
  %14506 = shufflevector <2 x i64> %14468, <2 x i64> undef, <1 x i32> <i32 1>
  %14507 = bitcast <1 x i64> %14506 to <4 x i16>
  %.cast2377 = bitcast <1 x i64> %14496 to <4 x i16>
  %14508 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2377, <4 x i16> %11862) #11
  %14509 = sext <4 x i16> %14495 to <4 x i32>
  %14510 = mul nsw <4 x i32> %14509, %11884
  %.cast2380 = bitcast <1 x i64> %14499 to <4 x i16>
  %14511 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2380, <4 x i16> %11867) #11
  %14512 = sext <4 x i16> %14498 to <4 x i32>
  %14513 = mul nsw <4 x i32> %14512, %11888
  %.cast2383 = bitcast <1 x i64> %14502 to <4 x i16>
  %14514 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2383, <4 x i16> %11872) #11
  %14515 = sext <4 x i16> %14501 to <4 x i32>
  %14516 = mul nsw <4 x i32> %14515, %11892
  %.cast2386 = bitcast <1 x i64> %14505 to <4 x i16>
  %14517 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2386, <4 x i16> %11877) #11
  %14518 = sext <4 x i16> %14504 to <4 x i32>
  %14519 = mul nsw <4 x i32> %14518, %11896
  %14520 = sext <4 x i16> %14507 to <4 x i32>
  %14521 = mul nsw <4 x i32> %14520, %11898
  %14522 = add <4 x i32> %14510, %11565
  %14523 = add <4 x i32> %14522, %14513
  %14524 = add <4 x i32> %14523, %14516
  %14525 = add <4 x i32> %14524, %14519
  %14526 = add <4 x i32> %14525, %14521
  %14527 = add <4 x i32> %14526, %14508
  %14528 = add <4 x i32> %14527, %14511
  %14529 = add <4 x i32> %14528, %14514
  %14530 = add <4 x i32> %14529, %14517
  %t3539 = add nsw i32 %t3202, %t2420560
  %14531 = sext i32 %t3539 to i64
  %14532 = shl nsw i64 %14531, 4
  %14533 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14532
  %14534 = bitcast i8* %14533 to <8 x i8>*
  %t3540 = load <8 x i8>, <8 x i8>* %14534, align 16, !tbaa !438
  %t3541 = add nsw i32 %t3202, %t2416559
  %14535 = sext i32 %t3541 to i64
  %14536 = shl nsw i64 %14535, 4
  %14537 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14536
  %14538 = bitcast i8* %14537 to <8 x i8>*
  %t3542 = load <8 x i8>, <8 x i8>* %14538, align 16, !tbaa !438
  %t3543 = add nsw i32 %t3202, %t2412540
  %14539 = sext i32 %t3543 to i64
  %14540 = shl nsw i64 %14539, 4
  %14541 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14540
  %14542 = bitcast i8* %14541 to <8 x i8>*
  %t3544 = load <8 x i8>, <8 x i8>* %14542, align 16, !tbaa !438
  %t3545 = add nsw i32 %t3202, %t2407558
  %14543 = sext i32 %t3545 to i64
  %14544 = shl nsw i64 %14543, 4
  %14545 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14544
  %14546 = bitcast i8* %14545 to <8 x i8>*
  %t3546 = load <8 x i8>, <8 x i8>* %14546, align 16, !tbaa !438
  %t3547 = add nsw i32 %t3202, %t2403557
  %14547 = sext i32 %t3547 to i64
  %14548 = shl nsw i64 %14547, 4
  %14549 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14548
  %14550 = bitcast i8* %14549 to <8 x i8>*
  %t3548 = load <8 x i8>, <8 x i8>* %14550, align 16, !tbaa !438
  %t3549 = add nsw i32 %t3202, %t2399539
  %14551 = sext i32 %t3549 to i64
  %14552 = shl nsw i64 %14551, 4
  %14553 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14552
  %14554 = bitcast i8* %14553 to <8 x i8>*
  %t3550 = load <8 x i8>, <8 x i8>* %14554, align 16, !tbaa !438
  %t3551 = add nsw i32 %t3202, %t2394548
  %14555 = sext i32 %t3551 to i64
  %14556 = shl nsw i64 %14555, 4
  %14557 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14556
  %14558 = bitcast i8* %14557 to <8 x i8>*
  %t3552 = load <8 x i8>, <8 x i8>* %14558, align 16, !tbaa !438
  %t3553 = add nsw i32 %t3202, %t2390547
  %14559 = sext i32 %t3553 to i64
  %14560 = shl nsw i64 %14559, 4
  %14561 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14560
  %14562 = bitcast i8* %14561 to <8 x i8>*
  %t3554 = load <8 x i8>, <8 x i8>* %14562, align 16, !tbaa !438
  %t3555 = add nsw i32 %t3202, %t2386534
  %14563 = sext i32 %t3555 to i64
  %14564 = shl nsw i64 %14563, 4
  %14565 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14564
  %14566 = bitcast i8* %14565 to <8 x i8>*
  %t3556 = load <8 x i8>, <8 x i8>* %14566, align 16, !tbaa !438
  %14567 = getelementptr inbounds i8, i8* %14533, i64 8
  %14568 = bitcast i8* %14567 to <8 x i8>*
  %t3557 = load <8 x i8>, <8 x i8>* %14568, align 8, !tbaa !438
  %14569 = getelementptr inbounds i8, i8* %14537, i64 8
  %14570 = bitcast i8* %14569 to <8 x i8>*
  %t3558 = load <8 x i8>, <8 x i8>* %14570, align 8, !tbaa !438
  %14571 = getelementptr inbounds i8, i8* %14541, i64 8
  %14572 = bitcast i8* %14571 to <8 x i8>*
  %t3559 = load <8 x i8>, <8 x i8>* %14572, align 8, !tbaa !438
  %14573 = getelementptr inbounds i8, i8* %14545, i64 8
  %14574 = bitcast i8* %14573 to <8 x i8>*
  %t3560 = load <8 x i8>, <8 x i8>* %14574, align 8, !tbaa !438
  %14575 = getelementptr inbounds i8, i8* %14549, i64 8
  %14576 = bitcast i8* %14575 to <8 x i8>*
  %t3561 = load <8 x i8>, <8 x i8>* %14576, align 8, !tbaa !438
  %14577 = getelementptr inbounds i8, i8* %14553, i64 8
  %14578 = bitcast i8* %14577 to <8 x i8>*
  %t3562 = load <8 x i8>, <8 x i8>* %14578, align 8, !tbaa !438
  %14579 = getelementptr inbounds i8, i8* %14557, i64 8
  %14580 = bitcast i8* %14579 to <8 x i8>*
  %t3563 = load <8 x i8>, <8 x i8>* %14580, align 8, !tbaa !438
  %14581 = getelementptr inbounds i8, i8* %14561, i64 8
  %14582 = bitcast i8* %14581 to <8 x i8>*
  %t3564 = load <8 x i8>, <8 x i8>* %14582, align 8, !tbaa !438
  %14583 = getelementptr inbounds i8, i8* %14565, i64 8
  %14584 = bitcast i8* %14583 to <8 x i8>*
  %t3565 = load <8 x i8>, <8 x i8>* %14584, align 8, !tbaa !438
  %14585 = zext <8 x i8> %t3540 to <8 x i16>
  %14586 = bitcast <8 x i16> %14585 to <2 x i64>
  %14587 = shufflevector <2 x i64> %14586, <2 x i64> undef, <1 x i32> zeroinitializer
  %14588 = bitcast <1 x i64> %14587 to <4 x i16>
  %14589 = zext <8 x i8> %t3542 to <8 x i16>
  %14590 = bitcast <8 x i16> %14589 to <2 x i64>
  %14591 = shufflevector <2 x i64> %14590, <2 x i64> undef, <1 x i32> zeroinitializer
  %14592 = zext <8 x i8> %t3544 to <8 x i16>
  %14593 = bitcast <8 x i16> %14592 to <2 x i64>
  %14594 = shufflevector <2 x i64> %14593, <2 x i64> undef, <1 x i32> zeroinitializer
  %14595 = bitcast <1 x i64> %14594 to <4 x i16>
  %14596 = zext <8 x i8> %t3546 to <8 x i16>
  %14597 = bitcast <8 x i16> %14596 to <2 x i64>
  %14598 = shufflevector <2 x i64> %14597, <2 x i64> undef, <1 x i32> zeroinitializer
  %14599 = zext <8 x i8> %t3548 to <8 x i16>
  %14600 = bitcast <8 x i16> %14599 to <2 x i64>
  %14601 = shufflevector <2 x i64> %14600, <2 x i64> undef, <1 x i32> zeroinitializer
  %14602 = bitcast <1 x i64> %14601 to <4 x i16>
  %14603 = zext <8 x i8> %t3550 to <8 x i16>
  %14604 = bitcast <8 x i16> %14603 to <2 x i64>
  %14605 = shufflevector <2 x i64> %14604, <2 x i64> undef, <1 x i32> zeroinitializer
  %14606 = zext <8 x i8> %t3552 to <8 x i16>
  %14607 = bitcast <8 x i16> %14606 to <2 x i64>
  %14608 = shufflevector <2 x i64> %14607, <2 x i64> undef, <1 x i32> zeroinitializer
  %14609 = bitcast <1 x i64> %14608 to <4 x i16>
  %14610 = zext <8 x i8> %t3554 to <8 x i16>
  %14611 = bitcast <8 x i16> %14610 to <2 x i64>
  %14612 = shufflevector <2 x i64> %14611, <2 x i64> undef, <1 x i32> zeroinitializer
  %14613 = zext <8 x i8> %t3556 to <8 x i16>
  %14614 = bitcast <8 x i16> %14613 to <2 x i64>
  %14615 = shufflevector <2 x i64> %14614, <2 x i64> undef, <1 x i32> zeroinitializer
  %14616 = bitcast <1 x i64> %14615 to <4 x i16>
  %.cast2389 = bitcast <1 x i64> %14591 to <4 x i16>
  %14617 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2389, <4 x i16> %11675) #11
  %14618 = sext <4 x i16> %14588 to <4 x i32>
  %14619 = mul nsw <4 x i32> %14618, %11713
  %.cast2392 = bitcast <1 x i64> %14598 to <4 x i16>
  %14620 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2392, <4 x i16> %11684) #11
  %14621 = sext <4 x i16> %14595 to <4 x i32>
  %14622 = mul nsw <4 x i32> %14621, %11717
  %.cast2395 = bitcast <1 x i64> %14605 to <4 x i16>
  %14623 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2395, <4 x i16> %11693) #11
  %14624 = sext <4 x i16> %14602 to <4 x i32>
  %14625 = mul nsw <4 x i32> %14624, %11721
  %.cast2398 = bitcast <1 x i64> %14612 to <4 x i16>
  %14626 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2398, <4 x i16> %11702) #11
  %14627 = sext <4 x i16> %14609 to <4 x i32>
  %14628 = mul nsw <4 x i32> %14627, %11725
  %14629 = sext <4 x i16> %14616 to <4 x i32>
  %14630 = mul nsw <4 x i32> %14629, %11727
  %14631 = add <4 x i32> %14619, %11562
  %14632 = add <4 x i32> %14631, %14622
  %14633 = add <4 x i32> %14632, %14625
  %14634 = add <4 x i32> %14633, %14628
  %14635 = add <4 x i32> %14634, %14630
  %14636 = add <4 x i32> %14635, %14617
  %14637 = add <4 x i32> %14636, %14620
  %14638 = add <4 x i32> %14637, %14623
  %14639 = add <4 x i32> %14638, %14626
  %14640 = shufflevector <2 x i64> %14586, <2 x i64> undef, <1 x i32> <i32 1>
  %14641 = bitcast <1 x i64> %14640 to <4 x i16>
  %14642 = shufflevector <2 x i64> %14590, <2 x i64> undef, <1 x i32> <i32 1>
  %14643 = shufflevector <2 x i64> %14593, <2 x i64> undef, <1 x i32> <i32 1>
  %14644 = bitcast <1 x i64> %14643 to <4 x i16>
  %14645 = shufflevector <2 x i64> %14597, <2 x i64> undef, <1 x i32> <i32 1>
  %14646 = shufflevector <2 x i64> %14600, <2 x i64> undef, <1 x i32> <i32 1>
  %14647 = bitcast <1 x i64> %14646 to <4 x i16>
  %14648 = shufflevector <2 x i64> %14604, <2 x i64> undef, <1 x i32> <i32 1>
  %14649 = shufflevector <2 x i64> %14607, <2 x i64> undef, <1 x i32> <i32 1>
  %14650 = bitcast <1 x i64> %14649 to <4 x i16>
  %14651 = shufflevector <2 x i64> %14611, <2 x i64> undef, <1 x i32> <i32 1>
  %14652 = shufflevector <2 x i64> %14614, <2 x i64> undef, <1 x i32> <i32 1>
  %14653 = bitcast <1 x i64> %14652 to <4 x i16>
  %.cast2401 = bitcast <1 x i64> %14642 to <4 x i16>
  %14654 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2401, <4 x i16> %11742) #11
  %14655 = sext <4 x i16> %14641 to <4 x i32>
  %14656 = mul nsw <4 x i32> %14655, %11764
  %.cast2404 = bitcast <1 x i64> %14645 to <4 x i16>
  %14657 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2404, <4 x i16> %11747) #11
  %14658 = sext <4 x i16> %14644 to <4 x i32>
  %14659 = mul nsw <4 x i32> %14658, %11768
  %.cast2407 = bitcast <1 x i64> %14648 to <4 x i16>
  %14660 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2407, <4 x i16> %11752) #11
  %14661 = sext <4 x i16> %14647 to <4 x i32>
  %14662 = mul nsw <4 x i32> %14661, %11772
  %.cast2410 = bitcast <1 x i64> %14651 to <4 x i16>
  %14663 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2410, <4 x i16> %11757) #11
  %14664 = sext <4 x i16> %14650 to <4 x i32>
  %14665 = mul nsw <4 x i32> %14664, %11776
  %14666 = sext <4 x i16> %14653 to <4 x i32>
  %14667 = mul nsw <4 x i32> %14666, %11778
  %14668 = add <4 x i32> %14656, %11563
  %14669 = add <4 x i32> %14668, %14659
  %14670 = add <4 x i32> %14669, %14662
  %14671 = add <4 x i32> %14670, %14665
  %14672 = add <4 x i32> %14671, %14667
  %14673 = add <4 x i32> %14672, %14654
  %14674 = add <4 x i32> %14673, %14657
  %14675 = add <4 x i32> %14674, %14660
  %14676 = add <4 x i32> %14675, %14663
  %14677 = zext <8 x i8> %t3557 to <8 x i16>
  %14678 = bitcast <8 x i16> %14677 to <2 x i64>
  %14679 = shufflevector <2 x i64> %14678, <2 x i64> undef, <1 x i32> zeroinitializer
  %14680 = bitcast <1 x i64> %14679 to <4 x i16>
  %14681 = zext <8 x i8> %t3558 to <8 x i16>
  %14682 = bitcast <8 x i16> %14681 to <2 x i64>
  %14683 = shufflevector <2 x i64> %14682, <2 x i64> undef, <1 x i32> zeroinitializer
  %14684 = zext <8 x i8> %t3559 to <8 x i16>
  %14685 = bitcast <8 x i16> %14684 to <2 x i64>
  %14686 = shufflevector <2 x i64> %14685, <2 x i64> undef, <1 x i32> zeroinitializer
  %14687 = bitcast <1 x i64> %14686 to <4 x i16>
  %14688 = zext <8 x i8> %t3560 to <8 x i16>
  %14689 = bitcast <8 x i16> %14688 to <2 x i64>
  %14690 = shufflevector <2 x i64> %14689, <2 x i64> undef, <1 x i32> zeroinitializer
  %14691 = zext <8 x i8> %t3561 to <8 x i16>
  %14692 = bitcast <8 x i16> %14691 to <2 x i64>
  %14693 = shufflevector <2 x i64> %14692, <2 x i64> undef, <1 x i32> zeroinitializer
  %14694 = bitcast <1 x i64> %14693 to <4 x i16>
  %14695 = zext <8 x i8> %t3562 to <8 x i16>
  %14696 = bitcast <8 x i16> %14695 to <2 x i64>
  %14697 = shufflevector <2 x i64> %14696, <2 x i64> undef, <1 x i32> zeroinitializer
  %14698 = zext <8 x i8> %t3563 to <8 x i16>
  %14699 = bitcast <8 x i16> %14698 to <2 x i64>
  %14700 = shufflevector <2 x i64> %14699, <2 x i64> undef, <1 x i32> zeroinitializer
  %14701 = bitcast <1 x i64> %14700 to <4 x i16>
  %14702 = zext <8 x i8> %t3564 to <8 x i16>
  %14703 = bitcast <8 x i16> %14702 to <2 x i64>
  %14704 = shufflevector <2 x i64> %14703, <2 x i64> undef, <1 x i32> zeroinitializer
  %14705 = zext <8 x i8> %t3565 to <8 x i16>
  %14706 = bitcast <8 x i16> %14705 to <2 x i64>
  %14707 = shufflevector <2 x i64> %14706, <2 x i64> undef, <1 x i32> zeroinitializer
  %14708 = bitcast <1 x i64> %14707 to <4 x i16>
  %.cast2413 = bitcast <1 x i64> %14683 to <4 x i16>
  %14709 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2413, <4 x i16> %11795) #11
  %14710 = sext <4 x i16> %14680 to <4 x i32>
  %14711 = mul nsw <4 x i32> %14710, %11833
  %.cast2416 = bitcast <1 x i64> %14690 to <4 x i16>
  %14712 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2416, <4 x i16> %11804) #11
  %14713 = sext <4 x i16> %14687 to <4 x i32>
  %14714 = mul nsw <4 x i32> %14713, %11837
  %.cast2419 = bitcast <1 x i64> %14697 to <4 x i16>
  %14715 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2419, <4 x i16> %11813) #11
  %14716 = sext <4 x i16> %14694 to <4 x i32>
  %14717 = mul nsw <4 x i32> %14716, %11841
  %.cast2422 = bitcast <1 x i64> %14704 to <4 x i16>
  %14718 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2422, <4 x i16> %11822) #11
  %14719 = sext <4 x i16> %14701 to <4 x i32>
  %14720 = mul nsw <4 x i32> %14719, %11845
  %14721 = sext <4 x i16> %14708 to <4 x i32>
  %14722 = mul nsw <4 x i32> %14721, %11847
  %14723 = add <4 x i32> %14711, %11564
  %14724 = add <4 x i32> %14723, %14714
  %14725 = add <4 x i32> %14724, %14717
  %14726 = add <4 x i32> %14725, %14720
  %14727 = add <4 x i32> %14726, %14722
  %14728 = add <4 x i32> %14727, %14709
  %14729 = add <4 x i32> %14728, %14712
  %14730 = add <4 x i32> %14729, %14715
  %14731 = add <4 x i32> %14730, %14718
  %14732 = shufflevector <2 x i64> %14678, <2 x i64> undef, <1 x i32> <i32 1>
  %14733 = bitcast <1 x i64> %14732 to <4 x i16>
  %14734 = shufflevector <2 x i64> %14682, <2 x i64> undef, <1 x i32> <i32 1>
  %14735 = shufflevector <2 x i64> %14685, <2 x i64> undef, <1 x i32> <i32 1>
  %14736 = bitcast <1 x i64> %14735 to <4 x i16>
  %14737 = shufflevector <2 x i64> %14689, <2 x i64> undef, <1 x i32> <i32 1>
  %14738 = shufflevector <2 x i64> %14692, <2 x i64> undef, <1 x i32> <i32 1>
  %14739 = bitcast <1 x i64> %14738 to <4 x i16>
  %14740 = shufflevector <2 x i64> %14696, <2 x i64> undef, <1 x i32> <i32 1>
  %14741 = shufflevector <2 x i64> %14699, <2 x i64> undef, <1 x i32> <i32 1>
  %14742 = bitcast <1 x i64> %14741 to <4 x i16>
  %14743 = shufflevector <2 x i64> %14703, <2 x i64> undef, <1 x i32> <i32 1>
  %14744 = shufflevector <2 x i64> %14706, <2 x i64> undef, <1 x i32> <i32 1>
  %14745 = bitcast <1 x i64> %14744 to <4 x i16>
  %.cast2425 = bitcast <1 x i64> %14734 to <4 x i16>
  %14746 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2425, <4 x i16> %11862) #11
  %14747 = sext <4 x i16> %14733 to <4 x i32>
  %14748 = mul nsw <4 x i32> %14747, %11884
  %.cast2428 = bitcast <1 x i64> %14737 to <4 x i16>
  %14749 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2428, <4 x i16> %11867) #11
  %14750 = sext <4 x i16> %14736 to <4 x i32>
  %14751 = mul nsw <4 x i32> %14750, %11888
  %.cast2431 = bitcast <1 x i64> %14740 to <4 x i16>
  %14752 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2431, <4 x i16> %11872) #11
  %14753 = sext <4 x i16> %14739 to <4 x i32>
  %14754 = mul nsw <4 x i32> %14753, %11892
  %.cast2434 = bitcast <1 x i64> %14743 to <4 x i16>
  %14755 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2434, <4 x i16> %11877) #11
  %14756 = sext <4 x i16> %14742 to <4 x i32>
  %14757 = mul nsw <4 x i32> %14756, %11896
  %14758 = sext <4 x i16> %14745 to <4 x i32>
  %14759 = mul nsw <4 x i32> %14758, %11898
  %14760 = add <4 x i32> %14748, %11565
  %14761 = add <4 x i32> %14760, %14751
  %14762 = add <4 x i32> %14761, %14754
  %14763 = add <4 x i32> %14762, %14757
  %14764 = add <4 x i32> %14763, %14759
  %14765 = add <4 x i32> %14764, %14746
  %14766 = add <4 x i32> %14765, %14749
  %14767 = add <4 x i32> %14766, %14752
  %14768 = add <4 x i32> %14767, %14755
  %t3567 = add nsw i32 %t3230, %t2420560
  %14769 = sext i32 %t3567 to i64
  %14770 = shl nsw i64 %14769, 4
  %14771 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14770
  %14772 = bitcast i8* %14771 to <8 x i8>*
  %t3568 = load <8 x i8>, <8 x i8>* %14772, align 16, !tbaa !438
  %t3569 = add nsw i32 %t3230, %t2416559
  %14773 = sext i32 %t3569 to i64
  %14774 = shl nsw i64 %14773, 4
  %14775 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14774
  %14776 = bitcast i8* %14775 to <8 x i8>*
  %t3570 = load <8 x i8>, <8 x i8>* %14776, align 16, !tbaa !438
  %t3571 = add nsw i32 %t3230, %t2412540
  %14777 = sext i32 %t3571 to i64
  %14778 = shl nsw i64 %14777, 4
  %14779 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14778
  %14780 = bitcast i8* %14779 to <8 x i8>*
  %t3572 = load <8 x i8>, <8 x i8>* %14780, align 16, !tbaa !438
  %t3573 = add nsw i32 %t3230, %t2407558
  %14781 = sext i32 %t3573 to i64
  %14782 = shl nsw i64 %14781, 4
  %14783 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14782
  %14784 = bitcast i8* %14783 to <8 x i8>*
  %t3574 = load <8 x i8>, <8 x i8>* %14784, align 16, !tbaa !438
  %t3575 = add nsw i32 %t3230, %t2403557
  %14785 = sext i32 %t3575 to i64
  %14786 = shl nsw i64 %14785, 4
  %14787 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14786
  %14788 = bitcast i8* %14787 to <8 x i8>*
  %t3576 = load <8 x i8>, <8 x i8>* %14788, align 16, !tbaa !438
  %t3577 = add nsw i32 %t3230, %t2399539
  %14789 = sext i32 %t3577 to i64
  %14790 = shl nsw i64 %14789, 4
  %14791 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14790
  %14792 = bitcast i8* %14791 to <8 x i8>*
  %t3578 = load <8 x i8>, <8 x i8>* %14792, align 16, !tbaa !438
  %t3579 = add nsw i32 %t3230, %t2394548
  %14793 = sext i32 %t3579 to i64
  %14794 = shl nsw i64 %14793, 4
  %14795 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14794
  %14796 = bitcast i8* %14795 to <8 x i8>*
  %t3580 = load <8 x i8>, <8 x i8>* %14796, align 16, !tbaa !438
  %t3581 = add nsw i32 %t3230, %t2390547
  %14797 = sext i32 %t3581 to i64
  %14798 = shl nsw i64 %14797, 4
  %14799 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14798
  %14800 = bitcast i8* %14799 to <8 x i8>*
  %t3582 = load <8 x i8>, <8 x i8>* %14800, align 16, !tbaa !438
  %t3583 = add nsw i32 %t3230, %t2386534
  %14801 = sext i32 %t3583 to i64
  %14802 = shl nsw i64 %14801, 4
  %14803 = getelementptr inbounds i8, i8* %resampled_input473, i64 %14802
  %14804 = bitcast i8* %14803 to <8 x i8>*
  %t3584 = load <8 x i8>, <8 x i8>* %14804, align 16, !tbaa !438
  %14805 = getelementptr inbounds i8, i8* %14771, i64 8
  %14806 = bitcast i8* %14805 to <8 x i8>*
  %t3585 = load <8 x i8>, <8 x i8>* %14806, align 8, !tbaa !438
  %14807 = getelementptr inbounds i8, i8* %14775, i64 8
  %14808 = bitcast i8* %14807 to <8 x i8>*
  %t3586 = load <8 x i8>, <8 x i8>* %14808, align 8, !tbaa !438
  %14809 = getelementptr inbounds i8, i8* %14779, i64 8
  %14810 = bitcast i8* %14809 to <8 x i8>*
  %t3587 = load <8 x i8>, <8 x i8>* %14810, align 8, !tbaa !438
  %14811 = getelementptr inbounds i8, i8* %14783, i64 8
  %14812 = bitcast i8* %14811 to <8 x i8>*
  %t3588 = load <8 x i8>, <8 x i8>* %14812, align 8, !tbaa !438
  %14813 = getelementptr inbounds i8, i8* %14787, i64 8
  %14814 = bitcast i8* %14813 to <8 x i8>*
  %t3589 = load <8 x i8>, <8 x i8>* %14814, align 8, !tbaa !438
  %14815 = getelementptr inbounds i8, i8* %14791, i64 8
  %14816 = bitcast i8* %14815 to <8 x i8>*
  %t3590 = load <8 x i8>, <8 x i8>* %14816, align 8, !tbaa !438
  %14817 = getelementptr inbounds i8, i8* %14795, i64 8
  %14818 = bitcast i8* %14817 to <8 x i8>*
  %t3591 = load <8 x i8>, <8 x i8>* %14818, align 8, !tbaa !438
  %14819 = getelementptr inbounds i8, i8* %14799, i64 8
  %14820 = bitcast i8* %14819 to <8 x i8>*
  %t3592 = load <8 x i8>, <8 x i8>* %14820, align 8, !tbaa !438
  %14821 = getelementptr inbounds i8, i8* %14803, i64 8
  %14822 = bitcast i8* %14821 to <8 x i8>*
  %t3593 = load <8 x i8>, <8 x i8>* %14822, align 8, !tbaa !438
  %14823 = zext <8 x i8> %t3568 to <8 x i16>
  %14824 = bitcast <8 x i16> %14823 to <2 x i64>
  %14825 = shufflevector <2 x i64> %14824, <2 x i64> undef, <1 x i32> zeroinitializer
  %14826 = bitcast <1 x i64> %14825 to <4 x i16>
  %14827 = zext <8 x i8> %t3570 to <8 x i16>
  %14828 = bitcast <8 x i16> %14827 to <2 x i64>
  %14829 = shufflevector <2 x i64> %14828, <2 x i64> undef, <1 x i32> zeroinitializer
  %14830 = zext <8 x i8> %t3572 to <8 x i16>
  %14831 = bitcast <8 x i16> %14830 to <2 x i64>
  %14832 = shufflevector <2 x i64> %14831, <2 x i64> undef, <1 x i32> zeroinitializer
  %14833 = bitcast <1 x i64> %14832 to <4 x i16>
  %14834 = zext <8 x i8> %t3574 to <8 x i16>
  %14835 = bitcast <8 x i16> %14834 to <2 x i64>
  %14836 = shufflevector <2 x i64> %14835, <2 x i64> undef, <1 x i32> zeroinitializer
  %14837 = zext <8 x i8> %t3576 to <8 x i16>
  %14838 = bitcast <8 x i16> %14837 to <2 x i64>
  %14839 = shufflevector <2 x i64> %14838, <2 x i64> undef, <1 x i32> zeroinitializer
  %14840 = bitcast <1 x i64> %14839 to <4 x i16>
  %14841 = zext <8 x i8> %t3578 to <8 x i16>
  %14842 = bitcast <8 x i16> %14841 to <2 x i64>
  %14843 = shufflevector <2 x i64> %14842, <2 x i64> undef, <1 x i32> zeroinitializer
  %14844 = zext <8 x i8> %t3580 to <8 x i16>
  %14845 = bitcast <8 x i16> %14844 to <2 x i64>
  %14846 = shufflevector <2 x i64> %14845, <2 x i64> undef, <1 x i32> zeroinitializer
  %14847 = bitcast <1 x i64> %14846 to <4 x i16>
  %14848 = zext <8 x i8> %t3582 to <8 x i16>
  %14849 = bitcast <8 x i16> %14848 to <2 x i64>
  %14850 = shufflevector <2 x i64> %14849, <2 x i64> undef, <1 x i32> zeroinitializer
  %14851 = zext <8 x i8> %t3584 to <8 x i16>
  %14852 = bitcast <8 x i16> %14851 to <2 x i64>
  %14853 = shufflevector <2 x i64> %14852, <2 x i64> undef, <1 x i32> zeroinitializer
  %14854 = bitcast <1 x i64> %14853 to <4 x i16>
  %.cast2437 = bitcast <1 x i64> %14829 to <4 x i16>
  %14855 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2437, <4 x i16> %11675) #11
  %14856 = sext <4 x i16> %14826 to <4 x i32>
  %14857 = mul nsw <4 x i32> %14856, %11713
  %.cast2440 = bitcast <1 x i64> %14836 to <4 x i16>
  %14858 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2440, <4 x i16> %11684) #11
  %14859 = sext <4 x i16> %14833 to <4 x i32>
  %14860 = mul nsw <4 x i32> %14859, %11717
  %.cast2443 = bitcast <1 x i64> %14843 to <4 x i16>
  %14861 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2443, <4 x i16> %11693) #11
  %14862 = sext <4 x i16> %14840 to <4 x i32>
  %14863 = mul nsw <4 x i32> %14862, %11721
  %.cast2446 = bitcast <1 x i64> %14850 to <4 x i16>
  %14864 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2446, <4 x i16> %11702) #11
  %14865 = sext <4 x i16> %14847 to <4 x i32>
  %14866 = mul nsw <4 x i32> %14865, %11725
  %14867 = sext <4 x i16> %14854 to <4 x i32>
  %14868 = mul nsw <4 x i32> %14867, %11727
  %14869 = add <4 x i32> %14857, %11562
  %14870 = add <4 x i32> %14869, %14860
  %14871 = add <4 x i32> %14870, %14863
  %14872 = add <4 x i32> %14871, %14866
  %14873 = add <4 x i32> %14872, %14868
  %14874 = add <4 x i32> %14873, %14855
  %14875 = add <4 x i32> %14874, %14858
  %14876 = add <4 x i32> %14875, %14861
  %14877 = add <4 x i32> %14876, %14864
  %14878 = shufflevector <2 x i64> %14824, <2 x i64> undef, <1 x i32> <i32 1>
  %14879 = bitcast <1 x i64> %14878 to <4 x i16>
  %14880 = shufflevector <2 x i64> %14828, <2 x i64> undef, <1 x i32> <i32 1>
  %14881 = shufflevector <2 x i64> %14831, <2 x i64> undef, <1 x i32> <i32 1>
  %14882 = bitcast <1 x i64> %14881 to <4 x i16>
  %14883 = shufflevector <2 x i64> %14835, <2 x i64> undef, <1 x i32> <i32 1>
  %14884 = shufflevector <2 x i64> %14838, <2 x i64> undef, <1 x i32> <i32 1>
  %14885 = bitcast <1 x i64> %14884 to <4 x i16>
  %14886 = shufflevector <2 x i64> %14842, <2 x i64> undef, <1 x i32> <i32 1>
  %14887 = shufflevector <2 x i64> %14845, <2 x i64> undef, <1 x i32> <i32 1>
  %14888 = bitcast <1 x i64> %14887 to <4 x i16>
  %14889 = shufflevector <2 x i64> %14849, <2 x i64> undef, <1 x i32> <i32 1>
  %14890 = shufflevector <2 x i64> %14852, <2 x i64> undef, <1 x i32> <i32 1>
  %14891 = bitcast <1 x i64> %14890 to <4 x i16>
  %.cast2449 = bitcast <1 x i64> %14880 to <4 x i16>
  %14892 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2449, <4 x i16> %11742) #11
  %14893 = sext <4 x i16> %14879 to <4 x i32>
  %14894 = mul nsw <4 x i32> %14893, %11764
  %.cast2452 = bitcast <1 x i64> %14883 to <4 x i16>
  %14895 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2452, <4 x i16> %11747) #11
  %14896 = sext <4 x i16> %14882 to <4 x i32>
  %14897 = mul nsw <4 x i32> %14896, %11768
  %.cast2455 = bitcast <1 x i64> %14886 to <4 x i16>
  %14898 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2455, <4 x i16> %11752) #11
  %14899 = sext <4 x i16> %14885 to <4 x i32>
  %14900 = mul nsw <4 x i32> %14899, %11772
  %.cast2458 = bitcast <1 x i64> %14889 to <4 x i16>
  %14901 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2458, <4 x i16> %11757) #11
  %14902 = sext <4 x i16> %14888 to <4 x i32>
  %14903 = mul nsw <4 x i32> %14902, %11776
  %14904 = sext <4 x i16> %14891 to <4 x i32>
  %14905 = mul nsw <4 x i32> %14904, %11778
  %14906 = add <4 x i32> %14894, %11563
  %14907 = add <4 x i32> %14906, %14897
  %14908 = add <4 x i32> %14907, %14900
  %14909 = add <4 x i32> %14908, %14903
  %14910 = add <4 x i32> %14909, %14905
  %14911 = add <4 x i32> %14910, %14892
  %14912 = add <4 x i32> %14911, %14895
  %14913 = add <4 x i32> %14912, %14898
  %14914 = add <4 x i32> %14913, %14901
  %14915 = zext <8 x i8> %t3585 to <8 x i16>
  %14916 = bitcast <8 x i16> %14915 to <2 x i64>
  %14917 = shufflevector <2 x i64> %14916, <2 x i64> undef, <1 x i32> zeroinitializer
  %14918 = bitcast <1 x i64> %14917 to <4 x i16>
  %14919 = zext <8 x i8> %t3586 to <8 x i16>
  %14920 = bitcast <8 x i16> %14919 to <2 x i64>
  %14921 = shufflevector <2 x i64> %14920, <2 x i64> undef, <1 x i32> zeroinitializer
  %14922 = zext <8 x i8> %t3587 to <8 x i16>
  %14923 = bitcast <8 x i16> %14922 to <2 x i64>
  %14924 = shufflevector <2 x i64> %14923, <2 x i64> undef, <1 x i32> zeroinitializer
  %14925 = bitcast <1 x i64> %14924 to <4 x i16>
  %14926 = zext <8 x i8> %t3588 to <8 x i16>
  %14927 = bitcast <8 x i16> %14926 to <2 x i64>
  %14928 = shufflevector <2 x i64> %14927, <2 x i64> undef, <1 x i32> zeroinitializer
  %14929 = zext <8 x i8> %t3589 to <8 x i16>
  %14930 = bitcast <8 x i16> %14929 to <2 x i64>
  %14931 = shufflevector <2 x i64> %14930, <2 x i64> undef, <1 x i32> zeroinitializer
  %14932 = bitcast <1 x i64> %14931 to <4 x i16>
  %14933 = zext <8 x i8> %t3590 to <8 x i16>
  %14934 = bitcast <8 x i16> %14933 to <2 x i64>
  %14935 = shufflevector <2 x i64> %14934, <2 x i64> undef, <1 x i32> zeroinitializer
  %14936 = zext <8 x i8> %t3591 to <8 x i16>
  %14937 = bitcast <8 x i16> %14936 to <2 x i64>
  %14938 = shufflevector <2 x i64> %14937, <2 x i64> undef, <1 x i32> zeroinitializer
  %14939 = bitcast <1 x i64> %14938 to <4 x i16>
  %14940 = zext <8 x i8> %t3592 to <8 x i16>
  %14941 = bitcast <8 x i16> %14940 to <2 x i64>
  %14942 = shufflevector <2 x i64> %14941, <2 x i64> undef, <1 x i32> zeroinitializer
  %14943 = zext <8 x i8> %t3593 to <8 x i16>
  %14944 = bitcast <8 x i16> %14943 to <2 x i64>
  %14945 = shufflevector <2 x i64> %14944, <2 x i64> undef, <1 x i32> zeroinitializer
  %14946 = bitcast <1 x i64> %14945 to <4 x i16>
  %.cast2461 = bitcast <1 x i64> %14921 to <4 x i16>
  %14947 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2461, <4 x i16> %11795) #11
  %14948 = sext <4 x i16> %14918 to <4 x i32>
  %14949 = mul nsw <4 x i32> %14948, %11833
  %.cast2464 = bitcast <1 x i64> %14928 to <4 x i16>
  %14950 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2464, <4 x i16> %11804) #11
  %14951 = sext <4 x i16> %14925 to <4 x i32>
  %14952 = mul nsw <4 x i32> %14951, %11837
  %.cast2467 = bitcast <1 x i64> %14935 to <4 x i16>
  %14953 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2467, <4 x i16> %11813) #11
  %14954 = sext <4 x i16> %14932 to <4 x i32>
  %14955 = mul nsw <4 x i32> %14954, %11841
  %.cast2470 = bitcast <1 x i64> %14942 to <4 x i16>
  %14956 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2470, <4 x i16> %11822) #11
  %14957 = sext <4 x i16> %14939 to <4 x i32>
  %14958 = mul nsw <4 x i32> %14957, %11845
  %14959 = sext <4 x i16> %14946 to <4 x i32>
  %14960 = mul nsw <4 x i32> %14959, %11847
  %14961 = add <4 x i32> %14949, %11564
  %14962 = add <4 x i32> %14961, %14952
  %14963 = add <4 x i32> %14962, %14955
  %14964 = add <4 x i32> %14963, %14958
  %14965 = add <4 x i32> %14964, %14960
  %14966 = add <4 x i32> %14965, %14947
  %14967 = add <4 x i32> %14966, %14950
  %14968 = add <4 x i32> %14967, %14953
  %14969 = add <4 x i32> %14968, %14956
  %14970 = shufflevector <2 x i64> %14916, <2 x i64> undef, <1 x i32> <i32 1>
  %14971 = bitcast <1 x i64> %14970 to <4 x i16>
  %14972 = shufflevector <2 x i64> %14920, <2 x i64> undef, <1 x i32> <i32 1>
  %14973 = shufflevector <2 x i64> %14923, <2 x i64> undef, <1 x i32> <i32 1>
  %14974 = bitcast <1 x i64> %14973 to <4 x i16>
  %14975 = shufflevector <2 x i64> %14927, <2 x i64> undef, <1 x i32> <i32 1>
  %14976 = shufflevector <2 x i64> %14930, <2 x i64> undef, <1 x i32> <i32 1>
  %14977 = bitcast <1 x i64> %14976 to <4 x i16>
  %14978 = shufflevector <2 x i64> %14934, <2 x i64> undef, <1 x i32> <i32 1>
  %14979 = shufflevector <2 x i64> %14937, <2 x i64> undef, <1 x i32> <i32 1>
  %14980 = bitcast <1 x i64> %14979 to <4 x i16>
  %14981 = shufflevector <2 x i64> %14941, <2 x i64> undef, <1 x i32> <i32 1>
  %14982 = shufflevector <2 x i64> %14944, <2 x i64> undef, <1 x i32> <i32 1>
  %14983 = bitcast <1 x i64> %14982 to <4 x i16>
  %.cast2473 = bitcast <1 x i64> %14972 to <4 x i16>
  %14984 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2473, <4 x i16> %11862) #11
  %14985 = sext <4 x i16> %14971 to <4 x i32>
  %14986 = mul nsw <4 x i32> %14985, %11884
  %.cast2476 = bitcast <1 x i64> %14975 to <4 x i16>
  %14987 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2476, <4 x i16> %11867) #11
  %14988 = sext <4 x i16> %14974 to <4 x i32>
  %14989 = mul nsw <4 x i32> %14988, %11888
  %.cast2479 = bitcast <1 x i64> %14978 to <4 x i16>
  %14990 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2479, <4 x i16> %11872) #11
  %14991 = sext <4 x i16> %14977 to <4 x i32>
  %14992 = mul nsw <4 x i32> %14991, %11892
  %.cast2482 = bitcast <1 x i64> %14981 to <4 x i16>
  %14993 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2482, <4 x i16> %11877) #11
  %14994 = sext <4 x i16> %14980 to <4 x i32>
  %14995 = mul nsw <4 x i32> %14994, %11896
  %14996 = sext <4 x i16> %14983 to <4 x i32>
  %14997 = mul nsw <4 x i32> %14996, %11898
  %14998 = add <4 x i32> %14986, %11565
  %14999 = add <4 x i32> %14998, %14989
  %15000 = add <4 x i32> %14999, %14992
  %15001 = add <4 x i32> %15000, %14995
  %15002 = add <4 x i32> %15001, %14997
  %15003 = add <4 x i32> %15002, %14984
  %15004 = add <4 x i32> %15003, %14987
  %15005 = add <4 x i32> %15004, %14990
  %15006 = add <4 x i32> %15005, %14993
  %t3595 = add nsw i32 %t3258, %t2420560
  %15007 = sext i32 %t3595 to i64
  %15008 = shl nsw i64 %15007, 4
  %15009 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15008
  %15010 = bitcast i8* %15009 to <8 x i8>*
  %t3596 = load <8 x i8>, <8 x i8>* %15010, align 16, !tbaa !438
  %t3597 = add nsw i32 %t3258, %t2416559
  %15011 = sext i32 %t3597 to i64
  %15012 = shl nsw i64 %15011, 4
  %15013 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15012
  %15014 = bitcast i8* %15013 to <8 x i8>*
  %t3598 = load <8 x i8>, <8 x i8>* %15014, align 16, !tbaa !438
  %t3599 = add nsw i32 %t3258, %t2412540
  %15015 = sext i32 %t3599 to i64
  %15016 = shl nsw i64 %15015, 4
  %15017 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15016
  %15018 = bitcast i8* %15017 to <8 x i8>*
  %t3600 = load <8 x i8>, <8 x i8>* %15018, align 16, !tbaa !438
  %t3601 = add nsw i32 %t3258, %t2407558
  %15019 = sext i32 %t3601 to i64
  %15020 = shl nsw i64 %15019, 4
  %15021 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15020
  %15022 = bitcast i8* %15021 to <8 x i8>*
  %t3602 = load <8 x i8>, <8 x i8>* %15022, align 16, !tbaa !438
  %t3603 = add nsw i32 %t3258, %t2403557
  %15023 = sext i32 %t3603 to i64
  %15024 = shl nsw i64 %15023, 4
  %15025 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15024
  %15026 = bitcast i8* %15025 to <8 x i8>*
  %t3604 = load <8 x i8>, <8 x i8>* %15026, align 16, !tbaa !438
  %t3605 = add nsw i32 %t3258, %t2399539
  %15027 = sext i32 %t3605 to i64
  %15028 = shl nsw i64 %15027, 4
  %15029 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15028
  %15030 = bitcast i8* %15029 to <8 x i8>*
  %t3606 = load <8 x i8>, <8 x i8>* %15030, align 16, !tbaa !438
  %t3607 = add nsw i32 %t3258, %t2394548
  %15031 = sext i32 %t3607 to i64
  %15032 = shl nsw i64 %15031, 4
  %15033 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15032
  %15034 = bitcast i8* %15033 to <8 x i8>*
  %t3608 = load <8 x i8>, <8 x i8>* %15034, align 16, !tbaa !438
  %t3609 = add nsw i32 %t3258, %t2390547
  %15035 = sext i32 %t3609 to i64
  %15036 = shl nsw i64 %15035, 4
  %15037 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15036
  %15038 = bitcast i8* %15037 to <8 x i8>*
  %t3610 = load <8 x i8>, <8 x i8>* %15038, align 16, !tbaa !438
  %t3611 = add nsw i32 %t3258, %t2386534
  %15039 = sext i32 %t3611 to i64
  %15040 = shl nsw i64 %15039, 4
  %15041 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15040
  %15042 = bitcast i8* %15041 to <8 x i8>*
  %t3612 = load <8 x i8>, <8 x i8>* %15042, align 16, !tbaa !438
  %15043 = getelementptr inbounds i8, i8* %15009, i64 8
  %15044 = bitcast i8* %15043 to <8 x i8>*
  %t3613 = load <8 x i8>, <8 x i8>* %15044, align 8, !tbaa !438
  %15045 = getelementptr inbounds i8, i8* %15013, i64 8
  %15046 = bitcast i8* %15045 to <8 x i8>*
  %t3614 = load <8 x i8>, <8 x i8>* %15046, align 8, !tbaa !438
  %15047 = getelementptr inbounds i8, i8* %15017, i64 8
  %15048 = bitcast i8* %15047 to <8 x i8>*
  %t3615 = load <8 x i8>, <8 x i8>* %15048, align 8, !tbaa !438
  %15049 = getelementptr inbounds i8, i8* %15021, i64 8
  %15050 = bitcast i8* %15049 to <8 x i8>*
  %t3616 = load <8 x i8>, <8 x i8>* %15050, align 8, !tbaa !438
  %15051 = getelementptr inbounds i8, i8* %15025, i64 8
  %15052 = bitcast i8* %15051 to <8 x i8>*
  %t3617 = load <8 x i8>, <8 x i8>* %15052, align 8, !tbaa !438
  %15053 = getelementptr inbounds i8, i8* %15029, i64 8
  %15054 = bitcast i8* %15053 to <8 x i8>*
  %t3618 = load <8 x i8>, <8 x i8>* %15054, align 8, !tbaa !438
  %15055 = getelementptr inbounds i8, i8* %15033, i64 8
  %15056 = bitcast i8* %15055 to <8 x i8>*
  %t3619 = load <8 x i8>, <8 x i8>* %15056, align 8, !tbaa !438
  %15057 = getelementptr inbounds i8, i8* %15037, i64 8
  %15058 = bitcast i8* %15057 to <8 x i8>*
  %t3620 = load <8 x i8>, <8 x i8>* %15058, align 8, !tbaa !438
  %15059 = getelementptr inbounds i8, i8* %15041, i64 8
  %15060 = bitcast i8* %15059 to <8 x i8>*
  %t3621 = load <8 x i8>, <8 x i8>* %15060, align 8, !tbaa !438
  %15061 = zext <8 x i8> %t3596 to <8 x i16>
  %15062 = bitcast <8 x i16> %15061 to <2 x i64>
  %15063 = shufflevector <2 x i64> %15062, <2 x i64> undef, <1 x i32> zeroinitializer
  %15064 = bitcast <1 x i64> %15063 to <4 x i16>
  %15065 = zext <8 x i8> %t3598 to <8 x i16>
  %15066 = bitcast <8 x i16> %15065 to <2 x i64>
  %15067 = shufflevector <2 x i64> %15066, <2 x i64> undef, <1 x i32> zeroinitializer
  %15068 = zext <8 x i8> %t3600 to <8 x i16>
  %15069 = bitcast <8 x i16> %15068 to <2 x i64>
  %15070 = shufflevector <2 x i64> %15069, <2 x i64> undef, <1 x i32> zeroinitializer
  %15071 = bitcast <1 x i64> %15070 to <4 x i16>
  %15072 = zext <8 x i8> %t3602 to <8 x i16>
  %15073 = bitcast <8 x i16> %15072 to <2 x i64>
  %15074 = shufflevector <2 x i64> %15073, <2 x i64> undef, <1 x i32> zeroinitializer
  %15075 = zext <8 x i8> %t3604 to <8 x i16>
  %15076 = bitcast <8 x i16> %15075 to <2 x i64>
  %15077 = shufflevector <2 x i64> %15076, <2 x i64> undef, <1 x i32> zeroinitializer
  %15078 = bitcast <1 x i64> %15077 to <4 x i16>
  %15079 = zext <8 x i8> %t3606 to <8 x i16>
  %15080 = bitcast <8 x i16> %15079 to <2 x i64>
  %15081 = shufflevector <2 x i64> %15080, <2 x i64> undef, <1 x i32> zeroinitializer
  %15082 = zext <8 x i8> %t3608 to <8 x i16>
  %15083 = bitcast <8 x i16> %15082 to <2 x i64>
  %15084 = shufflevector <2 x i64> %15083, <2 x i64> undef, <1 x i32> zeroinitializer
  %15085 = bitcast <1 x i64> %15084 to <4 x i16>
  %15086 = zext <8 x i8> %t3610 to <8 x i16>
  %15087 = bitcast <8 x i16> %15086 to <2 x i64>
  %15088 = shufflevector <2 x i64> %15087, <2 x i64> undef, <1 x i32> zeroinitializer
  %15089 = zext <8 x i8> %t3612 to <8 x i16>
  %15090 = bitcast <8 x i16> %15089 to <2 x i64>
  %15091 = shufflevector <2 x i64> %15090, <2 x i64> undef, <1 x i32> zeroinitializer
  %15092 = bitcast <1 x i64> %15091 to <4 x i16>
  %.cast2485 = bitcast <1 x i64> %15067 to <4 x i16>
  %15093 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2485, <4 x i16> %11675) #11
  %15094 = sext <4 x i16> %15064 to <4 x i32>
  %15095 = mul nsw <4 x i32> %15094, %11713
  %.cast2488 = bitcast <1 x i64> %15074 to <4 x i16>
  %15096 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2488, <4 x i16> %11684) #11
  %15097 = sext <4 x i16> %15071 to <4 x i32>
  %15098 = mul nsw <4 x i32> %15097, %11717
  %.cast2491 = bitcast <1 x i64> %15081 to <4 x i16>
  %15099 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2491, <4 x i16> %11693) #11
  %15100 = sext <4 x i16> %15078 to <4 x i32>
  %15101 = mul nsw <4 x i32> %15100, %11721
  %.cast2494 = bitcast <1 x i64> %15088 to <4 x i16>
  %15102 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2494, <4 x i16> %11702) #11
  %15103 = sext <4 x i16> %15085 to <4 x i32>
  %15104 = mul nsw <4 x i32> %15103, %11725
  %15105 = sext <4 x i16> %15092 to <4 x i32>
  %15106 = mul nsw <4 x i32> %15105, %11727
  %15107 = add <4 x i32> %15095, %11562
  %15108 = add <4 x i32> %15107, %15098
  %15109 = add <4 x i32> %15108, %15101
  %15110 = add <4 x i32> %15109, %15104
  %15111 = add <4 x i32> %15110, %15106
  %15112 = add <4 x i32> %15111, %15093
  %15113 = add <4 x i32> %15112, %15096
  %15114 = add <4 x i32> %15113, %15099
  %15115 = add <4 x i32> %15114, %15102
  %15116 = shufflevector <2 x i64> %15062, <2 x i64> undef, <1 x i32> <i32 1>
  %15117 = bitcast <1 x i64> %15116 to <4 x i16>
  %15118 = shufflevector <2 x i64> %15066, <2 x i64> undef, <1 x i32> <i32 1>
  %15119 = shufflevector <2 x i64> %15069, <2 x i64> undef, <1 x i32> <i32 1>
  %15120 = bitcast <1 x i64> %15119 to <4 x i16>
  %15121 = shufflevector <2 x i64> %15073, <2 x i64> undef, <1 x i32> <i32 1>
  %15122 = shufflevector <2 x i64> %15076, <2 x i64> undef, <1 x i32> <i32 1>
  %15123 = bitcast <1 x i64> %15122 to <4 x i16>
  %15124 = shufflevector <2 x i64> %15080, <2 x i64> undef, <1 x i32> <i32 1>
  %15125 = shufflevector <2 x i64> %15083, <2 x i64> undef, <1 x i32> <i32 1>
  %15126 = bitcast <1 x i64> %15125 to <4 x i16>
  %15127 = shufflevector <2 x i64> %15087, <2 x i64> undef, <1 x i32> <i32 1>
  %15128 = shufflevector <2 x i64> %15090, <2 x i64> undef, <1 x i32> <i32 1>
  %15129 = bitcast <1 x i64> %15128 to <4 x i16>
  %.cast2497 = bitcast <1 x i64> %15118 to <4 x i16>
  %15130 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2497, <4 x i16> %11742) #11
  %15131 = sext <4 x i16> %15117 to <4 x i32>
  %15132 = mul nsw <4 x i32> %15131, %11764
  %.cast2500 = bitcast <1 x i64> %15121 to <4 x i16>
  %15133 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2500, <4 x i16> %11747) #11
  %15134 = sext <4 x i16> %15120 to <4 x i32>
  %15135 = mul nsw <4 x i32> %15134, %11768
  %.cast2503 = bitcast <1 x i64> %15124 to <4 x i16>
  %15136 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2503, <4 x i16> %11752) #11
  %15137 = sext <4 x i16> %15123 to <4 x i32>
  %15138 = mul nsw <4 x i32> %15137, %11772
  %.cast2506 = bitcast <1 x i64> %15127 to <4 x i16>
  %15139 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2506, <4 x i16> %11757) #11
  %15140 = sext <4 x i16> %15126 to <4 x i32>
  %15141 = mul nsw <4 x i32> %15140, %11776
  %15142 = sext <4 x i16> %15129 to <4 x i32>
  %15143 = mul nsw <4 x i32> %15142, %11778
  %15144 = add <4 x i32> %15132, %11563
  %15145 = add <4 x i32> %15144, %15135
  %15146 = add <4 x i32> %15145, %15138
  %15147 = add <4 x i32> %15146, %15141
  %15148 = add <4 x i32> %15147, %15143
  %15149 = add <4 x i32> %15148, %15130
  %15150 = add <4 x i32> %15149, %15133
  %15151 = add <4 x i32> %15150, %15136
  %15152 = add <4 x i32> %15151, %15139
  %15153 = zext <8 x i8> %t3613 to <8 x i16>
  %15154 = bitcast <8 x i16> %15153 to <2 x i64>
  %15155 = shufflevector <2 x i64> %15154, <2 x i64> undef, <1 x i32> zeroinitializer
  %15156 = bitcast <1 x i64> %15155 to <4 x i16>
  %15157 = zext <8 x i8> %t3614 to <8 x i16>
  %15158 = bitcast <8 x i16> %15157 to <2 x i64>
  %15159 = shufflevector <2 x i64> %15158, <2 x i64> undef, <1 x i32> zeroinitializer
  %15160 = zext <8 x i8> %t3615 to <8 x i16>
  %15161 = bitcast <8 x i16> %15160 to <2 x i64>
  %15162 = shufflevector <2 x i64> %15161, <2 x i64> undef, <1 x i32> zeroinitializer
  %15163 = bitcast <1 x i64> %15162 to <4 x i16>
  %15164 = zext <8 x i8> %t3616 to <8 x i16>
  %15165 = bitcast <8 x i16> %15164 to <2 x i64>
  %15166 = shufflevector <2 x i64> %15165, <2 x i64> undef, <1 x i32> zeroinitializer
  %15167 = zext <8 x i8> %t3617 to <8 x i16>
  %15168 = bitcast <8 x i16> %15167 to <2 x i64>
  %15169 = shufflevector <2 x i64> %15168, <2 x i64> undef, <1 x i32> zeroinitializer
  %15170 = bitcast <1 x i64> %15169 to <4 x i16>
  %15171 = zext <8 x i8> %t3618 to <8 x i16>
  %15172 = bitcast <8 x i16> %15171 to <2 x i64>
  %15173 = shufflevector <2 x i64> %15172, <2 x i64> undef, <1 x i32> zeroinitializer
  %15174 = zext <8 x i8> %t3619 to <8 x i16>
  %15175 = bitcast <8 x i16> %15174 to <2 x i64>
  %15176 = shufflevector <2 x i64> %15175, <2 x i64> undef, <1 x i32> zeroinitializer
  %15177 = bitcast <1 x i64> %15176 to <4 x i16>
  %15178 = zext <8 x i8> %t3620 to <8 x i16>
  %15179 = bitcast <8 x i16> %15178 to <2 x i64>
  %15180 = shufflevector <2 x i64> %15179, <2 x i64> undef, <1 x i32> zeroinitializer
  %15181 = zext <8 x i8> %t3621 to <8 x i16>
  %15182 = bitcast <8 x i16> %15181 to <2 x i64>
  %15183 = shufflevector <2 x i64> %15182, <2 x i64> undef, <1 x i32> zeroinitializer
  %15184 = bitcast <1 x i64> %15183 to <4 x i16>
  %.cast2509 = bitcast <1 x i64> %15159 to <4 x i16>
  %15185 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2509, <4 x i16> %11795) #11
  %15186 = sext <4 x i16> %15156 to <4 x i32>
  %15187 = mul nsw <4 x i32> %15186, %11833
  %.cast2512 = bitcast <1 x i64> %15166 to <4 x i16>
  %15188 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2512, <4 x i16> %11804) #11
  %15189 = sext <4 x i16> %15163 to <4 x i32>
  %15190 = mul nsw <4 x i32> %15189, %11837
  %.cast2515 = bitcast <1 x i64> %15173 to <4 x i16>
  %15191 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2515, <4 x i16> %11813) #11
  %15192 = sext <4 x i16> %15170 to <4 x i32>
  %15193 = mul nsw <4 x i32> %15192, %11841
  %.cast2518 = bitcast <1 x i64> %15180 to <4 x i16>
  %15194 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2518, <4 x i16> %11822) #11
  %15195 = sext <4 x i16> %15177 to <4 x i32>
  %15196 = mul nsw <4 x i32> %15195, %11845
  %15197 = sext <4 x i16> %15184 to <4 x i32>
  %15198 = mul nsw <4 x i32> %15197, %11847
  %15199 = add <4 x i32> %15187, %11564
  %15200 = add <4 x i32> %15199, %15190
  %15201 = add <4 x i32> %15200, %15193
  %15202 = add <4 x i32> %15201, %15196
  %15203 = add <4 x i32> %15202, %15198
  %15204 = add <4 x i32> %15203, %15185
  %15205 = add <4 x i32> %15204, %15188
  %15206 = add <4 x i32> %15205, %15191
  %15207 = add <4 x i32> %15206, %15194
  %15208 = shufflevector <2 x i64> %15154, <2 x i64> undef, <1 x i32> <i32 1>
  %15209 = bitcast <1 x i64> %15208 to <4 x i16>
  %15210 = shufflevector <2 x i64> %15158, <2 x i64> undef, <1 x i32> <i32 1>
  %15211 = shufflevector <2 x i64> %15161, <2 x i64> undef, <1 x i32> <i32 1>
  %15212 = bitcast <1 x i64> %15211 to <4 x i16>
  %15213 = shufflevector <2 x i64> %15165, <2 x i64> undef, <1 x i32> <i32 1>
  %15214 = shufflevector <2 x i64> %15168, <2 x i64> undef, <1 x i32> <i32 1>
  %15215 = bitcast <1 x i64> %15214 to <4 x i16>
  %15216 = shufflevector <2 x i64> %15172, <2 x i64> undef, <1 x i32> <i32 1>
  %15217 = shufflevector <2 x i64> %15175, <2 x i64> undef, <1 x i32> <i32 1>
  %15218 = bitcast <1 x i64> %15217 to <4 x i16>
  %15219 = shufflevector <2 x i64> %15179, <2 x i64> undef, <1 x i32> <i32 1>
  %15220 = shufflevector <2 x i64> %15182, <2 x i64> undef, <1 x i32> <i32 1>
  %15221 = bitcast <1 x i64> %15220 to <4 x i16>
  %.cast2521 = bitcast <1 x i64> %15210 to <4 x i16>
  %15222 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2521, <4 x i16> %11862) #11
  %15223 = sext <4 x i16> %15209 to <4 x i32>
  %15224 = mul nsw <4 x i32> %15223, %11884
  %.cast2524 = bitcast <1 x i64> %15213 to <4 x i16>
  %15225 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2524, <4 x i16> %11867) #11
  %15226 = sext <4 x i16> %15212 to <4 x i32>
  %15227 = mul nsw <4 x i32> %15226, %11888
  %.cast2527 = bitcast <1 x i64> %15216 to <4 x i16>
  %15228 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2527, <4 x i16> %11872) #11
  %15229 = sext <4 x i16> %15215 to <4 x i32>
  %15230 = mul nsw <4 x i32> %15229, %11892
  %.cast2530 = bitcast <1 x i64> %15219 to <4 x i16>
  %15231 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2530, <4 x i16> %11877) #11
  %15232 = sext <4 x i16> %15218 to <4 x i32>
  %15233 = mul nsw <4 x i32> %15232, %11896
  %15234 = sext <4 x i16> %15221 to <4 x i32>
  %15235 = mul nsw <4 x i32> %15234, %11898
  %15236 = add <4 x i32> %15224, %11565
  %15237 = add <4 x i32> %15236, %15227
  %15238 = add <4 x i32> %15237, %15230
  %15239 = add <4 x i32> %15238, %15233
  %15240 = add <4 x i32> %15239, %15235
  %15241 = add <4 x i32> %15240, %15222
  %15242 = add <4 x i32> %15241, %15225
  %15243 = add <4 x i32> %15242, %15228
  %15244 = add <4 x i32> %15243, %15231
  %t3623 = add nsw i32 %t3286, %t2420560
  %15245 = sext i32 %t3623 to i64
  %15246 = shl nsw i64 %15245, 4
  %15247 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15246
  %15248 = bitcast i8* %15247 to <8 x i8>*
  %t3624 = load <8 x i8>, <8 x i8>* %15248, align 16, !tbaa !438
  %t3625 = add nsw i32 %t3286, %t2416559
  %15249 = sext i32 %t3625 to i64
  %15250 = shl nsw i64 %15249, 4
  %15251 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15250
  %15252 = bitcast i8* %15251 to <8 x i8>*
  %t3626 = load <8 x i8>, <8 x i8>* %15252, align 16, !tbaa !438
  %t3627 = add nsw i32 %t3286, %t2412540
  %15253 = sext i32 %t3627 to i64
  %15254 = shl nsw i64 %15253, 4
  %15255 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15254
  %15256 = bitcast i8* %15255 to <8 x i8>*
  %t3628 = load <8 x i8>, <8 x i8>* %15256, align 16, !tbaa !438
  %t3629 = add nsw i32 %t3286, %t2407558
  %15257 = sext i32 %t3629 to i64
  %15258 = shl nsw i64 %15257, 4
  %15259 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15258
  %15260 = bitcast i8* %15259 to <8 x i8>*
  %t3630 = load <8 x i8>, <8 x i8>* %15260, align 16, !tbaa !438
  %t3631 = add nsw i32 %t3286, %t2403557
  %15261 = sext i32 %t3631 to i64
  %15262 = shl nsw i64 %15261, 4
  %15263 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15262
  %15264 = bitcast i8* %15263 to <8 x i8>*
  %t3632 = load <8 x i8>, <8 x i8>* %15264, align 16, !tbaa !438
  %t3633 = add nsw i32 %t3286, %t2399539
  %15265 = sext i32 %t3633 to i64
  %15266 = shl nsw i64 %15265, 4
  %15267 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15266
  %15268 = bitcast i8* %15267 to <8 x i8>*
  %t3634 = load <8 x i8>, <8 x i8>* %15268, align 16, !tbaa !438
  %t3635 = add nsw i32 %t3286, %t2394548
  %15269 = sext i32 %t3635 to i64
  %15270 = shl nsw i64 %15269, 4
  %15271 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15270
  %15272 = bitcast i8* %15271 to <8 x i8>*
  %t3636 = load <8 x i8>, <8 x i8>* %15272, align 16, !tbaa !438
  %t3637 = add nsw i32 %t3286, %t2390547
  %15273 = sext i32 %t3637 to i64
  %15274 = shl nsw i64 %15273, 4
  %15275 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15274
  %15276 = bitcast i8* %15275 to <8 x i8>*
  %t3638 = load <8 x i8>, <8 x i8>* %15276, align 16, !tbaa !438
  %t3639 = add nsw i32 %t3286, %t2386534
  %15277 = sext i32 %t3639 to i64
  %15278 = shl nsw i64 %15277, 4
  %15279 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15278
  %15280 = bitcast i8* %15279 to <8 x i8>*
  %t3640 = load <8 x i8>, <8 x i8>* %15280, align 16, !tbaa !438
  %15281 = getelementptr inbounds i8, i8* %15247, i64 8
  %15282 = bitcast i8* %15281 to <8 x i8>*
  %t3641 = load <8 x i8>, <8 x i8>* %15282, align 8, !tbaa !438
  %15283 = getelementptr inbounds i8, i8* %15251, i64 8
  %15284 = bitcast i8* %15283 to <8 x i8>*
  %t3642 = load <8 x i8>, <8 x i8>* %15284, align 8, !tbaa !438
  %15285 = getelementptr inbounds i8, i8* %15255, i64 8
  %15286 = bitcast i8* %15285 to <8 x i8>*
  %t3643 = load <8 x i8>, <8 x i8>* %15286, align 8, !tbaa !438
  %15287 = getelementptr inbounds i8, i8* %15259, i64 8
  %15288 = bitcast i8* %15287 to <8 x i8>*
  %t3644 = load <8 x i8>, <8 x i8>* %15288, align 8, !tbaa !438
  %15289 = getelementptr inbounds i8, i8* %15263, i64 8
  %15290 = bitcast i8* %15289 to <8 x i8>*
  %t3645 = load <8 x i8>, <8 x i8>* %15290, align 8, !tbaa !438
  %15291 = getelementptr inbounds i8, i8* %15267, i64 8
  %15292 = bitcast i8* %15291 to <8 x i8>*
  %t3646 = load <8 x i8>, <8 x i8>* %15292, align 8, !tbaa !438
  %15293 = getelementptr inbounds i8, i8* %15271, i64 8
  %15294 = bitcast i8* %15293 to <8 x i8>*
  %t3647 = load <8 x i8>, <8 x i8>* %15294, align 8, !tbaa !438
  %15295 = getelementptr inbounds i8, i8* %15275, i64 8
  %15296 = bitcast i8* %15295 to <8 x i8>*
  %t3648 = load <8 x i8>, <8 x i8>* %15296, align 8, !tbaa !438
  %15297 = getelementptr inbounds i8, i8* %15279, i64 8
  %15298 = bitcast i8* %15297 to <8 x i8>*
  %t3649 = load <8 x i8>, <8 x i8>* %15298, align 8, !tbaa !438
  %15299 = zext <8 x i8> %t3624 to <8 x i16>
  %15300 = bitcast <8 x i16> %15299 to <2 x i64>
  %15301 = shufflevector <2 x i64> %15300, <2 x i64> undef, <1 x i32> zeroinitializer
  %15302 = bitcast <1 x i64> %15301 to <4 x i16>
  %15303 = zext <8 x i8> %t3626 to <8 x i16>
  %15304 = bitcast <8 x i16> %15303 to <2 x i64>
  %15305 = shufflevector <2 x i64> %15304, <2 x i64> undef, <1 x i32> zeroinitializer
  %15306 = zext <8 x i8> %t3628 to <8 x i16>
  %15307 = bitcast <8 x i16> %15306 to <2 x i64>
  %15308 = shufflevector <2 x i64> %15307, <2 x i64> undef, <1 x i32> zeroinitializer
  %15309 = bitcast <1 x i64> %15308 to <4 x i16>
  %15310 = zext <8 x i8> %t3630 to <8 x i16>
  %15311 = bitcast <8 x i16> %15310 to <2 x i64>
  %15312 = shufflevector <2 x i64> %15311, <2 x i64> undef, <1 x i32> zeroinitializer
  %15313 = zext <8 x i8> %t3632 to <8 x i16>
  %15314 = bitcast <8 x i16> %15313 to <2 x i64>
  %15315 = shufflevector <2 x i64> %15314, <2 x i64> undef, <1 x i32> zeroinitializer
  %15316 = bitcast <1 x i64> %15315 to <4 x i16>
  %15317 = zext <8 x i8> %t3634 to <8 x i16>
  %15318 = bitcast <8 x i16> %15317 to <2 x i64>
  %15319 = shufflevector <2 x i64> %15318, <2 x i64> undef, <1 x i32> zeroinitializer
  %15320 = zext <8 x i8> %t3636 to <8 x i16>
  %15321 = bitcast <8 x i16> %15320 to <2 x i64>
  %15322 = shufflevector <2 x i64> %15321, <2 x i64> undef, <1 x i32> zeroinitializer
  %15323 = bitcast <1 x i64> %15322 to <4 x i16>
  %15324 = zext <8 x i8> %t3638 to <8 x i16>
  %15325 = bitcast <8 x i16> %15324 to <2 x i64>
  %15326 = shufflevector <2 x i64> %15325, <2 x i64> undef, <1 x i32> zeroinitializer
  %15327 = zext <8 x i8> %t3640 to <8 x i16>
  %15328 = bitcast <8 x i16> %15327 to <2 x i64>
  %15329 = shufflevector <2 x i64> %15328, <2 x i64> undef, <1 x i32> zeroinitializer
  %15330 = bitcast <1 x i64> %15329 to <4 x i16>
  %.cast2533 = bitcast <1 x i64> %15305 to <4 x i16>
  %15331 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2533, <4 x i16> %11675) #11
  %15332 = sext <4 x i16> %15302 to <4 x i32>
  %15333 = mul nsw <4 x i32> %15332, %11713
  %.cast2536 = bitcast <1 x i64> %15312 to <4 x i16>
  %15334 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2536, <4 x i16> %11684) #11
  %15335 = sext <4 x i16> %15309 to <4 x i32>
  %15336 = mul nsw <4 x i32> %15335, %11717
  %.cast2539 = bitcast <1 x i64> %15319 to <4 x i16>
  %15337 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2539, <4 x i16> %11693) #11
  %15338 = sext <4 x i16> %15316 to <4 x i32>
  %15339 = mul nsw <4 x i32> %15338, %11721
  %.cast2542 = bitcast <1 x i64> %15326 to <4 x i16>
  %15340 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2542, <4 x i16> %11702) #11
  %15341 = sext <4 x i16> %15323 to <4 x i32>
  %15342 = mul nsw <4 x i32> %15341, %11725
  %15343 = sext <4 x i16> %15330 to <4 x i32>
  %15344 = mul nsw <4 x i32> %15343, %11727
  %15345 = add <4 x i32> %15333, %11562
  %15346 = add <4 x i32> %15345, %15336
  %15347 = add <4 x i32> %15346, %15339
  %15348 = add <4 x i32> %15347, %15342
  %15349 = add <4 x i32> %15348, %15344
  %15350 = add <4 x i32> %15349, %15331
  %15351 = add <4 x i32> %15350, %15334
  %15352 = add <4 x i32> %15351, %15337
  %15353 = add <4 x i32> %15352, %15340
  %15354 = shufflevector <2 x i64> %15300, <2 x i64> undef, <1 x i32> <i32 1>
  %15355 = bitcast <1 x i64> %15354 to <4 x i16>
  %15356 = shufflevector <2 x i64> %15304, <2 x i64> undef, <1 x i32> <i32 1>
  %15357 = shufflevector <2 x i64> %15307, <2 x i64> undef, <1 x i32> <i32 1>
  %15358 = bitcast <1 x i64> %15357 to <4 x i16>
  %15359 = shufflevector <2 x i64> %15311, <2 x i64> undef, <1 x i32> <i32 1>
  %15360 = shufflevector <2 x i64> %15314, <2 x i64> undef, <1 x i32> <i32 1>
  %15361 = bitcast <1 x i64> %15360 to <4 x i16>
  %15362 = shufflevector <2 x i64> %15318, <2 x i64> undef, <1 x i32> <i32 1>
  %15363 = shufflevector <2 x i64> %15321, <2 x i64> undef, <1 x i32> <i32 1>
  %15364 = bitcast <1 x i64> %15363 to <4 x i16>
  %15365 = shufflevector <2 x i64> %15325, <2 x i64> undef, <1 x i32> <i32 1>
  %15366 = shufflevector <2 x i64> %15328, <2 x i64> undef, <1 x i32> <i32 1>
  %15367 = bitcast <1 x i64> %15366 to <4 x i16>
  %.cast2545 = bitcast <1 x i64> %15356 to <4 x i16>
  %15368 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2545, <4 x i16> %11742) #11
  %15369 = sext <4 x i16> %15355 to <4 x i32>
  %15370 = mul nsw <4 x i32> %15369, %11764
  %.cast2548 = bitcast <1 x i64> %15359 to <4 x i16>
  %15371 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2548, <4 x i16> %11747) #11
  %15372 = sext <4 x i16> %15358 to <4 x i32>
  %15373 = mul nsw <4 x i32> %15372, %11768
  %.cast2551 = bitcast <1 x i64> %15362 to <4 x i16>
  %15374 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2551, <4 x i16> %11752) #11
  %15375 = sext <4 x i16> %15361 to <4 x i32>
  %15376 = mul nsw <4 x i32> %15375, %11772
  %.cast2554 = bitcast <1 x i64> %15365 to <4 x i16>
  %15377 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2554, <4 x i16> %11757) #11
  %15378 = sext <4 x i16> %15364 to <4 x i32>
  %15379 = mul nsw <4 x i32> %15378, %11776
  %15380 = sext <4 x i16> %15367 to <4 x i32>
  %15381 = mul nsw <4 x i32> %15380, %11778
  %15382 = add <4 x i32> %15370, %11563
  %15383 = add <4 x i32> %15382, %15373
  %15384 = add <4 x i32> %15383, %15376
  %15385 = add <4 x i32> %15384, %15379
  %15386 = add <4 x i32> %15385, %15381
  %15387 = add <4 x i32> %15386, %15368
  %15388 = add <4 x i32> %15387, %15371
  %15389 = add <4 x i32> %15388, %15374
  %15390 = add <4 x i32> %15389, %15377
  %15391 = zext <8 x i8> %t3641 to <8 x i16>
  %15392 = bitcast <8 x i16> %15391 to <2 x i64>
  %15393 = shufflevector <2 x i64> %15392, <2 x i64> undef, <1 x i32> zeroinitializer
  %15394 = bitcast <1 x i64> %15393 to <4 x i16>
  %15395 = zext <8 x i8> %t3642 to <8 x i16>
  %15396 = bitcast <8 x i16> %15395 to <2 x i64>
  %15397 = shufflevector <2 x i64> %15396, <2 x i64> undef, <1 x i32> zeroinitializer
  %15398 = zext <8 x i8> %t3643 to <8 x i16>
  %15399 = bitcast <8 x i16> %15398 to <2 x i64>
  %15400 = shufflevector <2 x i64> %15399, <2 x i64> undef, <1 x i32> zeroinitializer
  %15401 = bitcast <1 x i64> %15400 to <4 x i16>
  %15402 = zext <8 x i8> %t3644 to <8 x i16>
  %15403 = bitcast <8 x i16> %15402 to <2 x i64>
  %15404 = shufflevector <2 x i64> %15403, <2 x i64> undef, <1 x i32> zeroinitializer
  %15405 = zext <8 x i8> %t3645 to <8 x i16>
  %15406 = bitcast <8 x i16> %15405 to <2 x i64>
  %15407 = shufflevector <2 x i64> %15406, <2 x i64> undef, <1 x i32> zeroinitializer
  %15408 = bitcast <1 x i64> %15407 to <4 x i16>
  %15409 = zext <8 x i8> %t3646 to <8 x i16>
  %15410 = bitcast <8 x i16> %15409 to <2 x i64>
  %15411 = shufflevector <2 x i64> %15410, <2 x i64> undef, <1 x i32> zeroinitializer
  %15412 = zext <8 x i8> %t3647 to <8 x i16>
  %15413 = bitcast <8 x i16> %15412 to <2 x i64>
  %15414 = shufflevector <2 x i64> %15413, <2 x i64> undef, <1 x i32> zeroinitializer
  %15415 = bitcast <1 x i64> %15414 to <4 x i16>
  %15416 = zext <8 x i8> %t3648 to <8 x i16>
  %15417 = bitcast <8 x i16> %15416 to <2 x i64>
  %15418 = shufflevector <2 x i64> %15417, <2 x i64> undef, <1 x i32> zeroinitializer
  %15419 = zext <8 x i8> %t3649 to <8 x i16>
  %15420 = bitcast <8 x i16> %15419 to <2 x i64>
  %15421 = shufflevector <2 x i64> %15420, <2 x i64> undef, <1 x i32> zeroinitializer
  %15422 = bitcast <1 x i64> %15421 to <4 x i16>
  %.cast2557 = bitcast <1 x i64> %15397 to <4 x i16>
  %15423 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2557, <4 x i16> %11795) #11
  %15424 = sext <4 x i16> %15394 to <4 x i32>
  %15425 = mul nsw <4 x i32> %15424, %11833
  %.cast2560 = bitcast <1 x i64> %15404 to <4 x i16>
  %15426 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2560, <4 x i16> %11804) #11
  %15427 = sext <4 x i16> %15401 to <4 x i32>
  %15428 = mul nsw <4 x i32> %15427, %11837
  %.cast2563 = bitcast <1 x i64> %15411 to <4 x i16>
  %15429 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2563, <4 x i16> %11813) #11
  %15430 = sext <4 x i16> %15408 to <4 x i32>
  %15431 = mul nsw <4 x i32> %15430, %11841
  %.cast2566 = bitcast <1 x i64> %15418 to <4 x i16>
  %15432 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2566, <4 x i16> %11822) #11
  %15433 = sext <4 x i16> %15415 to <4 x i32>
  %15434 = mul nsw <4 x i32> %15433, %11845
  %15435 = sext <4 x i16> %15422 to <4 x i32>
  %15436 = mul nsw <4 x i32> %15435, %11847
  %15437 = add <4 x i32> %15425, %11564
  %15438 = add <4 x i32> %15437, %15428
  %15439 = add <4 x i32> %15438, %15431
  %15440 = add <4 x i32> %15439, %15434
  %15441 = add <4 x i32> %15440, %15436
  %15442 = add <4 x i32> %15441, %15423
  %15443 = add <4 x i32> %15442, %15426
  %15444 = add <4 x i32> %15443, %15429
  %15445 = add <4 x i32> %15444, %15432
  %15446 = shufflevector <2 x i64> %15392, <2 x i64> undef, <1 x i32> <i32 1>
  %15447 = bitcast <1 x i64> %15446 to <4 x i16>
  %15448 = shufflevector <2 x i64> %15396, <2 x i64> undef, <1 x i32> <i32 1>
  %15449 = shufflevector <2 x i64> %15399, <2 x i64> undef, <1 x i32> <i32 1>
  %15450 = bitcast <1 x i64> %15449 to <4 x i16>
  %15451 = shufflevector <2 x i64> %15403, <2 x i64> undef, <1 x i32> <i32 1>
  %15452 = shufflevector <2 x i64> %15406, <2 x i64> undef, <1 x i32> <i32 1>
  %15453 = bitcast <1 x i64> %15452 to <4 x i16>
  %15454 = shufflevector <2 x i64> %15410, <2 x i64> undef, <1 x i32> <i32 1>
  %15455 = shufflevector <2 x i64> %15413, <2 x i64> undef, <1 x i32> <i32 1>
  %15456 = bitcast <1 x i64> %15455 to <4 x i16>
  %15457 = shufflevector <2 x i64> %15417, <2 x i64> undef, <1 x i32> <i32 1>
  %15458 = shufflevector <2 x i64> %15420, <2 x i64> undef, <1 x i32> <i32 1>
  %15459 = bitcast <1 x i64> %15458 to <4 x i16>
  %.cast2569 = bitcast <1 x i64> %15448 to <4 x i16>
  %15460 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2569, <4 x i16> %11862) #11
  %15461 = sext <4 x i16> %15447 to <4 x i32>
  %15462 = mul nsw <4 x i32> %15461, %11884
  %.cast2572 = bitcast <1 x i64> %15451 to <4 x i16>
  %15463 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2572, <4 x i16> %11867) #11
  %15464 = sext <4 x i16> %15450 to <4 x i32>
  %15465 = mul nsw <4 x i32> %15464, %11888
  %.cast2575 = bitcast <1 x i64> %15454 to <4 x i16>
  %15466 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2575, <4 x i16> %11872) #11
  %15467 = sext <4 x i16> %15453 to <4 x i32>
  %15468 = mul nsw <4 x i32> %15467, %11892
  %.cast2578 = bitcast <1 x i64> %15457 to <4 x i16>
  %15469 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast2578, <4 x i16> %11877) #11
  %15470 = sext <4 x i16> %15456 to <4 x i32>
  %15471 = mul nsw <4 x i32> %15470, %11896
  %15472 = sext <4 x i16> %15459 to <4 x i32>
  %15473 = mul nsw <4 x i32> %15472, %11898
  %15474 = add <4 x i32> %15462, %11565
  %15475 = add <4 x i32> %15474, %15465
  %15476 = add <4 x i32> %15475, %15468
  %15477 = add <4 x i32> %15476, %15471
  %15478 = add <4 x i32> %15477, %15473
  %15479 = add <4 x i32> %15478, %15460
  %15480 = add <4 x i32> %15479, %15463
  %15481 = add <4 x i32> %15480, %15466
  %15482 = add <4 x i32> %15481, %15469
  br label %"consume convolved613"

next_bb579:                                       ; preds = %"for output.s0.x.xo573"
  br i1 %11140, label %"for convolved.s1.r19$y585.preheader", label %"consume convolved613", !prof !391

"for convolved.s1.r19$y585.preheader":            ; preds = %next_bb579
  %15483 = add nsw i32 %11615, 1
  %15484 = mul nsw i32 %15483, %stride_x
  %15485 = add nsw i32 %11615, 2
  %15486 = mul nsw i32 %15485, %stride_x
  %15487 = add nsw i32 %11615, 3
  %15488 = mul nsw i32 %15487, %stride_x
  %15489 = mul nsw i32 %11615, %stride_x
  %15490 = sub nsw i32 %15488, %11217
  %15491 = sub nsw i32 %15486, %11217
  %15492 = sub nsw i32 %15484, %11217
  %15493 = sub nsw i32 %15489, %11217
  br i1 %11138, label %"for convolved.s1.r19$y585.us", label %"consume convolved613", !prof !391

"for convolved.s1.r19$y585.us":                   ; preds = %"for convolved.s1.r19$y585.preheader", %"end for convolved.s1.r19$x611.loopexit.us"
  %indvars.iv6460 = phi i64 [ %indvars.iv.next6461, %"end for convolved.s1.r19$x611.loopexit.us" ], [ 0, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3624.8.us = phi <4 x i32> [ %16007, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3568.8.us = phi <4 x i32> [ %16002, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3512.8.us = phi <4 x i32> [ %15995, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3456.8.us = phi <4 x i32> [ %15990, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3400.8.us = phi <4 x i32> [ %15977, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3344.8.us = phi <4 x i32> [ %15972, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3288.8.us = phi <4 x i32> [ %15965, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3232.8.us = phi <4 x i32> [ %15960, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3176.8.us = phi <4 x i32> [ %15947, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3120.8.us = phi <4 x i32> [ %15942, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3064.8.us = phi <4 x i32> [ %15935, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.3008.8.us = phi <4 x i32> [ %15930, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2952.8.us = phi <4 x i32> [ %15917, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2896.8.us = phi <4 x i32> [ %15912, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2840.8.us = phi <4 x i32> [ %15905, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2784.8.us = phi <4 x i32> [ %15900, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2728.8.us = phi <4 x i32> [ %15887, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2672.8.us = phi <4 x i32> [ %15882, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2616.8.us = phi <4 x i32> [ %15875, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2560.8.us = phi <4 x i32> [ %15870, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2504.8.us = phi <4 x i32> [ %15857, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2448.8.us = phi <4 x i32> [ %15852, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2392.8.us = phi <4 x i32> [ %15845, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2336.8.us = phi <4 x i32> [ %15840, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2280.8.us = phi <4 x i32> [ %15827, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2224.8.us = phi <4 x i32> [ %15822, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2168.8.us = phi <4 x i32> [ %15815, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2112.8.us = phi <4 x i32> [ %15810, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2056.8.us = phi <4 x i32> [ %15797, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.2000.8.us = phi <4 x i32> [ %15792, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1944.8.us = phi <4 x i32> [ %15785, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1888.8.us = phi <4 x i32> [ %15780, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1832.8.us = phi <4 x i32> [ %15767, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1776.8.us = phi <4 x i32> [ %15762, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1720.8.us = phi <4 x i32> [ %15755, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1664.8.us = phi <4 x i32> [ %15750, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1608.8.us = phi <4 x i32> [ %15737, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1552.8.us = phi <4 x i32> [ %15732, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1496.8.us = phi <4 x i32> [ %15725, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1440.8.us = phi <4 x i32> [ %15720, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1384.8.us = phi <4 x i32> [ %15707, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1328.8.us = phi <4 x i32> [ %15702, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1272.8.us = phi <4 x i32> [ %15695, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1216.8.us = phi <4 x i32> [ %15690, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1160.8.us = phi <4 x i32> [ %15677, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1104.8.us = phi <4 x i32> [ %15672, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.1048.8.us = phi <4 x i32> [ %15665, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.992.8.us = phi <4 x i32> [ %15660, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.930.12.us = phi <4 x i32> [ %15647, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.868.12.us = phi <4 x i32> [ %15642, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.806.12.us = phi <4 x i32> [ %15635, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.744.12.us = phi <4 x i32> [ %15630, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.682.12.us = phi <4 x i32> [ %15617, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.620.12.us = phi <4 x i32> [ %15612, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.558.12.us = phi <4 x i32> [ %15605, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.496.12.us = phi <4 x i32> [ %15600, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.434.12.us = phi <4 x i32> [ %15587, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.372.12.us = phi <4 x i32> [ %15582, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.310.12.us = phi <4 x i32> [ %15575, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.248.12.us = phi <4 x i32> [ %15570, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.186.12.us = phi <4 x i32> [ %15557, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11565, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.124.12.us = phi <4 x i32> [ %15548, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11564, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.62.12.us = phi <4 x i32> [ %15537, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11563, %"for convolved.s1.r19$y585.preheader" ]
  %convolved944.sroa.0.12.us = phi <4 x i32> [ %15528, %"end for convolved.s1.r19$x611.loopexit.us" ], [ %11562, %"for convolved.s1.r19$y585.preheader" ]
  %15494 = mul i64 %indvars.iv6460, %11253
  %15495 = mul nsw i64 %indvars.iv6460, %11243
  %15496 = trunc i64 %15494 to i32
  %15497 = add i32 %11607, %15496
  %15498 = mul i32 %15497, %11223
  %15499 = trunc i64 %15494 to i32
  %15500 = add i32 %11604, %15499
  %15501 = mul i32 %15500, %11223
  %15502 = trunc i64 %15494 to i32
  %15503 = add i32 %11605, %15502
  %15504 = mul i32 %15503, %11223
  %15505 = trunc i64 %15494 to i32
  %15506 = add i32 %11606, %15505
  %15507 = mul i32 %15506, %11223
  br label %"for convolved.s1.r19$x610.us"

"for convolved.s1.r19$x610.us":                   ; preds = %"for convolved.s1.r19$y585.us", %"for convolved.s1.r19$x610.us"
  %indvars.iv6457 = phi i64 [ 0, %"for convolved.s1.r19$y585.us" ], [ %indvars.iv.next6458, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3624.9.us = phi <4 x i32> [ %convolved944.sroa.3624.8.us, %"for convolved.s1.r19$y585.us" ], [ %16007, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3568.9.us = phi <4 x i32> [ %convolved944.sroa.3568.8.us, %"for convolved.s1.r19$y585.us" ], [ %16002, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3512.9.us = phi <4 x i32> [ %convolved944.sroa.3512.8.us, %"for convolved.s1.r19$y585.us" ], [ %15995, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3456.9.us = phi <4 x i32> [ %convolved944.sroa.3456.8.us, %"for convolved.s1.r19$y585.us" ], [ %15990, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3400.9.us = phi <4 x i32> [ %convolved944.sroa.3400.8.us, %"for convolved.s1.r19$y585.us" ], [ %15977, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3344.9.us = phi <4 x i32> [ %convolved944.sroa.3344.8.us, %"for convolved.s1.r19$y585.us" ], [ %15972, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3288.9.us = phi <4 x i32> [ %convolved944.sroa.3288.8.us, %"for convolved.s1.r19$y585.us" ], [ %15965, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3232.9.us = phi <4 x i32> [ %convolved944.sroa.3232.8.us, %"for convolved.s1.r19$y585.us" ], [ %15960, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3176.9.us = phi <4 x i32> [ %convolved944.sroa.3176.8.us, %"for convolved.s1.r19$y585.us" ], [ %15947, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3120.9.us = phi <4 x i32> [ %convolved944.sroa.3120.8.us, %"for convolved.s1.r19$y585.us" ], [ %15942, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3064.9.us = phi <4 x i32> [ %convolved944.sroa.3064.8.us, %"for convolved.s1.r19$y585.us" ], [ %15935, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.3008.9.us = phi <4 x i32> [ %convolved944.sroa.3008.8.us, %"for convolved.s1.r19$y585.us" ], [ %15930, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2952.9.us = phi <4 x i32> [ %convolved944.sroa.2952.8.us, %"for convolved.s1.r19$y585.us" ], [ %15917, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2896.9.us = phi <4 x i32> [ %convolved944.sroa.2896.8.us, %"for convolved.s1.r19$y585.us" ], [ %15912, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2840.9.us = phi <4 x i32> [ %convolved944.sroa.2840.8.us, %"for convolved.s1.r19$y585.us" ], [ %15905, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2784.9.us = phi <4 x i32> [ %convolved944.sroa.2784.8.us, %"for convolved.s1.r19$y585.us" ], [ %15900, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2728.9.us = phi <4 x i32> [ %convolved944.sroa.2728.8.us, %"for convolved.s1.r19$y585.us" ], [ %15887, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2672.9.us = phi <4 x i32> [ %convolved944.sroa.2672.8.us, %"for convolved.s1.r19$y585.us" ], [ %15882, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2616.9.us = phi <4 x i32> [ %convolved944.sroa.2616.8.us, %"for convolved.s1.r19$y585.us" ], [ %15875, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2560.9.us = phi <4 x i32> [ %convolved944.sroa.2560.8.us, %"for convolved.s1.r19$y585.us" ], [ %15870, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2504.9.us = phi <4 x i32> [ %convolved944.sroa.2504.8.us, %"for convolved.s1.r19$y585.us" ], [ %15857, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2448.9.us = phi <4 x i32> [ %convolved944.sroa.2448.8.us, %"for convolved.s1.r19$y585.us" ], [ %15852, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2392.9.us = phi <4 x i32> [ %convolved944.sroa.2392.8.us, %"for convolved.s1.r19$y585.us" ], [ %15845, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2336.9.us = phi <4 x i32> [ %convolved944.sroa.2336.8.us, %"for convolved.s1.r19$y585.us" ], [ %15840, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2280.9.us = phi <4 x i32> [ %convolved944.sroa.2280.8.us, %"for convolved.s1.r19$y585.us" ], [ %15827, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2224.9.us = phi <4 x i32> [ %convolved944.sroa.2224.8.us, %"for convolved.s1.r19$y585.us" ], [ %15822, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2168.9.us = phi <4 x i32> [ %convolved944.sroa.2168.8.us, %"for convolved.s1.r19$y585.us" ], [ %15815, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2112.9.us = phi <4 x i32> [ %convolved944.sroa.2112.8.us, %"for convolved.s1.r19$y585.us" ], [ %15810, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2056.9.us = phi <4 x i32> [ %convolved944.sroa.2056.8.us, %"for convolved.s1.r19$y585.us" ], [ %15797, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.2000.9.us = phi <4 x i32> [ %convolved944.sroa.2000.8.us, %"for convolved.s1.r19$y585.us" ], [ %15792, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1944.9.us = phi <4 x i32> [ %convolved944.sroa.1944.8.us, %"for convolved.s1.r19$y585.us" ], [ %15785, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1888.9.us = phi <4 x i32> [ %convolved944.sroa.1888.8.us, %"for convolved.s1.r19$y585.us" ], [ %15780, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1832.9.us = phi <4 x i32> [ %convolved944.sroa.1832.8.us, %"for convolved.s1.r19$y585.us" ], [ %15767, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1776.9.us = phi <4 x i32> [ %convolved944.sroa.1776.8.us, %"for convolved.s1.r19$y585.us" ], [ %15762, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1720.9.us = phi <4 x i32> [ %convolved944.sroa.1720.8.us, %"for convolved.s1.r19$y585.us" ], [ %15755, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1664.9.us = phi <4 x i32> [ %convolved944.sroa.1664.8.us, %"for convolved.s1.r19$y585.us" ], [ %15750, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1608.9.us = phi <4 x i32> [ %convolved944.sroa.1608.8.us, %"for convolved.s1.r19$y585.us" ], [ %15737, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1552.9.us = phi <4 x i32> [ %convolved944.sroa.1552.8.us, %"for convolved.s1.r19$y585.us" ], [ %15732, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1496.9.us = phi <4 x i32> [ %convolved944.sroa.1496.8.us, %"for convolved.s1.r19$y585.us" ], [ %15725, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1440.9.us = phi <4 x i32> [ %convolved944.sroa.1440.8.us, %"for convolved.s1.r19$y585.us" ], [ %15720, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1384.9.us = phi <4 x i32> [ %convolved944.sroa.1384.8.us, %"for convolved.s1.r19$y585.us" ], [ %15707, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1328.9.us = phi <4 x i32> [ %convolved944.sroa.1328.8.us, %"for convolved.s1.r19$y585.us" ], [ %15702, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1272.9.us = phi <4 x i32> [ %convolved944.sroa.1272.8.us, %"for convolved.s1.r19$y585.us" ], [ %15695, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1216.9.us = phi <4 x i32> [ %convolved944.sroa.1216.8.us, %"for convolved.s1.r19$y585.us" ], [ %15690, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1160.9.us = phi <4 x i32> [ %convolved944.sroa.1160.8.us, %"for convolved.s1.r19$y585.us" ], [ %15677, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1104.9.us = phi <4 x i32> [ %convolved944.sroa.1104.8.us, %"for convolved.s1.r19$y585.us" ], [ %15672, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.1048.9.us = phi <4 x i32> [ %convolved944.sroa.1048.8.us, %"for convolved.s1.r19$y585.us" ], [ %15665, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.992.9.us = phi <4 x i32> [ %convolved944.sroa.992.8.us, %"for convolved.s1.r19$y585.us" ], [ %15660, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.930.13.us = phi <4 x i32> [ %convolved944.sroa.930.12.us, %"for convolved.s1.r19$y585.us" ], [ %15647, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.868.13.us = phi <4 x i32> [ %convolved944.sroa.868.12.us, %"for convolved.s1.r19$y585.us" ], [ %15642, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.806.13.us = phi <4 x i32> [ %convolved944.sroa.806.12.us, %"for convolved.s1.r19$y585.us" ], [ %15635, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.744.13.us = phi <4 x i32> [ %convolved944.sroa.744.12.us, %"for convolved.s1.r19$y585.us" ], [ %15630, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.682.13.us = phi <4 x i32> [ %convolved944.sroa.682.12.us, %"for convolved.s1.r19$y585.us" ], [ %15617, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.620.13.us = phi <4 x i32> [ %convolved944.sroa.620.12.us, %"for convolved.s1.r19$y585.us" ], [ %15612, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.558.13.us = phi <4 x i32> [ %convolved944.sroa.558.12.us, %"for convolved.s1.r19$y585.us" ], [ %15605, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.496.13.us = phi <4 x i32> [ %convolved944.sroa.496.12.us, %"for convolved.s1.r19$y585.us" ], [ %15600, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.434.13.us = phi <4 x i32> [ %convolved944.sroa.434.12.us, %"for convolved.s1.r19$y585.us" ], [ %15587, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.372.13.us = phi <4 x i32> [ %convolved944.sroa.372.12.us, %"for convolved.s1.r19$y585.us" ], [ %15582, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.310.13.us = phi <4 x i32> [ %convolved944.sroa.310.12.us, %"for convolved.s1.r19$y585.us" ], [ %15575, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.248.13.us = phi <4 x i32> [ %convolved944.sroa.248.12.us, %"for convolved.s1.r19$y585.us" ], [ %15570, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.186.13.us = phi <4 x i32> [ %convolved944.sroa.186.12.us, %"for convolved.s1.r19$y585.us" ], [ %15557, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.124.13.us = phi <4 x i32> [ %convolved944.sroa.124.12.us, %"for convolved.s1.r19$y585.us" ], [ %15548, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.62.13.us = phi <4 x i32> [ %convolved944.sroa.62.12.us, %"for convolved.s1.r19$y585.us" ], [ %15537, %"for convolved.s1.r19$x610.us" ]
  %convolved944.sroa.0.13.us = phi <4 x i32> [ %convolved944.sroa.0.12.us, %"for convolved.s1.r19$y585.us" ], [ %15528, %"for convolved.s1.r19$x610.us" ]
  %15508 = add nsw i64 %indvars.iv6457, %15495
  %15509 = trunc i64 %indvars.iv6457 to i32
  %15510 = mul i32 %15509, %a614
  %t2461597.us = add i32 %15510, %15498
  %t3723.us = add i32 %t2461597.us, %15493
  %15511 = sext i32 %t3723.us to i64
  %15512 = shl nsw i64 %15511, 4
  %15513 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15512
  %15514 = bitcast i8* %15513 to <8 x i8>*
  %t3724.us = load <8 x i8>, <8 x i8>* %15514, align 16, !tbaa !438
  %15515 = getelementptr inbounds i8, i8* %15513, i64 8
  %15516 = bitcast i8* %15515 to <8 x i8>*
  %t3725.us = load <8 x i8>, <8 x i8>* %15516, align 8, !tbaa !438
  %15517 = shl nsw i64 %15508, 4
  %15518 = getelementptr inbounds i16, i16* %filter_zeroed438, i64 %15517
  %15519 = bitcast i16* %15518 to <4 x i16>*
  %15520 = load <4 x i16>, <4 x i16>* %15519, align 16, !tbaa !395
  %15521 = zext <8 x i8> %t3724.us to <8 x i16>
  %15522 = bitcast <8 x i16> %15521 to <2 x i64>
  %15523 = shufflevector <2 x i64> %15522, <2 x i64> undef, <1 x i32> zeroinitializer
  %15524 = bitcast <1 x i64> %15523 to <4 x i16>
  %15525 = sext <4 x i16> %15520 to <4 x i32>
  %15526 = sext <4 x i16> %15524 to <4 x i32>
  %15527 = mul nsw <4 x i32> %15526, %15525
  %15528 = add <4 x i32> %15527, %convolved944.sroa.0.13.us
  %15529 = getelementptr inbounds i16, i16* %15518, i64 4
  %15530 = bitcast i16* %15529 to <4 x i16>*
  %15531 = load <4 x i16>, <4 x i16>* %15530, align 8, !tbaa !395
  %15532 = shufflevector <2 x i64> %15522, <2 x i64> undef, <1 x i32> <i32 1>
  %15533 = bitcast <1 x i64> %15532 to <4 x i16>
  %15534 = sext <4 x i16> %15531 to <4 x i32>
  %15535 = sext <4 x i16> %15533 to <4 x i32>
  %15536 = mul nsw <4 x i32> %15535, %15534
  %15537 = add <4 x i32> %15536, %convolved944.sroa.62.13.us
  %15538 = getelementptr inbounds i16, i16* %15518, i64 8
  %15539 = bitcast i16* %15538 to <4 x i16>*
  %15540 = load <4 x i16>, <4 x i16>* %15539, align 16, !tbaa !395
  %15541 = zext <8 x i8> %t3725.us to <8 x i16>
  %15542 = bitcast <8 x i16> %15541 to <2 x i64>
  %15543 = shufflevector <2 x i64> %15542, <2 x i64> undef, <1 x i32> zeroinitializer
  %15544 = bitcast <1 x i64> %15543 to <4 x i16>
  %15545 = sext <4 x i16> %15540 to <4 x i32>
  %15546 = sext <4 x i16> %15544 to <4 x i32>
  %15547 = mul nsw <4 x i32> %15546, %15545
  %15548 = add <4 x i32> %15547, %convolved944.sroa.124.13.us
  %15549 = getelementptr inbounds i16, i16* %15518, i64 12
  %15550 = bitcast i16* %15549 to <4 x i16>*
  %15551 = load <4 x i16>, <4 x i16>* %15550, align 8, !tbaa !395
  %15552 = shufflevector <2 x i64> %15542, <2 x i64> undef, <1 x i32> <i32 1>
  %15553 = bitcast <1 x i64> %15552 to <4 x i16>
  %15554 = sext <4 x i16> %15551 to <4 x i32>
  %15555 = sext <4 x i16> %15553 to <4 x i32>
  %15556 = mul nsw <4 x i32> %15554, %15555
  %15557 = add <4 x i32> %15556, %convolved944.sroa.186.13.us
  %t3727.us = add i32 %t2461597.us, %15492
  %15558 = sext i32 %t3727.us to i64
  %15559 = shl nsw i64 %15558, 4
  %15560 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15559
  %15561 = bitcast i8* %15560 to <8 x i8>*
  %t3728.us = load <8 x i8>, <8 x i8>* %15561, align 16, !tbaa !438
  %15562 = getelementptr inbounds i8, i8* %15560, i64 8
  %15563 = bitcast i8* %15562 to <8 x i8>*
  %t3729.us = load <8 x i8>, <8 x i8>* %15563, align 8, !tbaa !438
  %15564 = zext <8 x i8> %t3728.us to <8 x i16>
  %15565 = bitcast <8 x i16> %15564 to <2 x i64>
  %15566 = shufflevector <2 x i64> %15565, <2 x i64> undef, <1 x i32> zeroinitializer
  %15567 = bitcast <1 x i64> %15566 to <4 x i16>
  %15568 = sext <4 x i16> %15567 to <4 x i32>
  %15569 = mul nsw <4 x i32> %15568, %15525
  %15570 = add <4 x i32> %15569, %convolved944.sroa.248.13.us
  %15571 = shufflevector <2 x i64> %15565, <2 x i64> undef, <1 x i32> <i32 1>
  %15572 = bitcast <1 x i64> %15571 to <4 x i16>
  %15573 = sext <4 x i16> %15572 to <4 x i32>
  %15574 = mul nsw <4 x i32> %15573, %15534
  %15575 = add <4 x i32> %15574, %convolved944.sroa.310.13.us
  %15576 = zext <8 x i8> %t3729.us to <8 x i16>
  %15577 = bitcast <8 x i16> %15576 to <2 x i64>
  %15578 = shufflevector <2 x i64> %15577, <2 x i64> undef, <1 x i32> zeroinitializer
  %15579 = bitcast <1 x i64> %15578 to <4 x i16>
  %15580 = sext <4 x i16> %15579 to <4 x i32>
  %15581 = mul nsw <4 x i32> %15580, %15545
  %15582 = add <4 x i32> %15581, %convolved944.sroa.372.13.us
  %15583 = shufflevector <2 x i64> %15577, <2 x i64> undef, <1 x i32> <i32 1>
  %15584 = bitcast <1 x i64> %15583 to <4 x i16>
  %15585 = sext <4 x i16> %15584 to <4 x i32>
  %15586 = mul nsw <4 x i32> %15585, %15554
  %15587 = add <4 x i32> %15586, %convolved944.sroa.434.13.us
  %t3731.us = add i32 %t2461597.us, %15491
  %15588 = sext i32 %t3731.us to i64
  %15589 = shl nsw i64 %15588, 4
  %15590 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15589
  %15591 = bitcast i8* %15590 to <8 x i8>*
  %t3732.us = load <8 x i8>, <8 x i8>* %15591, align 16, !tbaa !438
  %15592 = getelementptr inbounds i8, i8* %15590, i64 8
  %15593 = bitcast i8* %15592 to <8 x i8>*
  %t3733.us = load <8 x i8>, <8 x i8>* %15593, align 8, !tbaa !438
  %15594 = zext <8 x i8> %t3732.us to <8 x i16>
  %15595 = bitcast <8 x i16> %15594 to <2 x i64>
  %15596 = shufflevector <2 x i64> %15595, <2 x i64> undef, <1 x i32> zeroinitializer
  %15597 = bitcast <1 x i64> %15596 to <4 x i16>
  %15598 = sext <4 x i16> %15597 to <4 x i32>
  %15599 = mul nsw <4 x i32> %15598, %15525
  %15600 = add <4 x i32> %15599, %convolved944.sroa.496.13.us
  %15601 = shufflevector <2 x i64> %15595, <2 x i64> undef, <1 x i32> <i32 1>
  %15602 = bitcast <1 x i64> %15601 to <4 x i16>
  %15603 = sext <4 x i16> %15602 to <4 x i32>
  %15604 = mul nsw <4 x i32> %15603, %15534
  %15605 = add <4 x i32> %15604, %convolved944.sroa.558.13.us
  %15606 = zext <8 x i8> %t3733.us to <8 x i16>
  %15607 = bitcast <8 x i16> %15606 to <2 x i64>
  %15608 = shufflevector <2 x i64> %15607, <2 x i64> undef, <1 x i32> zeroinitializer
  %15609 = bitcast <1 x i64> %15608 to <4 x i16>
  %15610 = sext <4 x i16> %15609 to <4 x i32>
  %15611 = mul nsw <4 x i32> %15610, %15545
  %15612 = add <4 x i32> %15611, %convolved944.sroa.620.13.us
  %15613 = shufflevector <2 x i64> %15607, <2 x i64> undef, <1 x i32> <i32 1>
  %15614 = bitcast <1 x i64> %15613 to <4 x i16>
  %15615 = sext <4 x i16> %15614 to <4 x i32>
  %15616 = mul nsw <4 x i32> %15615, %15554
  %15617 = add <4 x i32> %15616, %convolved944.sroa.682.13.us
  %t3735.us = add i32 %t2461597.us, %15490
  %15618 = sext i32 %t3735.us to i64
  %15619 = shl nsw i64 %15618, 4
  %15620 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15619
  %15621 = bitcast i8* %15620 to <8 x i8>*
  %t3736.us = load <8 x i8>, <8 x i8>* %15621, align 16, !tbaa !438
  %15622 = getelementptr inbounds i8, i8* %15620, i64 8
  %15623 = bitcast i8* %15622 to <8 x i8>*
  %t3737.us = load <8 x i8>, <8 x i8>* %15623, align 8, !tbaa !438
  %15624 = zext <8 x i8> %t3736.us to <8 x i16>
  %15625 = bitcast <8 x i16> %15624 to <2 x i64>
  %15626 = shufflevector <2 x i64> %15625, <2 x i64> undef, <1 x i32> zeroinitializer
  %15627 = bitcast <1 x i64> %15626 to <4 x i16>
  %15628 = sext <4 x i16> %15627 to <4 x i32>
  %15629 = mul nsw <4 x i32> %15628, %15525
  %15630 = add <4 x i32> %15629, %convolved944.sroa.744.13.us
  %15631 = shufflevector <2 x i64> %15625, <2 x i64> undef, <1 x i32> <i32 1>
  %15632 = bitcast <1 x i64> %15631 to <4 x i16>
  %15633 = sext <4 x i16> %15632 to <4 x i32>
  %15634 = mul nsw <4 x i32> %15633, %15534
  %15635 = add <4 x i32> %15634, %convolved944.sroa.806.13.us
  %15636 = zext <8 x i8> %t3737.us to <8 x i16>
  %15637 = bitcast <8 x i16> %15636 to <2 x i64>
  %15638 = shufflevector <2 x i64> %15637, <2 x i64> undef, <1 x i32> zeroinitializer
  %15639 = bitcast <1 x i64> %15638 to <4 x i16>
  %15640 = sext <4 x i16> %15639 to <4 x i32>
  %15641 = mul nsw <4 x i32> %15640, %15545
  %15642 = add <4 x i32> %15641, %convolved944.sroa.868.13.us
  %15643 = shufflevector <2 x i64> %15637, <2 x i64> undef, <1 x i32> <i32 1>
  %15644 = bitcast <1 x i64> %15643 to <4 x i16>
  %15645 = sext <4 x i16> %15644 to <4 x i32>
  %15646 = mul nsw <4 x i32> %15645, %15554
  %15647 = add <4 x i32> %15646, %convolved944.sroa.930.13.us
  %t2465609.us = add i32 %15510, %15501
  %t3739.us = add i32 %t2465609.us, %15493
  %15648 = sext i32 %t3739.us to i64
  %15649 = shl nsw i64 %15648, 4
  %15650 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15649
  %15651 = bitcast i8* %15650 to <8 x i8>*
  %t3740.us = load <8 x i8>, <8 x i8>* %15651, align 16, !tbaa !438
  %15652 = getelementptr inbounds i8, i8* %15650, i64 8
  %15653 = bitcast i8* %15652 to <8 x i8>*
  %t3741.us = load <8 x i8>, <8 x i8>* %15653, align 8, !tbaa !438
  %15654 = zext <8 x i8> %t3740.us to <8 x i16>
  %15655 = bitcast <8 x i16> %15654 to <2 x i64>
  %15656 = shufflevector <2 x i64> %15655, <2 x i64> undef, <1 x i32> zeroinitializer
  %15657 = bitcast <1 x i64> %15656 to <4 x i16>
  %15658 = sext <4 x i16> %15657 to <4 x i32>
  %15659 = mul nsw <4 x i32> %15658, %15525
  %15660 = add <4 x i32> %15659, %convolved944.sroa.992.9.us
  %15661 = shufflevector <2 x i64> %15655, <2 x i64> undef, <1 x i32> <i32 1>
  %15662 = bitcast <1 x i64> %15661 to <4 x i16>
  %15663 = sext <4 x i16> %15662 to <4 x i32>
  %15664 = mul nsw <4 x i32> %15663, %15534
  %15665 = add <4 x i32> %15664, %convolved944.sroa.1048.9.us
  %15666 = zext <8 x i8> %t3741.us to <8 x i16>
  %15667 = bitcast <8 x i16> %15666 to <2 x i64>
  %15668 = shufflevector <2 x i64> %15667, <2 x i64> undef, <1 x i32> zeroinitializer
  %15669 = bitcast <1 x i64> %15668 to <4 x i16>
  %15670 = sext <4 x i16> %15669 to <4 x i32>
  %15671 = mul nsw <4 x i32> %15670, %15545
  %15672 = add <4 x i32> %15671, %convolved944.sroa.1104.9.us
  %15673 = shufflevector <2 x i64> %15667, <2 x i64> undef, <1 x i32> <i32 1>
  %15674 = bitcast <1 x i64> %15673 to <4 x i16>
  %15675 = sext <4 x i16> %15674 to <4 x i32>
  %15676 = mul nsw <4 x i32> %15675, %15554
  %15677 = add <4 x i32> %15676, %convolved944.sroa.1160.9.us
  %t3743.us = add i32 %t2465609.us, %15492
  %15678 = sext i32 %t3743.us to i64
  %15679 = shl nsw i64 %15678, 4
  %15680 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15679
  %15681 = bitcast i8* %15680 to <8 x i8>*
  %t3744.us = load <8 x i8>, <8 x i8>* %15681, align 16, !tbaa !438
  %15682 = getelementptr inbounds i8, i8* %15680, i64 8
  %15683 = bitcast i8* %15682 to <8 x i8>*
  %t3745.us = load <8 x i8>, <8 x i8>* %15683, align 8, !tbaa !438
  %15684 = zext <8 x i8> %t3744.us to <8 x i16>
  %15685 = bitcast <8 x i16> %15684 to <2 x i64>
  %15686 = shufflevector <2 x i64> %15685, <2 x i64> undef, <1 x i32> zeroinitializer
  %15687 = bitcast <1 x i64> %15686 to <4 x i16>
  %15688 = sext <4 x i16> %15687 to <4 x i32>
  %15689 = mul nsw <4 x i32> %15688, %15525
  %15690 = add <4 x i32> %15689, %convolved944.sroa.1216.9.us
  %15691 = shufflevector <2 x i64> %15685, <2 x i64> undef, <1 x i32> <i32 1>
  %15692 = bitcast <1 x i64> %15691 to <4 x i16>
  %15693 = sext <4 x i16> %15692 to <4 x i32>
  %15694 = mul nsw <4 x i32> %15693, %15534
  %15695 = add <4 x i32> %15694, %convolved944.sroa.1272.9.us
  %15696 = zext <8 x i8> %t3745.us to <8 x i16>
  %15697 = bitcast <8 x i16> %15696 to <2 x i64>
  %15698 = shufflevector <2 x i64> %15697, <2 x i64> undef, <1 x i32> zeroinitializer
  %15699 = bitcast <1 x i64> %15698 to <4 x i16>
  %15700 = sext <4 x i16> %15699 to <4 x i32>
  %15701 = mul nsw <4 x i32> %15700, %15545
  %15702 = add <4 x i32> %15701, %convolved944.sroa.1328.9.us
  %15703 = shufflevector <2 x i64> %15697, <2 x i64> undef, <1 x i32> <i32 1>
  %15704 = bitcast <1 x i64> %15703 to <4 x i16>
  %15705 = sext <4 x i16> %15704 to <4 x i32>
  %15706 = mul nsw <4 x i32> %15705, %15554
  %15707 = add <4 x i32> %15706, %convolved944.sroa.1384.9.us
  %t3747.us = add i32 %t2465609.us, %15491
  %15708 = sext i32 %t3747.us to i64
  %15709 = shl nsw i64 %15708, 4
  %15710 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15709
  %15711 = bitcast i8* %15710 to <8 x i8>*
  %t3748.us = load <8 x i8>, <8 x i8>* %15711, align 16, !tbaa !438
  %15712 = getelementptr inbounds i8, i8* %15710, i64 8
  %15713 = bitcast i8* %15712 to <8 x i8>*
  %t3749.us = load <8 x i8>, <8 x i8>* %15713, align 8, !tbaa !438
  %15714 = zext <8 x i8> %t3748.us to <8 x i16>
  %15715 = bitcast <8 x i16> %15714 to <2 x i64>
  %15716 = shufflevector <2 x i64> %15715, <2 x i64> undef, <1 x i32> zeroinitializer
  %15717 = bitcast <1 x i64> %15716 to <4 x i16>
  %15718 = sext <4 x i16> %15717 to <4 x i32>
  %15719 = mul nsw <4 x i32> %15718, %15525
  %15720 = add <4 x i32> %15719, %convolved944.sroa.1440.9.us
  %15721 = shufflevector <2 x i64> %15715, <2 x i64> undef, <1 x i32> <i32 1>
  %15722 = bitcast <1 x i64> %15721 to <4 x i16>
  %15723 = sext <4 x i16> %15722 to <4 x i32>
  %15724 = mul nsw <4 x i32> %15723, %15534
  %15725 = add <4 x i32> %15724, %convolved944.sroa.1496.9.us
  %15726 = zext <8 x i8> %t3749.us to <8 x i16>
  %15727 = bitcast <8 x i16> %15726 to <2 x i64>
  %15728 = shufflevector <2 x i64> %15727, <2 x i64> undef, <1 x i32> zeroinitializer
  %15729 = bitcast <1 x i64> %15728 to <4 x i16>
  %15730 = sext <4 x i16> %15729 to <4 x i32>
  %15731 = mul nsw <4 x i32> %15730, %15545
  %15732 = add <4 x i32> %15731, %convolved944.sroa.1552.9.us
  %15733 = shufflevector <2 x i64> %15727, <2 x i64> undef, <1 x i32> <i32 1>
  %15734 = bitcast <1 x i64> %15733 to <4 x i16>
  %15735 = sext <4 x i16> %15734 to <4 x i32>
  %15736 = mul nsw <4 x i32> %15735, %15554
  %15737 = add <4 x i32> %15736, %convolved944.sroa.1608.9.us
  %t3751.us = add i32 %t2465609.us, %15490
  %15738 = sext i32 %t3751.us to i64
  %15739 = shl nsw i64 %15738, 4
  %15740 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15739
  %15741 = bitcast i8* %15740 to <8 x i8>*
  %t3752.us = load <8 x i8>, <8 x i8>* %15741, align 16, !tbaa !438
  %15742 = getelementptr inbounds i8, i8* %15740, i64 8
  %15743 = bitcast i8* %15742 to <8 x i8>*
  %t3753.us = load <8 x i8>, <8 x i8>* %15743, align 8, !tbaa !438
  %15744 = zext <8 x i8> %t3752.us to <8 x i16>
  %15745 = bitcast <8 x i16> %15744 to <2 x i64>
  %15746 = shufflevector <2 x i64> %15745, <2 x i64> undef, <1 x i32> zeroinitializer
  %15747 = bitcast <1 x i64> %15746 to <4 x i16>
  %15748 = sext <4 x i16> %15747 to <4 x i32>
  %15749 = mul nsw <4 x i32> %15748, %15525
  %15750 = add <4 x i32> %15749, %convolved944.sroa.1664.9.us
  %15751 = shufflevector <2 x i64> %15745, <2 x i64> undef, <1 x i32> <i32 1>
  %15752 = bitcast <1 x i64> %15751 to <4 x i16>
  %15753 = sext <4 x i16> %15752 to <4 x i32>
  %15754 = mul nsw <4 x i32> %15753, %15534
  %15755 = add <4 x i32> %15754, %convolved944.sroa.1720.9.us
  %15756 = zext <8 x i8> %t3753.us to <8 x i16>
  %15757 = bitcast <8 x i16> %15756 to <2 x i64>
  %15758 = shufflevector <2 x i64> %15757, <2 x i64> undef, <1 x i32> zeroinitializer
  %15759 = bitcast <1 x i64> %15758 to <4 x i16>
  %15760 = sext <4 x i16> %15759 to <4 x i32>
  %15761 = mul nsw <4 x i32> %15760, %15545
  %15762 = add <4 x i32> %15761, %convolved944.sroa.1776.9.us
  %15763 = shufflevector <2 x i64> %15757, <2 x i64> undef, <1 x i32> <i32 1>
  %15764 = bitcast <1 x i64> %15763 to <4 x i16>
  %15765 = sext <4 x i16> %15764 to <4 x i32>
  %15766 = mul nsw <4 x i32> %15765, %15554
  %15767 = add <4 x i32> %15766, %convolved944.sroa.1832.9.us
  %t2469605.us = add i32 %15510, %15504
  %t3755.us = add i32 %t2469605.us, %15493
  %15768 = sext i32 %t3755.us to i64
  %15769 = shl nsw i64 %15768, 4
  %15770 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15769
  %15771 = bitcast i8* %15770 to <8 x i8>*
  %t3756.us = load <8 x i8>, <8 x i8>* %15771, align 16, !tbaa !438
  %15772 = getelementptr inbounds i8, i8* %15770, i64 8
  %15773 = bitcast i8* %15772 to <8 x i8>*
  %t3757.us = load <8 x i8>, <8 x i8>* %15773, align 8, !tbaa !438
  %15774 = zext <8 x i8> %t3756.us to <8 x i16>
  %15775 = bitcast <8 x i16> %15774 to <2 x i64>
  %15776 = shufflevector <2 x i64> %15775, <2 x i64> undef, <1 x i32> zeroinitializer
  %15777 = bitcast <1 x i64> %15776 to <4 x i16>
  %15778 = sext <4 x i16> %15777 to <4 x i32>
  %15779 = mul nsw <4 x i32> %15778, %15525
  %15780 = add <4 x i32> %15779, %convolved944.sroa.1888.9.us
  %15781 = shufflevector <2 x i64> %15775, <2 x i64> undef, <1 x i32> <i32 1>
  %15782 = bitcast <1 x i64> %15781 to <4 x i16>
  %15783 = sext <4 x i16> %15782 to <4 x i32>
  %15784 = mul nsw <4 x i32> %15783, %15534
  %15785 = add <4 x i32> %15784, %convolved944.sroa.1944.9.us
  %15786 = zext <8 x i8> %t3757.us to <8 x i16>
  %15787 = bitcast <8 x i16> %15786 to <2 x i64>
  %15788 = shufflevector <2 x i64> %15787, <2 x i64> undef, <1 x i32> zeroinitializer
  %15789 = bitcast <1 x i64> %15788 to <4 x i16>
  %15790 = sext <4 x i16> %15789 to <4 x i32>
  %15791 = mul nsw <4 x i32> %15790, %15545
  %15792 = add <4 x i32> %15791, %convolved944.sroa.2000.9.us
  %15793 = shufflevector <2 x i64> %15787, <2 x i64> undef, <1 x i32> <i32 1>
  %15794 = bitcast <1 x i64> %15793 to <4 x i16>
  %15795 = sext <4 x i16> %15794 to <4 x i32>
  %15796 = mul nsw <4 x i32> %15795, %15554
  %15797 = add <4 x i32> %15796, %convolved944.sroa.2056.9.us
  %t3759.us = add i32 %t2469605.us, %15492
  %15798 = sext i32 %t3759.us to i64
  %15799 = shl nsw i64 %15798, 4
  %15800 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15799
  %15801 = bitcast i8* %15800 to <8 x i8>*
  %t3760.us = load <8 x i8>, <8 x i8>* %15801, align 16, !tbaa !438
  %15802 = getelementptr inbounds i8, i8* %15800, i64 8
  %15803 = bitcast i8* %15802 to <8 x i8>*
  %t3761.us = load <8 x i8>, <8 x i8>* %15803, align 8, !tbaa !438
  %15804 = zext <8 x i8> %t3760.us to <8 x i16>
  %15805 = bitcast <8 x i16> %15804 to <2 x i64>
  %15806 = shufflevector <2 x i64> %15805, <2 x i64> undef, <1 x i32> zeroinitializer
  %15807 = bitcast <1 x i64> %15806 to <4 x i16>
  %15808 = sext <4 x i16> %15807 to <4 x i32>
  %15809 = mul nsw <4 x i32> %15808, %15525
  %15810 = add <4 x i32> %15809, %convolved944.sroa.2112.9.us
  %15811 = shufflevector <2 x i64> %15805, <2 x i64> undef, <1 x i32> <i32 1>
  %15812 = bitcast <1 x i64> %15811 to <4 x i16>
  %15813 = sext <4 x i16> %15812 to <4 x i32>
  %15814 = mul nsw <4 x i32> %15813, %15534
  %15815 = add <4 x i32> %15814, %convolved944.sroa.2168.9.us
  %15816 = zext <8 x i8> %t3761.us to <8 x i16>
  %15817 = bitcast <8 x i16> %15816 to <2 x i64>
  %15818 = shufflevector <2 x i64> %15817, <2 x i64> undef, <1 x i32> zeroinitializer
  %15819 = bitcast <1 x i64> %15818 to <4 x i16>
  %15820 = sext <4 x i16> %15819 to <4 x i32>
  %15821 = mul nsw <4 x i32> %15820, %15545
  %15822 = add <4 x i32> %15821, %convolved944.sroa.2224.9.us
  %15823 = shufflevector <2 x i64> %15817, <2 x i64> undef, <1 x i32> <i32 1>
  %15824 = bitcast <1 x i64> %15823 to <4 x i16>
  %15825 = sext <4 x i16> %15824 to <4 x i32>
  %15826 = mul nsw <4 x i32> %15825, %15554
  %15827 = add <4 x i32> %15826, %convolved944.sroa.2280.9.us
  %t3763.us = add i32 %t2469605.us, %15491
  %15828 = sext i32 %t3763.us to i64
  %15829 = shl nsw i64 %15828, 4
  %15830 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15829
  %15831 = bitcast i8* %15830 to <8 x i8>*
  %t3764.us = load <8 x i8>, <8 x i8>* %15831, align 16, !tbaa !438
  %15832 = getelementptr inbounds i8, i8* %15830, i64 8
  %15833 = bitcast i8* %15832 to <8 x i8>*
  %t3765.us = load <8 x i8>, <8 x i8>* %15833, align 8, !tbaa !438
  %15834 = zext <8 x i8> %t3764.us to <8 x i16>
  %15835 = bitcast <8 x i16> %15834 to <2 x i64>
  %15836 = shufflevector <2 x i64> %15835, <2 x i64> undef, <1 x i32> zeroinitializer
  %15837 = bitcast <1 x i64> %15836 to <4 x i16>
  %15838 = sext <4 x i16> %15837 to <4 x i32>
  %15839 = mul nsw <4 x i32> %15838, %15525
  %15840 = add <4 x i32> %15839, %convolved944.sroa.2336.9.us
  %15841 = shufflevector <2 x i64> %15835, <2 x i64> undef, <1 x i32> <i32 1>
  %15842 = bitcast <1 x i64> %15841 to <4 x i16>
  %15843 = sext <4 x i16> %15842 to <4 x i32>
  %15844 = mul nsw <4 x i32> %15843, %15534
  %15845 = add <4 x i32> %15844, %convolved944.sroa.2392.9.us
  %15846 = zext <8 x i8> %t3765.us to <8 x i16>
  %15847 = bitcast <8 x i16> %15846 to <2 x i64>
  %15848 = shufflevector <2 x i64> %15847, <2 x i64> undef, <1 x i32> zeroinitializer
  %15849 = bitcast <1 x i64> %15848 to <4 x i16>
  %15850 = sext <4 x i16> %15849 to <4 x i32>
  %15851 = mul nsw <4 x i32> %15850, %15545
  %15852 = add <4 x i32> %15851, %convolved944.sroa.2448.9.us
  %15853 = shufflevector <2 x i64> %15847, <2 x i64> undef, <1 x i32> <i32 1>
  %15854 = bitcast <1 x i64> %15853 to <4 x i16>
  %15855 = sext <4 x i16> %15854 to <4 x i32>
  %15856 = mul nsw <4 x i32> %15855, %15554
  %15857 = add <4 x i32> %15856, %convolved944.sroa.2504.9.us
  %t3767.us = add i32 %t2469605.us, %15490
  %15858 = sext i32 %t3767.us to i64
  %15859 = shl nsw i64 %15858, 4
  %15860 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15859
  %15861 = bitcast i8* %15860 to <8 x i8>*
  %t3768.us = load <8 x i8>, <8 x i8>* %15861, align 16, !tbaa !438
  %15862 = getelementptr inbounds i8, i8* %15860, i64 8
  %15863 = bitcast i8* %15862 to <8 x i8>*
  %t3769.us = load <8 x i8>, <8 x i8>* %15863, align 8, !tbaa !438
  %15864 = zext <8 x i8> %t3768.us to <8 x i16>
  %15865 = bitcast <8 x i16> %15864 to <2 x i64>
  %15866 = shufflevector <2 x i64> %15865, <2 x i64> undef, <1 x i32> zeroinitializer
  %15867 = bitcast <1 x i64> %15866 to <4 x i16>
  %15868 = sext <4 x i16> %15867 to <4 x i32>
  %15869 = mul nsw <4 x i32> %15868, %15525
  %15870 = add <4 x i32> %15869, %convolved944.sroa.2560.9.us
  %15871 = shufflevector <2 x i64> %15865, <2 x i64> undef, <1 x i32> <i32 1>
  %15872 = bitcast <1 x i64> %15871 to <4 x i16>
  %15873 = sext <4 x i16> %15872 to <4 x i32>
  %15874 = mul nsw <4 x i32> %15873, %15534
  %15875 = add <4 x i32> %15874, %convolved944.sroa.2616.9.us
  %15876 = zext <8 x i8> %t3769.us to <8 x i16>
  %15877 = bitcast <8 x i16> %15876 to <2 x i64>
  %15878 = shufflevector <2 x i64> %15877, <2 x i64> undef, <1 x i32> zeroinitializer
  %15879 = bitcast <1 x i64> %15878 to <4 x i16>
  %15880 = sext <4 x i16> %15879 to <4 x i32>
  %15881 = mul nsw <4 x i32> %15880, %15545
  %15882 = add <4 x i32> %15881, %convolved944.sroa.2672.9.us
  %15883 = shufflevector <2 x i64> %15877, <2 x i64> undef, <1 x i32> <i32 1>
  %15884 = bitcast <1 x i64> %15883 to <4 x i16>
  %15885 = sext <4 x i16> %15884 to <4 x i32>
  %15886 = mul nsw <4 x i32> %15885, %15554
  %15887 = add <4 x i32> %15886, %convolved944.sroa.2728.9.us
  %t2473601.us = add i32 %15510, %15507
  %t3771.us = add i32 %t2473601.us, %15493
  %15888 = sext i32 %t3771.us to i64
  %15889 = shl nsw i64 %15888, 4
  %15890 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15889
  %15891 = bitcast i8* %15890 to <8 x i8>*
  %t3772.us = load <8 x i8>, <8 x i8>* %15891, align 16, !tbaa !438
  %15892 = getelementptr inbounds i8, i8* %15890, i64 8
  %15893 = bitcast i8* %15892 to <8 x i8>*
  %t3773.us = load <8 x i8>, <8 x i8>* %15893, align 8, !tbaa !438
  %15894 = zext <8 x i8> %t3772.us to <8 x i16>
  %15895 = bitcast <8 x i16> %15894 to <2 x i64>
  %15896 = shufflevector <2 x i64> %15895, <2 x i64> undef, <1 x i32> zeroinitializer
  %15897 = bitcast <1 x i64> %15896 to <4 x i16>
  %15898 = sext <4 x i16> %15897 to <4 x i32>
  %15899 = mul nsw <4 x i32> %15898, %15525
  %15900 = add <4 x i32> %15899, %convolved944.sroa.2784.9.us
  %15901 = shufflevector <2 x i64> %15895, <2 x i64> undef, <1 x i32> <i32 1>
  %15902 = bitcast <1 x i64> %15901 to <4 x i16>
  %15903 = sext <4 x i16> %15902 to <4 x i32>
  %15904 = mul nsw <4 x i32> %15903, %15534
  %15905 = add <4 x i32> %15904, %convolved944.sroa.2840.9.us
  %15906 = zext <8 x i8> %t3773.us to <8 x i16>
  %15907 = bitcast <8 x i16> %15906 to <2 x i64>
  %15908 = shufflevector <2 x i64> %15907, <2 x i64> undef, <1 x i32> zeroinitializer
  %15909 = bitcast <1 x i64> %15908 to <4 x i16>
  %15910 = sext <4 x i16> %15909 to <4 x i32>
  %15911 = mul nsw <4 x i32> %15910, %15545
  %15912 = add <4 x i32> %15911, %convolved944.sroa.2896.9.us
  %15913 = shufflevector <2 x i64> %15907, <2 x i64> undef, <1 x i32> <i32 1>
  %15914 = bitcast <1 x i64> %15913 to <4 x i16>
  %15915 = sext <4 x i16> %15914 to <4 x i32>
  %15916 = mul nsw <4 x i32> %15915, %15554
  %15917 = add <4 x i32> %15916, %convolved944.sroa.2952.9.us
  %t3775.us = add i32 %t2473601.us, %15492
  %15918 = sext i32 %t3775.us to i64
  %15919 = shl nsw i64 %15918, 4
  %15920 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15919
  %15921 = bitcast i8* %15920 to <8 x i8>*
  %t3776.us = load <8 x i8>, <8 x i8>* %15921, align 16, !tbaa !438
  %15922 = getelementptr inbounds i8, i8* %15920, i64 8
  %15923 = bitcast i8* %15922 to <8 x i8>*
  %t3777.us = load <8 x i8>, <8 x i8>* %15923, align 8, !tbaa !438
  %15924 = zext <8 x i8> %t3776.us to <8 x i16>
  %15925 = bitcast <8 x i16> %15924 to <2 x i64>
  %15926 = shufflevector <2 x i64> %15925, <2 x i64> undef, <1 x i32> zeroinitializer
  %15927 = bitcast <1 x i64> %15926 to <4 x i16>
  %15928 = sext <4 x i16> %15927 to <4 x i32>
  %15929 = mul nsw <4 x i32> %15928, %15525
  %15930 = add <4 x i32> %15929, %convolved944.sroa.3008.9.us
  %15931 = shufflevector <2 x i64> %15925, <2 x i64> undef, <1 x i32> <i32 1>
  %15932 = bitcast <1 x i64> %15931 to <4 x i16>
  %15933 = sext <4 x i16> %15932 to <4 x i32>
  %15934 = mul nsw <4 x i32> %15933, %15534
  %15935 = add <4 x i32> %15934, %convolved944.sroa.3064.9.us
  %15936 = zext <8 x i8> %t3777.us to <8 x i16>
  %15937 = bitcast <8 x i16> %15936 to <2 x i64>
  %15938 = shufflevector <2 x i64> %15937, <2 x i64> undef, <1 x i32> zeroinitializer
  %15939 = bitcast <1 x i64> %15938 to <4 x i16>
  %15940 = sext <4 x i16> %15939 to <4 x i32>
  %15941 = mul nsw <4 x i32> %15940, %15545
  %15942 = add <4 x i32> %15941, %convolved944.sroa.3120.9.us
  %15943 = shufflevector <2 x i64> %15937, <2 x i64> undef, <1 x i32> <i32 1>
  %15944 = bitcast <1 x i64> %15943 to <4 x i16>
  %15945 = sext <4 x i16> %15944 to <4 x i32>
  %15946 = mul nsw <4 x i32> %15945, %15554
  %15947 = add <4 x i32> %15946, %convolved944.sroa.3176.9.us
  %t3779.us = add i32 %t2473601.us, %15491
  %15948 = sext i32 %t3779.us to i64
  %15949 = shl nsw i64 %15948, 4
  %15950 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15949
  %15951 = bitcast i8* %15950 to <8 x i8>*
  %t3780.us = load <8 x i8>, <8 x i8>* %15951, align 16, !tbaa !438
  %15952 = getelementptr inbounds i8, i8* %15950, i64 8
  %15953 = bitcast i8* %15952 to <8 x i8>*
  %t3781.us = load <8 x i8>, <8 x i8>* %15953, align 8, !tbaa !438
  %15954 = zext <8 x i8> %t3780.us to <8 x i16>
  %15955 = bitcast <8 x i16> %15954 to <2 x i64>
  %15956 = shufflevector <2 x i64> %15955, <2 x i64> undef, <1 x i32> zeroinitializer
  %15957 = bitcast <1 x i64> %15956 to <4 x i16>
  %15958 = sext <4 x i16> %15957 to <4 x i32>
  %15959 = mul nsw <4 x i32> %15958, %15525
  %15960 = add <4 x i32> %15959, %convolved944.sroa.3232.9.us
  %15961 = shufflevector <2 x i64> %15955, <2 x i64> undef, <1 x i32> <i32 1>
  %15962 = bitcast <1 x i64> %15961 to <4 x i16>
  %15963 = sext <4 x i16> %15962 to <4 x i32>
  %15964 = mul nsw <4 x i32> %15963, %15534
  %15965 = add <4 x i32> %15964, %convolved944.sroa.3288.9.us
  %15966 = zext <8 x i8> %t3781.us to <8 x i16>
  %15967 = bitcast <8 x i16> %15966 to <2 x i64>
  %15968 = shufflevector <2 x i64> %15967, <2 x i64> undef, <1 x i32> zeroinitializer
  %15969 = bitcast <1 x i64> %15968 to <4 x i16>
  %15970 = sext <4 x i16> %15969 to <4 x i32>
  %15971 = mul nsw <4 x i32> %15970, %15545
  %15972 = add <4 x i32> %15971, %convolved944.sroa.3344.9.us
  %15973 = shufflevector <2 x i64> %15967, <2 x i64> undef, <1 x i32> <i32 1>
  %15974 = bitcast <1 x i64> %15973 to <4 x i16>
  %15975 = sext <4 x i16> %15974 to <4 x i32>
  %15976 = mul nsw <4 x i32> %15975, %15554
  %15977 = add <4 x i32> %15976, %convolved944.sroa.3400.9.us
  %t3783.us = add i32 %t2473601.us, %15490
  %15978 = sext i32 %t3783.us to i64
  %15979 = shl nsw i64 %15978, 4
  %15980 = getelementptr inbounds i8, i8* %resampled_input473, i64 %15979
  %15981 = bitcast i8* %15980 to <8 x i8>*
  %t3784.us = load <8 x i8>, <8 x i8>* %15981, align 16, !tbaa !438
  %15982 = getelementptr inbounds i8, i8* %15980, i64 8
  %15983 = bitcast i8* %15982 to <8 x i8>*
  %t3785.us = load <8 x i8>, <8 x i8>* %15983, align 8, !tbaa !438
  %15984 = zext <8 x i8> %t3784.us to <8 x i16>
  %15985 = bitcast <8 x i16> %15984 to <2 x i64>
  %15986 = shufflevector <2 x i64> %15985, <2 x i64> undef, <1 x i32> zeroinitializer
  %15987 = bitcast <1 x i64> %15986 to <4 x i16>
  %15988 = sext <4 x i16> %15987 to <4 x i32>
  %15989 = mul nsw <4 x i32> %15988, %15525
  %15990 = add <4 x i32> %15989, %convolved944.sroa.3456.9.us
  %15991 = shufflevector <2 x i64> %15985, <2 x i64> undef, <1 x i32> <i32 1>
  %15992 = bitcast <1 x i64> %15991 to <4 x i16>
  %15993 = sext <4 x i16> %15992 to <4 x i32>
  %15994 = mul nsw <4 x i32> %15993, %15534
  %15995 = add <4 x i32> %15994, %convolved944.sroa.3512.9.us
  %15996 = zext <8 x i8> %t3785.us to <8 x i16>
  %15997 = bitcast <8 x i16> %15996 to <2 x i64>
  %15998 = shufflevector <2 x i64> %15997, <2 x i64> undef, <1 x i32> zeroinitializer
  %15999 = bitcast <1 x i64> %15998 to <4 x i16>
  %16000 = sext <4 x i16> %15999 to <4 x i32>
  %16001 = mul nsw <4 x i32> %16000, %15545
  %16002 = add <4 x i32> %16001, %convolved944.sroa.3568.9.us
  %16003 = shufflevector <2 x i64> %15997, <2 x i64> undef, <1 x i32> <i32 1>
  %16004 = bitcast <1 x i64> %16003 to <4 x i16>
  %16005 = sext <4 x i16> %16004 to <4 x i32>
  %16006 = mul nsw <4 x i32> %16005, %15554
  %16007 = add <4 x i32> %16006, %convolved944.sroa.3624.9.us
  %indvars.iv.next6458 = add nuw nsw i64 %indvars.iv6457, 1
  %.not1810.us = icmp eq i64 %indvars.iv.next6458, %11242
  br i1 %.not1810.us, label %"end for convolved.s1.r19$x611.loopexit.us", label %"for convolved.s1.r19$x610.us"

"end for convolved.s1.r19$x611.loopexit.us":      ; preds = %"for convolved.s1.r19$x610.us"
  %indvars.iv.next6461 = add nuw nsw i64 %indvars.iv6460, 1
  %.not1809.us = icmp eq i64 %indvars.iv.next6461, %11245
  br i1 %.not1809.us, label %"consume convolved613", label %"for convolved.s1.r19$y585.us"

"consume convolved613":                           ; preds = %"end for convolved.s1.r19$x611.loopexit.us", %"for convolved.s1.r19$y585.preheader", %next_bb579, %then_bb578
  %convolved944.sroa.3624.11 = phi <4 x i32> [ %15482, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %16007, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3568.11 = phi <4 x i32> [ %15445, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %16002, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3512.11 = phi <4 x i32> [ %15390, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15995, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3456.11 = phi <4 x i32> [ %15353, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15990, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3400.11 = phi <4 x i32> [ %15244, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15977, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3344.11 = phi <4 x i32> [ %15207, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15972, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3288.11 = phi <4 x i32> [ %15152, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15965, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3232.11 = phi <4 x i32> [ %15115, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15960, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3176.11 = phi <4 x i32> [ %15006, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15947, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3120.11 = phi <4 x i32> [ %14969, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15942, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3064.11 = phi <4 x i32> [ %14914, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15935, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.3008.11 = phi <4 x i32> [ %14877, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15930, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2952.11 = phi <4 x i32> [ %14768, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15917, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2896.11 = phi <4 x i32> [ %14731, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15912, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2840.11 = phi <4 x i32> [ %14676, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15905, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2784.11 = phi <4 x i32> [ %14639, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15900, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2728.11 = phi <4 x i32> [ %14530, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15887, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2672.11 = phi <4 x i32> [ %14493, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15882, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2616.11 = phi <4 x i32> [ %14438, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15875, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2560.11 = phi <4 x i32> [ %14401, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15870, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2504.11 = phi <4 x i32> [ %14292, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15857, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2448.11 = phi <4 x i32> [ %14255, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15852, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2392.11 = phi <4 x i32> [ %14200, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15845, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2336.11 = phi <4 x i32> [ %14163, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15840, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2280.11 = phi <4 x i32> [ %14054, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15827, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2224.11 = phi <4 x i32> [ %14017, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15822, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2168.11 = phi <4 x i32> [ %13962, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15815, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2112.11 = phi <4 x i32> [ %13925, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15810, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2056.11 = phi <4 x i32> [ %13816, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15797, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.2000.11 = phi <4 x i32> [ %13779, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15792, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1944.11 = phi <4 x i32> [ %13724, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15785, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1888.11 = phi <4 x i32> [ %13687, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15780, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1832.11 = phi <4 x i32> [ %13578, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15767, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1776.11 = phi <4 x i32> [ %13541, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15762, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1720.11 = phi <4 x i32> [ %13486, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15755, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1664.11 = phi <4 x i32> [ %13449, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15750, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1608.11 = phi <4 x i32> [ %13340, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15737, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1552.11 = phi <4 x i32> [ %13303, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15732, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1496.11 = phi <4 x i32> [ %13248, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15725, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1440.11 = phi <4 x i32> [ %13211, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15720, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1384.11 = phi <4 x i32> [ %13102, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15707, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1328.11 = phi <4 x i32> [ %13065, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15702, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1272.11 = phi <4 x i32> [ %13010, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15695, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1216.11 = phi <4 x i32> [ %12973, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15690, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1160.11 = phi <4 x i32> [ %12864, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15677, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1104.11 = phi <4 x i32> [ %12827, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15672, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.1048.11 = phi <4 x i32> [ %12772, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15665, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.992.11 = phi <4 x i32> [ %12735, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15660, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.930.15 = phi <4 x i32> [ %12626, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15647, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.868.15 = phi <4 x i32> [ %12589, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15642, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.806.15 = phi <4 x i32> [ %12534, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15635, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.744.15 = phi <4 x i32> [ %12497, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15630, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.682.15 = phi <4 x i32> [ %12387, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15617, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.620.15 = phi <4 x i32> [ %12350, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15612, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.558.15 = phi <4 x i32> [ %12295, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15605, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.496.15 = phi <4 x i32> [ %12258, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15600, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.434.15 = phi <4 x i32> [ %12148, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15587, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.372.15 = phi <4 x i32> [ %12111, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15582, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.310.15 = phi <4 x i32> [ %12056, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15575, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.248.15 = phi <4 x i32> [ %12019, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15570, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.186.15 = phi <4 x i32> [ %11909, %then_bb578 ], [ %11565, %next_bb579 ], [ %11565, %"for convolved.s1.r19$y585.preheader" ], [ %15557, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.124.15 = phi <4 x i32> [ %11858, %then_bb578 ], [ %11564, %next_bb579 ], [ %11564, %"for convolved.s1.r19$y585.preheader" ], [ %15548, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.62.15 = phi <4 x i32> [ %11789, %then_bb578 ], [ %11563, %next_bb579 ], [ %11563, %"for convolved.s1.r19$y585.preheader" ], [ %15537, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %convolved944.sroa.0.15 = phi <4 x i32> [ %11738, %then_bb578 ], [ %11562, %next_bb579 ], [ %11562, %"for convolved.s1.r19$y585.preheader" ], [ %15528, %"end for convolved.s1.r19$x611.loopexit.us" ]
  %16008 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.0.15, <4 x i32> %11225) #11
  %16009 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16008, <4 x i32> %11228) #11
  %16010 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16009, <4 x i32> %11231) #11
  %16011 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16010) #11
  %16012 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.62.15, <4 x i32> %11225) #11
  %16013 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16012, <4 x i32> %11228) #11
  %16014 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16013, <4 x i32> %11231) #11
  %16015 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16014) #11
  %16016 = shufflevector <4 x i16> %16011, <4 x i16> %16015, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16017 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16016, <8 x i16> %11234) #11
  %16018 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16017) #11
  %16019 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.124.15, <4 x i32> %11225) #11
  %16020 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16019, <4 x i32> %11228) #11
  %16021 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16020, <4 x i32> %11231) #11
  %16022 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16021) #11
  %16023 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.186.15, <4 x i32> %11225) #11
  %16024 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16023, <4 x i32> %11228) #11
  %16025 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16024, <4 x i32> %11231) #11
  %16026 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16025) #11
  %16027 = shufflevector <4 x i16> %16022, <4 x i16> %16026, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16028 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16027, <8 x i16> %11234) #11
  %16029 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16028) #11
  %16030 = shufflevector <8 x i8> %16018, <8 x i8> %16029, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16031 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16030) #11
  %16032 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16031, <16 x i8> %11238) #11
  %16033 = shl nuw nsw i64 %indvars.iv6463, 2
  %16034 = add nsw i64 %16033, %11239
  %16035 = mul nsw i64 %16034, %11240
  %16036 = add nsw i64 %16035, %11608
  %16037 = getelementptr inbounds i8, i8* %59, i64 %16036
  %16038 = bitcast i8* %16037 to <16 x i8>*
  store <16 x i8> %16032, <16 x i8>* %16038, align 1, !tbaa !515
  %16039 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.248.15, <4 x i32> %11225) #11
  %16040 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16039, <4 x i32> %11228) #11
  %16041 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16040, <4 x i32> %11231) #11
  %16042 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16041) #11
  %16043 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.310.15, <4 x i32> %11225) #11
  %16044 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16043, <4 x i32> %11228) #11
  %16045 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16044, <4 x i32> %11231) #11
  %16046 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16045) #11
  %16047 = shufflevector <4 x i16> %16042, <4 x i16> %16046, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16048 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16047, <8 x i16> %11234) #11
  %16049 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16048) #11
  %16050 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.372.15, <4 x i32> %11225) #11
  %16051 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16050, <4 x i32> %11228) #11
  %16052 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16051, <4 x i32> %11231) #11
  %16053 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16052) #11
  %16054 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.434.15, <4 x i32> %11225) #11
  %16055 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16054, <4 x i32> %11228) #11
  %16056 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16055, <4 x i32> %11231) #11
  %16057 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16056) #11
  %16058 = shufflevector <4 x i16> %16053, <4 x i16> %16057, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16059 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16058, <8 x i16> %11234) #11
  %16060 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16059) #11
  %16061 = shufflevector <8 x i8> %16049, <8 x i8> %16060, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16062 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16061) #11
  %16063 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16062, <16 x i8> %11238) #11
  %16064 = add nsw i64 %16034, 1
  %16065 = mul nsw i64 %16064, %11240
  %16066 = add nsw i64 %16065, %11608
  %16067 = getelementptr inbounds i8, i8* %59, i64 %16066
  %16068 = bitcast i8* %16067 to <16 x i8>*
  store <16 x i8> %16063, <16 x i8>* %16068, align 1, !tbaa !515
  %16069 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.496.15, <4 x i32> %11225) #11
  %16070 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16069, <4 x i32> %11228) #11
  %16071 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16070, <4 x i32> %11231) #11
  %16072 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16071) #11
  %16073 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.558.15, <4 x i32> %11225) #11
  %16074 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16073, <4 x i32> %11228) #11
  %16075 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16074, <4 x i32> %11231) #11
  %16076 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16075) #11
  %16077 = shufflevector <4 x i16> %16072, <4 x i16> %16076, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16078 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16077, <8 x i16> %11234) #11
  %16079 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16078) #11
  %16080 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.620.15, <4 x i32> %11225) #11
  %16081 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16080, <4 x i32> %11228) #11
  %16082 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16081, <4 x i32> %11231) #11
  %16083 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16082) #11
  %16084 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.682.15, <4 x i32> %11225) #11
  %16085 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16084, <4 x i32> %11228) #11
  %16086 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16085, <4 x i32> %11231) #11
  %16087 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16086) #11
  %16088 = shufflevector <4 x i16> %16083, <4 x i16> %16087, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16089 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16088, <8 x i16> %11234) #11
  %16090 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16089) #11
  %16091 = shufflevector <8 x i8> %16079, <8 x i8> %16090, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16092 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16091) #11
  %16093 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16092, <16 x i8> %11238) #11
  %16094 = add nsw i64 %16034, 2
  %16095 = mul nsw i64 %16094, %11240
  %16096 = add nsw i64 %16095, %11608
  %16097 = getelementptr inbounds i8, i8* %59, i64 %16096
  %16098 = bitcast i8* %16097 to <16 x i8>*
  store <16 x i8> %16093, <16 x i8>* %16098, align 1, !tbaa !515
  %16099 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.744.15, <4 x i32> %11225) #11
  %16100 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16099, <4 x i32> %11228) #11
  %16101 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16100, <4 x i32> %11231) #11
  %16102 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16101) #11
  %16103 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.806.15, <4 x i32> %11225) #11
  %16104 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16103, <4 x i32> %11228) #11
  %16105 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16104, <4 x i32> %11231) #11
  %16106 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16105) #11
  %16107 = shufflevector <4 x i16> %16102, <4 x i16> %16106, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16108 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16107, <8 x i16> %11234) #11
  %16109 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16108) #11
  %16110 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.868.15, <4 x i32> %11225) #11
  %16111 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16110, <4 x i32> %11228) #11
  %16112 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16111, <4 x i32> %11231) #11
  %16113 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16112) #11
  %16114 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.930.15, <4 x i32> %11225) #11
  %16115 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16114, <4 x i32> %11228) #11
  %16116 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16115, <4 x i32> %11231) #11
  %16117 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16116) #11
  %16118 = shufflevector <4 x i16> %16113, <4 x i16> %16117, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16119 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16118, <8 x i16> %11234) #11
  %16120 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16119) #11
  %16121 = shufflevector <8 x i8> %16109, <8 x i8> %16120, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16122 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16121) #11
  %16123 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16122, <16 x i8> %11238) #11
  %16124 = add nsw i64 %16034, 3
  %16125 = mul nsw i64 %16124, %11240
  %16126 = add nsw i64 %16125, %11608
  %16127 = getelementptr inbounds i8, i8* %59, i64 %16126
  %16128 = bitcast i8* %16127 to <16 x i8>*
  store <16 x i8> %16123, <16 x i8>* %16128, align 1, !tbaa !515
  %16129 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.992.11, <4 x i32> %11225) #11
  %16130 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16129, <4 x i32> %11228) #11
  %16131 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16130, <4 x i32> %11231) #11
  %16132 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16131) #11
  %16133 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1048.11, <4 x i32> %11225) #11
  %16134 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16133, <4 x i32> %11228) #11
  %16135 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16134, <4 x i32> %11231) #11
  %16136 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16135) #11
  %16137 = shufflevector <4 x i16> %16132, <4 x i16> %16136, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16138 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16137, <8 x i16> %11234) #11
  %16139 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16138) #11
  %16140 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1104.11, <4 x i32> %11225) #11
  %16141 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16140, <4 x i32> %11228) #11
  %16142 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16141, <4 x i32> %11231) #11
  %16143 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16142) #11
  %16144 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1160.11, <4 x i32> %11225) #11
  %16145 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16144, <4 x i32> %11228) #11
  %16146 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16145, <4 x i32> %11231) #11
  %16147 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16146) #11
  %16148 = shufflevector <4 x i16> %16143, <4 x i16> %16147, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16149 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16148, <8 x i16> %11234) #11
  %16150 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16149) #11
  %16151 = shufflevector <8 x i8> %16139, <8 x i8> %16150, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16152 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16151) #11
  %16153 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16152, <16 x i8> %11238) #11
  %16154 = add nsw i64 %16035, %11609
  %16155 = getelementptr inbounds i8, i8* %59, i64 %16154
  %16156 = bitcast i8* %16155 to <16 x i8>*
  store <16 x i8> %16153, <16 x i8>* %16156, align 1, !tbaa !515
  %16157 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1216.11, <4 x i32> %11225) #11
  %16158 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16157, <4 x i32> %11228) #11
  %16159 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16158, <4 x i32> %11231) #11
  %16160 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16159) #11
  %16161 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1272.11, <4 x i32> %11225) #11
  %16162 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16161, <4 x i32> %11228) #11
  %16163 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16162, <4 x i32> %11231) #11
  %16164 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16163) #11
  %16165 = shufflevector <4 x i16> %16160, <4 x i16> %16164, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16166 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16165, <8 x i16> %11234) #11
  %16167 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16166) #11
  %16168 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1328.11, <4 x i32> %11225) #11
  %16169 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16168, <4 x i32> %11228) #11
  %16170 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16169, <4 x i32> %11231) #11
  %16171 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16170) #11
  %16172 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1384.11, <4 x i32> %11225) #11
  %16173 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16172, <4 x i32> %11228) #11
  %16174 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16173, <4 x i32> %11231) #11
  %16175 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16174) #11
  %16176 = shufflevector <4 x i16> %16171, <4 x i16> %16175, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16177 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16176, <8 x i16> %11234) #11
  %16178 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16177) #11
  %16179 = shufflevector <8 x i8> %16167, <8 x i8> %16178, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16180 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16179) #11
  %16181 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16180, <16 x i8> %11238) #11
  %16182 = add nsw i64 %16065, %11609
  %16183 = getelementptr inbounds i8, i8* %59, i64 %16182
  %16184 = bitcast i8* %16183 to <16 x i8>*
  store <16 x i8> %16181, <16 x i8>* %16184, align 1, !tbaa !515
  %16185 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1440.11, <4 x i32> %11225) #11
  %16186 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16185, <4 x i32> %11228) #11
  %16187 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16186, <4 x i32> %11231) #11
  %16188 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16187) #11
  %16189 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1496.11, <4 x i32> %11225) #11
  %16190 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16189, <4 x i32> %11228) #11
  %16191 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16190, <4 x i32> %11231) #11
  %16192 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16191) #11
  %16193 = shufflevector <4 x i16> %16188, <4 x i16> %16192, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16194 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16193, <8 x i16> %11234) #11
  %16195 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16194) #11
  %16196 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1552.11, <4 x i32> %11225) #11
  %16197 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16196, <4 x i32> %11228) #11
  %16198 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16197, <4 x i32> %11231) #11
  %16199 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16198) #11
  %16200 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1608.11, <4 x i32> %11225) #11
  %16201 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16200, <4 x i32> %11228) #11
  %16202 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16201, <4 x i32> %11231) #11
  %16203 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16202) #11
  %16204 = shufflevector <4 x i16> %16199, <4 x i16> %16203, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16205 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16204, <8 x i16> %11234) #11
  %16206 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16205) #11
  %16207 = shufflevector <8 x i8> %16195, <8 x i8> %16206, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16208 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16207) #11
  %16209 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16208, <16 x i8> %11238) #11
  %16210 = add nsw i64 %16095, %11609
  %16211 = getelementptr inbounds i8, i8* %59, i64 %16210
  %16212 = bitcast i8* %16211 to <16 x i8>*
  store <16 x i8> %16209, <16 x i8>* %16212, align 1, !tbaa !515
  %16213 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1664.11, <4 x i32> %11225) #11
  %16214 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16213, <4 x i32> %11228) #11
  %16215 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16214, <4 x i32> %11231) #11
  %16216 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16215) #11
  %16217 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1720.11, <4 x i32> %11225) #11
  %16218 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16217, <4 x i32> %11228) #11
  %16219 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16218, <4 x i32> %11231) #11
  %16220 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16219) #11
  %16221 = shufflevector <4 x i16> %16216, <4 x i16> %16220, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16222 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16221, <8 x i16> %11234) #11
  %16223 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16222) #11
  %16224 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1776.11, <4 x i32> %11225) #11
  %16225 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16224, <4 x i32> %11228) #11
  %16226 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16225, <4 x i32> %11231) #11
  %16227 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16226) #11
  %16228 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1832.11, <4 x i32> %11225) #11
  %16229 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16228, <4 x i32> %11228) #11
  %16230 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16229, <4 x i32> %11231) #11
  %16231 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16230) #11
  %16232 = shufflevector <4 x i16> %16227, <4 x i16> %16231, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16233 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16232, <8 x i16> %11234) #11
  %16234 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16233) #11
  %16235 = shufflevector <8 x i8> %16223, <8 x i8> %16234, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16236 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16235) #11
  %16237 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16236, <16 x i8> %11238) #11
  %16238 = add nsw i64 %16125, %11609
  %16239 = getelementptr inbounds i8, i8* %59, i64 %16238
  %16240 = bitcast i8* %16239 to <16 x i8>*
  store <16 x i8> %16237, <16 x i8>* %16240, align 1, !tbaa !515
  %16241 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1888.11, <4 x i32> %11225) #11
  %16242 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16241, <4 x i32> %11228) #11
  %16243 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16242, <4 x i32> %11231) #11
  %16244 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16243) #11
  %16245 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.1944.11, <4 x i32> %11225) #11
  %16246 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16245, <4 x i32> %11228) #11
  %16247 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16246, <4 x i32> %11231) #11
  %16248 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16247) #11
  %16249 = shufflevector <4 x i16> %16244, <4 x i16> %16248, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16250 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16249, <8 x i16> %11234) #11
  %16251 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16250) #11
  %16252 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2000.11, <4 x i32> %11225) #11
  %16253 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16252, <4 x i32> %11228) #11
  %16254 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16253, <4 x i32> %11231) #11
  %16255 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16254) #11
  %16256 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2056.11, <4 x i32> %11225) #11
  %16257 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16256, <4 x i32> %11228) #11
  %16258 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16257, <4 x i32> %11231) #11
  %16259 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16258) #11
  %16260 = shufflevector <4 x i16> %16255, <4 x i16> %16259, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16261 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16260, <8 x i16> %11234) #11
  %16262 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16261) #11
  %16263 = shufflevector <8 x i8> %16251, <8 x i8> %16262, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16264 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16263) #11
  %16265 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16264, <16 x i8> %11238) #11
  %16266 = add nsw i64 %16035, %11610
  %16267 = getelementptr inbounds i8, i8* %59, i64 %16266
  %16268 = bitcast i8* %16267 to <16 x i8>*
  store <16 x i8> %16265, <16 x i8>* %16268, align 1, !tbaa !515
  %16269 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2112.11, <4 x i32> %11225) #11
  %16270 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16269, <4 x i32> %11228) #11
  %16271 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16270, <4 x i32> %11231) #11
  %16272 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16271) #11
  %16273 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2168.11, <4 x i32> %11225) #11
  %16274 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16273, <4 x i32> %11228) #11
  %16275 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16274, <4 x i32> %11231) #11
  %16276 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16275) #11
  %16277 = shufflevector <4 x i16> %16272, <4 x i16> %16276, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16278 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16277, <8 x i16> %11234) #11
  %16279 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16278) #11
  %16280 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2224.11, <4 x i32> %11225) #11
  %16281 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16280, <4 x i32> %11228) #11
  %16282 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16281, <4 x i32> %11231) #11
  %16283 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16282) #11
  %16284 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2280.11, <4 x i32> %11225) #11
  %16285 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16284, <4 x i32> %11228) #11
  %16286 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16285, <4 x i32> %11231) #11
  %16287 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16286) #11
  %16288 = shufflevector <4 x i16> %16283, <4 x i16> %16287, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16289 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16288, <8 x i16> %11234) #11
  %16290 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16289) #11
  %16291 = shufflevector <8 x i8> %16279, <8 x i8> %16290, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16292 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16291) #11
  %16293 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16292, <16 x i8> %11238) #11
  %16294 = add nsw i64 %16065, %11610
  %16295 = getelementptr inbounds i8, i8* %59, i64 %16294
  %16296 = bitcast i8* %16295 to <16 x i8>*
  store <16 x i8> %16293, <16 x i8>* %16296, align 1, !tbaa !515
  %16297 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2336.11, <4 x i32> %11225) #11
  %16298 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16297, <4 x i32> %11228) #11
  %16299 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16298, <4 x i32> %11231) #11
  %16300 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16299) #11
  %16301 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2392.11, <4 x i32> %11225) #11
  %16302 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16301, <4 x i32> %11228) #11
  %16303 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16302, <4 x i32> %11231) #11
  %16304 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16303) #11
  %16305 = shufflevector <4 x i16> %16300, <4 x i16> %16304, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16306 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16305, <8 x i16> %11234) #11
  %16307 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16306) #11
  %16308 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2448.11, <4 x i32> %11225) #11
  %16309 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16308, <4 x i32> %11228) #11
  %16310 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16309, <4 x i32> %11231) #11
  %16311 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16310) #11
  %16312 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2504.11, <4 x i32> %11225) #11
  %16313 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16312, <4 x i32> %11228) #11
  %16314 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16313, <4 x i32> %11231) #11
  %16315 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16314) #11
  %16316 = shufflevector <4 x i16> %16311, <4 x i16> %16315, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16317 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16316, <8 x i16> %11234) #11
  %16318 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16317) #11
  %16319 = shufflevector <8 x i8> %16307, <8 x i8> %16318, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16320 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16319) #11
  %16321 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16320, <16 x i8> %11238) #11
  %16322 = add nsw i64 %16095, %11610
  %16323 = getelementptr inbounds i8, i8* %59, i64 %16322
  %16324 = bitcast i8* %16323 to <16 x i8>*
  store <16 x i8> %16321, <16 x i8>* %16324, align 1, !tbaa !515
  %16325 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2560.11, <4 x i32> %11225) #11
  %16326 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16325, <4 x i32> %11228) #11
  %16327 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16326, <4 x i32> %11231) #11
  %16328 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16327) #11
  %16329 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2616.11, <4 x i32> %11225) #11
  %16330 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16329, <4 x i32> %11228) #11
  %16331 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16330, <4 x i32> %11231) #11
  %16332 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16331) #11
  %16333 = shufflevector <4 x i16> %16328, <4 x i16> %16332, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16334 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16333, <8 x i16> %11234) #11
  %16335 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16334) #11
  %16336 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2672.11, <4 x i32> %11225) #11
  %16337 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16336, <4 x i32> %11228) #11
  %16338 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16337, <4 x i32> %11231) #11
  %16339 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16338) #11
  %16340 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2728.11, <4 x i32> %11225) #11
  %16341 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16340, <4 x i32> %11228) #11
  %16342 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16341, <4 x i32> %11231) #11
  %16343 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16342) #11
  %16344 = shufflevector <4 x i16> %16339, <4 x i16> %16343, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16345 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16344, <8 x i16> %11234) #11
  %16346 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16345) #11
  %16347 = shufflevector <8 x i8> %16335, <8 x i8> %16346, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16348 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16347) #11
  %16349 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16348, <16 x i8> %11238) #11
  %16350 = add nsw i64 %16125, %11610
  %16351 = getelementptr inbounds i8, i8* %59, i64 %16350
  %16352 = bitcast i8* %16351 to <16 x i8>*
  store <16 x i8> %16349, <16 x i8>* %16352, align 1, !tbaa !515
  %16353 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2784.11, <4 x i32> %11225) #11
  %16354 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16353, <4 x i32> %11228) #11
  %16355 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16354, <4 x i32> %11231) #11
  %16356 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16355) #11
  %16357 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2840.11, <4 x i32> %11225) #11
  %16358 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16357, <4 x i32> %11228) #11
  %16359 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16358, <4 x i32> %11231) #11
  %16360 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16359) #11
  %16361 = shufflevector <4 x i16> %16356, <4 x i16> %16360, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16362 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16361, <8 x i16> %11234) #11
  %16363 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16362) #11
  %16364 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2896.11, <4 x i32> %11225) #11
  %16365 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16364, <4 x i32> %11228) #11
  %16366 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16365, <4 x i32> %11231) #11
  %16367 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16366) #11
  %16368 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.2952.11, <4 x i32> %11225) #11
  %16369 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16368, <4 x i32> %11228) #11
  %16370 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16369, <4 x i32> %11231) #11
  %16371 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16370) #11
  %16372 = shufflevector <4 x i16> %16367, <4 x i16> %16371, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16373 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16372, <8 x i16> %11234) #11
  %16374 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16373) #11
  %16375 = shufflevector <8 x i8> %16363, <8 x i8> %16374, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16376 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16375) #11
  %16377 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16376, <16 x i8> %11238) #11
  %16378 = add nsw i64 %16035, %11611
  %16379 = getelementptr inbounds i8, i8* %59, i64 %16378
  %16380 = bitcast i8* %16379 to <16 x i8>*
  store <16 x i8> %16377, <16 x i8>* %16380, align 1, !tbaa !515
  %16381 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3008.11, <4 x i32> %11225) #11
  %16382 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16381, <4 x i32> %11228) #11
  %16383 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16382, <4 x i32> %11231) #11
  %16384 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16383) #11
  %16385 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3064.11, <4 x i32> %11225) #11
  %16386 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16385, <4 x i32> %11228) #11
  %16387 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16386, <4 x i32> %11231) #11
  %16388 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16387) #11
  %16389 = shufflevector <4 x i16> %16384, <4 x i16> %16388, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16390 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16389, <8 x i16> %11234) #11
  %16391 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16390) #11
  %16392 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3120.11, <4 x i32> %11225) #11
  %16393 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16392, <4 x i32> %11228) #11
  %16394 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16393, <4 x i32> %11231) #11
  %16395 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16394) #11
  %16396 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3176.11, <4 x i32> %11225) #11
  %16397 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16396, <4 x i32> %11228) #11
  %16398 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16397, <4 x i32> %11231) #11
  %16399 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16398) #11
  %16400 = shufflevector <4 x i16> %16395, <4 x i16> %16399, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16401 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16400, <8 x i16> %11234) #11
  %16402 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16401) #11
  %16403 = shufflevector <8 x i8> %16391, <8 x i8> %16402, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16404 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16403) #11
  %16405 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16404, <16 x i8> %11238) #11
  %16406 = add nsw i64 %16065, %11611
  %16407 = getelementptr inbounds i8, i8* %59, i64 %16406
  %16408 = bitcast i8* %16407 to <16 x i8>*
  store <16 x i8> %16405, <16 x i8>* %16408, align 1, !tbaa !515
  %16409 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3232.11, <4 x i32> %11225) #11
  %16410 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16409, <4 x i32> %11228) #11
  %16411 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16410, <4 x i32> %11231) #11
  %16412 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16411) #11
  %16413 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3288.11, <4 x i32> %11225) #11
  %16414 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16413, <4 x i32> %11228) #11
  %16415 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16414, <4 x i32> %11231) #11
  %16416 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16415) #11
  %16417 = shufflevector <4 x i16> %16412, <4 x i16> %16416, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16418 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16417, <8 x i16> %11234) #11
  %16419 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16418) #11
  %16420 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3344.11, <4 x i32> %11225) #11
  %16421 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16420, <4 x i32> %11228) #11
  %16422 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16421, <4 x i32> %11231) #11
  %16423 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16422) #11
  %16424 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3400.11, <4 x i32> %11225) #11
  %16425 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16424, <4 x i32> %11228) #11
  %16426 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16425, <4 x i32> %11231) #11
  %16427 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16426) #11
  %16428 = shufflevector <4 x i16> %16423, <4 x i16> %16427, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16429 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16428, <8 x i16> %11234) #11
  %16430 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16429) #11
  %16431 = shufflevector <8 x i8> %16419, <8 x i8> %16430, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16432 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16431) #11
  %16433 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16432, <16 x i8> %11238) #11
  %16434 = add nsw i64 %16095, %11611
  %16435 = getelementptr inbounds i8, i8* %59, i64 %16434
  %16436 = bitcast i8* %16435 to <16 x i8>*
  store <16 x i8> %16433, <16 x i8>* %16436, align 1, !tbaa !515
  %16437 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3456.11, <4 x i32> %11225) #11
  %16438 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16437, <4 x i32> %11228) #11
  %16439 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16438, <4 x i32> %11231) #11
  %16440 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16439) #11
  %16441 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3512.11, <4 x i32> %11225) #11
  %16442 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16441, <4 x i32> %11228) #11
  %16443 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16442, <4 x i32> %11231) #11
  %16444 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16443) #11
  %16445 = shufflevector <4 x i16> %16440, <4 x i16> %16444, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16446 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16445, <8 x i16> %11234) #11
  %16447 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16446) #11
  %16448 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3568.11, <4 x i32> %11225) #11
  %16449 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16448, <4 x i32> %11228) #11
  %16450 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16449, <4 x i32> %11231) #11
  %16451 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16450) #11
  %16452 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %convolved944.sroa.3624.11, <4 x i32> %11225) #11
  %16453 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %16452, <4 x i32> %11228) #11
  %16454 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %16453, <4 x i32> %11231) #11
  %16455 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %16454) #11
  %16456 = shufflevector <4 x i16> %16451, <4 x i16> %16455, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16457 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %16456, <8 x i16> %11234) #11
  %16458 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %16457) #11
  %16459 = shufflevector <8 x i8> %16447, <8 x i8> %16458, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16460 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %11236, <16 x i8> %16459) #11
  %16461 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %16460, <16 x i8> %11238) #11
  %16462 = add nsw i64 %16125, %11611
  %16463 = getelementptr inbounds i8, i8* %59, i64 %16462
  %16464 = bitcast i8* %16463 to <16 x i8>*
  store <16 x i8> %16461, <16 x i8>* %16464, align 1, !tbaa !515
  %indvars.iv.next6464 = add nuw nsw i64 %indvars.iv6463, 1
  %.not1808 = icmp eq i64 %indvars.iv.next6464, %11254
  br i1 %.not1808, label %"end for output.s0.x.xo574", label %"for output.s0.x.xo573"

then_bb614:                                       ; preds = %next_bb408
  %16465 = icmp eq i32 %46, 3
  %16466 = icmp eq i32 %48, 3
  %16467 = and i1 %16465, %16466
  %16468 = icmp sgt i32 %46, 0
  %16469 = select i1 %16468, i32 %46, i32 0
  %t4615 = zext i32 %16469 to i64
  %16470 = icmp sgt i32 %48, 0
  %16471 = select i1 %16470, i32 %48, i32 0
  %t4616 = zext i32 %16471 to i64
  %16472 = shl nuw nsw i64 %t4615, 5
  %16473 = mul i64 %16472, %t4616
  %16474 = or i64 %16473, 6
  %16475 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8*
  %16476 = load i64, i64* %.fca.1.gep, align 8, !tbaa !385
  %cmp.i3236 = icmp ult i64 %16476, %16474
  %16477 = load i8*, i8** %.fca.0.gep, align 8, !tbaa !387
  br i1 %cmp.i3236, label %if.then.i3239, label %pseudostack_alloc.exit3252, !prof !388

if.then.i3239:                                    ; preds = %then_bb614
  %tobool1.not.i3238 = icmp ne i8* %16477, null
  %16478 = load i64, i64* %.fca.2.gep, align 8
  %cmp2.i3241 = icmp ugt i64 %16478, 16384
  %or.cond5134 = and i1 %tobool1.not.i3238, %cmp2.i3241
  br i1 %or.cond5134, label %if.then3.i3243, label %if.end.i3247

if.then3.i3243:                                   ; preds = %if.then.i3239
  call void @halide_free(i8* null, i8* nonnull %16477) #15
  %.pre6507 = load i64, i64* %.fca.2.gep, align 8, !tbaa !389
  br label %if.end.i3247

if.end.i3247:                                     ; preds = %if.then3.i3243, %if.then.i3239
  %16479 = phi i64 [ %.pre6507, %if.then3.i3243 ], [ %16478, %if.then.i3239 ]
  %add.i3245 = add i64 %16479, %16474
  store i64 %add.i3245, i64* %.fca.2.gep, align 8, !tbaa !389
  %cmp7.i3246 = icmp ugt i64 %add.i3245, 16384
  br i1 %cmp7.i3246, label %if.then8.i3249, label %if.end11.i3251

if.then8.i3249:                                   ; preds = %if.end.i3247
  %call.i3248 = call i8* @halide_malloc(i8* null, i64 %16474) #15
  br label %if.end11.i3251

if.end11.i3251:                                   ; preds = %if.then8.i3249, %if.end.i3247
  %storemerge.i3250 = phi i8* [ %call.i3248, %if.then8.i3249 ], [ null, %if.end.i3247 ]
  store i8* %storemerge.i3250, i8** %.fca.0.gep, align 8, !tbaa !387
  store i64 %16474, i64* %.fca.1.gep, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3252

pseudostack_alloc.exit3252:                       ; preds = %then_bb614, %if.end11.i3251
  %16480 = phi i8* [ %storemerge.i3250, %if.end11.i3251 ], [ %16477, %then_bb614 ]
  %16481 = bitcast i8* %16480 to i16*
  %.not = icmp eq i8* %16480, null
  br i1 %.not, label %then_bb617, label %"produce filter_zeroed619", !prof !390

then_bb617:                                       ; preds = %pseudostack_alloc.exit3252
  %16482 = alloca i8*, i64 %16474, align 16
  %16483 = bitcast i8** %16482 to i16*
  %16484 = bitcast %struct.halide_pseudostack_slot_t* %filter_zeroed.pseudostack_slot to i8***
  store i8** %16482, i8*** %16484, align 8
  br label %"produce filter_zeroed619"

"produce filter_zeroed619":                       ; preds = %pseudostack_alloc.exit3252, %then_bb617
  %filter_zeroed618 = phi i16* [ %16483, %then_bb617 ], [ %16481, %pseudostack_alloc.exit3252 ]
  %t2483620 = icmp sgt i32 %45, 8
  %t2482621 = icmp sgt i32 %45, 7
  br i1 %16470, label %"for filter_zeroed.s0.y622.preheader", label %"produce sum_filter639", !prof !391

"for filter_zeroed.s0.y622.preheader":            ; preds = %"produce filter_zeroed619"
  %16485 = insertelement <8 x i32> undef, i32 %45, i32 0
  %16486 = shufflevector <8 x i32> %16485, <8 x i32> undef, <8 x i32> zeroinitializer
  %16487 = icmp sgt <8 x i32> %16486, <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16488 = sext i32 %47 to i64
  %16489 = zext i8 %filter_zero to i16
  %16490 = insertelement <8 x i16> undef, i16 %16489, i32 0
  %16491 = shufflevector <8 x i16> %16490, <8 x i16> undef, <8 x i32> zeroinitializer
  %16492 = icmp sgt <8 x i32> %16486, <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  br i1 %16468, label %"for filter_zeroed.s0.y622.us.preheader", label %"produce sum_filter639", !prof !391

"for filter_zeroed.s0.y622.us.preheader":         ; preds = %"for filter_zeroed.s0.y622.preheader"
  %16493 = zext i32 %46 to i64
  %16494 = zext i32 %46 to i64
  %16495 = sext i32 %49 to i64
  %16496 = zext i32 %48 to i64
  br label %"for filter_zeroed.s0.y622.us"

"for filter_zeroed.s0.y622.us":                   ; preds = %"for filter_zeroed.s0.y622.us.preheader", %"end for filter_zeroed.s0.x628.loopexit.us"
  %indvars.iv6398 = phi i64 [ 0, %"for filter_zeroed.s0.y622.us.preheader" ], [ %indvars.iv.next6399, %"end for filter_zeroed.s0.x628.loopexit.us" ]
  %16497 = mul nsw i64 %indvars.iv6398, %16494
  %16498 = mul nsw i64 %indvars.iv6398, %16495
  br i1 %t2482621, label %"for filter_zeroed.s0.x627.preheader.split.us.us", label %"for filter_zeroed.s0.x627.us5731"

"for filter_zeroed.s0.x627.us5731":               ; preds = %"for filter_zeroed.s0.y622.us", %"for filter_zeroed.s0.x627.us5731"
  %indvars.iv6392 = phi i64 [ %indvars.iv.next6393, %"for filter_zeroed.s0.x627.us5731" ], [ 0, %"for filter_zeroed.s0.y622.us" ]
  %16499 = mul nsw i64 %indvars.iv6392, %16488
  %16500 = add nsw i64 %16499, %16498
  %16501 = getelementptr inbounds i8, i8* %43, i64 %16500
  %16502 = bitcast i8* %16501 to <8 x i8>*
  %16503 = call <8 x i8> @llvm.masked.load.v8i8.p0v8i8(<8 x i8>* %16502, i32 1, <8 x i1> %16487, <8 x i8> undef), !tbaa !392
  %16504 = zext <8 x i8> %16503 to <8 x i16>
  %16505 = sub nsw <8 x i16> %16504, %16491
  %16506 = add nuw nsw i64 %indvars.iv6392, %16497
  %16507 = shl nsw i64 %16506, 4
  %16508 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 %16507
  %16509 = bitcast i16* %16508 to <8 x i16>*
  call void @llvm.masked.store.v8i16.p0v8i16(<8 x i16> %16505, <8 x i16>* %16509, i32 16, <8 x i1> %16487), !tbaa !395
  %indvars.iv.next6393 = add nuw nsw i64 %indvars.iv6392, 1
  %.not1798.us5733 = icmp eq i64 %indvars.iv.next6393, %16493
  br i1 %.not1798.us5733, label %"end for filter_zeroed.s0.x628.loopexit.us", label %"for filter_zeroed.s0.x627.us5731"

"end for filter_zeroed.s0.x628.loopexit.us":      ; preds = %"for filter_zeroed.s0.x627.us5731", %"for filter_zeroed.s0.x627.us.us5736", %"for filter_zeroed.s0.x627.us.us.us"
  %indvars.iv.next6399 = add nuw nsw i64 %indvars.iv6398, 1
  %.not1797.us = icmp eq i64 %indvars.iv.next6399, %16496
  br i1 %.not1797.us, label %"produce sum_filter639", label %"for filter_zeroed.s0.y622.us"

"for filter_zeroed.s0.x627.preheader.split.us.us": ; preds = %"for filter_zeroed.s0.y622.us"
  br i1 %t2483620, label %"for filter_zeroed.s0.x627.us.us.us", label %"for filter_zeroed.s0.x627.us.us5736"

"for filter_zeroed.s0.x627.us.us5736":            ; preds = %"for filter_zeroed.s0.x627.preheader.split.us.us", %"for filter_zeroed.s0.x627.us.us5736"
  %indvars.iv6394 = phi i64 [ %indvars.iv.next6395, %"for filter_zeroed.s0.x627.us.us5736" ], [ 0, %"for filter_zeroed.s0.x627.preheader.split.us.us" ]
  %16510 = mul nsw i64 %indvars.iv6394, %16488
  %16511 = add nsw i64 %16510, %16498
  %16512 = getelementptr inbounds i8, i8* %43, i64 %16511
  %16513 = bitcast i8* %16512 to <8 x i8>*
  %16514 = load <8 x i8>, <8 x i8>* %16513, align 1, !tbaa !392
  %16515 = zext <8 x i8> %16514 to <8 x i16>
  %16516 = sub nsw <8 x i16> %16515, %16491
  %16517 = add nuw nsw i64 %indvars.iv6394, %16497
  %16518 = shl nsw i64 %16517, 4
  %16519 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 %16518
  %16520 = bitcast i16* %16519 to <8 x i16>*
  store <8 x i16> %16516, <8 x i16>* %16520, align 16, !tbaa !395
  %indvars.iv.next6395 = add nuw nsw i64 %indvars.iv6394, 1
  %.not1798.us.us5738 = icmp eq i64 %indvars.iv.next6395, %16493
  br i1 %.not1798.us.us5738, label %"end for filter_zeroed.s0.x628.loopexit.us", label %"for filter_zeroed.s0.x627.us.us5736"

"for filter_zeroed.s0.x627.us.us.us":             ; preds = %"for filter_zeroed.s0.x627.preheader.split.us.us", %"for filter_zeroed.s0.x627.us.us.us"
  %indvars.iv6396 = phi i64 [ %indvars.iv.next6397, %"for filter_zeroed.s0.x627.us.us.us" ], [ 0, %"for filter_zeroed.s0.x627.preheader.split.us.us" ]
  %16521 = mul nsw i64 %indvars.iv6396, %16488
  %16522 = add nsw i64 %16521, %16498
  %16523 = getelementptr inbounds i8, i8* %43, i64 %16522
  %16524 = bitcast i8* %16523 to <8 x i8>*
  %16525 = load <8 x i8>, <8 x i8>* %16524, align 1, !tbaa !392
  %16526 = zext <8 x i8> %16525 to <8 x i16>
  %16527 = sub nsw <8 x i16> %16526, %16491
  %16528 = add nuw nsw i64 %indvars.iv6396, %16497
  %16529 = shl nsw i64 %16528, 4
  %16530 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 %16529
  %16531 = bitcast i16* %16530 to <8 x i16>*
  store <8 x i16> %16527, <8 x i16>* %16531, align 16, !tbaa !395
  %16532 = getelementptr inbounds i8, i8* %16523, i64 8
  %16533 = bitcast i8* %16532 to <8 x i8>*
  %16534 = call <8 x i8> @llvm.masked.load.v8i8.p0v8i8(<8 x i8>* nonnull %16533, i32 1, <8 x i1> %16492, <8 x i8> undef), !tbaa !392
  %16535 = zext <8 x i8> %16534 to <8 x i16>
  %16536 = sub nsw <8 x i16> %16535, %16491
  %16537 = getelementptr inbounds i16, i16* %16530, i64 8
  %16538 = bitcast i16* %16537 to <8 x i16>*
  call void @llvm.masked.store.v8i16.p0v8i16(<8 x i16> %16536, <8 x i16>* nonnull %16538, i32 16, <8 x i1> %16492), !tbaa !395
  %indvars.iv.next6397 = add nuw nsw i64 %indvars.iv6396, 1
  %.not1798.us.us.us = icmp eq i64 %indvars.iv.next6397, %16493
  br i1 %.not1798.us.us.us, label %"end for filter_zeroed.s0.x628.loopexit.us", label %"for filter_zeroed.s0.x627.us.us.us"

"produce sum_filter639":                          ; preds = %"end for filter_zeroed.s0.x628.loopexit.us", %"for filter_zeroed.s0.y622.preheader", %"produce filter_zeroed619"
  %16539 = insertelement <16 x i32> undef, i32 %45, i32 0
  %16540 = shufflevector <16 x i32> %16539, <16 x i32> undef, <16 x i32> zeroinitializer
  %16541 = icmp sgt <16 x i32> %16540, <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16542 = bitcast [64 x i32]* %sum_filter945 to <4 x i32>*
  %16543 = shufflevector <16 x i1> %16541, <16 x i1> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> zeroinitializer, <4 x i32>* nonnull %16542, i32 16, <4 x i1> %16543), !tbaa !441
  %16544 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 4
  %16545 = bitcast i32* %16544 to <4 x i32>*
  %16546 = shufflevector <16 x i1> %16541, <16 x i1> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> zeroinitializer, <4 x i32>* nonnull %16545, i32 16, <4 x i1> %16546), !tbaa !452
  %16547 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 8
  %16548 = bitcast i32* %16547 to <4 x i32>*
  %16549 = shufflevector <16 x i1> %16541, <16 x i1> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> zeroinitializer, <4 x i32>* nonnull %16548, i32 16, <4 x i1> %16549), !tbaa !454
  %16550 = getelementptr inbounds [64 x i32], [64 x i32]* %sum_filter945, i64 0, i64 12
  %16551 = bitcast i32* %16550 to <4 x i32>*
  %16552 = shufflevector <16 x i1> %16541, <16 x i1> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> zeroinitializer, <4 x i32>* nonnull %16551, i32 16, <4 x i1> %16552), !tbaa !457
  %16553 = insertelement <4 x i32> undef, i32 %45, i32 0
  %16554 = shufflevector <4 x i32> %16553, <4 x i32> undef, <4 x i32> zeroinitializer
  br i1 %16470, label %"for sum_filter.s1.r19$y640.preheader", label %"consume sum_filter648", !prof !391

"for sum_filter.s1.r19$y640.preheader":           ; preds = %"produce sum_filter639"
  %t3802 = icmp sgt <4 x i32> %16554, <i32 0, i32 1, i32 2, i32 3>
  %t3804 = icmp sgt <4 x i32> %16554, <i32 4, i32 5, i32 6, i32 7>
  %t3805 = icmp sgt <4 x i32> %16554, <i32 8, i32 9, i32 10, i32 11>
  %t3806 = icmp sgt <4 x i32> %16554, <i32 12, i32 13, i32 14, i32 15>
  br i1 %16468, label %"for sum_filter.s1.r19$y640.us.preheader", label %"consume sum_filter648", !prof !391

"for sum_filter.s1.r19$y640.us.preheader":        ; preds = %"for sum_filter.s1.r19$y640.preheader"
  %16555 = zext i32 %46 to i64
  %16556 = zext i32 %46 to i64
  %16557 = zext i32 %48 to i64
  br label %"for sum_filter.s1.r19$y640.us"

"for sum_filter.s1.r19$y640.us":                  ; preds = %"for sum_filter.s1.r19$y640.us.preheader", %"end for sum_filter.s1.r19$x645.loopexit.us"
  %indvars.iv6405 = phi i64 [ 0, %"for sum_filter.s1.r19$y640.us.preheader" ], [ %indvars.iv.next6406, %"end for sum_filter.s1.r19$x645.loopexit.us" ]
  %16558 = mul nsw i64 %indvars.iv6405, %16556
  br label %"for sum_filter.s1.r19$x644.us"

"for sum_filter.s1.r19$x644.us":                  ; preds = %"for sum_filter.s1.r19$y640.us", %"for sum_filter.s1.r19$x644.us"
  %indvars.iv6402 = phi i64 [ 0, %"for sum_filter.s1.r19$y640.us" ], [ %indvars.iv.next6403, %"for sum_filter.s1.r19$x644.us" ]
  %16559 = add nuw nsw i64 %indvars.iv6402, %16558
  %unmaskedload1792.us = load <4 x i32>, <4 x i32>* %16542, align 16
  %16560 = select <4 x i1> %t3802, <4 x i32> %unmaskedload1792.us, <4 x i32> undef
  %16561 = shl nsw i64 %16559, 4
  %16562 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 %16561
  %16563 = bitcast i16* %16562 to <4 x i16>*
  %16564 = call <4 x i16> @llvm.masked.load.v4i16.p0v4i16(<4 x i16>* %16563, i32 16, <4 x i1> %t3802, <4 x i16> undef), !tbaa !395
  %16565 = sext <4 x i16> %16564 to <4 x i32>
  %16566 = add <4 x i32> %16560, %16565
  %unmaskedload1793.us = load <4 x i32>, <4 x i32>* %16545, align 16
  %16567 = select <4 x i1> %t3804, <4 x i32> %unmaskedload1793.us, <4 x i32> undef
  %16568 = getelementptr inbounds i16, i16* %16562, i64 4
  %16569 = bitcast i16* %16568 to <4 x i16>*
  %16570 = call <4 x i16> @llvm.masked.load.v4i16.p0v4i16(<4 x i16>* nonnull %16569, i32 8, <4 x i1> %t3804, <4 x i16> undef), !tbaa !395
  %16571 = sext <4 x i16> %16570 to <4 x i32>
  %16572 = add <4 x i32> %16567, %16571
  %unmaskedload1794.us = load <4 x i32>, <4 x i32>* %16548, align 16
  %16573 = select <4 x i1> %t3805, <4 x i32> %unmaskedload1794.us, <4 x i32> undef
  %16574 = getelementptr inbounds i16, i16* %16562, i64 8
  %16575 = bitcast i16* %16574 to <4 x i16>*
  %16576 = call <4 x i16> @llvm.masked.load.v4i16.p0v4i16(<4 x i16>* nonnull %16575, i32 16, <4 x i1> %t3805, <4 x i16> undef), !tbaa !395
  %16577 = sext <4 x i16> %16576 to <4 x i32>
  %16578 = add <4 x i32> %16573, %16577
  %unmaskedload1795.us = load <4 x i32>, <4 x i32>* %16551, align 16
  %16579 = select <4 x i1> %t3806, <4 x i32> %unmaskedload1795.us, <4 x i32> undef
  %16580 = getelementptr inbounds i16, i16* %16562, i64 12
  %16581 = bitcast i16* %16580 to <4 x i16>*
  %16582 = call <4 x i16> @llvm.masked.load.v4i16.p0v4i16(<4 x i16>* nonnull %16581, i32 8, <4 x i1> %t3806, <4 x i16> undef), !tbaa !395
  %16583 = sext <4 x i16> %16582 to <4 x i32>
  %16584 = add <4 x i32> %16579, %16583
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %16566, <4 x i32>* nonnull %16542, i32 16, <4 x i1> %16543), !tbaa !441
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %16572, <4 x i32>* nonnull %16545, i32 16, <4 x i1> %16546), !tbaa !452
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %16578, <4 x i32>* nonnull %16548, i32 16, <4 x i1> %16549), !tbaa !454
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %16584, <4 x i32>* nonnull %16551, i32 16, <4 x i1> %16552), !tbaa !457
  %indvars.iv.next6403 = add nuw nsw i64 %indvars.iv6402, 1
  %.not1796.us = icmp eq i64 %indvars.iv.next6403, %16555
  br i1 %.not1796.us, label %"end for sum_filter.s1.r19$x645.loopexit.us", label %"for sum_filter.s1.r19$x644.us"

"end for sum_filter.s1.r19$x645.loopexit.us":     ; preds = %"for sum_filter.s1.r19$x644.us"
  %indvars.iv.next6406 = add nuw nsw i64 %indvars.iv6405, 1
  %.not1791.us = icmp eq i64 %indvars.iv.next6406, %16557
  br i1 %.not1791.us, label %"consume sum_filter648", label %"for sum_filter.s1.r19$y640.us"

"consume sum_filter648":                          ; preds = %"end for sum_filter.s1.r19$x645.loopexit.us", %"produce sum_filter639", %"for sum_filter.s1.r19$y640.preheader"
  %t3807 = icmp sgt <4 x i32> %16554, <i32 0, i32 1, i32 2, i32 3>
  %t3808 = icmp sgt <4 x i32> %16554, <i32 4, i32 5, i32 6, i32 7>
  %t3809 = icmp sgt <4 x i32> %16554, <i32 8, i32 9, i32 10, i32 11>
  %t3810 = icmp sgt <4 x i32> %16554, <i32 12, i32 13, i32 14, i32 15>
  %16585 = bitcast i8* %42 to <4 x i32>*
  %16586 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* %16585, i32 4, <4 x i1> %t3807, <4 x i32> undef), !tbaa !517
  %unmaskedload = load <4 x i32>, <4 x i32>* %16542, align 16
  %16587 = select <4 x i1> %t3807, <4 x i32> %unmaskedload, <4 x i32> undef
  %16588 = zext i8 %input_zero to i32
  %16589 = insertelement <4 x i32> undef, i32 %16588, i32 0
  %16590 = shufflevector <4 x i32> %16589, <4 x i32> undef, <4 x i32> zeroinitializer
  %16591 = mul <4 x i32> %16587, %16590
  %16592 = sub <4 x i32> %16586, %16591
  %16593 = getelementptr inbounds i8, i8* %42, i64 16
  %16594 = bitcast i8* %16593 to <4 x i32>*
  %16595 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* nonnull %16594, i32 4, <4 x i1> %t3808, <4 x i32> undef), !tbaa !527
  %unmaskedload948 = load <4 x i32>, <4 x i32>* %16545, align 16
  %16596 = select <4 x i1> %t3808, <4 x i32> %unmaskedload948, <4 x i32> undef
  %16597 = mul <4 x i32> %16596, %16590
  %16598 = sub <4 x i32> %16595, %16597
  %16599 = getelementptr inbounds i8, i8* %42, i64 32
  %16600 = bitcast i8* %16599 to <4 x i32>*
  %16601 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* nonnull %16600, i32 4, <4 x i1> %t3809, <4 x i32> undef), !tbaa !529
  %unmaskedload949 = load <4 x i32>, <4 x i32>* %16548, align 16
  %16602 = select <4 x i1> %t3809, <4 x i32> %unmaskedload949, <4 x i32> undef
  %16603 = mul <4 x i32> %16602, %16590
  %16604 = sub <4 x i32> %16601, %16603
  %16605 = getelementptr inbounds i8, i8* %42, i64 48
  %16606 = bitcast i8* %16605 to <4 x i32>*
  %16607 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* nonnull %16606, i32 4, <4 x i1> %t3810, <4 x i32> undef), !tbaa !532
  %unmaskedload950 = load <4 x i32>, <4 x i32>* %16551, align 16
  %16608 = select <4 x i1> %t3810, <4 x i32> %unmaskedload950, <4 x i32> undef
  %16609 = mul <4 x i32> %16608, %16590
  %16610 = sub <4 x i32> %16607, %16609
  %16611 = bitcast [16 x i32]* %offset_c636946 to <4 x i32>*
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %16592, <4 x i32>* nonnull %16611, i32 16, <4 x i1> %16543), !tbaa !397
  %16612 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 4
  %16613 = bitcast i32* %16612 to <4 x i32>*
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %16598, <4 x i32>* nonnull %16613, i32 16, <4 x i1> %16546), !tbaa !408
  %16614 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 8
  %16615 = bitcast i32* %16614 to <4 x i32>*
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %16604, <4 x i32>* nonnull %16615, i32 16, <4 x i1> %16549), !tbaa !410
  %16616 = getelementptr inbounds [16 x i32], [16 x i32]* %offset_c636946, i64 0, i64 12
  %16617 = bitcast i32* %16616 to <4 x i32>*
  call void @llvm.masked.store.v4i32.p0v4i32(<4 x i32> %16610, <4 x i32>* nonnull %16617, i32 16, <4 x i1> %16552), !tbaa !413
  %t2515652 = icmp slt i32 %a614, 0
  %16618 = add nsw i32 %46, -1
  %t2516653 = mul nsw i32 %16618, %a614
  %t2517654 = icmp slt i32 %stride_x, 0
  %16619 = add nsw i32 %62, 3
  %t2519656 = and i32 %16619, -4
  %t2520657 = select i1 %t2515652, i32 %t2516653, i32 0
  %t2521658 = icmp slt i32 %a613, 0
  %16620 = add nsw i32 %48, -1
  %t2522659 = mul nsw i32 %16620, %a613
  %t2523660 = icmp slt i32 %stride_y, 0
  %16621 = add nsw i32 %65, 3
  %t2525662 = and i32 %16621, -4
  %t2526663 = select i1 %t2521658, i32 %t2522659, i32 0
  %a614.op = shl i32 %a614, 1
  %16622 = add nsw i32 %62, -1
  %t2529666 = and i32 %16622, -4
  %a613.op = shl i32 %a613, 1
  %16623 = add nsw i32 %65, -1
  %t2531668 = and i32 %16623, -4
  %t2506669 = icmp eq i32 %depth_multiplier, 1
  %16624 = select i1 %t2523660, i32 %t2531668, i32 -3
  %16625 = add i32 %64, 3
  %16626 = add i32 %16625, %16624
  %16627 = mul nsw i32 %16626, %stride_y
  %16628 = select i1 %t2517654, i32 %t2529666, i32 -3
  %16629 = add i32 %61, 3
  %16630 = add i32 %16629, %16628
  %16631 = mul nsw i32 %16630, %stride_x
  %16632 = select i1 %t2523660, i32 %t2525662, i32 1
  %16633 = add i32 %64, -1
  %16634 = add i32 %16633, %16632
  %16635 = mul nsw i32 %16634, %stride_y
  %16636 = select i1 %t2521658, i32 0, i32 %t2522659
  %16637 = select i1 %t2517654, i32 %t2519656, i32 1
  %16638 = add i32 %61, -1
  %16639 = add i32 %16638, %16637
  %16640 = mul nsw i32 %16639, %stride_x
  %t2490 = add nsw i32 %16640, %t2520657
  %16641 = select i1 %t2515652, i32 0, i32 %t2516653
  %16642 = add nsw i32 %t2519656, -1
  %16643 = select i1 %t2517654, i32 0, i32 %16642
  %16644 = add nsw i32 %16643, %61
  %16645 = mul nsw i32 %16644, %stride_x
  %16646 = icmp sgt i32 %57, 0
  br i1 %16646, label %"for output.s0.b.rebased674.preheader", label %after_bb236, !prof !391

"for output.s0.b.rebased674.preheader":           ; preds = %"consume sum_filter648"
  %16647 = mul nsw i32 %58, %56
  %16648 = mul nsw i32 %53, %52
  %16649 = add nsw i32 %16647, %16648
  %16650 = mul nsw i32 %55, %54
  %t2524661 = ashr i32 %16621, 2
  %t2518655 = ashr i32 %16619, 2
  %16651 = add nsw i32 %t2525662, -1
  %16652 = select i1 %t2523660, i32 0, i32 %16651
  %16653 = add nsw i32 %16652, %64
  %16654 = mul nsw i32 %16653, %stride_y
  %t2528665 = select i1 %t2515652, i32 %a614.op, i32 0
  %16655 = select i1 %16467, i32 %t2528665, i32 %t2520657
  %t2530667 = select i1 %t2521658, i32 %a613.op, i32 0
  %16656 = select i1 %16467, i32 %t2530667, i32 %t2526663
  %16657 = icmp sgt i32 %a614, 0
  %16658 = select i1 %16657, i32 %a614, i32 0
  %t2503671 = shl nuw nsw i32 %16658, 1
  %16659 = icmp sgt i32 %a613, 0
  %16660 = select i1 %16659, i32 %a613, i32 0
  %t2496670 = shl nuw nsw i32 %16660, 1
  %a615 = add nsw i32 %16635, %t2526663
  %b617 = add nsw i32 %16627, %16656
  %16661 = select i1 %16467, i32 %t2496670, i32 %16636
  %16662 = or i32 %16623, 3
  %16663 = select i1 %t2523660, i32 0, i32 %16662
  %16664 = add nsw i32 %16663, %64
  %16665 = mul nsw i32 %16664, %stride_y
  %a616 = add nsw i32 %16665, %16661
  %b618 = add nsw i32 %16654, %16636
  %16666 = icmp sgt i32 %a616, %b618
  %16667 = select i1 %16666, i32 %a616, i32 %b618
  %16668 = icmp slt i32 %b617, %a615
  %16669 = select i1 %16668, i32 %b617, i32 %a615
  %16670 = sub nsw i32 %16667, %16669
  %a624 = add nsw i32 %16670, 1
  %b620 = add nsw i32 %16631, %16655
  %16671 = select i1 %16467, i32 %t2503671, i32 %16641
  %16672 = or i32 %16622, 3
  %16673 = select i1 %t2517654, i32 0, i32 %16672
  %16674 = add nsw i32 %16673, %61
  %16675 = mul nsw i32 %16674, %stride_x
  %a619 = add nsw i32 %16675, %16671
  %b621 = add nsw i32 %16645, %16641
  %16676 = icmp sgt i32 %a619, %b621
  %16677 = select i1 %16676, i32 %a619, i32 %b621
  %16678 = icmp slt i32 %b620, %t2490
  %16679 = select i1 %16678, i32 %b620, i32 %t2490
  %16680 = sub nsw i32 %16677, %16679
  %a623 = add nsw i32 %16680, 1
  %.inv = icmp slt i32 %16680, 0
  %16681 = select i1 %.inv, i32 0, i32 %a623
  %t4617 = zext i32 %16681 to i64
  %.inv951 = icmp slt i32 %16670, 0
  %16682 = select i1 %.inv951, i32 0, i32 %a624
  %t4618 = zext i32 %16682 to i64
  %t4619 = shl nuw nsw i64 %t4617, 4
  %16683 = mul i64 %t4619, %t4618
  %16684 = or i64 %16683, 3
  %16685 = bitcast %struct.halide_pseudostack_slot_t* %resampled_input.pseudostack_slot to i8***
  %t2540701 = sub nsw i32 %b621, %t2490
  %t2539703 = sub nsw i32 %b618, %a615
  %16686 = sub i32 %a615, %16669
  %16687 = sub i32 %t2490, %16679
  %16688 = add i32 %16649, %16650
  %16689 = sext i32 %t2490 to i64
  %16690 = sext i32 %53 to i64
  %16691 = icmp sgt i32 %45, 1
  %16692 = icmp eq i32 %depth_multiplier, 0
  %t4621 = sext i1 %16692 to i32
  %16693 = sub nsw i32 %depth_multiplier, %t4621
  %16694 = add i32 %16693, 1
  %16695 = icmp ult i32 %16694, 3
  %16696 = select i1 %16695, i32 %16693, i32 0
  %16697 = xor i32 %t4621, -1
  %16698 = and i32 %16696, %16697
  %16699 = sext i32 %16698 to i64
  %.not5139 = icmp eq i32 %45, 2
  %16700 = icmp sgt i32 %45, 3
  %.not5140 = icmp eq i32 %45, 4
  %16701 = icmp sgt i32 %45, 5
  %.not5141 = icmp eq i32 %45, 6
  %.not5136 = icmp eq i32 %45, 9
  %16702 = icmp sgt i32 %45, 10
  %.not5137 = icmp eq i32 %45, 11
  %16703 = icmp sgt i32 %45, 12
  %.not5138 = icmp eq i32 %45, 13
  %16704 = icmp sgt i32 %45, 14
  %16705 = icmp sgt i32 %65, 0
  %16706 = sub nsw i32 %a613.op, %16669
  %16707 = sub nsw i32 %a613, %16669
  %16708 = sub nsw i32 %a614, %16679
  %16709 = sub nsw i32 %a614.op, %16679
  %.neg5172 = mul i32 %66, %64
  %.neg5173 = mul i32 %63, %61
  %.neg5174 = mul i32 %67, %56
  %reass.add5177 = add i32 %.neg5172, %.neg5173
  %reass.add5178 = add i32 %reass.add5177, %.neg5174
  %16710 = icmp sgt i32 %62, 0
  %b827 = add nsw i32 %16631, %t2520657
  %16711 = icmp slt i32 %16640, %16631
  %16712 = select i1 %16711, i32 %t2490, i32 %b827
  %b831 = add nsw i32 %16627, %t2526663
  %16713 = icmp slt i32 %16635, %16627
  %16714 = select i1 %16713, i32 %a615, i32 %b831
  %a830 = add nsw i32 %16675, %16641
  %16715 = icmp sgt i32 %16675, %16645
  %16716 = select i1 %16715, i32 %a830, i32 %b621
  %16717 = sub nsw i32 %16716, %16712
  %16718 = add nsw i32 %16717, 1
  %16719 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 96
  %16720 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 128
  %16721 = bitcast i16* %16720 to <4 x i16>*
  %16722 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 112
  %16723 = bitcast i16* %16722 to <4 x i16>*
  %16724 = bitcast i16* %16719 to <4 x i16>*
  %16725 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 48
  %16726 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 80
  %16727 = bitcast i16* %16726 to <4 x i16>*
  %16728 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 64
  %16729 = bitcast i16* %16728 to <4 x i16>*
  %16730 = bitcast i16* %16725 to <4 x i16>*
  %16731 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 32
  %16732 = bitcast i16* %16731 to <4 x i16>*
  %16733 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 16
  %16734 = bitcast i16* %16733 to <4 x i16>*
  %16735 = bitcast i16* %filter_zeroed618 to <4 x i16>*
  %16736 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 132
  %16737 = bitcast i16* %16736 to <4 x i16>*
  %16738 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 116
  %16739 = bitcast i16* %16738 to <4 x i16>*
  %16740 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 100
  %16741 = bitcast i16* %16740 to <4 x i16>*
  %16742 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 84
  %16743 = bitcast i16* %16742 to <4 x i16>*
  %16744 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 68
  %16745 = bitcast i16* %16744 to <4 x i16>*
  %16746 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 52
  %16747 = bitcast i16* %16746 to <4 x i16>*
  %16748 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 36
  %16749 = bitcast i16* %16748 to <4 x i16>*
  %16750 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 20
  %16751 = bitcast i16* %16750 to <4 x i16>*
  %16752 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 4
  %16753 = bitcast i16* %16752 to <4 x i16>*
  %16754 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 136
  %16755 = bitcast i16* %16754 to <4 x i16>*
  %16756 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 120
  %16757 = bitcast i16* %16756 to <4 x i16>*
  %16758 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 104
  %16759 = bitcast i16* %16758 to <4 x i16>*
  %16760 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 88
  %16761 = bitcast i16* %16760 to <4 x i16>*
  %16762 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 72
  %16763 = bitcast i16* %16762 to <4 x i16>*
  %16764 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 56
  %16765 = bitcast i16* %16764 to <4 x i16>*
  %16766 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 40
  %16767 = bitcast i16* %16766 to <4 x i16>*
  %16768 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 24
  %16769 = bitcast i16* %16768 to <4 x i16>*
  %16770 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 8
  %16771 = bitcast i16* %16770 to <4 x i16>*
  %16772 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 140
  %16773 = bitcast i16* %16772 to <4 x i16>*
  %16774 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 124
  %16775 = bitcast i16* %16774 to <4 x i16>*
  %16776 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 108
  %16777 = bitcast i16* %16776 to <4 x i16>*
  %16778 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 92
  %16779 = bitcast i16* %16778 to <4 x i16>*
  %16780 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 76
  %16781 = bitcast i16* %16780 to <4 x i16>*
  %16782 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 60
  %16783 = bitcast i16* %16782 to <4 x i16>*
  %16784 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 44
  %16785 = bitcast i16* %16784 to <4 x i16>*
  %16786 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 28
  %16787 = bitcast i16* %16786 to <4 x i16>*
  %16788 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 12
  %16789 = bitcast i16* %16788 to <4 x i16>*
  %t4581 = shl nuw i32 1, %output_shift
  %16790 = insertelement <4 x i32> undef, i32 %output_multiplier, i32 0
  %16791 = shufflevector <4 x i32> %16790, <4 x i32> undef, <4 x i32> zeroinitializer
  %16792 = ashr i32 %t4581, 1
  %16793 = insertelement <4 x i32> undef, i32 %16792, i32 0
  %16794 = shufflevector <4 x i32> %16793, <4 x i32> undef, <4 x i32> zeroinitializer
  %16795 = sub i32 0, %output_shift
  %16796 = insertelement <4 x i32> undef, i32 %16795, i32 0
  %16797 = shufflevector <4 x i32> %16796, <4 x i32> undef, <4 x i32> zeroinitializer
  %16798 = zext i8 %output_zero to i16
  %16799 = insertelement <8 x i16> undef, i16 %16798, i32 0
  %16800 = shufflevector <8 x i16> %16799, <8 x i16> undef, <8 x i32> zeroinitializer
  %16801 = insertelement <16 x i8> undef, i8 %output_max, i32 0
  %16802 = shufflevector <16 x i8> %16801, <16 x i8> undef, <16 x i32> zeroinitializer
  %16803 = insertelement <16 x i8> undef, i8 %output_min, i32 0
  %16804 = shufflevector <16 x i8> %16803, <16 x i8> undef, <16 x i32> zeroinitializer
  %16805 = sext i32 %61 to i64
  %16806 = sext i32 %63 to i64
  %16807 = zext i32 %t2540701 to i64
  %16808 = sext i32 %16686 to i64
  %16809 = sext i32 %a623 to i64
  %16810 = sext i32 %16687 to i64
  %16811 = sext i32 %a615 to i64
  %16812 = sext i32 %55 to i64
  %16813 = zext i32 %t2539703 to i64
  %16814 = zext i32 %46 to i64
  %16815 = zext i32 %a613 to i64
  %16816 = sext i32 %46 to i64
  %16817 = zext i32 %48 to i64
  %16818 = zext i32 %t2518655 to i64
  %16819 = zext i32 %t2524661 to i64
  %zext6432 = zext i32 %57 to i64
  %16820 = or i32 %t2539703, %t2540701
  %16821 = icmp slt i32 %16820, 0
  %.not6877 = icmp sgt i32 %16820, -1
  br label %"for output.s0.b.rebased674"

"for output.s0.b.rebased674":                     ; preds = %"for output.s0.b.rebased674.preheader", %"end for output.s0.y.yo765"
  %indvars.iv6430 = phi i64 [ 0, %"for output.s0.b.rebased674.preheader" ], [ %indvars.iv.next6431, %"end for output.s0.y.yo765" ]
  %16822 = load i64, i64* %.fca.1.gep875, align 8, !tbaa !385
  %cmp.i3254 = icmp ult i64 %16822, %16684
  %16823 = load i8*, i8** %.fca.0.gep874, align 8, !tbaa !387
  br i1 %cmp.i3254, label %if.then.i3257, label %pseudostack_alloc.exit3270, !prof !388

if.then.i3257:                                    ; preds = %"for output.s0.b.rebased674"
  %tobool1.not.i3256 = icmp ne i8* %16823, null
  %16824 = load i64, i64* %.fca.2.gep876, align 8
  %cmp2.i3259 = icmp ugt i64 %16824, 16384
  %or.cond5135 = and i1 %tobool1.not.i3256, %cmp2.i3259
  br i1 %or.cond5135, label %if.then3.i3261, label %if.end.i3265

if.then3.i3261:                                   ; preds = %if.then.i3257
  call void @halide_free(i8* null, i8* nonnull %16823) #15
  %.pre6508 = load i64, i64* %.fca.2.gep876, align 8, !tbaa !389
  br label %if.end.i3265

if.end.i3265:                                     ; preds = %if.then3.i3261, %if.then.i3257
  %16825 = phi i64 [ %.pre6508, %if.then3.i3261 ], [ %16824, %if.then.i3257 ]
  %add.i3263 = add i64 %16825, %16684
  store i64 %add.i3263, i64* %.fca.2.gep876, align 8, !tbaa !389
  %cmp7.i3264 = icmp ugt i64 %add.i3263, 16384
  br i1 %cmp7.i3264, label %if.then8.i3267, label %if.end11.i3269

if.then8.i3267:                                   ; preds = %if.end.i3265
  %call.i3266 = call i8* @halide_malloc(i8* null, i64 %16684) #15
  br label %if.end11.i3269

if.end11.i3269:                                   ; preds = %if.then8.i3267, %if.end.i3265
  %storemerge.i3268 = phi i8* [ %call.i3266, %if.then8.i3267 ], [ null, %if.end.i3265 ]
  store i8* %storemerge.i3268, i8** %.fca.0.gep874, align 8, !tbaa !387
  store i64 %16684, i64* %.fca.1.gep875, align 8, !tbaa !385
  br label %pseudostack_alloc.exit3270

pseudostack_alloc.exit3270:                       ; preds = %"for output.s0.b.rebased674", %if.end11.i3269
  %16826 = phi i8* [ %storemerge.i3268, %if.end11.i3269 ], [ %16823, %"for output.s0.b.rebased674" ]
  %.not952 = icmp eq i8* %16826, null
  br i1 %.not952, label %then_bb680, label %"produce resampled_input682", !prof !390

then_bb680:                                       ; preds = %pseudostack_alloc.exit3270
  %16827 = alloca i8*, i64 %16684, align 16
  %16828 = bitcast i8** %16827 to i8*
  store i8** %16827, i8*** %16685, align 8
  br label %"produce resampled_input682"

"produce resampled_input682":                     ; preds = %pseudostack_alloc.exit3270, %then_bb680
  %resampled_input681 = phi i8* [ %16828, %then_bb680 ], [ %16826, %pseudostack_alloc.exit3270 ]
  br i1 %t2506669, label %then_bb684, label %next_bb685

then_bb684:                                       ; preds = %"produce resampled_input682"
  br i1 %.not6877, label %"for resampled_input.s0.y.rebased691.us.preheader", label %"consume resampled_input760", !prof !435

"for resampled_input.s0.y.rebased691.us.preheader": ; preds = %then_bb684
  %16829 = trunc i64 %indvars.iv6430 to i32
  %16830 = add i32 %56, %16829
  %16831 = mul i32 %16830, %58
  %16832 = sub i32 %16831, %16688
  %16833 = sext i32 %16832 to i64
  br label %"for resampled_input.s0.y.rebased691.us"

"for resampled_input.s0.y.rebased691.us":         ; preds = %"for resampled_input.s0.y.rebased691.us.preheader", %"end for resampled_input.s0.x.rebased697.loopexit.us"
  %indvars.iv6416 = phi i64 [ 0, %"for resampled_input.s0.y.rebased691.us.preheader" ], [ %indvars.iv.next6417, %"end for resampled_input.s0.x.rebased697.loopexit.us" ]
  %16834 = add nsw i64 %indvars.iv6416, %16808
  %16835 = mul nsw i64 %16834, %16809
  %16836 = add nsw i64 %16835, %16810
  %16837 = add nsw i64 %indvars.iv6416, %16811
  %16838 = mul nsw i64 %16837, %16812
  %16839 = add nsw i64 %16838, %16833
  br label %"for resampled_input.s0.x.rebased696.us"

"for resampled_input.s0.x.rebased696.us":         ; preds = %"for resampled_input.s0.y.rebased691.us", %"for resampled_input.s0.x.rebased696.us"
  %indvars.iv6414 = phi i64 [ 0, %"for resampled_input.s0.y.rebased691.us" ], [ %indvars.iv.next6415, %"for resampled_input.s0.x.rebased696.us" ]
  %16840 = add nsw i64 %indvars.iv6414, %16689
  %16841 = mul nsw i64 %16840, %16690
  %16842 = add nsw i64 %16841, %16839
  %16843 = getelementptr inbounds i8, i8* %50, i64 %16842
  %16844 = bitcast i8* %16843 to <16 x i8>*
  %16845 = call <16 x i8> @llvm.masked.load.v16i8.p0v16i8(<16 x i8>* %16844, i32 1, <16 x i1> %16541, <16 x i8> undef), !tbaa !436
  %16846 = add nsw i64 %indvars.iv6414, %16836
  %16847 = shl nsw i64 %16846, 4
  %16848 = getelementptr inbounds i8, i8* %resampled_input681, i64 %16847
  %16849 = bitcast i8* %16848 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %16845, <16 x i8>* %16849, i32 16, <16 x i1> %16541), !tbaa !438
  %indvars.iv.next6415 = add nuw nsw i64 %indvars.iv6414, 1
  %.not1790.us = icmp eq i64 %indvars.iv6414, %16807
  br i1 %.not1790.us, label %"end for resampled_input.s0.x.rebased697.loopexit.us", label %"for resampled_input.s0.x.rebased696.us"

"end for resampled_input.s0.x.rebased697.loopexit.us": ; preds = %"for resampled_input.s0.x.rebased696.us"
  %indvars.iv.next6417 = add nuw nsw i64 %indvars.iv6416, 1
  %.not1789.us = icmp eq i64 %indvars.iv6416, %16813
  br i1 %.not1789.us, label %"consume resampled_input760", label %"for resampled_input.s0.y.rebased691.us"

next_bb685:                                       ; preds = %"produce resampled_input682"
  br i1 %16821, label %"consume resampled_input760", label %"for resampled_input.s0.y.rebased704.us.preheader", !prof !440

"for resampled_input.s0.y.rebased704.us.preheader": ; preds = %next_bb685
  %16850 = trunc i64 %indvars.iv6430 to i32
  %16851 = add i32 %56, %16850
  %16852 = mul i32 %16851, %58
  %16853 = sub i32 %16852, %16688
  %16854 = sext i32 %16853 to i64
  br label %"for resampled_input.s0.y.rebased704.us"

"for resampled_input.s0.y.rebased704.us":         ; preds = %"for resampled_input.s0.y.rebased704.us.preheader", %"end for resampled_input.s0.x.rebased710.loopexit.us"
  %indvars.iv6410 = phi i64 [ 0, %"for resampled_input.s0.y.rebased704.us.preheader" ], [ %indvars.iv.next6411, %"end for resampled_input.s0.x.rebased710.loopexit.us" ]
  %16855 = add nsw i64 %indvars.iv6410, %16808
  %16856 = mul nsw i64 %16855, %16809
  %16857 = add nsw i64 %16856, %16810
  %16858 = add nsw i64 %indvars.iv6410, %16811
  %16859 = mul nsw i64 %16858, %16812
  %16860 = add nsw i64 %16859, %16854
  br label %"for resampled_input.s0.x.rebased709.us"

"for resampled_input.s0.x.rebased709.us":         ; preds = %"for resampled_input.s0.y.rebased704.us", %after_bb759.us
  %indvars.iv6408 = phi i64 [ 0, %"for resampled_input.s0.y.rebased704.us" ], [ %indvars.iv.next6409, %after_bb759.us ]
  %16861 = add nsw i64 %indvars.iv6408, %16689
  %16862 = mul nsw i64 %16861, %16690
  %16863 = add nsw i64 %16862, %16860
  %16864 = getelementptr inbounds i8, i8* %50, i64 %16863
  %16865 = load i8, i8* %16864, align 1, !tbaa !436
  br i1 %16691, label %after_bb717.us, label %after_bb732.us

after_bb717.us:                                   ; preds = %"for resampled_input.s0.x.rebased709.us"
  %16866 = add nsw i64 %16863, %16699
  %16867 = getelementptr inbounds i8, i8* %50, i64 %16866
  %16868 = load i8, i8* %16867, align 1, !tbaa !436
  br i1 %.not5139, label %after_bb732.us, label %after_bb720.us

after_bb720.us:                                   ; preds = %after_bb717.us
  %16869 = sdiv i32 2, %16693
  %16870 = and i32 %16869, %16697
  %16871 = sext i32 %16870 to i64
  %16872 = add nsw i64 %16863, %16871
  %16873 = getelementptr inbounds i8, i8* %50, i64 %16872
  %16874 = load i8, i8* %16873, align 1, !tbaa !436
  br i1 %16700, label %after_bb723.us, label %after_bb732.us

after_bb723.us:                                   ; preds = %after_bb720.us
  %16875 = sdiv i32 3, %16693
  %16876 = and i32 %16875, %16697
  %16877 = sext i32 %16876 to i64
  %16878 = add nsw i64 %16863, %16877
  %16879 = getelementptr inbounds i8, i8* %50, i64 %16878
  %16880 = load i8, i8* %16879, align 1, !tbaa !436
  br i1 %.not5140, label %after_bb732.us, label %after_bb726.us

after_bb726.us:                                   ; preds = %after_bb723.us
  %16881 = sdiv i32 4, %16693
  %16882 = and i32 %16881, %16697
  %16883 = sext i32 %16882 to i64
  %16884 = add nsw i64 %16863, %16883
  %16885 = getelementptr inbounds i8, i8* %50, i64 %16884
  %16886 = load i8, i8* %16885, align 1, !tbaa !436
  br i1 %16701, label %after_bb729.us, label %after_bb732.us

after_bb729.us:                                   ; preds = %after_bb726.us
  %16887 = sdiv i32 5, %16693
  %16888 = and i32 %16887, %16697
  %16889 = sext i32 %16888 to i64
  %16890 = add nsw i64 %16863, %16889
  %16891 = getelementptr inbounds i8, i8* %50, i64 %16890
  %16892 = load i8, i8* %16891, align 1, !tbaa !436
  br i1 %.not5141, label %after_bb732.us, label %true_bb730.us

true_bb730.us:                                    ; preds = %after_bb729.us
  %16893 = sdiv i32 6, %16693
  %16894 = and i32 %16893, %16697
  %16895 = sext i32 %16894 to i64
  %16896 = add nsw i64 %16863, %16895
  %16897 = getelementptr inbounds i8, i8* %50, i64 %16896
  %16898 = load i8, i8* %16897, align 1, !tbaa !436
  br label %after_bb732.us

after_bb732.us:                                   ; preds = %true_bb730.us, %after_bb729.us, %after_bb726.us, %after_bb723.us, %after_bb720.us, %after_bb717.us, %"for resampled_input.s0.x.rebased709.us"
  %16899 = phi i8 [ %16892, %true_bb730.us ], [ %16892, %after_bb729.us ], [ 0, %after_bb726.us ], [ 0, %after_bb723.us ], [ 0, %after_bb720.us ], [ 0, %after_bb717.us ], [ 0, %"for resampled_input.s0.x.rebased709.us" ]
  %16900 = phi i8 [ %16880, %true_bb730.us ], [ %16880, %after_bb729.us ], [ %16880, %after_bb726.us ], [ %16880, %after_bb723.us ], [ 0, %after_bb720.us ], [ 0, %after_bb717.us ], [ 0, %"for resampled_input.s0.x.rebased709.us" ]
  %16901 = phi i8 [ %16868, %true_bb730.us ], [ %16868, %after_bb729.us ], [ %16868, %after_bb726.us ], [ %16868, %after_bb723.us ], [ %16868, %after_bb720.us ], [ %16868, %after_bb717.us ], [ 0, %"for resampled_input.s0.x.rebased709.us" ]
  %16902 = phi i8 [ %16874, %true_bb730.us ], [ %16874, %after_bb729.us ], [ %16874, %after_bb726.us ], [ %16874, %after_bb723.us ], [ %16874, %after_bb720.us ], [ 0, %after_bb717.us ], [ 0, %"for resampled_input.s0.x.rebased709.us" ]
  %16903 = phi i8 [ %16886, %true_bb730.us ], [ %16886, %after_bb729.us ], [ %16886, %after_bb726.us ], [ 0, %after_bb723.us ], [ 0, %after_bb720.us ], [ 0, %after_bb717.us ], [ 0, %"for resampled_input.s0.x.rebased709.us" ]
  %16904 = phi i8 [ %16898, %true_bb730.us ], [ 0, %after_bb729.us ], [ 0, %after_bb726.us ], [ 0, %after_bb723.us ], [ 0, %after_bb720.us ], [ 0, %after_bb717.us ], [ 0, %"for resampled_input.s0.x.rebased709.us" ]
  br i1 %t2482621, label %true_bb733.us, label %after_bb735.us

true_bb733.us:                                    ; preds = %after_bb732.us
  %16905 = sdiv i32 7, %16693
  %16906 = and i32 %16905, %16697
  %16907 = sext i32 %16906 to i64
  %16908 = add nsw i64 %16863, %16907
  %16909 = getelementptr inbounds i8, i8* %50, i64 %16908
  %16910 = load i8, i8* %16909, align 1, !tbaa !436
  br label %after_bb735.us

after_bb735.us:                                   ; preds = %true_bb733.us, %after_bb732.us
  %16911 = phi i8 [ %16910, %true_bb733.us ], [ 0, %after_bb732.us ]
  br i1 %t2483620, label %after_bb738.us, label %after_bb759.us

after_bb738.us:                                   ; preds = %after_bb735.us
  %16912 = sdiv i32 8, %16693
  %16913 = and i32 %16912, %16697
  %16914 = sext i32 %16913 to i64
  %16915 = add nsw i64 %16863, %16914
  %16916 = getelementptr inbounds i8, i8* %50, i64 %16915
  %16917 = load i8, i8* %16916, align 1, !tbaa !436
  br i1 %.not5136, label %after_bb759.us, label %after_bb741.us

after_bb741.us:                                   ; preds = %after_bb738.us
  %16918 = sdiv i32 9, %16693
  %16919 = and i32 %16918, %16697
  %16920 = sext i32 %16919 to i64
  %16921 = add nsw i64 %16863, %16920
  %16922 = getelementptr inbounds i8, i8* %50, i64 %16921
  %16923 = load i8, i8* %16922, align 1, !tbaa !436
  br i1 %16702, label %after_bb744.us, label %after_bb759.us

after_bb744.us:                                   ; preds = %after_bb741.us
  %16924 = sdiv i32 10, %16693
  %16925 = and i32 %16924, %16697
  %16926 = sext i32 %16925 to i64
  %16927 = add nsw i64 %16863, %16926
  %16928 = getelementptr inbounds i8, i8* %50, i64 %16927
  %16929 = load i8, i8* %16928, align 1, !tbaa !436
  br i1 %.not5137, label %after_bb759.us, label %after_bb747.us

after_bb747.us:                                   ; preds = %after_bb744.us
  %16930 = sdiv i32 11, %16693
  %16931 = and i32 %16930, %16697
  %16932 = sext i32 %16931 to i64
  %16933 = add nsw i64 %16863, %16932
  %16934 = getelementptr inbounds i8, i8* %50, i64 %16933
  %16935 = load i8, i8* %16934, align 1, !tbaa !436
  br i1 %16703, label %after_bb750.us, label %after_bb759.us

after_bb750.us:                                   ; preds = %after_bb747.us
  %16936 = sdiv i32 12, %16693
  %16937 = and i32 %16936, %16697
  %16938 = sext i32 %16937 to i64
  %16939 = add nsw i64 %16863, %16938
  %16940 = getelementptr inbounds i8, i8* %50, i64 %16939
  %16941 = load i8, i8* %16940, align 1, !tbaa !436
  br i1 %.not5138, label %after_bb759.us, label %after_bb753.us

after_bb753.us:                                   ; preds = %after_bb750.us
  %16942 = sdiv i32 13, %16693
  %16943 = and i32 %16942, %16697
  %16944 = sext i32 %16943 to i64
  %16945 = add nsw i64 %16863, %16944
  %16946 = getelementptr inbounds i8, i8* %50, i64 %16945
  %16947 = load i8, i8* %16946, align 1, !tbaa !436
  br i1 %16704, label %true_bb754.us, label %after_bb759.us

true_bb754.us:                                    ; preds = %after_bb753.us
  %16948 = sdiv i32 14, %16693
  %16949 = and i32 %16948, %16697
  %16950 = sext i32 %16949 to i64
  %16951 = add nsw i64 %16863, %16950
  %16952 = getelementptr inbounds i8, i8* %50, i64 %16951
  %16953 = load i8, i8* %16952, align 1, !tbaa !436
  br label %after_bb759.us

after_bb759.us:                                   ; preds = %true_bb754.us, %after_bb753.us, %after_bb750.us, %after_bb747.us, %after_bb744.us, %after_bb741.us, %after_bb738.us, %after_bb735.us
  %16954 = phi i8 [ %16947, %true_bb754.us ], [ %16947, %after_bb753.us ], [ 0, %after_bb750.us ], [ 0, %after_bb747.us ], [ 0, %after_bb744.us ], [ 0, %after_bb741.us ], [ 0, %after_bb738.us ], [ 0, %after_bb735.us ]
  %16955 = phi i8 [ %16935, %true_bb754.us ], [ %16935, %after_bb753.us ], [ %16935, %after_bb750.us ], [ %16935, %after_bb747.us ], [ 0, %after_bb744.us ], [ 0, %after_bb741.us ], [ 0, %after_bb738.us ], [ 0, %after_bb735.us ]
  %16956 = phi i8 [ %16923, %true_bb754.us ], [ %16923, %after_bb753.us ], [ %16923, %after_bb750.us ], [ %16923, %after_bb747.us ], [ %16923, %after_bb744.us ], [ %16923, %after_bb741.us ], [ 0, %after_bb738.us ], [ 0, %after_bb735.us ]
  %16957 = phi i8 [ %16917, %true_bb754.us ], [ %16917, %after_bb753.us ], [ %16917, %after_bb750.us ], [ %16917, %after_bb747.us ], [ %16917, %after_bb744.us ], [ %16917, %after_bb741.us ], [ %16917, %after_bb738.us ], [ 0, %after_bb735.us ]
  %16958 = phi i8 [ %16929, %true_bb754.us ], [ %16929, %after_bb753.us ], [ %16929, %after_bb750.us ], [ %16929, %after_bb747.us ], [ %16929, %after_bb744.us ], [ 0, %after_bb741.us ], [ 0, %after_bb738.us ], [ 0, %after_bb735.us ]
  %16959 = phi i8 [ %16941, %true_bb754.us ], [ %16941, %after_bb753.us ], [ %16941, %after_bb750.us ], [ 0, %after_bb747.us ], [ 0, %after_bb744.us ], [ 0, %after_bb741.us ], [ 0, %after_bb738.us ], [ 0, %after_bb735.us ]
  %16960 = phi i8 [ %16953, %true_bb754.us ], [ 0, %after_bb753.us ], [ 0, %after_bb750.us ], [ 0, %after_bb747.us ], [ 0, %after_bb744.us ], [ 0, %after_bb741.us ], [ 0, %after_bb738.us ], [ 0, %after_bb735.us ]
  %16961 = insertelement <16 x i8> <i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 poison, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0>, i8 %16865, i32 0
  %16962 = insertelement <16 x i8> %16961, i8 %16901, i32 1
  %16963 = insertelement <16 x i8> %16962, i8 %16902, i32 2
  %16964 = insertelement <16 x i8> %16963, i8 %16900, i32 3
  %16965 = insertelement <16 x i8> %16964, i8 %16903, i32 4
  %16966 = insertelement <16 x i8> %16965, i8 %16899, i32 5
  %16967 = insertelement <16 x i8> %16966, i8 %16904, i32 6
  %16968 = insertelement <16 x i8> %16967, i8 %16911, i32 7
  %16969 = insertelement <16 x i8> %16968, i8 %16957, i32 8
  %16970 = insertelement <16 x i8> %16969, i8 %16956, i32 9
  %16971 = insertelement <16 x i8> %16970, i8 %16958, i32 10
  %16972 = insertelement <16 x i8> %16971, i8 %16955, i32 11
  %16973 = insertelement <16 x i8> %16972, i8 %16959, i32 12
  %16974 = insertelement <16 x i8> %16973, i8 %16954, i32 13
  %16975 = insertelement <16 x i8> %16974, i8 %16960, i32 14
  %16976 = add nsw i64 %indvars.iv6408, %16857
  %16977 = shl nsw i64 %16976, 4
  %16978 = getelementptr inbounds i8, i8* %resampled_input681, i64 %16977
  %16979 = bitcast i8* %16978 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %16975, <16 x i8>* %16979, i32 16, <16 x i1> %16541), !tbaa !438
  %indvars.iv.next6409 = add nuw nsw i64 %indvars.iv6408, 1
  %.not1788.us = icmp eq i64 %indvars.iv6408, %16807
  br i1 %.not1788.us, label %"end for resampled_input.s0.x.rebased710.loopexit.us", label %"for resampled_input.s0.x.rebased709.us"

"end for resampled_input.s0.x.rebased710.loopexit.us": ; preds = %after_bb759.us
  %indvars.iv.next6411 = add nuw nsw i64 %indvars.iv6410, 1
  %.not1787.us = icmp eq i64 %indvars.iv6410, %16813
  br i1 %.not1787.us, label %"consume resampled_input760", label %"for resampled_input.s0.y.rebased704.us"

"consume resampled_input760":                     ; preds = %"end for resampled_input.s0.x.rebased710.loopexit.us", %"end for resampled_input.s0.x.rebased697.loopexit.us", %next_bb685, %then_bb684
  br i1 %16705, label %"for output.s0.y.yo764.preheader", label %"end for output.s0.y.yo765", !prof !391

"for output.s0.y.yo764.preheader":                ; preds = %"consume resampled_input760"
  %16980 = trunc i64 %indvars.iv6430 to i32
  %16981 = add i32 %56, %16980
  %16982 = mul i32 %16981, %67
  %16983 = sub i32 %16982, %reass.add5178
  %16984 = load <4 x i32>, <4 x i32>* %16611, align 16
  %16985 = load <4 x i32>, <4 x i32>* %16613, align 16
  %16986 = load <4 x i32>, <4 x i32>* %16615, align 16
  %16987 = load <4 x i32>, <4 x i32>* %16617, align 16
  br label %"for output.s0.y.yo764"

"for output.s0.y.yo764":                          ; preds = %"for output.s0.y.yo764.preheader", %"end for output.s0.x.xo834"
  %indvars.iv6428 = phi i64 [ 0, %"for output.s0.y.yo764.preheader" ], [ %indvars.iv.next6429, %"end for output.s0.x.xo834" ]
  %16988 = trunc i64 %indvars.iv6428 to i32
  %16989 = shl nsw i32 %16988, 2
  %16990 = add nsw i32 %16989, %64
  %16991 = add nsw i32 %16990, 1
  %16992 = mul nsw i32 %16991, %stride_y
  %16993 = add nsw i32 %16992, %16706
  %16994 = add nsw i32 %16992, %16707
  %16995 = add nsw i32 %16990, 2
  %16996 = mul nsw i32 %16995, %stride_y
  %16997 = add nsw i32 %16996, %16706
  %16998 = add nsw i32 %16996, %16707
  %16999 = add nsw i32 %16990, 3
  %17000 = mul nsw i32 %16999, %stride_y
  %17001 = add nsw i32 %17000, %16706
  %17002 = add nsw i32 %17000, %16707
  %17003 = mul nsw i32 %16990, %stride_y
  %17004 = add nsw i32 %17003, %16706
  %17005 = add nsw i32 %17003, %16707
  %17006 = sub nsw i32 %16992, %16669
  %17007 = sub nsw i32 %16996, %16669
  %17008 = sub nsw i32 %17000, %16669
  %17009 = sub nsw i32 %17003, %16669
  %17010 = mul nsw i32 %17009, %a623
  %t2553793 = sub nsw i32 %17010, %16679
  %17011 = mul nsw i32 %17008, %a623
  %t2556794 = sub nsw i32 %17011, %16679
  %17012 = mul nsw i32 %17007, %a623
  %t2555795 = sub nsw i32 %17012, %16679
  %17013 = mul nsw i32 %17006, %a623
  %t2554796 = sub nsw i32 %17013, %16679
  %17014 = mul nsw i32 %17005, %a623
  %t2566797 = sub nsw i32 %17014, %16679
  %17015 = mul nsw i32 %17004, %a623
  %t2579798 = sub nsw i32 %17015, %16679
  %17016 = mul nsw i32 %17002, %a623
  %t2569799 = sub nsw i32 %17016, %16679
  %17017 = mul nsw i32 %17001, %a623
  %t2582800 = sub nsw i32 %17017, %16679
  %17018 = mul nsw i32 %16998, %a623
  %t2568801 = sub nsw i32 %17018, %16679
  %17019 = mul nsw i32 %16997, %a623
  %t2581802 = sub nsw i32 %17019, %16679
  %17020 = mul nsw i32 %16994, %a623
  %t2567803 = sub nsw i32 %17020, %16679
  %17021 = mul nsw i32 %16993, %a623
  %t2580804 = sub nsw i32 %17021, %16679
  %t2557805 = add nsw i32 %17010, %16708
  %t2561806 = add nsw i32 %17010, %16709
  %t2560807 = add nsw i32 %17011, %16708
  %t2564808 = add nsw i32 %17011, %16709
  %t2559809 = add nsw i32 %17012, %16708
  %t2563810 = add nsw i32 %17012, %16709
  %t2558811 = add nsw i32 %17013, %16708
  %t2562812 = add nsw i32 %17013, %16709
  %t2570813 = add nsw i32 %17014, %16708
  %t2574814 = add nsw i32 %17014, %16709
  %t2583815 = add nsw i32 %17015, %16708
  %t2587816 = add nsw i32 %17015, %16709
  %t2573817 = add nsw i32 %17016, %16708
  %t2577818 = add nsw i32 %17016, %16709
  %t2586819 = add nsw i32 %17017, %16708
  %t2590820 = add nsw i32 %17017, %16709
  %t2572821 = add nsw i32 %17018, %16708
  %t2576822 = add nsw i32 %17018, %16709
  %t2585823 = add nsw i32 %17019, %16708
  %t2589824 = add nsw i32 %17019, %16709
  %t2571825 = add nsw i32 %17020, %16708
  %t2575826 = add nsw i32 %17020, %16709
  %t2584827 = add nsw i32 %17021, %16708
  %t2588828 = add nsw i32 %17021, %16709
  br i1 %16710, label %"for output.s0.x.xo833.preheader", label %"end for output.s0.x.xo834", !prof !391

"for output.s0.x.xo833.preheader":                ; preds = %"for output.s0.y.yo764"
  %17022 = mul nsw i32 %16991, %66
  %t2596832 = add nsw i32 %17022, %16983
  %17023 = mul nsw i32 %16995, %66
  %t2597831 = add nsw i32 %17023, %16983
  %17024 = mul nsw i32 %16999, %66
  %t2598830 = add nsw i32 %17024, %16983
  %17025 = mul nsw i32 %16990, %66
  %t2595829 = add nsw i32 %17025, %16983
  %17026 = sub i32 %16992, %16714
  %17027 = sub i32 %16996, %16714
  %17028 = sub i32 %17000, %16714
  %17029 = sub i32 %17003, %16714
  %17030 = sext i32 %t2595829 to i64
  %17031 = sext i32 %t2596832 to i64
  %17032 = sext i32 %t2597831 to i64
  %17033 = sext i32 %t2598830 to i64
  br label %"for output.s0.x.xo833"

"end for output.s0.y.yo765":                      ; preds = %"end for output.s0.x.xo834", %"consume resampled_input760"
  %indvars.iv.next6431 = add nuw nsw i64 %indvars.iv6430, 1
  %17034 = icmp eq i64 %indvars.iv.next6431, %zext6432
  br i1 %17034, label %if.then.i3157.loopexit, label %"for output.s0.b.rebased674"

"for output.s0.x.xo833":                          ; preds = %"for output.s0.x.xo833.preheader", %"consume convolved873"
  %indvars.iv6426 = phi i64 [ 0, %"for output.s0.x.xo833.preheader" ], [ %indvars.iv.next6427, %"consume convolved873" ]
  %17035 = trunc i64 %indvars.iv6426 to i32
  %17036 = shl nsw i32 %17035, 2
  %17037 = add nsw i32 %17036, %61
  br i1 %16467, label %then_bb838, label %next_bb839

"end for output.s0.x.xo834":                      ; preds = %"consume convolved873", %"for output.s0.y.yo764"
  %indvars.iv.next6429 = add nuw nsw i64 %indvars.iv6428, 1
  %.not954 = icmp eq i64 %indvars.iv.next6429, %16819
  br i1 %.not954, label %"end for output.s0.y.yo765", label %"for output.s0.y.yo764"

then_bb838:                                       ; preds = %"for output.s0.x.xo833"
  %t3997 = mul nsw i32 %17037, %stride_x
  %t3998 = add nsw i32 %t3997, %t2587816
  %17038 = sext i32 %t3998 to i64
  %17039 = shl nsw i64 %17038, 4
  %17040 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17039
  %17041 = bitcast i8* %17040 to <8 x i8>*
  %t3999 = load <8 x i8>, <8 x i8>* %17041, align 16, !tbaa !438
  %t4000 = add nsw i32 %t3997, %t2583815
  %17042 = sext i32 %t4000 to i64
  %17043 = shl nsw i64 %17042, 4
  %17044 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17043
  %17045 = bitcast i8* %17044 to <8 x i8>*
  %t4001 = load <8 x i8>, <8 x i8>* %17045, align 16, !tbaa !438
  %t4002 = add nsw i32 %t3997, %t2579798
  %17046 = sext i32 %t4002 to i64
  %17047 = shl nsw i64 %17046, 4
  %17048 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17047
  %17049 = bitcast i8* %17048 to <8 x i8>*
  %t4003 = load <8 x i8>, <8 x i8>* %17049, align 16, !tbaa !438
  %t4004 = add nsw i32 %t3997, %t2574814
  %17050 = sext i32 %t4004 to i64
  %17051 = shl nsw i64 %17050, 4
  %17052 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17051
  %17053 = bitcast i8* %17052 to <8 x i8>*
  %t4005 = load <8 x i8>, <8 x i8>* %17053, align 16, !tbaa !438
  %t4006 = add nsw i32 %t3997, %t2570813
  %17054 = sext i32 %t4006 to i64
  %17055 = shl nsw i64 %17054, 4
  %17056 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17055
  %17057 = bitcast i8* %17056 to <8 x i8>*
  %t4007 = load <8 x i8>, <8 x i8>* %17057, align 16, !tbaa !438
  %t4008 = add nsw i32 %t3997, %t2566797
  %17058 = sext i32 %t4008 to i64
  %17059 = shl nsw i64 %17058, 4
  %17060 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17059
  %17061 = bitcast i8* %17060 to <8 x i8>*
  %t4009 = load <8 x i8>, <8 x i8>* %17061, align 16, !tbaa !438
  %t4010 = add nsw i32 %t3997, %t2561806
  %17062 = sext i32 %t4010 to i64
  %17063 = shl nsw i64 %17062, 4
  %17064 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17063
  %17065 = bitcast i8* %17064 to <8 x i8>*
  %t4011 = load <8 x i8>, <8 x i8>* %17065, align 16, !tbaa !438
  %t4012 = add nsw i32 %t3997, %t2557805
  %17066 = sext i32 %t4012 to i64
  %17067 = shl nsw i64 %17066, 4
  %17068 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17067
  %17069 = bitcast i8* %17068 to <8 x i8>*
  %t4013 = load <8 x i8>, <8 x i8>* %17069, align 16, !tbaa !438
  %t4014 = add nsw i32 %t3997, %t2553793
  %17070 = sext i32 %t4014 to i64
  %17071 = shl nsw i64 %17070, 4
  %17072 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17071
  %17073 = bitcast i8* %17072 to <8 x i8>*
  %t4015 = load <8 x i8>, <8 x i8>* %17073, align 16, !tbaa !438
  %17074 = getelementptr inbounds i8, i8* %17040, i64 8
  %17075 = bitcast i8* %17074 to <8 x i8>*
  %t4016 = load <8 x i8>, <8 x i8>* %17075, align 8, !tbaa !438
  %17076 = getelementptr inbounds i8, i8* %17044, i64 8
  %17077 = bitcast i8* %17076 to <8 x i8>*
  %t4017 = load <8 x i8>, <8 x i8>* %17077, align 8, !tbaa !438
  %17078 = getelementptr inbounds i8, i8* %17048, i64 8
  %17079 = bitcast i8* %17078 to <8 x i8>*
  %t4018 = load <8 x i8>, <8 x i8>* %17079, align 8, !tbaa !438
  %17080 = getelementptr inbounds i8, i8* %17052, i64 8
  %17081 = bitcast i8* %17080 to <8 x i8>*
  %t4019 = load <8 x i8>, <8 x i8>* %17081, align 8, !tbaa !438
  %17082 = getelementptr inbounds i8, i8* %17056, i64 8
  %17083 = bitcast i8* %17082 to <8 x i8>*
  %t4020 = load <8 x i8>, <8 x i8>* %17083, align 8, !tbaa !438
  %17084 = getelementptr inbounds i8, i8* %17060, i64 8
  %17085 = bitcast i8* %17084 to <8 x i8>*
  %t4021 = load <8 x i8>, <8 x i8>* %17085, align 8, !tbaa !438
  %17086 = getelementptr inbounds i8, i8* %17064, i64 8
  %17087 = bitcast i8* %17086 to <8 x i8>*
  %t4022 = load <8 x i8>, <8 x i8>* %17087, align 8, !tbaa !438
  %17088 = getelementptr inbounds i8, i8* %17068, i64 8
  %17089 = bitcast i8* %17088 to <8 x i8>*
  %t4023 = load <8 x i8>, <8 x i8>* %17089, align 8, !tbaa !438
  %17090 = getelementptr inbounds i8, i8* %17072, i64 8
  %17091 = bitcast i8* %17090 to <8 x i8>*
  %t4024 = load <8 x i8>, <8 x i8>* %17091, align 8, !tbaa !438
  %17092 = load <4 x i16>, <4 x i16>* %16721, align 16, !tbaa !395
  %17093 = zext <8 x i8> %t3999 to <8 x i16>
  %17094 = bitcast <8 x i16> %17093 to <2 x i64>
  %17095 = shufflevector <2 x i64> %17094, <2 x i64> undef, <1 x i32> zeroinitializer
  %17096 = bitcast <1 x i64> %17095 to <4 x i16>
  %17097 = load <4 x i16>, <4 x i16>* %16723, align 16, !tbaa !395
  %17098 = zext <8 x i8> %t4001 to <8 x i16>
  %17099 = bitcast <8 x i16> %17098 to <2 x i64>
  %17100 = shufflevector <2 x i64> %17099, <2 x i64> undef, <1 x i32> zeroinitializer
  %17101 = load <4 x i16>, <4 x i16>* %16724, align 16, !tbaa !395
  %17102 = zext <8 x i8> %t4003 to <8 x i16>
  %17103 = bitcast <8 x i16> %17102 to <2 x i64>
  %17104 = shufflevector <2 x i64> %17103, <2 x i64> undef, <1 x i32> zeroinitializer
  %17105 = bitcast <1 x i64> %17104 to <4 x i16>
  %17106 = load <4 x i16>, <4 x i16>* %16727, align 16, !tbaa !395
  %17107 = zext <8 x i8> %t4005 to <8 x i16>
  %17108 = bitcast <8 x i16> %17107 to <2 x i64>
  %17109 = shufflevector <2 x i64> %17108, <2 x i64> undef, <1 x i32> zeroinitializer
  %17110 = load <4 x i16>, <4 x i16>* %16729, align 16, !tbaa !395
  %17111 = zext <8 x i8> %t4007 to <8 x i16>
  %17112 = bitcast <8 x i16> %17111 to <2 x i64>
  %17113 = shufflevector <2 x i64> %17112, <2 x i64> undef, <1 x i32> zeroinitializer
  %17114 = bitcast <1 x i64> %17113 to <4 x i16>
  %17115 = load <4 x i16>, <4 x i16>* %16730, align 16, !tbaa !395
  %17116 = zext <8 x i8> %t4009 to <8 x i16>
  %17117 = bitcast <8 x i16> %17116 to <2 x i64>
  %17118 = shufflevector <2 x i64> %17117, <2 x i64> undef, <1 x i32> zeroinitializer
  %17119 = load <4 x i16>, <4 x i16>* %16732, align 16, !tbaa !534
  %17120 = zext <8 x i8> %t4011 to <8 x i16>
  %17121 = bitcast <8 x i16> %17120 to <2 x i64>
  %17122 = shufflevector <2 x i64> %17121, <2 x i64> undef, <1 x i32> zeroinitializer
  %17123 = bitcast <1 x i64> %17122 to <4 x i16>
  %17124 = load <4 x i16>, <4 x i16>* %16734, align 16, !tbaa !536
  %17125 = zext <8 x i8> %t4013 to <8 x i16>
  %17126 = bitcast <8 x i16> %17125 to <2 x i64>
  %17127 = shufflevector <2 x i64> %17126, <2 x i64> undef, <1 x i32> zeroinitializer
  %17128 = load <4 x i16>, <4 x i16>* %16735, align 16, !tbaa !538
  %17129 = zext <8 x i8> %t4015 to <8 x i16>
  %17130 = bitcast <8 x i16> %17129 to <2 x i64>
  %17131 = shufflevector <2 x i64> %17130, <2 x i64> undef, <1 x i32> zeroinitializer
  %17132 = bitcast <1 x i64> %17131 to <4 x i16>
  %.cast = bitcast <1 x i64> %17100 to <4 x i16>
  %17133 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast, <4 x i16> %17097) #11
  %17134 = sext <4 x i16> %17096 to <4 x i32>
  %17135 = sext <4 x i16> %17092 to <4 x i32>
  %17136 = mul nsw <4 x i32> %17135, %17134
  %.cast1024 = bitcast <1 x i64> %17109 to <4 x i16>
  %17137 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1024, <4 x i16> %17106) #11
  %17138 = sext <4 x i16> %17105 to <4 x i32>
  %17139 = sext <4 x i16> %17101 to <4 x i32>
  %17140 = mul nsw <4 x i32> %17139, %17138
  %.cast1027 = bitcast <1 x i64> %17118 to <4 x i16>
  %17141 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1027, <4 x i16> %17115) #11
  %17142 = sext <4 x i16> %17114 to <4 x i32>
  %17143 = sext <4 x i16> %17110 to <4 x i32>
  %17144 = mul nsw <4 x i32> %17143, %17142
  %.cast1030 = bitcast <1 x i64> %17127 to <4 x i16>
  %17145 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1030, <4 x i16> %17124) #11
  %17146 = sext <4 x i16> %17123 to <4 x i32>
  %17147 = sext <4 x i16> %17119 to <4 x i32>
  %17148 = mul nsw <4 x i32> %17147, %17146
  %17149 = sext <4 x i16> %17128 to <4 x i32>
  %17150 = sext <4 x i16> %17132 to <4 x i32>
  %17151 = mul nsw <4 x i32> %17149, %17150
  %17152 = add <4 x i32> %17136, %16984
  %17153 = add <4 x i32> %17152, %17140
  %17154 = add <4 x i32> %17153, %17144
  %17155 = add <4 x i32> %17154, %17148
  %17156 = add <4 x i32> %17155, %17133
  %17157 = add <4 x i32> %17156, %17137
  %17158 = add <4 x i32> %17157, %17151
  %17159 = add <4 x i32> %17158, %17141
  %17160 = add <4 x i32> %17159, %17145
  %17161 = load <4 x i16>, <4 x i16>* %16737, align 8, !tbaa !395
  %17162 = shufflevector <2 x i64> %17094, <2 x i64> undef, <1 x i32> <i32 1>
  %17163 = bitcast <1 x i64> %17162 to <4 x i16>
  %17164 = load <4 x i16>, <4 x i16>* %16739, align 8, !tbaa !395
  %17165 = shufflevector <2 x i64> %17099, <2 x i64> undef, <1 x i32> <i32 1>
  %17166 = load <4 x i16>, <4 x i16>* %16741, align 8, !tbaa !395
  %17167 = shufflevector <2 x i64> %17103, <2 x i64> undef, <1 x i32> <i32 1>
  %17168 = bitcast <1 x i64> %17167 to <4 x i16>
  %17169 = load <4 x i16>, <4 x i16>* %16743, align 8, !tbaa !395
  %17170 = shufflevector <2 x i64> %17108, <2 x i64> undef, <1 x i32> <i32 1>
  %17171 = load <4 x i16>, <4 x i16>* %16745, align 8, !tbaa !395
  %17172 = shufflevector <2 x i64> %17112, <2 x i64> undef, <1 x i32> <i32 1>
  %17173 = bitcast <1 x i64> %17172 to <4 x i16>
  %17174 = load <4 x i16>, <4 x i16>* %16747, align 8, !tbaa !395
  %17175 = shufflevector <2 x i64> %17117, <2 x i64> undef, <1 x i32> <i32 1>
  %17176 = load <4 x i16>, <4 x i16>* %16749, align 16, !tbaa !540
  %17177 = shufflevector <2 x i64> %17121, <2 x i64> undef, <1 x i32> <i32 1>
  %17178 = bitcast <1 x i64> %17177 to <4 x i16>
  %17179 = load <4 x i16>, <4 x i16>* %16751, align 16, !tbaa !542
  %17180 = shufflevector <2 x i64> %17126, <2 x i64> undef, <1 x i32> <i32 1>
  %17181 = load <4 x i16>, <4 x i16>* %16753, align 16, !tbaa !544
  %17182 = shufflevector <2 x i64> %17130, <2 x i64> undef, <1 x i32> <i32 1>
  %17183 = bitcast <1 x i64> %17182 to <4 x i16>
  %.cast1033 = bitcast <1 x i64> %17165 to <4 x i16>
  %17184 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1033, <4 x i16> %17164) #11
  %17185 = sext <4 x i16> %17163 to <4 x i32>
  %17186 = sext <4 x i16> %17161 to <4 x i32>
  %17187 = mul nsw <4 x i32> %17186, %17185
  %.cast1036 = bitcast <1 x i64> %17170 to <4 x i16>
  %17188 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1036, <4 x i16> %17169) #11
  %17189 = sext <4 x i16> %17168 to <4 x i32>
  %17190 = sext <4 x i16> %17166 to <4 x i32>
  %17191 = mul nsw <4 x i32> %17190, %17189
  %.cast1039 = bitcast <1 x i64> %17175 to <4 x i16>
  %17192 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1039, <4 x i16> %17174) #11
  %17193 = sext <4 x i16> %17173 to <4 x i32>
  %17194 = sext <4 x i16> %17171 to <4 x i32>
  %17195 = mul nsw <4 x i32> %17194, %17193
  %.cast1042 = bitcast <1 x i64> %17180 to <4 x i16>
  %17196 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1042, <4 x i16> %17179) #11
  %17197 = sext <4 x i16> %17178 to <4 x i32>
  %17198 = sext <4 x i16> %17176 to <4 x i32>
  %17199 = mul nsw <4 x i32> %17198, %17197
  %17200 = sext <4 x i16> %17181 to <4 x i32>
  %17201 = sext <4 x i16> %17183 to <4 x i32>
  %17202 = mul nsw <4 x i32> %17200, %17201
  %17203 = add <4 x i32> %17187, %16985
  %17204 = add <4 x i32> %17203, %17191
  %17205 = add <4 x i32> %17204, %17195
  %17206 = add <4 x i32> %17205, %17199
  %17207 = add <4 x i32> %17206, %17184
  %17208 = add <4 x i32> %17207, %17188
  %17209 = add <4 x i32> %17208, %17202
  %17210 = add <4 x i32> %17209, %17192
  %17211 = add <4 x i32> %17210, %17196
  %17212 = load <4 x i16>, <4 x i16>* %16755, align 16, !tbaa !395
  %17213 = zext <8 x i8> %t4016 to <8 x i16>
  %17214 = bitcast <8 x i16> %17213 to <2 x i64>
  %17215 = shufflevector <2 x i64> %17214, <2 x i64> undef, <1 x i32> zeroinitializer
  %17216 = bitcast <1 x i64> %17215 to <4 x i16>
  %17217 = load <4 x i16>, <4 x i16>* %16757, align 16, !tbaa !395
  %17218 = zext <8 x i8> %t4017 to <8 x i16>
  %17219 = bitcast <8 x i16> %17218 to <2 x i64>
  %17220 = shufflevector <2 x i64> %17219, <2 x i64> undef, <1 x i32> zeroinitializer
  %17221 = load <4 x i16>, <4 x i16>* %16759, align 16, !tbaa !395
  %17222 = zext <8 x i8> %t4018 to <8 x i16>
  %17223 = bitcast <8 x i16> %17222 to <2 x i64>
  %17224 = shufflevector <2 x i64> %17223, <2 x i64> undef, <1 x i32> zeroinitializer
  %17225 = bitcast <1 x i64> %17224 to <4 x i16>
  %17226 = load <4 x i16>, <4 x i16>* %16761, align 16, !tbaa !395
  %17227 = zext <8 x i8> %t4019 to <8 x i16>
  %17228 = bitcast <8 x i16> %17227 to <2 x i64>
  %17229 = shufflevector <2 x i64> %17228, <2 x i64> undef, <1 x i32> zeroinitializer
  %17230 = load <4 x i16>, <4 x i16>* %16763, align 16, !tbaa !395
  %17231 = zext <8 x i8> %t4020 to <8 x i16>
  %17232 = bitcast <8 x i16> %17231 to <2 x i64>
  %17233 = shufflevector <2 x i64> %17232, <2 x i64> undef, <1 x i32> zeroinitializer
  %17234 = bitcast <1 x i64> %17233 to <4 x i16>
  %17235 = load <4 x i16>, <4 x i16>* %16765, align 16, !tbaa !395
  %17236 = zext <8 x i8> %t4021 to <8 x i16>
  %17237 = bitcast <8 x i16> %17236 to <2 x i64>
  %17238 = shufflevector <2 x i64> %17237, <2 x i64> undef, <1 x i32> zeroinitializer
  %17239 = load <4 x i16>, <4 x i16>* %16767, align 16, !tbaa !546
  %17240 = zext <8 x i8> %t4022 to <8 x i16>
  %17241 = bitcast <8 x i16> %17240 to <2 x i64>
  %17242 = shufflevector <2 x i64> %17241, <2 x i64> undef, <1 x i32> zeroinitializer
  %17243 = bitcast <1 x i64> %17242 to <4 x i16>
  %17244 = load <4 x i16>, <4 x i16>* %16769, align 16, !tbaa !548
  %17245 = zext <8 x i8> %t4023 to <8 x i16>
  %17246 = bitcast <8 x i16> %17245 to <2 x i64>
  %17247 = shufflevector <2 x i64> %17246, <2 x i64> undef, <1 x i32> zeroinitializer
  %17248 = load <4 x i16>, <4 x i16>* %16771, align 16, !tbaa !550
  %17249 = zext <8 x i8> %t4024 to <8 x i16>
  %17250 = bitcast <8 x i16> %17249 to <2 x i64>
  %17251 = shufflevector <2 x i64> %17250, <2 x i64> undef, <1 x i32> zeroinitializer
  %17252 = bitcast <1 x i64> %17251 to <4 x i16>
  %.cast1045 = bitcast <1 x i64> %17220 to <4 x i16>
  %17253 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1045, <4 x i16> %17217) #11
  %17254 = sext <4 x i16> %17216 to <4 x i32>
  %17255 = sext <4 x i16> %17212 to <4 x i32>
  %17256 = mul nsw <4 x i32> %17255, %17254
  %.cast1048 = bitcast <1 x i64> %17229 to <4 x i16>
  %17257 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1048, <4 x i16> %17226) #11
  %17258 = sext <4 x i16> %17225 to <4 x i32>
  %17259 = sext <4 x i16> %17221 to <4 x i32>
  %17260 = mul nsw <4 x i32> %17259, %17258
  %.cast1051 = bitcast <1 x i64> %17238 to <4 x i16>
  %17261 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1051, <4 x i16> %17235) #11
  %17262 = sext <4 x i16> %17234 to <4 x i32>
  %17263 = sext <4 x i16> %17230 to <4 x i32>
  %17264 = mul nsw <4 x i32> %17263, %17262
  %.cast1054 = bitcast <1 x i64> %17247 to <4 x i16>
  %17265 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1054, <4 x i16> %17244) #11
  %17266 = sext <4 x i16> %17243 to <4 x i32>
  %17267 = sext <4 x i16> %17239 to <4 x i32>
  %17268 = mul nsw <4 x i32> %17267, %17266
  %17269 = sext <4 x i16> %17248 to <4 x i32>
  %17270 = sext <4 x i16> %17252 to <4 x i32>
  %17271 = mul nsw <4 x i32> %17269, %17270
  %17272 = add <4 x i32> %17256, %16986
  %17273 = add <4 x i32> %17272, %17260
  %17274 = add <4 x i32> %17273, %17264
  %17275 = add <4 x i32> %17274, %17268
  %17276 = add <4 x i32> %17275, %17253
  %17277 = add <4 x i32> %17276, %17257
  %17278 = add <4 x i32> %17277, %17271
  %17279 = add <4 x i32> %17278, %17261
  %17280 = add <4 x i32> %17279, %17265
  %17281 = load <4 x i16>, <4 x i16>* %16773, align 8, !tbaa !395
  %17282 = shufflevector <2 x i64> %17214, <2 x i64> undef, <1 x i32> <i32 1>
  %17283 = bitcast <1 x i64> %17282 to <4 x i16>
  %17284 = load <4 x i16>, <4 x i16>* %16775, align 8, !tbaa !395
  %17285 = shufflevector <2 x i64> %17219, <2 x i64> undef, <1 x i32> <i32 1>
  %17286 = load <4 x i16>, <4 x i16>* %16777, align 8, !tbaa !395
  %17287 = shufflevector <2 x i64> %17223, <2 x i64> undef, <1 x i32> <i32 1>
  %17288 = bitcast <1 x i64> %17287 to <4 x i16>
  %17289 = load <4 x i16>, <4 x i16>* %16779, align 8, !tbaa !395
  %17290 = shufflevector <2 x i64> %17228, <2 x i64> undef, <1 x i32> <i32 1>
  %17291 = load <4 x i16>, <4 x i16>* %16781, align 8, !tbaa !395
  %17292 = shufflevector <2 x i64> %17232, <2 x i64> undef, <1 x i32> <i32 1>
  %17293 = bitcast <1 x i64> %17292 to <4 x i16>
  %17294 = load <4 x i16>, <4 x i16>* %16783, align 8, !tbaa !395
  %17295 = shufflevector <2 x i64> %17237, <2 x i64> undef, <1 x i32> <i32 1>
  %17296 = load <4 x i16>, <4 x i16>* %16785, align 16, !tbaa !552
  %17297 = shufflevector <2 x i64> %17241, <2 x i64> undef, <1 x i32> <i32 1>
  %17298 = bitcast <1 x i64> %17297 to <4 x i16>
  %17299 = load <4 x i16>, <4 x i16>* %16787, align 16, !tbaa !554
  %17300 = shufflevector <2 x i64> %17246, <2 x i64> undef, <1 x i32> <i32 1>
  %17301 = load <4 x i16>, <4 x i16>* %16789, align 16, !tbaa !556
  %17302 = shufflevector <2 x i64> %17250, <2 x i64> undef, <1 x i32> <i32 1>
  %17303 = bitcast <1 x i64> %17302 to <4 x i16>
  %.cast1057 = bitcast <1 x i64> %17285 to <4 x i16>
  %17304 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1057, <4 x i16> %17284) #11
  %17305 = sext <4 x i16> %17283 to <4 x i32>
  %17306 = sext <4 x i16> %17281 to <4 x i32>
  %17307 = mul nsw <4 x i32> %17306, %17305
  %.cast1060 = bitcast <1 x i64> %17290 to <4 x i16>
  %17308 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1060, <4 x i16> %17289) #11
  %17309 = sext <4 x i16> %17288 to <4 x i32>
  %17310 = sext <4 x i16> %17286 to <4 x i32>
  %17311 = mul nsw <4 x i32> %17310, %17309
  %.cast1063 = bitcast <1 x i64> %17295 to <4 x i16>
  %17312 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1063, <4 x i16> %17294) #11
  %17313 = sext <4 x i16> %17293 to <4 x i32>
  %17314 = sext <4 x i16> %17291 to <4 x i32>
  %17315 = mul nsw <4 x i32> %17314, %17313
  %.cast1066 = bitcast <1 x i64> %17300 to <4 x i16>
  %17316 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1066, <4 x i16> %17299) #11
  %17317 = sext <4 x i16> %17298 to <4 x i32>
  %17318 = sext <4 x i16> %17296 to <4 x i32>
  %17319 = mul nsw <4 x i32> %17318, %17317
  %17320 = sext <4 x i16> %17301 to <4 x i32>
  %17321 = sext <4 x i16> %17303 to <4 x i32>
  %17322 = mul nsw <4 x i32> %17320, %17321
  %17323 = add <4 x i32> %17307, %16987
  %17324 = add <4 x i32> %17323, %17311
  %17325 = add <4 x i32> %17324, %17315
  %17326 = add <4 x i32> %17325, %17319
  %17327 = add <4 x i32> %17326, %17304
  %17328 = add <4 x i32> %17327, %17308
  %17329 = add <4 x i32> %17328, %17322
  %17330 = add <4 x i32> %17329, %17312
  %17331 = add <4 x i32> %17330, %17316
  %17332 = add nsw i32 %17037, 1
  %t4025 = mul nsw i32 %17332, %stride_x
  %t4026 = add nsw i32 %t4025, %t2587816
  %17333 = sext i32 %t4026 to i64
  %17334 = shl nsw i64 %17333, 4
  %17335 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17334
  %17336 = bitcast i8* %17335 to <8 x i8>*
  %t4027 = load <8 x i8>, <8 x i8>* %17336, align 16, !tbaa !438
  %t4028 = add nsw i32 %t4025, %t2583815
  %17337 = sext i32 %t4028 to i64
  %17338 = shl nsw i64 %17337, 4
  %17339 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17338
  %17340 = bitcast i8* %17339 to <8 x i8>*
  %t4029 = load <8 x i8>, <8 x i8>* %17340, align 16, !tbaa !438
  %t4030 = add nsw i32 %t4025, %t2579798
  %17341 = sext i32 %t4030 to i64
  %17342 = shl nsw i64 %17341, 4
  %17343 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17342
  %17344 = bitcast i8* %17343 to <8 x i8>*
  %t4031 = load <8 x i8>, <8 x i8>* %17344, align 16, !tbaa !438
  %t4032 = add nsw i32 %t4025, %t2574814
  %17345 = sext i32 %t4032 to i64
  %17346 = shl nsw i64 %17345, 4
  %17347 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17346
  %17348 = bitcast i8* %17347 to <8 x i8>*
  %t4033 = load <8 x i8>, <8 x i8>* %17348, align 16, !tbaa !438
  %t4034 = add nsw i32 %t4025, %t2570813
  %17349 = sext i32 %t4034 to i64
  %17350 = shl nsw i64 %17349, 4
  %17351 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17350
  %17352 = bitcast i8* %17351 to <8 x i8>*
  %t4035 = load <8 x i8>, <8 x i8>* %17352, align 16, !tbaa !438
  %t4036 = add nsw i32 %t4025, %t2566797
  %17353 = sext i32 %t4036 to i64
  %17354 = shl nsw i64 %17353, 4
  %17355 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17354
  %17356 = bitcast i8* %17355 to <8 x i8>*
  %t4037 = load <8 x i8>, <8 x i8>* %17356, align 16, !tbaa !438
  %t4038 = add nsw i32 %t4025, %t2561806
  %17357 = sext i32 %t4038 to i64
  %17358 = shl nsw i64 %17357, 4
  %17359 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17358
  %17360 = bitcast i8* %17359 to <8 x i8>*
  %t4039 = load <8 x i8>, <8 x i8>* %17360, align 16, !tbaa !438
  %t4040 = add nsw i32 %t4025, %t2557805
  %17361 = sext i32 %t4040 to i64
  %17362 = shl nsw i64 %17361, 4
  %17363 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17362
  %17364 = bitcast i8* %17363 to <8 x i8>*
  %t4041 = load <8 x i8>, <8 x i8>* %17364, align 16, !tbaa !438
  %t4042 = add nsw i32 %t4025, %t2553793
  %17365 = sext i32 %t4042 to i64
  %17366 = shl nsw i64 %17365, 4
  %17367 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17366
  %17368 = bitcast i8* %17367 to <8 x i8>*
  %t4043 = load <8 x i8>, <8 x i8>* %17368, align 16, !tbaa !438
  %17369 = getelementptr inbounds i8, i8* %17335, i64 8
  %17370 = bitcast i8* %17369 to <8 x i8>*
  %t4044 = load <8 x i8>, <8 x i8>* %17370, align 8, !tbaa !438
  %17371 = getelementptr inbounds i8, i8* %17339, i64 8
  %17372 = bitcast i8* %17371 to <8 x i8>*
  %t4045 = load <8 x i8>, <8 x i8>* %17372, align 8, !tbaa !438
  %17373 = getelementptr inbounds i8, i8* %17343, i64 8
  %17374 = bitcast i8* %17373 to <8 x i8>*
  %t4046 = load <8 x i8>, <8 x i8>* %17374, align 8, !tbaa !438
  %17375 = getelementptr inbounds i8, i8* %17347, i64 8
  %17376 = bitcast i8* %17375 to <8 x i8>*
  %t4047 = load <8 x i8>, <8 x i8>* %17376, align 8, !tbaa !438
  %17377 = getelementptr inbounds i8, i8* %17351, i64 8
  %17378 = bitcast i8* %17377 to <8 x i8>*
  %t4048 = load <8 x i8>, <8 x i8>* %17378, align 8, !tbaa !438
  %17379 = getelementptr inbounds i8, i8* %17355, i64 8
  %17380 = bitcast i8* %17379 to <8 x i8>*
  %t4049 = load <8 x i8>, <8 x i8>* %17380, align 8, !tbaa !438
  %17381 = getelementptr inbounds i8, i8* %17359, i64 8
  %17382 = bitcast i8* %17381 to <8 x i8>*
  %t4050 = load <8 x i8>, <8 x i8>* %17382, align 8, !tbaa !438
  %17383 = getelementptr inbounds i8, i8* %17363, i64 8
  %17384 = bitcast i8* %17383 to <8 x i8>*
  %t4051 = load <8 x i8>, <8 x i8>* %17384, align 8, !tbaa !438
  %17385 = getelementptr inbounds i8, i8* %17367, i64 8
  %17386 = bitcast i8* %17385 to <8 x i8>*
  %t4052 = load <8 x i8>, <8 x i8>* %17386, align 8, !tbaa !438
  %17387 = zext <8 x i8> %t4027 to <8 x i16>
  %17388 = bitcast <8 x i16> %17387 to <2 x i64>
  %17389 = shufflevector <2 x i64> %17388, <2 x i64> undef, <1 x i32> zeroinitializer
  %17390 = bitcast <1 x i64> %17389 to <4 x i16>
  %17391 = zext <8 x i8> %t4029 to <8 x i16>
  %17392 = bitcast <8 x i16> %17391 to <2 x i64>
  %17393 = shufflevector <2 x i64> %17392, <2 x i64> undef, <1 x i32> zeroinitializer
  %17394 = zext <8 x i8> %t4031 to <8 x i16>
  %17395 = bitcast <8 x i16> %17394 to <2 x i64>
  %17396 = shufflevector <2 x i64> %17395, <2 x i64> undef, <1 x i32> zeroinitializer
  %17397 = bitcast <1 x i64> %17396 to <4 x i16>
  %17398 = zext <8 x i8> %t4033 to <8 x i16>
  %17399 = bitcast <8 x i16> %17398 to <2 x i64>
  %17400 = shufflevector <2 x i64> %17399, <2 x i64> undef, <1 x i32> zeroinitializer
  %17401 = zext <8 x i8> %t4035 to <8 x i16>
  %17402 = bitcast <8 x i16> %17401 to <2 x i64>
  %17403 = shufflevector <2 x i64> %17402, <2 x i64> undef, <1 x i32> zeroinitializer
  %17404 = bitcast <1 x i64> %17403 to <4 x i16>
  %17405 = zext <8 x i8> %t4037 to <8 x i16>
  %17406 = bitcast <8 x i16> %17405 to <2 x i64>
  %17407 = shufflevector <2 x i64> %17406, <2 x i64> undef, <1 x i32> zeroinitializer
  %17408 = zext <8 x i8> %t4039 to <8 x i16>
  %17409 = bitcast <8 x i16> %17408 to <2 x i64>
  %17410 = shufflevector <2 x i64> %17409, <2 x i64> undef, <1 x i32> zeroinitializer
  %17411 = bitcast <1 x i64> %17410 to <4 x i16>
  %17412 = zext <8 x i8> %t4041 to <8 x i16>
  %17413 = bitcast <8 x i16> %17412 to <2 x i64>
  %17414 = shufflevector <2 x i64> %17413, <2 x i64> undef, <1 x i32> zeroinitializer
  %17415 = zext <8 x i8> %t4043 to <8 x i16>
  %17416 = bitcast <8 x i16> %17415 to <2 x i64>
  %17417 = shufflevector <2 x i64> %17416, <2 x i64> undef, <1 x i32> zeroinitializer
  %17418 = bitcast <1 x i64> %17417 to <4 x i16>
  %.cast1069 = bitcast <1 x i64> %17393 to <4 x i16>
  %17419 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1069, <4 x i16> %17097) #11
  %17420 = sext <4 x i16> %17390 to <4 x i32>
  %17421 = mul nsw <4 x i32> %17420, %17135
  %.cast1072 = bitcast <1 x i64> %17400 to <4 x i16>
  %17422 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1072, <4 x i16> %17106) #11
  %17423 = sext <4 x i16> %17397 to <4 x i32>
  %17424 = mul nsw <4 x i32> %17423, %17139
  %.cast1075 = bitcast <1 x i64> %17407 to <4 x i16>
  %17425 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1075, <4 x i16> %17115) #11
  %17426 = sext <4 x i16> %17404 to <4 x i32>
  %17427 = mul nsw <4 x i32> %17426, %17143
  %.cast1078 = bitcast <1 x i64> %17414 to <4 x i16>
  %17428 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1078, <4 x i16> %17124) #11
  %17429 = sext <4 x i16> %17411 to <4 x i32>
  %17430 = mul nsw <4 x i32> %17429, %17147
  %17431 = sext <4 x i16> %17418 to <4 x i32>
  %17432 = mul nsw <4 x i32> %17431, %17149
  %17433 = add <4 x i32> %17421, %16984
  %17434 = add <4 x i32> %17433, %17424
  %17435 = add <4 x i32> %17434, %17427
  %17436 = add <4 x i32> %17435, %17430
  %17437 = add <4 x i32> %17436, %17432
  %17438 = add <4 x i32> %17437, %17419
  %17439 = add <4 x i32> %17438, %17422
  %17440 = add <4 x i32> %17439, %17425
  %17441 = add <4 x i32> %17440, %17428
  %17442 = shufflevector <2 x i64> %17388, <2 x i64> undef, <1 x i32> <i32 1>
  %17443 = bitcast <1 x i64> %17442 to <4 x i16>
  %17444 = shufflevector <2 x i64> %17392, <2 x i64> undef, <1 x i32> <i32 1>
  %17445 = shufflevector <2 x i64> %17395, <2 x i64> undef, <1 x i32> <i32 1>
  %17446 = bitcast <1 x i64> %17445 to <4 x i16>
  %17447 = shufflevector <2 x i64> %17399, <2 x i64> undef, <1 x i32> <i32 1>
  %17448 = shufflevector <2 x i64> %17402, <2 x i64> undef, <1 x i32> <i32 1>
  %17449 = bitcast <1 x i64> %17448 to <4 x i16>
  %17450 = shufflevector <2 x i64> %17406, <2 x i64> undef, <1 x i32> <i32 1>
  %17451 = shufflevector <2 x i64> %17409, <2 x i64> undef, <1 x i32> <i32 1>
  %17452 = bitcast <1 x i64> %17451 to <4 x i16>
  %17453 = shufflevector <2 x i64> %17413, <2 x i64> undef, <1 x i32> <i32 1>
  %17454 = shufflevector <2 x i64> %17416, <2 x i64> undef, <1 x i32> <i32 1>
  %17455 = bitcast <1 x i64> %17454 to <4 x i16>
  %.cast1081 = bitcast <1 x i64> %17444 to <4 x i16>
  %17456 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1081, <4 x i16> %17164) #11
  %17457 = sext <4 x i16> %17443 to <4 x i32>
  %17458 = mul nsw <4 x i32> %17457, %17186
  %.cast1084 = bitcast <1 x i64> %17447 to <4 x i16>
  %17459 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1084, <4 x i16> %17169) #11
  %17460 = sext <4 x i16> %17446 to <4 x i32>
  %17461 = mul nsw <4 x i32> %17460, %17190
  %.cast1087 = bitcast <1 x i64> %17450 to <4 x i16>
  %17462 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1087, <4 x i16> %17174) #11
  %17463 = sext <4 x i16> %17449 to <4 x i32>
  %17464 = mul nsw <4 x i32> %17463, %17194
  %.cast1090 = bitcast <1 x i64> %17453 to <4 x i16>
  %17465 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1090, <4 x i16> %17179) #11
  %17466 = sext <4 x i16> %17452 to <4 x i32>
  %17467 = mul nsw <4 x i32> %17466, %17198
  %17468 = sext <4 x i16> %17455 to <4 x i32>
  %17469 = mul nsw <4 x i32> %17468, %17200
  %17470 = add <4 x i32> %17458, %16985
  %17471 = add <4 x i32> %17470, %17461
  %17472 = add <4 x i32> %17471, %17464
  %17473 = add <4 x i32> %17472, %17467
  %17474 = add <4 x i32> %17473, %17469
  %17475 = add <4 x i32> %17474, %17456
  %17476 = add <4 x i32> %17475, %17459
  %17477 = add <4 x i32> %17476, %17462
  %17478 = add <4 x i32> %17477, %17465
  %17479 = zext <8 x i8> %t4044 to <8 x i16>
  %17480 = bitcast <8 x i16> %17479 to <2 x i64>
  %17481 = shufflevector <2 x i64> %17480, <2 x i64> undef, <1 x i32> zeroinitializer
  %17482 = bitcast <1 x i64> %17481 to <4 x i16>
  %17483 = zext <8 x i8> %t4045 to <8 x i16>
  %17484 = bitcast <8 x i16> %17483 to <2 x i64>
  %17485 = shufflevector <2 x i64> %17484, <2 x i64> undef, <1 x i32> zeroinitializer
  %17486 = zext <8 x i8> %t4046 to <8 x i16>
  %17487 = bitcast <8 x i16> %17486 to <2 x i64>
  %17488 = shufflevector <2 x i64> %17487, <2 x i64> undef, <1 x i32> zeroinitializer
  %17489 = bitcast <1 x i64> %17488 to <4 x i16>
  %17490 = zext <8 x i8> %t4047 to <8 x i16>
  %17491 = bitcast <8 x i16> %17490 to <2 x i64>
  %17492 = shufflevector <2 x i64> %17491, <2 x i64> undef, <1 x i32> zeroinitializer
  %17493 = zext <8 x i8> %t4048 to <8 x i16>
  %17494 = bitcast <8 x i16> %17493 to <2 x i64>
  %17495 = shufflevector <2 x i64> %17494, <2 x i64> undef, <1 x i32> zeroinitializer
  %17496 = bitcast <1 x i64> %17495 to <4 x i16>
  %17497 = zext <8 x i8> %t4049 to <8 x i16>
  %17498 = bitcast <8 x i16> %17497 to <2 x i64>
  %17499 = shufflevector <2 x i64> %17498, <2 x i64> undef, <1 x i32> zeroinitializer
  %17500 = zext <8 x i8> %t4050 to <8 x i16>
  %17501 = bitcast <8 x i16> %17500 to <2 x i64>
  %17502 = shufflevector <2 x i64> %17501, <2 x i64> undef, <1 x i32> zeroinitializer
  %17503 = bitcast <1 x i64> %17502 to <4 x i16>
  %17504 = zext <8 x i8> %t4051 to <8 x i16>
  %17505 = bitcast <8 x i16> %17504 to <2 x i64>
  %17506 = shufflevector <2 x i64> %17505, <2 x i64> undef, <1 x i32> zeroinitializer
  %17507 = zext <8 x i8> %t4052 to <8 x i16>
  %17508 = bitcast <8 x i16> %17507 to <2 x i64>
  %17509 = shufflevector <2 x i64> %17508, <2 x i64> undef, <1 x i32> zeroinitializer
  %17510 = bitcast <1 x i64> %17509 to <4 x i16>
  %.cast1093 = bitcast <1 x i64> %17485 to <4 x i16>
  %17511 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1093, <4 x i16> %17217) #11
  %17512 = sext <4 x i16> %17482 to <4 x i32>
  %17513 = mul nsw <4 x i32> %17512, %17255
  %.cast1096 = bitcast <1 x i64> %17492 to <4 x i16>
  %17514 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1096, <4 x i16> %17226) #11
  %17515 = sext <4 x i16> %17489 to <4 x i32>
  %17516 = mul nsw <4 x i32> %17515, %17259
  %.cast1099 = bitcast <1 x i64> %17499 to <4 x i16>
  %17517 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1099, <4 x i16> %17235) #11
  %17518 = sext <4 x i16> %17496 to <4 x i32>
  %17519 = mul nsw <4 x i32> %17518, %17263
  %.cast1102 = bitcast <1 x i64> %17506 to <4 x i16>
  %17520 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1102, <4 x i16> %17244) #11
  %17521 = sext <4 x i16> %17503 to <4 x i32>
  %17522 = mul nsw <4 x i32> %17521, %17267
  %17523 = sext <4 x i16> %17510 to <4 x i32>
  %17524 = mul nsw <4 x i32> %17523, %17269
  %17525 = add <4 x i32> %17513, %16986
  %17526 = add <4 x i32> %17525, %17516
  %17527 = add <4 x i32> %17526, %17519
  %17528 = add <4 x i32> %17527, %17522
  %17529 = add <4 x i32> %17528, %17524
  %17530 = add <4 x i32> %17529, %17511
  %17531 = add <4 x i32> %17530, %17514
  %17532 = add <4 x i32> %17531, %17517
  %17533 = add <4 x i32> %17532, %17520
  %17534 = shufflevector <2 x i64> %17480, <2 x i64> undef, <1 x i32> <i32 1>
  %17535 = bitcast <1 x i64> %17534 to <4 x i16>
  %17536 = shufflevector <2 x i64> %17484, <2 x i64> undef, <1 x i32> <i32 1>
  %17537 = shufflevector <2 x i64> %17487, <2 x i64> undef, <1 x i32> <i32 1>
  %17538 = bitcast <1 x i64> %17537 to <4 x i16>
  %17539 = shufflevector <2 x i64> %17491, <2 x i64> undef, <1 x i32> <i32 1>
  %17540 = shufflevector <2 x i64> %17494, <2 x i64> undef, <1 x i32> <i32 1>
  %17541 = bitcast <1 x i64> %17540 to <4 x i16>
  %17542 = shufflevector <2 x i64> %17498, <2 x i64> undef, <1 x i32> <i32 1>
  %17543 = shufflevector <2 x i64> %17501, <2 x i64> undef, <1 x i32> <i32 1>
  %17544 = bitcast <1 x i64> %17543 to <4 x i16>
  %17545 = shufflevector <2 x i64> %17505, <2 x i64> undef, <1 x i32> <i32 1>
  %17546 = shufflevector <2 x i64> %17508, <2 x i64> undef, <1 x i32> <i32 1>
  %17547 = bitcast <1 x i64> %17546 to <4 x i16>
  %.cast1105 = bitcast <1 x i64> %17536 to <4 x i16>
  %17548 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1105, <4 x i16> %17284) #11
  %17549 = sext <4 x i16> %17535 to <4 x i32>
  %17550 = mul nsw <4 x i32> %17549, %17306
  %.cast1108 = bitcast <1 x i64> %17539 to <4 x i16>
  %17551 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1108, <4 x i16> %17289) #11
  %17552 = sext <4 x i16> %17538 to <4 x i32>
  %17553 = mul nsw <4 x i32> %17552, %17310
  %.cast1111 = bitcast <1 x i64> %17542 to <4 x i16>
  %17554 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1111, <4 x i16> %17294) #11
  %17555 = sext <4 x i16> %17541 to <4 x i32>
  %17556 = mul nsw <4 x i32> %17555, %17314
  %.cast1114 = bitcast <1 x i64> %17545 to <4 x i16>
  %17557 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1114, <4 x i16> %17299) #11
  %17558 = sext <4 x i16> %17544 to <4 x i32>
  %17559 = mul nsw <4 x i32> %17558, %17318
  %17560 = sext <4 x i16> %17547 to <4 x i32>
  %17561 = mul nsw <4 x i32> %17560, %17320
  %17562 = add <4 x i32> %17550, %16987
  %17563 = add <4 x i32> %17562, %17553
  %17564 = add <4 x i32> %17563, %17556
  %17565 = add <4 x i32> %17564, %17559
  %17566 = add <4 x i32> %17565, %17561
  %17567 = add <4 x i32> %17566, %17548
  %17568 = add <4 x i32> %17567, %17551
  %17569 = add <4 x i32> %17568, %17554
  %17570 = add <4 x i32> %17569, %17557
  %17571 = add nsw i32 %17037, 2
  %t4053 = mul nsw i32 %17571, %stride_x
  %t4054 = add nsw i32 %t4053, %t2587816
  %17572 = sext i32 %t4054 to i64
  %17573 = shl nsw i64 %17572, 4
  %17574 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17573
  %17575 = bitcast i8* %17574 to <8 x i8>*
  %t4055 = load <8 x i8>, <8 x i8>* %17575, align 16, !tbaa !438
  %t4056 = add nsw i32 %t4053, %t2583815
  %17576 = sext i32 %t4056 to i64
  %17577 = shl nsw i64 %17576, 4
  %17578 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17577
  %17579 = bitcast i8* %17578 to <8 x i8>*
  %t4057 = load <8 x i8>, <8 x i8>* %17579, align 16, !tbaa !438
  %t4058 = add nsw i32 %t4053, %t2579798
  %17580 = sext i32 %t4058 to i64
  %17581 = shl nsw i64 %17580, 4
  %17582 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17581
  %17583 = bitcast i8* %17582 to <8 x i8>*
  %t4059 = load <8 x i8>, <8 x i8>* %17583, align 16, !tbaa !438
  %t4060 = add nsw i32 %t4053, %t2574814
  %17584 = sext i32 %t4060 to i64
  %17585 = shl nsw i64 %17584, 4
  %17586 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17585
  %17587 = bitcast i8* %17586 to <8 x i8>*
  %t4061 = load <8 x i8>, <8 x i8>* %17587, align 16, !tbaa !438
  %t4062 = add nsw i32 %t4053, %t2570813
  %17588 = sext i32 %t4062 to i64
  %17589 = shl nsw i64 %17588, 4
  %17590 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17589
  %17591 = bitcast i8* %17590 to <8 x i8>*
  %t4063 = load <8 x i8>, <8 x i8>* %17591, align 16, !tbaa !438
  %t4064 = add nsw i32 %t4053, %t2566797
  %17592 = sext i32 %t4064 to i64
  %17593 = shl nsw i64 %17592, 4
  %17594 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17593
  %17595 = bitcast i8* %17594 to <8 x i8>*
  %t4065 = load <8 x i8>, <8 x i8>* %17595, align 16, !tbaa !438
  %t4066 = add nsw i32 %t4053, %t2561806
  %17596 = sext i32 %t4066 to i64
  %17597 = shl nsw i64 %17596, 4
  %17598 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17597
  %17599 = bitcast i8* %17598 to <8 x i8>*
  %t4067 = load <8 x i8>, <8 x i8>* %17599, align 16, !tbaa !438
  %t4068 = add nsw i32 %t4053, %t2557805
  %17600 = sext i32 %t4068 to i64
  %17601 = shl nsw i64 %17600, 4
  %17602 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17601
  %17603 = bitcast i8* %17602 to <8 x i8>*
  %t4069 = load <8 x i8>, <8 x i8>* %17603, align 16, !tbaa !438
  %t4070 = add nsw i32 %t4053, %t2553793
  %17604 = sext i32 %t4070 to i64
  %17605 = shl nsw i64 %17604, 4
  %17606 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17605
  %17607 = bitcast i8* %17606 to <8 x i8>*
  %t4071 = load <8 x i8>, <8 x i8>* %17607, align 16, !tbaa !438
  %17608 = getelementptr inbounds i8, i8* %17574, i64 8
  %17609 = bitcast i8* %17608 to <8 x i8>*
  %t4072 = load <8 x i8>, <8 x i8>* %17609, align 8, !tbaa !438
  %17610 = getelementptr inbounds i8, i8* %17578, i64 8
  %17611 = bitcast i8* %17610 to <8 x i8>*
  %t4073 = load <8 x i8>, <8 x i8>* %17611, align 8, !tbaa !438
  %17612 = getelementptr inbounds i8, i8* %17582, i64 8
  %17613 = bitcast i8* %17612 to <8 x i8>*
  %t4074 = load <8 x i8>, <8 x i8>* %17613, align 8, !tbaa !438
  %17614 = getelementptr inbounds i8, i8* %17586, i64 8
  %17615 = bitcast i8* %17614 to <8 x i8>*
  %t4075 = load <8 x i8>, <8 x i8>* %17615, align 8, !tbaa !438
  %17616 = getelementptr inbounds i8, i8* %17590, i64 8
  %17617 = bitcast i8* %17616 to <8 x i8>*
  %t4076 = load <8 x i8>, <8 x i8>* %17617, align 8, !tbaa !438
  %17618 = getelementptr inbounds i8, i8* %17594, i64 8
  %17619 = bitcast i8* %17618 to <8 x i8>*
  %t4077 = load <8 x i8>, <8 x i8>* %17619, align 8, !tbaa !438
  %17620 = getelementptr inbounds i8, i8* %17598, i64 8
  %17621 = bitcast i8* %17620 to <8 x i8>*
  %t4078 = load <8 x i8>, <8 x i8>* %17621, align 8, !tbaa !438
  %17622 = getelementptr inbounds i8, i8* %17602, i64 8
  %17623 = bitcast i8* %17622 to <8 x i8>*
  %t4079 = load <8 x i8>, <8 x i8>* %17623, align 8, !tbaa !438
  %17624 = getelementptr inbounds i8, i8* %17606, i64 8
  %17625 = bitcast i8* %17624 to <8 x i8>*
  %t4080 = load <8 x i8>, <8 x i8>* %17625, align 8, !tbaa !438
  %17626 = zext <8 x i8> %t4055 to <8 x i16>
  %17627 = bitcast <8 x i16> %17626 to <2 x i64>
  %17628 = shufflevector <2 x i64> %17627, <2 x i64> undef, <1 x i32> zeroinitializer
  %17629 = bitcast <1 x i64> %17628 to <4 x i16>
  %17630 = zext <8 x i8> %t4057 to <8 x i16>
  %17631 = bitcast <8 x i16> %17630 to <2 x i64>
  %17632 = shufflevector <2 x i64> %17631, <2 x i64> undef, <1 x i32> zeroinitializer
  %17633 = zext <8 x i8> %t4059 to <8 x i16>
  %17634 = bitcast <8 x i16> %17633 to <2 x i64>
  %17635 = shufflevector <2 x i64> %17634, <2 x i64> undef, <1 x i32> zeroinitializer
  %17636 = bitcast <1 x i64> %17635 to <4 x i16>
  %17637 = zext <8 x i8> %t4061 to <8 x i16>
  %17638 = bitcast <8 x i16> %17637 to <2 x i64>
  %17639 = shufflevector <2 x i64> %17638, <2 x i64> undef, <1 x i32> zeroinitializer
  %17640 = zext <8 x i8> %t4063 to <8 x i16>
  %17641 = bitcast <8 x i16> %17640 to <2 x i64>
  %17642 = shufflevector <2 x i64> %17641, <2 x i64> undef, <1 x i32> zeroinitializer
  %17643 = bitcast <1 x i64> %17642 to <4 x i16>
  %17644 = zext <8 x i8> %t4065 to <8 x i16>
  %17645 = bitcast <8 x i16> %17644 to <2 x i64>
  %17646 = shufflevector <2 x i64> %17645, <2 x i64> undef, <1 x i32> zeroinitializer
  %17647 = zext <8 x i8> %t4067 to <8 x i16>
  %17648 = bitcast <8 x i16> %17647 to <2 x i64>
  %17649 = shufflevector <2 x i64> %17648, <2 x i64> undef, <1 x i32> zeroinitializer
  %17650 = bitcast <1 x i64> %17649 to <4 x i16>
  %17651 = zext <8 x i8> %t4069 to <8 x i16>
  %17652 = bitcast <8 x i16> %17651 to <2 x i64>
  %17653 = shufflevector <2 x i64> %17652, <2 x i64> undef, <1 x i32> zeroinitializer
  %17654 = zext <8 x i8> %t4071 to <8 x i16>
  %17655 = bitcast <8 x i16> %17654 to <2 x i64>
  %17656 = shufflevector <2 x i64> %17655, <2 x i64> undef, <1 x i32> zeroinitializer
  %17657 = bitcast <1 x i64> %17656 to <4 x i16>
  %.cast1117 = bitcast <1 x i64> %17632 to <4 x i16>
  %17658 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1117, <4 x i16> %17097) #11
  %17659 = sext <4 x i16> %17629 to <4 x i32>
  %17660 = mul nsw <4 x i32> %17659, %17135
  %.cast1120 = bitcast <1 x i64> %17639 to <4 x i16>
  %17661 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1120, <4 x i16> %17106) #11
  %17662 = sext <4 x i16> %17636 to <4 x i32>
  %17663 = mul nsw <4 x i32> %17662, %17139
  %.cast1123 = bitcast <1 x i64> %17646 to <4 x i16>
  %17664 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1123, <4 x i16> %17115) #11
  %17665 = sext <4 x i16> %17643 to <4 x i32>
  %17666 = mul nsw <4 x i32> %17665, %17143
  %.cast1126 = bitcast <1 x i64> %17653 to <4 x i16>
  %17667 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1126, <4 x i16> %17124) #11
  %17668 = sext <4 x i16> %17650 to <4 x i32>
  %17669 = mul nsw <4 x i32> %17668, %17147
  %17670 = sext <4 x i16> %17657 to <4 x i32>
  %17671 = mul nsw <4 x i32> %17670, %17149
  %17672 = add <4 x i32> %17660, %16984
  %17673 = add <4 x i32> %17672, %17663
  %17674 = add <4 x i32> %17673, %17666
  %17675 = add <4 x i32> %17674, %17669
  %17676 = add <4 x i32> %17675, %17671
  %17677 = add <4 x i32> %17676, %17658
  %17678 = add <4 x i32> %17677, %17661
  %17679 = add <4 x i32> %17678, %17664
  %17680 = add <4 x i32> %17679, %17667
  %17681 = shufflevector <2 x i64> %17627, <2 x i64> undef, <1 x i32> <i32 1>
  %17682 = bitcast <1 x i64> %17681 to <4 x i16>
  %17683 = shufflevector <2 x i64> %17631, <2 x i64> undef, <1 x i32> <i32 1>
  %17684 = shufflevector <2 x i64> %17634, <2 x i64> undef, <1 x i32> <i32 1>
  %17685 = bitcast <1 x i64> %17684 to <4 x i16>
  %17686 = shufflevector <2 x i64> %17638, <2 x i64> undef, <1 x i32> <i32 1>
  %17687 = shufflevector <2 x i64> %17641, <2 x i64> undef, <1 x i32> <i32 1>
  %17688 = bitcast <1 x i64> %17687 to <4 x i16>
  %17689 = shufflevector <2 x i64> %17645, <2 x i64> undef, <1 x i32> <i32 1>
  %17690 = shufflevector <2 x i64> %17648, <2 x i64> undef, <1 x i32> <i32 1>
  %17691 = bitcast <1 x i64> %17690 to <4 x i16>
  %17692 = shufflevector <2 x i64> %17652, <2 x i64> undef, <1 x i32> <i32 1>
  %17693 = shufflevector <2 x i64> %17655, <2 x i64> undef, <1 x i32> <i32 1>
  %17694 = bitcast <1 x i64> %17693 to <4 x i16>
  %.cast1129 = bitcast <1 x i64> %17683 to <4 x i16>
  %17695 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1129, <4 x i16> %17164) #11
  %17696 = sext <4 x i16> %17682 to <4 x i32>
  %17697 = mul nsw <4 x i32> %17696, %17186
  %.cast1132 = bitcast <1 x i64> %17686 to <4 x i16>
  %17698 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1132, <4 x i16> %17169) #11
  %17699 = sext <4 x i16> %17685 to <4 x i32>
  %17700 = mul nsw <4 x i32> %17699, %17190
  %.cast1135 = bitcast <1 x i64> %17689 to <4 x i16>
  %17701 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1135, <4 x i16> %17174) #11
  %17702 = sext <4 x i16> %17688 to <4 x i32>
  %17703 = mul nsw <4 x i32> %17702, %17194
  %.cast1138 = bitcast <1 x i64> %17692 to <4 x i16>
  %17704 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1138, <4 x i16> %17179) #11
  %17705 = sext <4 x i16> %17691 to <4 x i32>
  %17706 = mul nsw <4 x i32> %17705, %17198
  %17707 = sext <4 x i16> %17694 to <4 x i32>
  %17708 = mul nsw <4 x i32> %17707, %17200
  %17709 = add <4 x i32> %17697, %16985
  %17710 = add <4 x i32> %17709, %17700
  %17711 = add <4 x i32> %17710, %17703
  %17712 = add <4 x i32> %17711, %17706
  %17713 = add <4 x i32> %17712, %17708
  %17714 = add <4 x i32> %17713, %17695
  %17715 = add <4 x i32> %17714, %17698
  %17716 = add <4 x i32> %17715, %17701
  %17717 = add <4 x i32> %17716, %17704
  %17718 = zext <8 x i8> %t4072 to <8 x i16>
  %17719 = bitcast <8 x i16> %17718 to <2 x i64>
  %17720 = shufflevector <2 x i64> %17719, <2 x i64> undef, <1 x i32> zeroinitializer
  %17721 = bitcast <1 x i64> %17720 to <4 x i16>
  %17722 = zext <8 x i8> %t4073 to <8 x i16>
  %17723 = bitcast <8 x i16> %17722 to <2 x i64>
  %17724 = shufflevector <2 x i64> %17723, <2 x i64> undef, <1 x i32> zeroinitializer
  %17725 = zext <8 x i8> %t4074 to <8 x i16>
  %17726 = bitcast <8 x i16> %17725 to <2 x i64>
  %17727 = shufflevector <2 x i64> %17726, <2 x i64> undef, <1 x i32> zeroinitializer
  %17728 = bitcast <1 x i64> %17727 to <4 x i16>
  %17729 = zext <8 x i8> %t4075 to <8 x i16>
  %17730 = bitcast <8 x i16> %17729 to <2 x i64>
  %17731 = shufflevector <2 x i64> %17730, <2 x i64> undef, <1 x i32> zeroinitializer
  %17732 = zext <8 x i8> %t4076 to <8 x i16>
  %17733 = bitcast <8 x i16> %17732 to <2 x i64>
  %17734 = shufflevector <2 x i64> %17733, <2 x i64> undef, <1 x i32> zeroinitializer
  %17735 = bitcast <1 x i64> %17734 to <4 x i16>
  %17736 = zext <8 x i8> %t4077 to <8 x i16>
  %17737 = bitcast <8 x i16> %17736 to <2 x i64>
  %17738 = shufflevector <2 x i64> %17737, <2 x i64> undef, <1 x i32> zeroinitializer
  %17739 = zext <8 x i8> %t4078 to <8 x i16>
  %17740 = bitcast <8 x i16> %17739 to <2 x i64>
  %17741 = shufflevector <2 x i64> %17740, <2 x i64> undef, <1 x i32> zeroinitializer
  %17742 = bitcast <1 x i64> %17741 to <4 x i16>
  %17743 = zext <8 x i8> %t4079 to <8 x i16>
  %17744 = bitcast <8 x i16> %17743 to <2 x i64>
  %17745 = shufflevector <2 x i64> %17744, <2 x i64> undef, <1 x i32> zeroinitializer
  %17746 = zext <8 x i8> %t4080 to <8 x i16>
  %17747 = bitcast <8 x i16> %17746 to <2 x i64>
  %17748 = shufflevector <2 x i64> %17747, <2 x i64> undef, <1 x i32> zeroinitializer
  %17749 = bitcast <1 x i64> %17748 to <4 x i16>
  %.cast1141 = bitcast <1 x i64> %17724 to <4 x i16>
  %17750 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1141, <4 x i16> %17217) #11
  %17751 = sext <4 x i16> %17721 to <4 x i32>
  %17752 = mul nsw <4 x i32> %17751, %17255
  %.cast1144 = bitcast <1 x i64> %17731 to <4 x i16>
  %17753 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1144, <4 x i16> %17226) #11
  %17754 = sext <4 x i16> %17728 to <4 x i32>
  %17755 = mul nsw <4 x i32> %17754, %17259
  %.cast1147 = bitcast <1 x i64> %17738 to <4 x i16>
  %17756 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1147, <4 x i16> %17235) #11
  %17757 = sext <4 x i16> %17735 to <4 x i32>
  %17758 = mul nsw <4 x i32> %17757, %17263
  %.cast1150 = bitcast <1 x i64> %17745 to <4 x i16>
  %17759 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1150, <4 x i16> %17244) #11
  %17760 = sext <4 x i16> %17742 to <4 x i32>
  %17761 = mul nsw <4 x i32> %17760, %17267
  %17762 = sext <4 x i16> %17749 to <4 x i32>
  %17763 = mul nsw <4 x i32> %17762, %17269
  %17764 = add <4 x i32> %17752, %16986
  %17765 = add <4 x i32> %17764, %17755
  %17766 = add <4 x i32> %17765, %17758
  %17767 = add <4 x i32> %17766, %17761
  %17768 = add <4 x i32> %17767, %17763
  %17769 = add <4 x i32> %17768, %17750
  %17770 = add <4 x i32> %17769, %17753
  %17771 = add <4 x i32> %17770, %17756
  %17772 = add <4 x i32> %17771, %17759
  %17773 = shufflevector <2 x i64> %17719, <2 x i64> undef, <1 x i32> <i32 1>
  %17774 = bitcast <1 x i64> %17773 to <4 x i16>
  %17775 = shufflevector <2 x i64> %17723, <2 x i64> undef, <1 x i32> <i32 1>
  %17776 = shufflevector <2 x i64> %17726, <2 x i64> undef, <1 x i32> <i32 1>
  %17777 = bitcast <1 x i64> %17776 to <4 x i16>
  %17778 = shufflevector <2 x i64> %17730, <2 x i64> undef, <1 x i32> <i32 1>
  %17779 = shufflevector <2 x i64> %17733, <2 x i64> undef, <1 x i32> <i32 1>
  %17780 = bitcast <1 x i64> %17779 to <4 x i16>
  %17781 = shufflevector <2 x i64> %17737, <2 x i64> undef, <1 x i32> <i32 1>
  %17782 = shufflevector <2 x i64> %17740, <2 x i64> undef, <1 x i32> <i32 1>
  %17783 = bitcast <1 x i64> %17782 to <4 x i16>
  %17784 = shufflevector <2 x i64> %17744, <2 x i64> undef, <1 x i32> <i32 1>
  %17785 = shufflevector <2 x i64> %17747, <2 x i64> undef, <1 x i32> <i32 1>
  %17786 = bitcast <1 x i64> %17785 to <4 x i16>
  %.cast1153 = bitcast <1 x i64> %17775 to <4 x i16>
  %17787 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1153, <4 x i16> %17284) #11
  %17788 = sext <4 x i16> %17774 to <4 x i32>
  %17789 = mul nsw <4 x i32> %17788, %17306
  %.cast1156 = bitcast <1 x i64> %17778 to <4 x i16>
  %17790 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1156, <4 x i16> %17289) #11
  %17791 = sext <4 x i16> %17777 to <4 x i32>
  %17792 = mul nsw <4 x i32> %17791, %17310
  %.cast1159 = bitcast <1 x i64> %17781 to <4 x i16>
  %17793 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1159, <4 x i16> %17294) #11
  %17794 = sext <4 x i16> %17780 to <4 x i32>
  %17795 = mul nsw <4 x i32> %17794, %17314
  %.cast1162 = bitcast <1 x i64> %17784 to <4 x i16>
  %17796 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1162, <4 x i16> %17299) #11
  %17797 = sext <4 x i16> %17783 to <4 x i32>
  %17798 = mul nsw <4 x i32> %17797, %17318
  %17799 = sext <4 x i16> %17786 to <4 x i32>
  %17800 = mul nsw <4 x i32> %17799, %17320
  %17801 = add <4 x i32> %17789, %16987
  %17802 = add <4 x i32> %17801, %17792
  %17803 = add <4 x i32> %17802, %17795
  %17804 = add <4 x i32> %17803, %17798
  %17805 = add <4 x i32> %17804, %17800
  %17806 = add <4 x i32> %17805, %17787
  %17807 = add <4 x i32> %17806, %17790
  %17808 = add <4 x i32> %17807, %17793
  %17809 = add <4 x i32> %17808, %17796
  %17810 = add nsw i32 %17037, 3
  %t4081 = mul nsw i32 %17810, %stride_x
  %t4082 = add nsw i32 %t4081, %t2587816
  %17811 = sext i32 %t4082 to i64
  %17812 = shl nsw i64 %17811, 4
  %17813 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17812
  %17814 = bitcast i8* %17813 to <8 x i8>*
  %t4083 = load <8 x i8>, <8 x i8>* %17814, align 16, !tbaa !438
  %t4084 = add nsw i32 %t4081, %t2583815
  %17815 = sext i32 %t4084 to i64
  %17816 = shl nsw i64 %17815, 4
  %17817 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17816
  %17818 = bitcast i8* %17817 to <8 x i8>*
  %t4085 = load <8 x i8>, <8 x i8>* %17818, align 16, !tbaa !438
  %t4086 = add nsw i32 %t4081, %t2579798
  %17819 = sext i32 %t4086 to i64
  %17820 = shl nsw i64 %17819, 4
  %17821 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17820
  %17822 = bitcast i8* %17821 to <8 x i8>*
  %t4087 = load <8 x i8>, <8 x i8>* %17822, align 16, !tbaa !438
  %t4088 = add nsw i32 %t4081, %t2574814
  %17823 = sext i32 %t4088 to i64
  %17824 = shl nsw i64 %17823, 4
  %17825 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17824
  %17826 = bitcast i8* %17825 to <8 x i8>*
  %t4089 = load <8 x i8>, <8 x i8>* %17826, align 16, !tbaa !438
  %t4090 = add nsw i32 %t4081, %t2570813
  %17827 = sext i32 %t4090 to i64
  %17828 = shl nsw i64 %17827, 4
  %17829 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17828
  %17830 = bitcast i8* %17829 to <8 x i8>*
  %t4091 = load <8 x i8>, <8 x i8>* %17830, align 16, !tbaa !438
  %t4092 = add nsw i32 %t4081, %t2566797
  %17831 = sext i32 %t4092 to i64
  %17832 = shl nsw i64 %17831, 4
  %17833 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17832
  %17834 = bitcast i8* %17833 to <8 x i8>*
  %t4093 = load <8 x i8>, <8 x i8>* %17834, align 16, !tbaa !438
  %t4094 = add nsw i32 %t4081, %t2561806
  %17835 = sext i32 %t4094 to i64
  %17836 = shl nsw i64 %17835, 4
  %17837 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17836
  %17838 = bitcast i8* %17837 to <8 x i8>*
  %t4095 = load <8 x i8>, <8 x i8>* %17838, align 16, !tbaa !438
  %t4096 = add nsw i32 %t4081, %t2557805
  %17839 = sext i32 %t4096 to i64
  %17840 = shl nsw i64 %17839, 4
  %17841 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17840
  %17842 = bitcast i8* %17841 to <8 x i8>*
  %t4097 = load <8 x i8>, <8 x i8>* %17842, align 16, !tbaa !438
  %t4098 = add nsw i32 %t4081, %t2553793
  %17843 = sext i32 %t4098 to i64
  %17844 = shl nsw i64 %17843, 4
  %17845 = getelementptr inbounds i8, i8* %resampled_input681, i64 %17844
  %17846 = bitcast i8* %17845 to <8 x i8>*
  %t4099 = load <8 x i8>, <8 x i8>* %17846, align 16, !tbaa !438
  %17847 = getelementptr inbounds i8, i8* %17813, i64 8
  %17848 = bitcast i8* %17847 to <8 x i8>*
  %t4100 = load <8 x i8>, <8 x i8>* %17848, align 8, !tbaa !438
  %17849 = getelementptr inbounds i8, i8* %17817, i64 8
  %17850 = bitcast i8* %17849 to <8 x i8>*
  %t4101 = load <8 x i8>, <8 x i8>* %17850, align 8, !tbaa !438
  %17851 = getelementptr inbounds i8, i8* %17821, i64 8
  %17852 = bitcast i8* %17851 to <8 x i8>*
  %t4102 = load <8 x i8>, <8 x i8>* %17852, align 8, !tbaa !438
  %17853 = getelementptr inbounds i8, i8* %17825, i64 8
  %17854 = bitcast i8* %17853 to <8 x i8>*
  %t4103 = load <8 x i8>, <8 x i8>* %17854, align 8, !tbaa !438
  %17855 = getelementptr inbounds i8, i8* %17829, i64 8
  %17856 = bitcast i8* %17855 to <8 x i8>*
  %t4104 = load <8 x i8>, <8 x i8>* %17856, align 8, !tbaa !438
  %17857 = getelementptr inbounds i8, i8* %17833, i64 8
  %17858 = bitcast i8* %17857 to <8 x i8>*
  %t4105 = load <8 x i8>, <8 x i8>* %17858, align 8, !tbaa !438
  %17859 = getelementptr inbounds i8, i8* %17837, i64 8
  %17860 = bitcast i8* %17859 to <8 x i8>*
  %t4106 = load <8 x i8>, <8 x i8>* %17860, align 8, !tbaa !438
  %17861 = getelementptr inbounds i8, i8* %17841, i64 8
  %17862 = bitcast i8* %17861 to <8 x i8>*
  %t4107 = load <8 x i8>, <8 x i8>* %17862, align 8, !tbaa !438
  %17863 = getelementptr inbounds i8, i8* %17845, i64 8
  %17864 = bitcast i8* %17863 to <8 x i8>*
  %t4108 = load <8 x i8>, <8 x i8>* %17864, align 8, !tbaa !438
  %17865 = zext <8 x i8> %t4083 to <8 x i16>
  %17866 = bitcast <8 x i16> %17865 to <2 x i64>
  %17867 = shufflevector <2 x i64> %17866, <2 x i64> undef, <1 x i32> zeroinitializer
  %17868 = bitcast <1 x i64> %17867 to <4 x i16>
  %17869 = zext <8 x i8> %t4085 to <8 x i16>
  %17870 = bitcast <8 x i16> %17869 to <2 x i64>
  %17871 = shufflevector <2 x i64> %17870, <2 x i64> undef, <1 x i32> zeroinitializer
  %17872 = zext <8 x i8> %t4087 to <8 x i16>
  %17873 = bitcast <8 x i16> %17872 to <2 x i64>
  %17874 = shufflevector <2 x i64> %17873, <2 x i64> undef, <1 x i32> zeroinitializer
  %17875 = bitcast <1 x i64> %17874 to <4 x i16>
  %17876 = zext <8 x i8> %t4089 to <8 x i16>
  %17877 = bitcast <8 x i16> %17876 to <2 x i64>
  %17878 = shufflevector <2 x i64> %17877, <2 x i64> undef, <1 x i32> zeroinitializer
  %17879 = zext <8 x i8> %t4091 to <8 x i16>
  %17880 = bitcast <8 x i16> %17879 to <2 x i64>
  %17881 = shufflevector <2 x i64> %17880, <2 x i64> undef, <1 x i32> zeroinitializer
  %17882 = bitcast <1 x i64> %17881 to <4 x i16>
  %17883 = zext <8 x i8> %t4093 to <8 x i16>
  %17884 = bitcast <8 x i16> %17883 to <2 x i64>
  %17885 = shufflevector <2 x i64> %17884, <2 x i64> undef, <1 x i32> zeroinitializer
  %17886 = zext <8 x i8> %t4095 to <8 x i16>
  %17887 = bitcast <8 x i16> %17886 to <2 x i64>
  %17888 = shufflevector <2 x i64> %17887, <2 x i64> undef, <1 x i32> zeroinitializer
  %17889 = bitcast <1 x i64> %17888 to <4 x i16>
  %17890 = zext <8 x i8> %t4097 to <8 x i16>
  %17891 = bitcast <8 x i16> %17890 to <2 x i64>
  %17892 = shufflevector <2 x i64> %17891, <2 x i64> undef, <1 x i32> zeroinitializer
  %17893 = zext <8 x i8> %t4099 to <8 x i16>
  %17894 = bitcast <8 x i16> %17893 to <2 x i64>
  %17895 = shufflevector <2 x i64> %17894, <2 x i64> undef, <1 x i32> zeroinitializer
  %17896 = bitcast <1 x i64> %17895 to <4 x i16>
  %.cast1165 = bitcast <1 x i64> %17871 to <4 x i16>
  %17897 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1165, <4 x i16> %17097) #11
  %17898 = sext <4 x i16> %17868 to <4 x i32>
  %17899 = mul nsw <4 x i32> %17898, %17135
  %.cast1168 = bitcast <1 x i64> %17878 to <4 x i16>
  %17900 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1168, <4 x i16> %17106) #11
  %17901 = sext <4 x i16> %17875 to <4 x i32>
  %17902 = mul nsw <4 x i32> %17901, %17139
  %.cast1171 = bitcast <1 x i64> %17885 to <4 x i16>
  %17903 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1171, <4 x i16> %17115) #11
  %17904 = sext <4 x i16> %17882 to <4 x i32>
  %17905 = mul nsw <4 x i32> %17904, %17143
  %.cast1174 = bitcast <1 x i64> %17892 to <4 x i16>
  %17906 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1174, <4 x i16> %17124) #11
  %17907 = sext <4 x i16> %17889 to <4 x i32>
  %17908 = mul nsw <4 x i32> %17907, %17147
  %17909 = sext <4 x i16> %17896 to <4 x i32>
  %17910 = mul nsw <4 x i32> %17909, %17149
  %17911 = add <4 x i32> %17899, %16984
  %17912 = add <4 x i32> %17911, %17902
  %17913 = add <4 x i32> %17912, %17905
  %17914 = add <4 x i32> %17913, %17908
  %17915 = add <4 x i32> %17914, %17910
  %17916 = add <4 x i32> %17915, %17897
  %17917 = add <4 x i32> %17916, %17900
  %17918 = add <4 x i32> %17917, %17903
  %17919 = add <4 x i32> %17918, %17906
  %17920 = shufflevector <2 x i64> %17866, <2 x i64> undef, <1 x i32> <i32 1>
  %17921 = bitcast <1 x i64> %17920 to <4 x i16>
  %17922 = shufflevector <2 x i64> %17870, <2 x i64> undef, <1 x i32> <i32 1>
  %17923 = shufflevector <2 x i64> %17873, <2 x i64> undef, <1 x i32> <i32 1>
  %17924 = bitcast <1 x i64> %17923 to <4 x i16>
  %17925 = shufflevector <2 x i64> %17877, <2 x i64> undef, <1 x i32> <i32 1>
  %17926 = shufflevector <2 x i64> %17880, <2 x i64> undef, <1 x i32> <i32 1>
  %17927 = bitcast <1 x i64> %17926 to <4 x i16>
  %17928 = shufflevector <2 x i64> %17884, <2 x i64> undef, <1 x i32> <i32 1>
  %17929 = shufflevector <2 x i64> %17887, <2 x i64> undef, <1 x i32> <i32 1>
  %17930 = bitcast <1 x i64> %17929 to <4 x i16>
  %17931 = shufflevector <2 x i64> %17891, <2 x i64> undef, <1 x i32> <i32 1>
  %17932 = shufflevector <2 x i64> %17894, <2 x i64> undef, <1 x i32> <i32 1>
  %17933 = bitcast <1 x i64> %17932 to <4 x i16>
  %.cast1177 = bitcast <1 x i64> %17922 to <4 x i16>
  %17934 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1177, <4 x i16> %17164) #11
  %17935 = sext <4 x i16> %17921 to <4 x i32>
  %17936 = mul nsw <4 x i32> %17935, %17186
  %.cast1180 = bitcast <1 x i64> %17925 to <4 x i16>
  %17937 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1180, <4 x i16> %17169) #11
  %17938 = sext <4 x i16> %17924 to <4 x i32>
  %17939 = mul nsw <4 x i32> %17938, %17190
  %.cast1183 = bitcast <1 x i64> %17928 to <4 x i16>
  %17940 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1183, <4 x i16> %17174) #11
  %17941 = sext <4 x i16> %17927 to <4 x i32>
  %17942 = mul nsw <4 x i32> %17941, %17194
  %.cast1186 = bitcast <1 x i64> %17931 to <4 x i16>
  %17943 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1186, <4 x i16> %17179) #11
  %17944 = sext <4 x i16> %17930 to <4 x i32>
  %17945 = mul nsw <4 x i32> %17944, %17198
  %17946 = sext <4 x i16> %17933 to <4 x i32>
  %17947 = mul nsw <4 x i32> %17946, %17200
  %17948 = add <4 x i32> %17936, %16985
  %17949 = add <4 x i32> %17948, %17939
  %17950 = add <4 x i32> %17949, %17942
  %17951 = add <4 x i32> %17950, %17945
  %17952 = add <4 x i32> %17951, %17947
  %17953 = add <4 x i32> %17952, %17934
  %17954 = add <4 x i32> %17953, %17937
  %17955 = add <4 x i32> %17954, %17940
  %17956 = add <4 x i32> %17955, %17943
  %17957 = zext <8 x i8> %t4100 to <8 x i16>
  %17958 = bitcast <8 x i16> %17957 to <2 x i64>
  %17959 = shufflevector <2 x i64> %17958, <2 x i64> undef, <1 x i32> zeroinitializer
  %17960 = bitcast <1 x i64> %17959 to <4 x i16>
  %17961 = zext <8 x i8> %t4101 to <8 x i16>
  %17962 = bitcast <8 x i16> %17961 to <2 x i64>
  %17963 = shufflevector <2 x i64> %17962, <2 x i64> undef, <1 x i32> zeroinitializer
  %17964 = zext <8 x i8> %t4102 to <8 x i16>
  %17965 = bitcast <8 x i16> %17964 to <2 x i64>
  %17966 = shufflevector <2 x i64> %17965, <2 x i64> undef, <1 x i32> zeroinitializer
  %17967 = bitcast <1 x i64> %17966 to <4 x i16>
  %17968 = zext <8 x i8> %t4103 to <8 x i16>
  %17969 = bitcast <8 x i16> %17968 to <2 x i64>
  %17970 = shufflevector <2 x i64> %17969, <2 x i64> undef, <1 x i32> zeroinitializer
  %17971 = zext <8 x i8> %t4104 to <8 x i16>
  %17972 = bitcast <8 x i16> %17971 to <2 x i64>
  %17973 = shufflevector <2 x i64> %17972, <2 x i64> undef, <1 x i32> zeroinitializer
  %17974 = bitcast <1 x i64> %17973 to <4 x i16>
  %17975 = zext <8 x i8> %t4105 to <8 x i16>
  %17976 = bitcast <8 x i16> %17975 to <2 x i64>
  %17977 = shufflevector <2 x i64> %17976, <2 x i64> undef, <1 x i32> zeroinitializer
  %17978 = zext <8 x i8> %t4106 to <8 x i16>
  %17979 = bitcast <8 x i16> %17978 to <2 x i64>
  %17980 = shufflevector <2 x i64> %17979, <2 x i64> undef, <1 x i32> zeroinitializer
  %17981 = bitcast <1 x i64> %17980 to <4 x i16>
  %17982 = zext <8 x i8> %t4107 to <8 x i16>
  %17983 = bitcast <8 x i16> %17982 to <2 x i64>
  %17984 = shufflevector <2 x i64> %17983, <2 x i64> undef, <1 x i32> zeroinitializer
  %17985 = zext <8 x i8> %t4108 to <8 x i16>
  %17986 = bitcast <8 x i16> %17985 to <2 x i64>
  %17987 = shufflevector <2 x i64> %17986, <2 x i64> undef, <1 x i32> zeroinitializer
  %17988 = bitcast <1 x i64> %17987 to <4 x i16>
  %.cast1189 = bitcast <1 x i64> %17963 to <4 x i16>
  %17989 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1189, <4 x i16> %17217) #11
  %17990 = sext <4 x i16> %17960 to <4 x i32>
  %17991 = mul nsw <4 x i32> %17990, %17255
  %.cast1192 = bitcast <1 x i64> %17970 to <4 x i16>
  %17992 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1192, <4 x i16> %17226) #11
  %17993 = sext <4 x i16> %17967 to <4 x i32>
  %17994 = mul nsw <4 x i32> %17993, %17259
  %.cast1195 = bitcast <1 x i64> %17977 to <4 x i16>
  %17995 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1195, <4 x i16> %17235) #11
  %17996 = sext <4 x i16> %17974 to <4 x i32>
  %17997 = mul nsw <4 x i32> %17996, %17263
  %.cast1198 = bitcast <1 x i64> %17984 to <4 x i16>
  %17998 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1198, <4 x i16> %17244) #11
  %17999 = sext <4 x i16> %17981 to <4 x i32>
  %18000 = mul nsw <4 x i32> %17999, %17267
  %18001 = sext <4 x i16> %17988 to <4 x i32>
  %18002 = mul nsw <4 x i32> %18001, %17269
  %18003 = add <4 x i32> %17991, %16986
  %18004 = add <4 x i32> %18003, %17994
  %18005 = add <4 x i32> %18004, %17997
  %18006 = add <4 x i32> %18005, %18000
  %18007 = add <4 x i32> %18006, %18002
  %18008 = add <4 x i32> %18007, %17989
  %18009 = add <4 x i32> %18008, %17992
  %18010 = add <4 x i32> %18009, %17995
  %18011 = add <4 x i32> %18010, %17998
  %18012 = shufflevector <2 x i64> %17958, <2 x i64> undef, <1 x i32> <i32 1>
  %18013 = bitcast <1 x i64> %18012 to <4 x i16>
  %18014 = shufflevector <2 x i64> %17962, <2 x i64> undef, <1 x i32> <i32 1>
  %18015 = shufflevector <2 x i64> %17965, <2 x i64> undef, <1 x i32> <i32 1>
  %18016 = bitcast <1 x i64> %18015 to <4 x i16>
  %18017 = shufflevector <2 x i64> %17969, <2 x i64> undef, <1 x i32> <i32 1>
  %18018 = shufflevector <2 x i64> %17972, <2 x i64> undef, <1 x i32> <i32 1>
  %18019 = bitcast <1 x i64> %18018 to <4 x i16>
  %18020 = shufflevector <2 x i64> %17976, <2 x i64> undef, <1 x i32> <i32 1>
  %18021 = shufflevector <2 x i64> %17979, <2 x i64> undef, <1 x i32> <i32 1>
  %18022 = bitcast <1 x i64> %18021 to <4 x i16>
  %18023 = shufflevector <2 x i64> %17983, <2 x i64> undef, <1 x i32> <i32 1>
  %18024 = shufflevector <2 x i64> %17986, <2 x i64> undef, <1 x i32> <i32 1>
  %18025 = bitcast <1 x i64> %18024 to <4 x i16>
  %.cast1201 = bitcast <1 x i64> %18014 to <4 x i16>
  %18026 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1201, <4 x i16> %17284) #11
  %18027 = sext <4 x i16> %18013 to <4 x i32>
  %18028 = mul nsw <4 x i32> %18027, %17306
  %.cast1204 = bitcast <1 x i64> %18017 to <4 x i16>
  %18029 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1204, <4 x i16> %17289) #11
  %18030 = sext <4 x i16> %18016 to <4 x i32>
  %18031 = mul nsw <4 x i32> %18030, %17310
  %.cast1207 = bitcast <1 x i64> %18020 to <4 x i16>
  %18032 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1207, <4 x i16> %17294) #11
  %18033 = sext <4 x i16> %18019 to <4 x i32>
  %18034 = mul nsw <4 x i32> %18033, %17314
  %.cast1210 = bitcast <1 x i64> %18023 to <4 x i16>
  %18035 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1210, <4 x i16> %17299) #11
  %18036 = sext <4 x i16> %18022 to <4 x i32>
  %18037 = mul nsw <4 x i32> %18036, %17318
  %18038 = sext <4 x i16> %18025 to <4 x i32>
  %18039 = mul nsw <4 x i32> %18038, %17320
  %18040 = add <4 x i32> %18028, %16987
  %18041 = add <4 x i32> %18040, %18031
  %18042 = add <4 x i32> %18041, %18034
  %18043 = add <4 x i32> %18042, %18037
  %18044 = add <4 x i32> %18043, %18039
  %18045 = add <4 x i32> %18044, %18026
  %18046 = add <4 x i32> %18045, %18029
  %18047 = add <4 x i32> %18046, %18032
  %18048 = add <4 x i32> %18047, %18035
  %t4110 = add nsw i32 %t3997, %t2588828
  %18049 = sext i32 %t4110 to i64
  %18050 = shl nsw i64 %18049, 4
  %18051 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18050
  %18052 = bitcast i8* %18051 to <8 x i8>*
  %t4111 = load <8 x i8>, <8 x i8>* %18052, align 16, !tbaa !438
  %t4112 = add nsw i32 %t3997, %t2584827
  %18053 = sext i32 %t4112 to i64
  %18054 = shl nsw i64 %18053, 4
  %18055 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18054
  %18056 = bitcast i8* %18055 to <8 x i8>*
  %t4113 = load <8 x i8>, <8 x i8>* %18056, align 16, !tbaa !438
  %t4114 = add nsw i32 %t3997, %t2580804
  %18057 = sext i32 %t4114 to i64
  %18058 = shl nsw i64 %18057, 4
  %18059 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18058
  %18060 = bitcast i8* %18059 to <8 x i8>*
  %t4115 = load <8 x i8>, <8 x i8>* %18060, align 16, !tbaa !438
  %t4116 = add nsw i32 %t3997, %t2575826
  %18061 = sext i32 %t4116 to i64
  %18062 = shl nsw i64 %18061, 4
  %18063 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18062
  %18064 = bitcast i8* %18063 to <8 x i8>*
  %t4117 = load <8 x i8>, <8 x i8>* %18064, align 16, !tbaa !438
  %t4118 = add nsw i32 %t3997, %t2571825
  %18065 = sext i32 %t4118 to i64
  %18066 = shl nsw i64 %18065, 4
  %18067 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18066
  %18068 = bitcast i8* %18067 to <8 x i8>*
  %t4119 = load <8 x i8>, <8 x i8>* %18068, align 16, !tbaa !438
  %t4120 = add nsw i32 %t3997, %t2567803
  %18069 = sext i32 %t4120 to i64
  %18070 = shl nsw i64 %18069, 4
  %18071 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18070
  %18072 = bitcast i8* %18071 to <8 x i8>*
  %t4121 = load <8 x i8>, <8 x i8>* %18072, align 16, !tbaa !438
  %t4122 = add nsw i32 %t3997, %t2562812
  %18073 = sext i32 %t4122 to i64
  %18074 = shl nsw i64 %18073, 4
  %18075 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18074
  %18076 = bitcast i8* %18075 to <8 x i8>*
  %t4123 = load <8 x i8>, <8 x i8>* %18076, align 16, !tbaa !438
  %t4124 = add nsw i32 %t3997, %t2558811
  %18077 = sext i32 %t4124 to i64
  %18078 = shl nsw i64 %18077, 4
  %18079 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18078
  %18080 = bitcast i8* %18079 to <8 x i8>*
  %t4125 = load <8 x i8>, <8 x i8>* %18080, align 16, !tbaa !438
  %t4126 = add nsw i32 %t3997, %t2554796
  %18081 = sext i32 %t4126 to i64
  %18082 = shl nsw i64 %18081, 4
  %18083 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18082
  %18084 = bitcast i8* %18083 to <8 x i8>*
  %t4127 = load <8 x i8>, <8 x i8>* %18084, align 16, !tbaa !438
  %18085 = getelementptr inbounds i8, i8* %18051, i64 8
  %18086 = bitcast i8* %18085 to <8 x i8>*
  %t4128 = load <8 x i8>, <8 x i8>* %18086, align 8, !tbaa !438
  %18087 = getelementptr inbounds i8, i8* %18055, i64 8
  %18088 = bitcast i8* %18087 to <8 x i8>*
  %t4129 = load <8 x i8>, <8 x i8>* %18088, align 8, !tbaa !438
  %18089 = getelementptr inbounds i8, i8* %18059, i64 8
  %18090 = bitcast i8* %18089 to <8 x i8>*
  %t4130 = load <8 x i8>, <8 x i8>* %18090, align 8, !tbaa !438
  %18091 = getelementptr inbounds i8, i8* %18063, i64 8
  %18092 = bitcast i8* %18091 to <8 x i8>*
  %t4131 = load <8 x i8>, <8 x i8>* %18092, align 8, !tbaa !438
  %18093 = getelementptr inbounds i8, i8* %18067, i64 8
  %18094 = bitcast i8* %18093 to <8 x i8>*
  %t4132 = load <8 x i8>, <8 x i8>* %18094, align 8, !tbaa !438
  %18095 = getelementptr inbounds i8, i8* %18071, i64 8
  %18096 = bitcast i8* %18095 to <8 x i8>*
  %t4133 = load <8 x i8>, <8 x i8>* %18096, align 8, !tbaa !438
  %18097 = getelementptr inbounds i8, i8* %18075, i64 8
  %18098 = bitcast i8* %18097 to <8 x i8>*
  %t4134 = load <8 x i8>, <8 x i8>* %18098, align 8, !tbaa !438
  %18099 = getelementptr inbounds i8, i8* %18079, i64 8
  %18100 = bitcast i8* %18099 to <8 x i8>*
  %t4135 = load <8 x i8>, <8 x i8>* %18100, align 8, !tbaa !438
  %18101 = getelementptr inbounds i8, i8* %18083, i64 8
  %18102 = bitcast i8* %18101 to <8 x i8>*
  %t4136 = load <8 x i8>, <8 x i8>* %18102, align 8, !tbaa !438
  %18103 = zext <8 x i8> %t4111 to <8 x i16>
  %18104 = bitcast <8 x i16> %18103 to <2 x i64>
  %18105 = shufflevector <2 x i64> %18104, <2 x i64> undef, <1 x i32> zeroinitializer
  %18106 = bitcast <1 x i64> %18105 to <4 x i16>
  %18107 = zext <8 x i8> %t4113 to <8 x i16>
  %18108 = bitcast <8 x i16> %18107 to <2 x i64>
  %18109 = shufflevector <2 x i64> %18108, <2 x i64> undef, <1 x i32> zeroinitializer
  %18110 = zext <8 x i8> %t4115 to <8 x i16>
  %18111 = bitcast <8 x i16> %18110 to <2 x i64>
  %18112 = shufflevector <2 x i64> %18111, <2 x i64> undef, <1 x i32> zeroinitializer
  %18113 = bitcast <1 x i64> %18112 to <4 x i16>
  %18114 = zext <8 x i8> %t4117 to <8 x i16>
  %18115 = bitcast <8 x i16> %18114 to <2 x i64>
  %18116 = shufflevector <2 x i64> %18115, <2 x i64> undef, <1 x i32> zeroinitializer
  %18117 = zext <8 x i8> %t4119 to <8 x i16>
  %18118 = bitcast <8 x i16> %18117 to <2 x i64>
  %18119 = shufflevector <2 x i64> %18118, <2 x i64> undef, <1 x i32> zeroinitializer
  %18120 = bitcast <1 x i64> %18119 to <4 x i16>
  %18121 = zext <8 x i8> %t4121 to <8 x i16>
  %18122 = bitcast <8 x i16> %18121 to <2 x i64>
  %18123 = shufflevector <2 x i64> %18122, <2 x i64> undef, <1 x i32> zeroinitializer
  %18124 = zext <8 x i8> %t4123 to <8 x i16>
  %18125 = bitcast <8 x i16> %18124 to <2 x i64>
  %18126 = shufflevector <2 x i64> %18125, <2 x i64> undef, <1 x i32> zeroinitializer
  %18127 = bitcast <1 x i64> %18126 to <4 x i16>
  %18128 = zext <8 x i8> %t4125 to <8 x i16>
  %18129 = bitcast <8 x i16> %18128 to <2 x i64>
  %18130 = shufflevector <2 x i64> %18129, <2 x i64> undef, <1 x i32> zeroinitializer
  %18131 = zext <8 x i8> %t4127 to <8 x i16>
  %18132 = bitcast <8 x i16> %18131 to <2 x i64>
  %18133 = shufflevector <2 x i64> %18132, <2 x i64> undef, <1 x i32> zeroinitializer
  %18134 = bitcast <1 x i64> %18133 to <4 x i16>
  %.cast1213 = bitcast <1 x i64> %18109 to <4 x i16>
  %18135 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1213, <4 x i16> %17097) #11
  %18136 = sext <4 x i16> %18106 to <4 x i32>
  %18137 = mul nsw <4 x i32> %18136, %17135
  %.cast1216 = bitcast <1 x i64> %18116 to <4 x i16>
  %18138 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1216, <4 x i16> %17106) #11
  %18139 = sext <4 x i16> %18113 to <4 x i32>
  %18140 = mul nsw <4 x i32> %18139, %17139
  %.cast1219 = bitcast <1 x i64> %18123 to <4 x i16>
  %18141 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1219, <4 x i16> %17115) #11
  %18142 = sext <4 x i16> %18120 to <4 x i32>
  %18143 = mul nsw <4 x i32> %18142, %17143
  %.cast1222 = bitcast <1 x i64> %18130 to <4 x i16>
  %18144 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1222, <4 x i16> %17124) #11
  %18145 = sext <4 x i16> %18127 to <4 x i32>
  %18146 = mul nsw <4 x i32> %18145, %17147
  %18147 = sext <4 x i16> %18134 to <4 x i32>
  %18148 = mul nsw <4 x i32> %18147, %17149
  %18149 = add <4 x i32> %18137, %16984
  %18150 = add <4 x i32> %18149, %18140
  %18151 = add <4 x i32> %18150, %18143
  %18152 = add <4 x i32> %18151, %18146
  %18153 = add <4 x i32> %18152, %18148
  %18154 = add <4 x i32> %18153, %18135
  %18155 = add <4 x i32> %18154, %18138
  %18156 = add <4 x i32> %18155, %18141
  %18157 = add <4 x i32> %18156, %18144
  %18158 = shufflevector <2 x i64> %18104, <2 x i64> undef, <1 x i32> <i32 1>
  %18159 = bitcast <1 x i64> %18158 to <4 x i16>
  %18160 = shufflevector <2 x i64> %18108, <2 x i64> undef, <1 x i32> <i32 1>
  %18161 = shufflevector <2 x i64> %18111, <2 x i64> undef, <1 x i32> <i32 1>
  %18162 = bitcast <1 x i64> %18161 to <4 x i16>
  %18163 = shufflevector <2 x i64> %18115, <2 x i64> undef, <1 x i32> <i32 1>
  %18164 = shufflevector <2 x i64> %18118, <2 x i64> undef, <1 x i32> <i32 1>
  %18165 = bitcast <1 x i64> %18164 to <4 x i16>
  %18166 = shufflevector <2 x i64> %18122, <2 x i64> undef, <1 x i32> <i32 1>
  %18167 = shufflevector <2 x i64> %18125, <2 x i64> undef, <1 x i32> <i32 1>
  %18168 = bitcast <1 x i64> %18167 to <4 x i16>
  %18169 = shufflevector <2 x i64> %18129, <2 x i64> undef, <1 x i32> <i32 1>
  %18170 = shufflevector <2 x i64> %18132, <2 x i64> undef, <1 x i32> <i32 1>
  %18171 = bitcast <1 x i64> %18170 to <4 x i16>
  %.cast1225 = bitcast <1 x i64> %18160 to <4 x i16>
  %18172 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1225, <4 x i16> %17164) #11
  %18173 = sext <4 x i16> %18159 to <4 x i32>
  %18174 = mul nsw <4 x i32> %18173, %17186
  %.cast1228 = bitcast <1 x i64> %18163 to <4 x i16>
  %18175 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1228, <4 x i16> %17169) #11
  %18176 = sext <4 x i16> %18162 to <4 x i32>
  %18177 = mul nsw <4 x i32> %18176, %17190
  %.cast1231 = bitcast <1 x i64> %18166 to <4 x i16>
  %18178 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1231, <4 x i16> %17174) #11
  %18179 = sext <4 x i16> %18165 to <4 x i32>
  %18180 = mul nsw <4 x i32> %18179, %17194
  %.cast1234 = bitcast <1 x i64> %18169 to <4 x i16>
  %18181 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1234, <4 x i16> %17179) #11
  %18182 = sext <4 x i16> %18168 to <4 x i32>
  %18183 = mul nsw <4 x i32> %18182, %17198
  %18184 = sext <4 x i16> %18171 to <4 x i32>
  %18185 = mul nsw <4 x i32> %18184, %17200
  %18186 = add <4 x i32> %18174, %16985
  %18187 = add <4 x i32> %18186, %18177
  %18188 = add <4 x i32> %18187, %18180
  %18189 = add <4 x i32> %18188, %18183
  %18190 = add <4 x i32> %18189, %18185
  %18191 = add <4 x i32> %18190, %18172
  %18192 = add <4 x i32> %18191, %18175
  %18193 = add <4 x i32> %18192, %18178
  %18194 = add <4 x i32> %18193, %18181
  %18195 = zext <8 x i8> %t4128 to <8 x i16>
  %18196 = bitcast <8 x i16> %18195 to <2 x i64>
  %18197 = shufflevector <2 x i64> %18196, <2 x i64> undef, <1 x i32> zeroinitializer
  %18198 = bitcast <1 x i64> %18197 to <4 x i16>
  %18199 = zext <8 x i8> %t4129 to <8 x i16>
  %18200 = bitcast <8 x i16> %18199 to <2 x i64>
  %18201 = shufflevector <2 x i64> %18200, <2 x i64> undef, <1 x i32> zeroinitializer
  %18202 = zext <8 x i8> %t4130 to <8 x i16>
  %18203 = bitcast <8 x i16> %18202 to <2 x i64>
  %18204 = shufflevector <2 x i64> %18203, <2 x i64> undef, <1 x i32> zeroinitializer
  %18205 = bitcast <1 x i64> %18204 to <4 x i16>
  %18206 = zext <8 x i8> %t4131 to <8 x i16>
  %18207 = bitcast <8 x i16> %18206 to <2 x i64>
  %18208 = shufflevector <2 x i64> %18207, <2 x i64> undef, <1 x i32> zeroinitializer
  %18209 = zext <8 x i8> %t4132 to <8 x i16>
  %18210 = bitcast <8 x i16> %18209 to <2 x i64>
  %18211 = shufflevector <2 x i64> %18210, <2 x i64> undef, <1 x i32> zeroinitializer
  %18212 = bitcast <1 x i64> %18211 to <4 x i16>
  %18213 = zext <8 x i8> %t4133 to <8 x i16>
  %18214 = bitcast <8 x i16> %18213 to <2 x i64>
  %18215 = shufflevector <2 x i64> %18214, <2 x i64> undef, <1 x i32> zeroinitializer
  %18216 = zext <8 x i8> %t4134 to <8 x i16>
  %18217 = bitcast <8 x i16> %18216 to <2 x i64>
  %18218 = shufflevector <2 x i64> %18217, <2 x i64> undef, <1 x i32> zeroinitializer
  %18219 = bitcast <1 x i64> %18218 to <4 x i16>
  %18220 = zext <8 x i8> %t4135 to <8 x i16>
  %18221 = bitcast <8 x i16> %18220 to <2 x i64>
  %18222 = shufflevector <2 x i64> %18221, <2 x i64> undef, <1 x i32> zeroinitializer
  %18223 = zext <8 x i8> %t4136 to <8 x i16>
  %18224 = bitcast <8 x i16> %18223 to <2 x i64>
  %18225 = shufflevector <2 x i64> %18224, <2 x i64> undef, <1 x i32> zeroinitializer
  %18226 = bitcast <1 x i64> %18225 to <4 x i16>
  %.cast1237 = bitcast <1 x i64> %18201 to <4 x i16>
  %18227 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1237, <4 x i16> %17217) #11
  %18228 = sext <4 x i16> %18198 to <4 x i32>
  %18229 = mul nsw <4 x i32> %18228, %17255
  %.cast1240 = bitcast <1 x i64> %18208 to <4 x i16>
  %18230 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1240, <4 x i16> %17226) #11
  %18231 = sext <4 x i16> %18205 to <4 x i32>
  %18232 = mul nsw <4 x i32> %18231, %17259
  %.cast1243 = bitcast <1 x i64> %18215 to <4 x i16>
  %18233 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1243, <4 x i16> %17235) #11
  %18234 = sext <4 x i16> %18212 to <4 x i32>
  %18235 = mul nsw <4 x i32> %18234, %17263
  %.cast1246 = bitcast <1 x i64> %18222 to <4 x i16>
  %18236 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1246, <4 x i16> %17244) #11
  %18237 = sext <4 x i16> %18219 to <4 x i32>
  %18238 = mul nsw <4 x i32> %18237, %17267
  %18239 = sext <4 x i16> %18226 to <4 x i32>
  %18240 = mul nsw <4 x i32> %18239, %17269
  %18241 = add <4 x i32> %18229, %16986
  %18242 = add <4 x i32> %18241, %18232
  %18243 = add <4 x i32> %18242, %18235
  %18244 = add <4 x i32> %18243, %18238
  %18245 = add <4 x i32> %18244, %18240
  %18246 = add <4 x i32> %18245, %18227
  %18247 = add <4 x i32> %18246, %18230
  %18248 = add <4 x i32> %18247, %18233
  %18249 = add <4 x i32> %18248, %18236
  %18250 = shufflevector <2 x i64> %18196, <2 x i64> undef, <1 x i32> <i32 1>
  %18251 = bitcast <1 x i64> %18250 to <4 x i16>
  %18252 = shufflevector <2 x i64> %18200, <2 x i64> undef, <1 x i32> <i32 1>
  %18253 = shufflevector <2 x i64> %18203, <2 x i64> undef, <1 x i32> <i32 1>
  %18254 = bitcast <1 x i64> %18253 to <4 x i16>
  %18255 = shufflevector <2 x i64> %18207, <2 x i64> undef, <1 x i32> <i32 1>
  %18256 = shufflevector <2 x i64> %18210, <2 x i64> undef, <1 x i32> <i32 1>
  %18257 = bitcast <1 x i64> %18256 to <4 x i16>
  %18258 = shufflevector <2 x i64> %18214, <2 x i64> undef, <1 x i32> <i32 1>
  %18259 = shufflevector <2 x i64> %18217, <2 x i64> undef, <1 x i32> <i32 1>
  %18260 = bitcast <1 x i64> %18259 to <4 x i16>
  %18261 = shufflevector <2 x i64> %18221, <2 x i64> undef, <1 x i32> <i32 1>
  %18262 = shufflevector <2 x i64> %18224, <2 x i64> undef, <1 x i32> <i32 1>
  %18263 = bitcast <1 x i64> %18262 to <4 x i16>
  %.cast1249 = bitcast <1 x i64> %18252 to <4 x i16>
  %18264 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1249, <4 x i16> %17284) #11
  %18265 = sext <4 x i16> %18251 to <4 x i32>
  %18266 = mul nsw <4 x i32> %18265, %17306
  %.cast1252 = bitcast <1 x i64> %18255 to <4 x i16>
  %18267 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1252, <4 x i16> %17289) #11
  %18268 = sext <4 x i16> %18254 to <4 x i32>
  %18269 = mul nsw <4 x i32> %18268, %17310
  %.cast1255 = bitcast <1 x i64> %18258 to <4 x i16>
  %18270 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1255, <4 x i16> %17294) #11
  %18271 = sext <4 x i16> %18257 to <4 x i32>
  %18272 = mul nsw <4 x i32> %18271, %17314
  %.cast1258 = bitcast <1 x i64> %18261 to <4 x i16>
  %18273 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1258, <4 x i16> %17299) #11
  %18274 = sext <4 x i16> %18260 to <4 x i32>
  %18275 = mul nsw <4 x i32> %18274, %17318
  %18276 = sext <4 x i16> %18263 to <4 x i32>
  %18277 = mul nsw <4 x i32> %18276, %17320
  %18278 = add <4 x i32> %18266, %16987
  %18279 = add <4 x i32> %18278, %18269
  %18280 = add <4 x i32> %18279, %18272
  %18281 = add <4 x i32> %18280, %18275
  %18282 = add <4 x i32> %18281, %18277
  %18283 = add <4 x i32> %18282, %18264
  %18284 = add <4 x i32> %18283, %18267
  %18285 = add <4 x i32> %18284, %18270
  %18286 = add <4 x i32> %18285, %18273
  %t4138 = add nsw i32 %t4025, %t2588828
  %18287 = sext i32 %t4138 to i64
  %18288 = shl nsw i64 %18287, 4
  %18289 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18288
  %18290 = bitcast i8* %18289 to <8 x i8>*
  %t4139 = load <8 x i8>, <8 x i8>* %18290, align 16, !tbaa !438
  %t4140 = add nsw i32 %t4025, %t2584827
  %18291 = sext i32 %t4140 to i64
  %18292 = shl nsw i64 %18291, 4
  %18293 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18292
  %18294 = bitcast i8* %18293 to <8 x i8>*
  %t4141 = load <8 x i8>, <8 x i8>* %18294, align 16, !tbaa !438
  %t4142 = add nsw i32 %t4025, %t2580804
  %18295 = sext i32 %t4142 to i64
  %18296 = shl nsw i64 %18295, 4
  %18297 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18296
  %18298 = bitcast i8* %18297 to <8 x i8>*
  %t4143 = load <8 x i8>, <8 x i8>* %18298, align 16, !tbaa !438
  %t4144 = add nsw i32 %t4025, %t2575826
  %18299 = sext i32 %t4144 to i64
  %18300 = shl nsw i64 %18299, 4
  %18301 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18300
  %18302 = bitcast i8* %18301 to <8 x i8>*
  %t4145 = load <8 x i8>, <8 x i8>* %18302, align 16, !tbaa !438
  %t4146 = add nsw i32 %t4025, %t2571825
  %18303 = sext i32 %t4146 to i64
  %18304 = shl nsw i64 %18303, 4
  %18305 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18304
  %18306 = bitcast i8* %18305 to <8 x i8>*
  %t4147 = load <8 x i8>, <8 x i8>* %18306, align 16, !tbaa !438
  %t4148 = add nsw i32 %t4025, %t2567803
  %18307 = sext i32 %t4148 to i64
  %18308 = shl nsw i64 %18307, 4
  %18309 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18308
  %18310 = bitcast i8* %18309 to <8 x i8>*
  %t4149 = load <8 x i8>, <8 x i8>* %18310, align 16, !tbaa !438
  %t4150 = add nsw i32 %t4025, %t2562812
  %18311 = sext i32 %t4150 to i64
  %18312 = shl nsw i64 %18311, 4
  %18313 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18312
  %18314 = bitcast i8* %18313 to <8 x i8>*
  %t4151 = load <8 x i8>, <8 x i8>* %18314, align 16, !tbaa !438
  %t4152 = add nsw i32 %t4025, %t2558811
  %18315 = sext i32 %t4152 to i64
  %18316 = shl nsw i64 %18315, 4
  %18317 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18316
  %18318 = bitcast i8* %18317 to <8 x i8>*
  %t4153 = load <8 x i8>, <8 x i8>* %18318, align 16, !tbaa !438
  %t4154 = add nsw i32 %t4025, %t2554796
  %18319 = sext i32 %t4154 to i64
  %18320 = shl nsw i64 %18319, 4
  %18321 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18320
  %18322 = bitcast i8* %18321 to <8 x i8>*
  %t4155 = load <8 x i8>, <8 x i8>* %18322, align 16, !tbaa !438
  %18323 = getelementptr inbounds i8, i8* %18289, i64 8
  %18324 = bitcast i8* %18323 to <8 x i8>*
  %t4156 = load <8 x i8>, <8 x i8>* %18324, align 8, !tbaa !438
  %18325 = getelementptr inbounds i8, i8* %18293, i64 8
  %18326 = bitcast i8* %18325 to <8 x i8>*
  %t4157 = load <8 x i8>, <8 x i8>* %18326, align 8, !tbaa !438
  %18327 = getelementptr inbounds i8, i8* %18297, i64 8
  %18328 = bitcast i8* %18327 to <8 x i8>*
  %t4158 = load <8 x i8>, <8 x i8>* %18328, align 8, !tbaa !438
  %18329 = getelementptr inbounds i8, i8* %18301, i64 8
  %18330 = bitcast i8* %18329 to <8 x i8>*
  %t4159 = load <8 x i8>, <8 x i8>* %18330, align 8, !tbaa !438
  %18331 = getelementptr inbounds i8, i8* %18305, i64 8
  %18332 = bitcast i8* %18331 to <8 x i8>*
  %t4160 = load <8 x i8>, <8 x i8>* %18332, align 8, !tbaa !438
  %18333 = getelementptr inbounds i8, i8* %18309, i64 8
  %18334 = bitcast i8* %18333 to <8 x i8>*
  %t4161 = load <8 x i8>, <8 x i8>* %18334, align 8, !tbaa !438
  %18335 = getelementptr inbounds i8, i8* %18313, i64 8
  %18336 = bitcast i8* %18335 to <8 x i8>*
  %t4162 = load <8 x i8>, <8 x i8>* %18336, align 8, !tbaa !438
  %18337 = getelementptr inbounds i8, i8* %18317, i64 8
  %18338 = bitcast i8* %18337 to <8 x i8>*
  %t4163 = load <8 x i8>, <8 x i8>* %18338, align 8, !tbaa !438
  %18339 = getelementptr inbounds i8, i8* %18321, i64 8
  %18340 = bitcast i8* %18339 to <8 x i8>*
  %t4164 = load <8 x i8>, <8 x i8>* %18340, align 8, !tbaa !438
  %18341 = zext <8 x i8> %t4139 to <8 x i16>
  %18342 = bitcast <8 x i16> %18341 to <2 x i64>
  %18343 = shufflevector <2 x i64> %18342, <2 x i64> undef, <1 x i32> zeroinitializer
  %18344 = bitcast <1 x i64> %18343 to <4 x i16>
  %18345 = zext <8 x i8> %t4141 to <8 x i16>
  %18346 = bitcast <8 x i16> %18345 to <2 x i64>
  %18347 = shufflevector <2 x i64> %18346, <2 x i64> undef, <1 x i32> zeroinitializer
  %18348 = zext <8 x i8> %t4143 to <8 x i16>
  %18349 = bitcast <8 x i16> %18348 to <2 x i64>
  %18350 = shufflevector <2 x i64> %18349, <2 x i64> undef, <1 x i32> zeroinitializer
  %18351 = bitcast <1 x i64> %18350 to <4 x i16>
  %18352 = zext <8 x i8> %t4145 to <8 x i16>
  %18353 = bitcast <8 x i16> %18352 to <2 x i64>
  %18354 = shufflevector <2 x i64> %18353, <2 x i64> undef, <1 x i32> zeroinitializer
  %18355 = zext <8 x i8> %t4147 to <8 x i16>
  %18356 = bitcast <8 x i16> %18355 to <2 x i64>
  %18357 = shufflevector <2 x i64> %18356, <2 x i64> undef, <1 x i32> zeroinitializer
  %18358 = bitcast <1 x i64> %18357 to <4 x i16>
  %18359 = zext <8 x i8> %t4149 to <8 x i16>
  %18360 = bitcast <8 x i16> %18359 to <2 x i64>
  %18361 = shufflevector <2 x i64> %18360, <2 x i64> undef, <1 x i32> zeroinitializer
  %18362 = zext <8 x i8> %t4151 to <8 x i16>
  %18363 = bitcast <8 x i16> %18362 to <2 x i64>
  %18364 = shufflevector <2 x i64> %18363, <2 x i64> undef, <1 x i32> zeroinitializer
  %18365 = bitcast <1 x i64> %18364 to <4 x i16>
  %18366 = zext <8 x i8> %t4153 to <8 x i16>
  %18367 = bitcast <8 x i16> %18366 to <2 x i64>
  %18368 = shufflevector <2 x i64> %18367, <2 x i64> undef, <1 x i32> zeroinitializer
  %18369 = zext <8 x i8> %t4155 to <8 x i16>
  %18370 = bitcast <8 x i16> %18369 to <2 x i64>
  %18371 = shufflevector <2 x i64> %18370, <2 x i64> undef, <1 x i32> zeroinitializer
  %18372 = bitcast <1 x i64> %18371 to <4 x i16>
  %.cast1261 = bitcast <1 x i64> %18347 to <4 x i16>
  %18373 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1261, <4 x i16> %17097) #11
  %18374 = sext <4 x i16> %18344 to <4 x i32>
  %18375 = mul nsw <4 x i32> %18374, %17135
  %.cast1264 = bitcast <1 x i64> %18354 to <4 x i16>
  %18376 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1264, <4 x i16> %17106) #11
  %18377 = sext <4 x i16> %18351 to <4 x i32>
  %18378 = mul nsw <4 x i32> %18377, %17139
  %.cast1267 = bitcast <1 x i64> %18361 to <4 x i16>
  %18379 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1267, <4 x i16> %17115) #11
  %18380 = sext <4 x i16> %18358 to <4 x i32>
  %18381 = mul nsw <4 x i32> %18380, %17143
  %.cast1270 = bitcast <1 x i64> %18368 to <4 x i16>
  %18382 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1270, <4 x i16> %17124) #11
  %18383 = sext <4 x i16> %18365 to <4 x i32>
  %18384 = mul nsw <4 x i32> %18383, %17147
  %18385 = sext <4 x i16> %18372 to <4 x i32>
  %18386 = mul nsw <4 x i32> %18385, %17149
  %18387 = add <4 x i32> %18375, %16984
  %18388 = add <4 x i32> %18387, %18378
  %18389 = add <4 x i32> %18388, %18381
  %18390 = add <4 x i32> %18389, %18384
  %18391 = add <4 x i32> %18390, %18386
  %18392 = add <4 x i32> %18391, %18373
  %18393 = add <4 x i32> %18392, %18376
  %18394 = add <4 x i32> %18393, %18379
  %18395 = add <4 x i32> %18394, %18382
  %18396 = shufflevector <2 x i64> %18342, <2 x i64> undef, <1 x i32> <i32 1>
  %18397 = bitcast <1 x i64> %18396 to <4 x i16>
  %18398 = shufflevector <2 x i64> %18346, <2 x i64> undef, <1 x i32> <i32 1>
  %18399 = shufflevector <2 x i64> %18349, <2 x i64> undef, <1 x i32> <i32 1>
  %18400 = bitcast <1 x i64> %18399 to <4 x i16>
  %18401 = shufflevector <2 x i64> %18353, <2 x i64> undef, <1 x i32> <i32 1>
  %18402 = shufflevector <2 x i64> %18356, <2 x i64> undef, <1 x i32> <i32 1>
  %18403 = bitcast <1 x i64> %18402 to <4 x i16>
  %18404 = shufflevector <2 x i64> %18360, <2 x i64> undef, <1 x i32> <i32 1>
  %18405 = shufflevector <2 x i64> %18363, <2 x i64> undef, <1 x i32> <i32 1>
  %18406 = bitcast <1 x i64> %18405 to <4 x i16>
  %18407 = shufflevector <2 x i64> %18367, <2 x i64> undef, <1 x i32> <i32 1>
  %18408 = shufflevector <2 x i64> %18370, <2 x i64> undef, <1 x i32> <i32 1>
  %18409 = bitcast <1 x i64> %18408 to <4 x i16>
  %.cast1273 = bitcast <1 x i64> %18398 to <4 x i16>
  %18410 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1273, <4 x i16> %17164) #11
  %18411 = sext <4 x i16> %18397 to <4 x i32>
  %18412 = mul nsw <4 x i32> %18411, %17186
  %.cast1276 = bitcast <1 x i64> %18401 to <4 x i16>
  %18413 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1276, <4 x i16> %17169) #11
  %18414 = sext <4 x i16> %18400 to <4 x i32>
  %18415 = mul nsw <4 x i32> %18414, %17190
  %.cast1279 = bitcast <1 x i64> %18404 to <4 x i16>
  %18416 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1279, <4 x i16> %17174) #11
  %18417 = sext <4 x i16> %18403 to <4 x i32>
  %18418 = mul nsw <4 x i32> %18417, %17194
  %.cast1282 = bitcast <1 x i64> %18407 to <4 x i16>
  %18419 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1282, <4 x i16> %17179) #11
  %18420 = sext <4 x i16> %18406 to <4 x i32>
  %18421 = mul nsw <4 x i32> %18420, %17198
  %18422 = sext <4 x i16> %18409 to <4 x i32>
  %18423 = mul nsw <4 x i32> %18422, %17200
  %18424 = add <4 x i32> %18412, %16985
  %18425 = add <4 x i32> %18424, %18415
  %18426 = add <4 x i32> %18425, %18418
  %18427 = add <4 x i32> %18426, %18421
  %18428 = add <4 x i32> %18427, %18423
  %18429 = add <4 x i32> %18428, %18410
  %18430 = add <4 x i32> %18429, %18413
  %18431 = add <4 x i32> %18430, %18416
  %18432 = add <4 x i32> %18431, %18419
  %18433 = zext <8 x i8> %t4156 to <8 x i16>
  %18434 = bitcast <8 x i16> %18433 to <2 x i64>
  %18435 = shufflevector <2 x i64> %18434, <2 x i64> undef, <1 x i32> zeroinitializer
  %18436 = bitcast <1 x i64> %18435 to <4 x i16>
  %18437 = zext <8 x i8> %t4157 to <8 x i16>
  %18438 = bitcast <8 x i16> %18437 to <2 x i64>
  %18439 = shufflevector <2 x i64> %18438, <2 x i64> undef, <1 x i32> zeroinitializer
  %18440 = zext <8 x i8> %t4158 to <8 x i16>
  %18441 = bitcast <8 x i16> %18440 to <2 x i64>
  %18442 = shufflevector <2 x i64> %18441, <2 x i64> undef, <1 x i32> zeroinitializer
  %18443 = bitcast <1 x i64> %18442 to <4 x i16>
  %18444 = zext <8 x i8> %t4159 to <8 x i16>
  %18445 = bitcast <8 x i16> %18444 to <2 x i64>
  %18446 = shufflevector <2 x i64> %18445, <2 x i64> undef, <1 x i32> zeroinitializer
  %18447 = zext <8 x i8> %t4160 to <8 x i16>
  %18448 = bitcast <8 x i16> %18447 to <2 x i64>
  %18449 = shufflevector <2 x i64> %18448, <2 x i64> undef, <1 x i32> zeroinitializer
  %18450 = bitcast <1 x i64> %18449 to <4 x i16>
  %18451 = zext <8 x i8> %t4161 to <8 x i16>
  %18452 = bitcast <8 x i16> %18451 to <2 x i64>
  %18453 = shufflevector <2 x i64> %18452, <2 x i64> undef, <1 x i32> zeroinitializer
  %18454 = zext <8 x i8> %t4162 to <8 x i16>
  %18455 = bitcast <8 x i16> %18454 to <2 x i64>
  %18456 = shufflevector <2 x i64> %18455, <2 x i64> undef, <1 x i32> zeroinitializer
  %18457 = bitcast <1 x i64> %18456 to <4 x i16>
  %18458 = zext <8 x i8> %t4163 to <8 x i16>
  %18459 = bitcast <8 x i16> %18458 to <2 x i64>
  %18460 = shufflevector <2 x i64> %18459, <2 x i64> undef, <1 x i32> zeroinitializer
  %18461 = zext <8 x i8> %t4164 to <8 x i16>
  %18462 = bitcast <8 x i16> %18461 to <2 x i64>
  %18463 = shufflevector <2 x i64> %18462, <2 x i64> undef, <1 x i32> zeroinitializer
  %18464 = bitcast <1 x i64> %18463 to <4 x i16>
  %.cast1285 = bitcast <1 x i64> %18439 to <4 x i16>
  %18465 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1285, <4 x i16> %17217) #11
  %18466 = sext <4 x i16> %18436 to <4 x i32>
  %18467 = mul nsw <4 x i32> %18466, %17255
  %.cast1288 = bitcast <1 x i64> %18446 to <4 x i16>
  %18468 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1288, <4 x i16> %17226) #11
  %18469 = sext <4 x i16> %18443 to <4 x i32>
  %18470 = mul nsw <4 x i32> %18469, %17259
  %.cast1291 = bitcast <1 x i64> %18453 to <4 x i16>
  %18471 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1291, <4 x i16> %17235) #11
  %18472 = sext <4 x i16> %18450 to <4 x i32>
  %18473 = mul nsw <4 x i32> %18472, %17263
  %.cast1294 = bitcast <1 x i64> %18460 to <4 x i16>
  %18474 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1294, <4 x i16> %17244) #11
  %18475 = sext <4 x i16> %18457 to <4 x i32>
  %18476 = mul nsw <4 x i32> %18475, %17267
  %18477 = sext <4 x i16> %18464 to <4 x i32>
  %18478 = mul nsw <4 x i32> %18477, %17269
  %18479 = add <4 x i32> %18467, %16986
  %18480 = add <4 x i32> %18479, %18470
  %18481 = add <4 x i32> %18480, %18473
  %18482 = add <4 x i32> %18481, %18476
  %18483 = add <4 x i32> %18482, %18478
  %18484 = add <4 x i32> %18483, %18465
  %18485 = add <4 x i32> %18484, %18468
  %18486 = add <4 x i32> %18485, %18471
  %18487 = add <4 x i32> %18486, %18474
  %18488 = shufflevector <2 x i64> %18434, <2 x i64> undef, <1 x i32> <i32 1>
  %18489 = bitcast <1 x i64> %18488 to <4 x i16>
  %18490 = shufflevector <2 x i64> %18438, <2 x i64> undef, <1 x i32> <i32 1>
  %18491 = shufflevector <2 x i64> %18441, <2 x i64> undef, <1 x i32> <i32 1>
  %18492 = bitcast <1 x i64> %18491 to <4 x i16>
  %18493 = shufflevector <2 x i64> %18445, <2 x i64> undef, <1 x i32> <i32 1>
  %18494 = shufflevector <2 x i64> %18448, <2 x i64> undef, <1 x i32> <i32 1>
  %18495 = bitcast <1 x i64> %18494 to <4 x i16>
  %18496 = shufflevector <2 x i64> %18452, <2 x i64> undef, <1 x i32> <i32 1>
  %18497 = shufflevector <2 x i64> %18455, <2 x i64> undef, <1 x i32> <i32 1>
  %18498 = bitcast <1 x i64> %18497 to <4 x i16>
  %18499 = shufflevector <2 x i64> %18459, <2 x i64> undef, <1 x i32> <i32 1>
  %18500 = shufflevector <2 x i64> %18462, <2 x i64> undef, <1 x i32> <i32 1>
  %18501 = bitcast <1 x i64> %18500 to <4 x i16>
  %.cast1297 = bitcast <1 x i64> %18490 to <4 x i16>
  %18502 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1297, <4 x i16> %17284) #11
  %18503 = sext <4 x i16> %18489 to <4 x i32>
  %18504 = mul nsw <4 x i32> %18503, %17306
  %.cast1300 = bitcast <1 x i64> %18493 to <4 x i16>
  %18505 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1300, <4 x i16> %17289) #11
  %18506 = sext <4 x i16> %18492 to <4 x i32>
  %18507 = mul nsw <4 x i32> %18506, %17310
  %.cast1303 = bitcast <1 x i64> %18496 to <4 x i16>
  %18508 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1303, <4 x i16> %17294) #11
  %18509 = sext <4 x i16> %18495 to <4 x i32>
  %18510 = mul nsw <4 x i32> %18509, %17314
  %.cast1306 = bitcast <1 x i64> %18499 to <4 x i16>
  %18511 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1306, <4 x i16> %17299) #11
  %18512 = sext <4 x i16> %18498 to <4 x i32>
  %18513 = mul nsw <4 x i32> %18512, %17318
  %18514 = sext <4 x i16> %18501 to <4 x i32>
  %18515 = mul nsw <4 x i32> %18514, %17320
  %18516 = add <4 x i32> %18504, %16987
  %18517 = add <4 x i32> %18516, %18507
  %18518 = add <4 x i32> %18517, %18510
  %18519 = add <4 x i32> %18518, %18513
  %18520 = add <4 x i32> %18519, %18515
  %18521 = add <4 x i32> %18520, %18502
  %18522 = add <4 x i32> %18521, %18505
  %18523 = add <4 x i32> %18522, %18508
  %18524 = add <4 x i32> %18523, %18511
  %t4166 = add nsw i32 %t4053, %t2588828
  %18525 = sext i32 %t4166 to i64
  %18526 = shl nsw i64 %18525, 4
  %18527 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18526
  %18528 = bitcast i8* %18527 to <8 x i8>*
  %t4167 = load <8 x i8>, <8 x i8>* %18528, align 16, !tbaa !438
  %t4168 = add nsw i32 %t4053, %t2584827
  %18529 = sext i32 %t4168 to i64
  %18530 = shl nsw i64 %18529, 4
  %18531 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18530
  %18532 = bitcast i8* %18531 to <8 x i8>*
  %t4169 = load <8 x i8>, <8 x i8>* %18532, align 16, !tbaa !438
  %t4170 = add nsw i32 %t4053, %t2580804
  %18533 = sext i32 %t4170 to i64
  %18534 = shl nsw i64 %18533, 4
  %18535 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18534
  %18536 = bitcast i8* %18535 to <8 x i8>*
  %t4171 = load <8 x i8>, <8 x i8>* %18536, align 16, !tbaa !438
  %t4172 = add nsw i32 %t4053, %t2575826
  %18537 = sext i32 %t4172 to i64
  %18538 = shl nsw i64 %18537, 4
  %18539 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18538
  %18540 = bitcast i8* %18539 to <8 x i8>*
  %t4173 = load <8 x i8>, <8 x i8>* %18540, align 16, !tbaa !438
  %t4174 = add nsw i32 %t4053, %t2571825
  %18541 = sext i32 %t4174 to i64
  %18542 = shl nsw i64 %18541, 4
  %18543 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18542
  %18544 = bitcast i8* %18543 to <8 x i8>*
  %t4175 = load <8 x i8>, <8 x i8>* %18544, align 16, !tbaa !438
  %t4176 = add nsw i32 %t4053, %t2567803
  %18545 = sext i32 %t4176 to i64
  %18546 = shl nsw i64 %18545, 4
  %18547 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18546
  %18548 = bitcast i8* %18547 to <8 x i8>*
  %t4177 = load <8 x i8>, <8 x i8>* %18548, align 16, !tbaa !438
  %t4178 = add nsw i32 %t4053, %t2562812
  %18549 = sext i32 %t4178 to i64
  %18550 = shl nsw i64 %18549, 4
  %18551 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18550
  %18552 = bitcast i8* %18551 to <8 x i8>*
  %t4179 = load <8 x i8>, <8 x i8>* %18552, align 16, !tbaa !438
  %t4180 = add nsw i32 %t4053, %t2558811
  %18553 = sext i32 %t4180 to i64
  %18554 = shl nsw i64 %18553, 4
  %18555 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18554
  %18556 = bitcast i8* %18555 to <8 x i8>*
  %t4181 = load <8 x i8>, <8 x i8>* %18556, align 16, !tbaa !438
  %t4182 = add nsw i32 %t4053, %t2554796
  %18557 = sext i32 %t4182 to i64
  %18558 = shl nsw i64 %18557, 4
  %18559 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18558
  %18560 = bitcast i8* %18559 to <8 x i8>*
  %t4183 = load <8 x i8>, <8 x i8>* %18560, align 16, !tbaa !438
  %18561 = getelementptr inbounds i8, i8* %18527, i64 8
  %18562 = bitcast i8* %18561 to <8 x i8>*
  %t4184 = load <8 x i8>, <8 x i8>* %18562, align 8, !tbaa !438
  %18563 = getelementptr inbounds i8, i8* %18531, i64 8
  %18564 = bitcast i8* %18563 to <8 x i8>*
  %t4185 = load <8 x i8>, <8 x i8>* %18564, align 8, !tbaa !438
  %18565 = getelementptr inbounds i8, i8* %18535, i64 8
  %18566 = bitcast i8* %18565 to <8 x i8>*
  %t4186 = load <8 x i8>, <8 x i8>* %18566, align 8, !tbaa !438
  %18567 = getelementptr inbounds i8, i8* %18539, i64 8
  %18568 = bitcast i8* %18567 to <8 x i8>*
  %t4187 = load <8 x i8>, <8 x i8>* %18568, align 8, !tbaa !438
  %18569 = getelementptr inbounds i8, i8* %18543, i64 8
  %18570 = bitcast i8* %18569 to <8 x i8>*
  %t4188 = load <8 x i8>, <8 x i8>* %18570, align 8, !tbaa !438
  %18571 = getelementptr inbounds i8, i8* %18547, i64 8
  %18572 = bitcast i8* %18571 to <8 x i8>*
  %t4189 = load <8 x i8>, <8 x i8>* %18572, align 8, !tbaa !438
  %18573 = getelementptr inbounds i8, i8* %18551, i64 8
  %18574 = bitcast i8* %18573 to <8 x i8>*
  %t4190 = load <8 x i8>, <8 x i8>* %18574, align 8, !tbaa !438
  %18575 = getelementptr inbounds i8, i8* %18555, i64 8
  %18576 = bitcast i8* %18575 to <8 x i8>*
  %t4191 = load <8 x i8>, <8 x i8>* %18576, align 8, !tbaa !438
  %18577 = getelementptr inbounds i8, i8* %18559, i64 8
  %18578 = bitcast i8* %18577 to <8 x i8>*
  %t4192 = load <8 x i8>, <8 x i8>* %18578, align 8, !tbaa !438
  %18579 = zext <8 x i8> %t4167 to <8 x i16>
  %18580 = bitcast <8 x i16> %18579 to <2 x i64>
  %18581 = shufflevector <2 x i64> %18580, <2 x i64> undef, <1 x i32> zeroinitializer
  %18582 = bitcast <1 x i64> %18581 to <4 x i16>
  %18583 = zext <8 x i8> %t4169 to <8 x i16>
  %18584 = bitcast <8 x i16> %18583 to <2 x i64>
  %18585 = shufflevector <2 x i64> %18584, <2 x i64> undef, <1 x i32> zeroinitializer
  %18586 = zext <8 x i8> %t4171 to <8 x i16>
  %18587 = bitcast <8 x i16> %18586 to <2 x i64>
  %18588 = shufflevector <2 x i64> %18587, <2 x i64> undef, <1 x i32> zeroinitializer
  %18589 = bitcast <1 x i64> %18588 to <4 x i16>
  %18590 = zext <8 x i8> %t4173 to <8 x i16>
  %18591 = bitcast <8 x i16> %18590 to <2 x i64>
  %18592 = shufflevector <2 x i64> %18591, <2 x i64> undef, <1 x i32> zeroinitializer
  %18593 = zext <8 x i8> %t4175 to <8 x i16>
  %18594 = bitcast <8 x i16> %18593 to <2 x i64>
  %18595 = shufflevector <2 x i64> %18594, <2 x i64> undef, <1 x i32> zeroinitializer
  %18596 = bitcast <1 x i64> %18595 to <4 x i16>
  %18597 = zext <8 x i8> %t4177 to <8 x i16>
  %18598 = bitcast <8 x i16> %18597 to <2 x i64>
  %18599 = shufflevector <2 x i64> %18598, <2 x i64> undef, <1 x i32> zeroinitializer
  %18600 = zext <8 x i8> %t4179 to <8 x i16>
  %18601 = bitcast <8 x i16> %18600 to <2 x i64>
  %18602 = shufflevector <2 x i64> %18601, <2 x i64> undef, <1 x i32> zeroinitializer
  %18603 = bitcast <1 x i64> %18602 to <4 x i16>
  %18604 = zext <8 x i8> %t4181 to <8 x i16>
  %18605 = bitcast <8 x i16> %18604 to <2 x i64>
  %18606 = shufflevector <2 x i64> %18605, <2 x i64> undef, <1 x i32> zeroinitializer
  %18607 = zext <8 x i8> %t4183 to <8 x i16>
  %18608 = bitcast <8 x i16> %18607 to <2 x i64>
  %18609 = shufflevector <2 x i64> %18608, <2 x i64> undef, <1 x i32> zeroinitializer
  %18610 = bitcast <1 x i64> %18609 to <4 x i16>
  %.cast1309 = bitcast <1 x i64> %18585 to <4 x i16>
  %18611 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1309, <4 x i16> %17097) #11
  %18612 = sext <4 x i16> %18582 to <4 x i32>
  %18613 = mul nsw <4 x i32> %18612, %17135
  %.cast1312 = bitcast <1 x i64> %18592 to <4 x i16>
  %18614 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1312, <4 x i16> %17106) #11
  %18615 = sext <4 x i16> %18589 to <4 x i32>
  %18616 = mul nsw <4 x i32> %18615, %17139
  %.cast1315 = bitcast <1 x i64> %18599 to <4 x i16>
  %18617 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1315, <4 x i16> %17115) #11
  %18618 = sext <4 x i16> %18596 to <4 x i32>
  %18619 = mul nsw <4 x i32> %18618, %17143
  %.cast1318 = bitcast <1 x i64> %18606 to <4 x i16>
  %18620 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1318, <4 x i16> %17124) #11
  %18621 = sext <4 x i16> %18603 to <4 x i32>
  %18622 = mul nsw <4 x i32> %18621, %17147
  %18623 = sext <4 x i16> %18610 to <4 x i32>
  %18624 = mul nsw <4 x i32> %18623, %17149
  %18625 = add <4 x i32> %18613, %16984
  %18626 = add <4 x i32> %18625, %18616
  %18627 = add <4 x i32> %18626, %18619
  %18628 = add <4 x i32> %18627, %18622
  %18629 = add <4 x i32> %18628, %18624
  %18630 = add <4 x i32> %18629, %18611
  %18631 = add <4 x i32> %18630, %18614
  %18632 = add <4 x i32> %18631, %18617
  %18633 = add <4 x i32> %18632, %18620
  %18634 = shufflevector <2 x i64> %18580, <2 x i64> undef, <1 x i32> <i32 1>
  %18635 = bitcast <1 x i64> %18634 to <4 x i16>
  %18636 = shufflevector <2 x i64> %18584, <2 x i64> undef, <1 x i32> <i32 1>
  %18637 = shufflevector <2 x i64> %18587, <2 x i64> undef, <1 x i32> <i32 1>
  %18638 = bitcast <1 x i64> %18637 to <4 x i16>
  %18639 = shufflevector <2 x i64> %18591, <2 x i64> undef, <1 x i32> <i32 1>
  %18640 = shufflevector <2 x i64> %18594, <2 x i64> undef, <1 x i32> <i32 1>
  %18641 = bitcast <1 x i64> %18640 to <4 x i16>
  %18642 = shufflevector <2 x i64> %18598, <2 x i64> undef, <1 x i32> <i32 1>
  %18643 = shufflevector <2 x i64> %18601, <2 x i64> undef, <1 x i32> <i32 1>
  %18644 = bitcast <1 x i64> %18643 to <4 x i16>
  %18645 = shufflevector <2 x i64> %18605, <2 x i64> undef, <1 x i32> <i32 1>
  %18646 = shufflevector <2 x i64> %18608, <2 x i64> undef, <1 x i32> <i32 1>
  %18647 = bitcast <1 x i64> %18646 to <4 x i16>
  %.cast1321 = bitcast <1 x i64> %18636 to <4 x i16>
  %18648 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1321, <4 x i16> %17164) #11
  %18649 = sext <4 x i16> %18635 to <4 x i32>
  %18650 = mul nsw <4 x i32> %18649, %17186
  %.cast1324 = bitcast <1 x i64> %18639 to <4 x i16>
  %18651 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1324, <4 x i16> %17169) #11
  %18652 = sext <4 x i16> %18638 to <4 x i32>
  %18653 = mul nsw <4 x i32> %18652, %17190
  %.cast1327 = bitcast <1 x i64> %18642 to <4 x i16>
  %18654 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1327, <4 x i16> %17174) #11
  %18655 = sext <4 x i16> %18641 to <4 x i32>
  %18656 = mul nsw <4 x i32> %18655, %17194
  %.cast1330 = bitcast <1 x i64> %18645 to <4 x i16>
  %18657 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1330, <4 x i16> %17179) #11
  %18658 = sext <4 x i16> %18644 to <4 x i32>
  %18659 = mul nsw <4 x i32> %18658, %17198
  %18660 = sext <4 x i16> %18647 to <4 x i32>
  %18661 = mul nsw <4 x i32> %18660, %17200
  %18662 = add <4 x i32> %18650, %16985
  %18663 = add <4 x i32> %18662, %18653
  %18664 = add <4 x i32> %18663, %18656
  %18665 = add <4 x i32> %18664, %18659
  %18666 = add <4 x i32> %18665, %18661
  %18667 = add <4 x i32> %18666, %18648
  %18668 = add <4 x i32> %18667, %18651
  %18669 = add <4 x i32> %18668, %18654
  %18670 = add <4 x i32> %18669, %18657
  %18671 = zext <8 x i8> %t4184 to <8 x i16>
  %18672 = bitcast <8 x i16> %18671 to <2 x i64>
  %18673 = shufflevector <2 x i64> %18672, <2 x i64> undef, <1 x i32> zeroinitializer
  %18674 = bitcast <1 x i64> %18673 to <4 x i16>
  %18675 = zext <8 x i8> %t4185 to <8 x i16>
  %18676 = bitcast <8 x i16> %18675 to <2 x i64>
  %18677 = shufflevector <2 x i64> %18676, <2 x i64> undef, <1 x i32> zeroinitializer
  %18678 = zext <8 x i8> %t4186 to <8 x i16>
  %18679 = bitcast <8 x i16> %18678 to <2 x i64>
  %18680 = shufflevector <2 x i64> %18679, <2 x i64> undef, <1 x i32> zeroinitializer
  %18681 = bitcast <1 x i64> %18680 to <4 x i16>
  %18682 = zext <8 x i8> %t4187 to <8 x i16>
  %18683 = bitcast <8 x i16> %18682 to <2 x i64>
  %18684 = shufflevector <2 x i64> %18683, <2 x i64> undef, <1 x i32> zeroinitializer
  %18685 = zext <8 x i8> %t4188 to <8 x i16>
  %18686 = bitcast <8 x i16> %18685 to <2 x i64>
  %18687 = shufflevector <2 x i64> %18686, <2 x i64> undef, <1 x i32> zeroinitializer
  %18688 = bitcast <1 x i64> %18687 to <4 x i16>
  %18689 = zext <8 x i8> %t4189 to <8 x i16>
  %18690 = bitcast <8 x i16> %18689 to <2 x i64>
  %18691 = shufflevector <2 x i64> %18690, <2 x i64> undef, <1 x i32> zeroinitializer
  %18692 = zext <8 x i8> %t4190 to <8 x i16>
  %18693 = bitcast <8 x i16> %18692 to <2 x i64>
  %18694 = shufflevector <2 x i64> %18693, <2 x i64> undef, <1 x i32> zeroinitializer
  %18695 = bitcast <1 x i64> %18694 to <4 x i16>
  %18696 = zext <8 x i8> %t4191 to <8 x i16>
  %18697 = bitcast <8 x i16> %18696 to <2 x i64>
  %18698 = shufflevector <2 x i64> %18697, <2 x i64> undef, <1 x i32> zeroinitializer
  %18699 = zext <8 x i8> %t4192 to <8 x i16>
  %18700 = bitcast <8 x i16> %18699 to <2 x i64>
  %18701 = shufflevector <2 x i64> %18700, <2 x i64> undef, <1 x i32> zeroinitializer
  %18702 = bitcast <1 x i64> %18701 to <4 x i16>
  %.cast1333 = bitcast <1 x i64> %18677 to <4 x i16>
  %18703 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1333, <4 x i16> %17217) #11
  %18704 = sext <4 x i16> %18674 to <4 x i32>
  %18705 = mul nsw <4 x i32> %18704, %17255
  %.cast1336 = bitcast <1 x i64> %18684 to <4 x i16>
  %18706 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1336, <4 x i16> %17226) #11
  %18707 = sext <4 x i16> %18681 to <4 x i32>
  %18708 = mul nsw <4 x i32> %18707, %17259
  %.cast1339 = bitcast <1 x i64> %18691 to <4 x i16>
  %18709 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1339, <4 x i16> %17235) #11
  %18710 = sext <4 x i16> %18688 to <4 x i32>
  %18711 = mul nsw <4 x i32> %18710, %17263
  %.cast1342 = bitcast <1 x i64> %18698 to <4 x i16>
  %18712 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1342, <4 x i16> %17244) #11
  %18713 = sext <4 x i16> %18695 to <4 x i32>
  %18714 = mul nsw <4 x i32> %18713, %17267
  %18715 = sext <4 x i16> %18702 to <4 x i32>
  %18716 = mul nsw <4 x i32> %18715, %17269
  %18717 = add <4 x i32> %18705, %16986
  %18718 = add <4 x i32> %18717, %18708
  %18719 = add <4 x i32> %18718, %18711
  %18720 = add <4 x i32> %18719, %18714
  %18721 = add <4 x i32> %18720, %18716
  %18722 = add <4 x i32> %18721, %18703
  %18723 = add <4 x i32> %18722, %18706
  %18724 = add <4 x i32> %18723, %18709
  %18725 = add <4 x i32> %18724, %18712
  %18726 = shufflevector <2 x i64> %18672, <2 x i64> undef, <1 x i32> <i32 1>
  %18727 = bitcast <1 x i64> %18726 to <4 x i16>
  %18728 = shufflevector <2 x i64> %18676, <2 x i64> undef, <1 x i32> <i32 1>
  %18729 = shufflevector <2 x i64> %18679, <2 x i64> undef, <1 x i32> <i32 1>
  %18730 = bitcast <1 x i64> %18729 to <4 x i16>
  %18731 = shufflevector <2 x i64> %18683, <2 x i64> undef, <1 x i32> <i32 1>
  %18732 = shufflevector <2 x i64> %18686, <2 x i64> undef, <1 x i32> <i32 1>
  %18733 = bitcast <1 x i64> %18732 to <4 x i16>
  %18734 = shufflevector <2 x i64> %18690, <2 x i64> undef, <1 x i32> <i32 1>
  %18735 = shufflevector <2 x i64> %18693, <2 x i64> undef, <1 x i32> <i32 1>
  %18736 = bitcast <1 x i64> %18735 to <4 x i16>
  %18737 = shufflevector <2 x i64> %18697, <2 x i64> undef, <1 x i32> <i32 1>
  %18738 = shufflevector <2 x i64> %18700, <2 x i64> undef, <1 x i32> <i32 1>
  %18739 = bitcast <1 x i64> %18738 to <4 x i16>
  %.cast1345 = bitcast <1 x i64> %18728 to <4 x i16>
  %18740 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1345, <4 x i16> %17284) #11
  %18741 = sext <4 x i16> %18727 to <4 x i32>
  %18742 = mul nsw <4 x i32> %18741, %17306
  %.cast1348 = bitcast <1 x i64> %18731 to <4 x i16>
  %18743 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1348, <4 x i16> %17289) #11
  %18744 = sext <4 x i16> %18730 to <4 x i32>
  %18745 = mul nsw <4 x i32> %18744, %17310
  %.cast1351 = bitcast <1 x i64> %18734 to <4 x i16>
  %18746 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1351, <4 x i16> %17294) #11
  %18747 = sext <4 x i16> %18733 to <4 x i32>
  %18748 = mul nsw <4 x i32> %18747, %17314
  %.cast1354 = bitcast <1 x i64> %18737 to <4 x i16>
  %18749 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1354, <4 x i16> %17299) #11
  %18750 = sext <4 x i16> %18736 to <4 x i32>
  %18751 = mul nsw <4 x i32> %18750, %17318
  %18752 = sext <4 x i16> %18739 to <4 x i32>
  %18753 = mul nsw <4 x i32> %18752, %17320
  %18754 = add <4 x i32> %18742, %16987
  %18755 = add <4 x i32> %18754, %18745
  %18756 = add <4 x i32> %18755, %18748
  %18757 = add <4 x i32> %18756, %18751
  %18758 = add <4 x i32> %18757, %18753
  %18759 = add <4 x i32> %18758, %18740
  %18760 = add <4 x i32> %18759, %18743
  %18761 = add <4 x i32> %18760, %18746
  %18762 = add <4 x i32> %18761, %18749
  %t4194 = add nsw i32 %t4081, %t2588828
  %18763 = sext i32 %t4194 to i64
  %18764 = shl nsw i64 %18763, 4
  %18765 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18764
  %18766 = bitcast i8* %18765 to <8 x i8>*
  %t4195 = load <8 x i8>, <8 x i8>* %18766, align 16, !tbaa !438
  %t4196 = add nsw i32 %t4081, %t2584827
  %18767 = sext i32 %t4196 to i64
  %18768 = shl nsw i64 %18767, 4
  %18769 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18768
  %18770 = bitcast i8* %18769 to <8 x i8>*
  %t4197 = load <8 x i8>, <8 x i8>* %18770, align 16, !tbaa !438
  %t4198 = add nsw i32 %t4081, %t2580804
  %18771 = sext i32 %t4198 to i64
  %18772 = shl nsw i64 %18771, 4
  %18773 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18772
  %18774 = bitcast i8* %18773 to <8 x i8>*
  %t4199 = load <8 x i8>, <8 x i8>* %18774, align 16, !tbaa !438
  %t4200 = add nsw i32 %t4081, %t2575826
  %18775 = sext i32 %t4200 to i64
  %18776 = shl nsw i64 %18775, 4
  %18777 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18776
  %18778 = bitcast i8* %18777 to <8 x i8>*
  %t4201 = load <8 x i8>, <8 x i8>* %18778, align 16, !tbaa !438
  %t4202 = add nsw i32 %t4081, %t2571825
  %18779 = sext i32 %t4202 to i64
  %18780 = shl nsw i64 %18779, 4
  %18781 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18780
  %18782 = bitcast i8* %18781 to <8 x i8>*
  %t4203 = load <8 x i8>, <8 x i8>* %18782, align 16, !tbaa !438
  %t4204 = add nsw i32 %t4081, %t2567803
  %18783 = sext i32 %t4204 to i64
  %18784 = shl nsw i64 %18783, 4
  %18785 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18784
  %18786 = bitcast i8* %18785 to <8 x i8>*
  %t4205 = load <8 x i8>, <8 x i8>* %18786, align 16, !tbaa !438
  %t4206 = add nsw i32 %t4081, %t2562812
  %18787 = sext i32 %t4206 to i64
  %18788 = shl nsw i64 %18787, 4
  %18789 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18788
  %18790 = bitcast i8* %18789 to <8 x i8>*
  %t4207 = load <8 x i8>, <8 x i8>* %18790, align 16, !tbaa !438
  %t4208 = add nsw i32 %t4081, %t2558811
  %18791 = sext i32 %t4208 to i64
  %18792 = shl nsw i64 %18791, 4
  %18793 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18792
  %18794 = bitcast i8* %18793 to <8 x i8>*
  %t4209 = load <8 x i8>, <8 x i8>* %18794, align 16, !tbaa !438
  %t4210 = add nsw i32 %t4081, %t2554796
  %18795 = sext i32 %t4210 to i64
  %18796 = shl nsw i64 %18795, 4
  %18797 = getelementptr inbounds i8, i8* %resampled_input681, i64 %18796
  %18798 = bitcast i8* %18797 to <8 x i8>*
  %t4211 = load <8 x i8>, <8 x i8>* %18798, align 16, !tbaa !438
  %18799 = getelementptr inbounds i8, i8* %18765, i64 8
  %18800 = bitcast i8* %18799 to <8 x i8>*
  %t4212 = load <8 x i8>, <8 x i8>* %18800, align 8, !tbaa !438
  %18801 = getelementptr inbounds i8, i8* %18769, i64 8
  %18802 = bitcast i8* %18801 to <8 x i8>*
  %t4213 = load <8 x i8>, <8 x i8>* %18802, align 8, !tbaa !438
  %18803 = getelementptr inbounds i8, i8* %18773, i64 8
  %18804 = bitcast i8* %18803 to <8 x i8>*
  %t4214 = load <8 x i8>, <8 x i8>* %18804, align 8, !tbaa !438
  %18805 = getelementptr inbounds i8, i8* %18777, i64 8
  %18806 = bitcast i8* %18805 to <8 x i8>*
  %t4215 = load <8 x i8>, <8 x i8>* %18806, align 8, !tbaa !438
  %18807 = getelementptr inbounds i8, i8* %18781, i64 8
  %18808 = bitcast i8* %18807 to <8 x i8>*
  %t4216 = load <8 x i8>, <8 x i8>* %18808, align 8, !tbaa !438
  %18809 = getelementptr inbounds i8, i8* %18785, i64 8
  %18810 = bitcast i8* %18809 to <8 x i8>*
  %t4217 = load <8 x i8>, <8 x i8>* %18810, align 8, !tbaa !438
  %18811 = getelementptr inbounds i8, i8* %18789, i64 8
  %18812 = bitcast i8* %18811 to <8 x i8>*
  %t4218 = load <8 x i8>, <8 x i8>* %18812, align 8, !tbaa !438
  %18813 = getelementptr inbounds i8, i8* %18793, i64 8
  %18814 = bitcast i8* %18813 to <8 x i8>*
  %t4219 = load <8 x i8>, <8 x i8>* %18814, align 8, !tbaa !438
  %18815 = getelementptr inbounds i8, i8* %18797, i64 8
  %18816 = bitcast i8* %18815 to <8 x i8>*
  %t4220 = load <8 x i8>, <8 x i8>* %18816, align 8, !tbaa !438
  %18817 = zext <8 x i8> %t4195 to <8 x i16>
  %18818 = bitcast <8 x i16> %18817 to <2 x i64>
  %18819 = shufflevector <2 x i64> %18818, <2 x i64> undef, <1 x i32> zeroinitializer
  %18820 = bitcast <1 x i64> %18819 to <4 x i16>
  %18821 = zext <8 x i8> %t4197 to <8 x i16>
  %18822 = bitcast <8 x i16> %18821 to <2 x i64>
  %18823 = shufflevector <2 x i64> %18822, <2 x i64> undef, <1 x i32> zeroinitializer
  %18824 = zext <8 x i8> %t4199 to <8 x i16>
  %18825 = bitcast <8 x i16> %18824 to <2 x i64>
  %18826 = shufflevector <2 x i64> %18825, <2 x i64> undef, <1 x i32> zeroinitializer
  %18827 = bitcast <1 x i64> %18826 to <4 x i16>
  %18828 = zext <8 x i8> %t4201 to <8 x i16>
  %18829 = bitcast <8 x i16> %18828 to <2 x i64>
  %18830 = shufflevector <2 x i64> %18829, <2 x i64> undef, <1 x i32> zeroinitializer
  %18831 = zext <8 x i8> %t4203 to <8 x i16>
  %18832 = bitcast <8 x i16> %18831 to <2 x i64>
  %18833 = shufflevector <2 x i64> %18832, <2 x i64> undef, <1 x i32> zeroinitializer
  %18834 = bitcast <1 x i64> %18833 to <4 x i16>
  %18835 = zext <8 x i8> %t4205 to <8 x i16>
  %18836 = bitcast <8 x i16> %18835 to <2 x i64>
  %18837 = shufflevector <2 x i64> %18836, <2 x i64> undef, <1 x i32> zeroinitializer
  %18838 = zext <8 x i8> %t4207 to <8 x i16>
  %18839 = bitcast <8 x i16> %18838 to <2 x i64>
  %18840 = shufflevector <2 x i64> %18839, <2 x i64> undef, <1 x i32> zeroinitializer
  %18841 = bitcast <1 x i64> %18840 to <4 x i16>
  %18842 = zext <8 x i8> %t4209 to <8 x i16>
  %18843 = bitcast <8 x i16> %18842 to <2 x i64>
  %18844 = shufflevector <2 x i64> %18843, <2 x i64> undef, <1 x i32> zeroinitializer
  %18845 = zext <8 x i8> %t4211 to <8 x i16>
  %18846 = bitcast <8 x i16> %18845 to <2 x i64>
  %18847 = shufflevector <2 x i64> %18846, <2 x i64> undef, <1 x i32> zeroinitializer
  %18848 = bitcast <1 x i64> %18847 to <4 x i16>
  %.cast1357 = bitcast <1 x i64> %18823 to <4 x i16>
  %18849 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1357, <4 x i16> %17097) #11
  %18850 = sext <4 x i16> %18820 to <4 x i32>
  %18851 = mul nsw <4 x i32> %18850, %17135
  %.cast1360 = bitcast <1 x i64> %18830 to <4 x i16>
  %18852 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1360, <4 x i16> %17106) #11
  %18853 = sext <4 x i16> %18827 to <4 x i32>
  %18854 = mul nsw <4 x i32> %18853, %17139
  %.cast1363 = bitcast <1 x i64> %18837 to <4 x i16>
  %18855 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1363, <4 x i16> %17115) #11
  %18856 = sext <4 x i16> %18834 to <4 x i32>
  %18857 = mul nsw <4 x i32> %18856, %17143
  %.cast1366 = bitcast <1 x i64> %18844 to <4 x i16>
  %18858 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1366, <4 x i16> %17124) #11
  %18859 = sext <4 x i16> %18841 to <4 x i32>
  %18860 = mul nsw <4 x i32> %18859, %17147
  %18861 = sext <4 x i16> %18848 to <4 x i32>
  %18862 = mul nsw <4 x i32> %18861, %17149
  %18863 = add <4 x i32> %18851, %16984
  %18864 = add <4 x i32> %18863, %18854
  %18865 = add <4 x i32> %18864, %18857
  %18866 = add <4 x i32> %18865, %18860
  %18867 = add <4 x i32> %18866, %18862
  %18868 = add <4 x i32> %18867, %18849
  %18869 = add <4 x i32> %18868, %18852
  %18870 = add <4 x i32> %18869, %18855
  %18871 = add <4 x i32> %18870, %18858
  %18872 = shufflevector <2 x i64> %18818, <2 x i64> undef, <1 x i32> <i32 1>
  %18873 = bitcast <1 x i64> %18872 to <4 x i16>
  %18874 = shufflevector <2 x i64> %18822, <2 x i64> undef, <1 x i32> <i32 1>
  %18875 = shufflevector <2 x i64> %18825, <2 x i64> undef, <1 x i32> <i32 1>
  %18876 = bitcast <1 x i64> %18875 to <4 x i16>
  %18877 = shufflevector <2 x i64> %18829, <2 x i64> undef, <1 x i32> <i32 1>
  %18878 = shufflevector <2 x i64> %18832, <2 x i64> undef, <1 x i32> <i32 1>
  %18879 = bitcast <1 x i64> %18878 to <4 x i16>
  %18880 = shufflevector <2 x i64> %18836, <2 x i64> undef, <1 x i32> <i32 1>
  %18881 = shufflevector <2 x i64> %18839, <2 x i64> undef, <1 x i32> <i32 1>
  %18882 = bitcast <1 x i64> %18881 to <4 x i16>
  %18883 = shufflevector <2 x i64> %18843, <2 x i64> undef, <1 x i32> <i32 1>
  %18884 = shufflevector <2 x i64> %18846, <2 x i64> undef, <1 x i32> <i32 1>
  %18885 = bitcast <1 x i64> %18884 to <4 x i16>
  %.cast1369 = bitcast <1 x i64> %18874 to <4 x i16>
  %18886 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1369, <4 x i16> %17164) #11
  %18887 = sext <4 x i16> %18873 to <4 x i32>
  %18888 = mul nsw <4 x i32> %18887, %17186
  %.cast1372 = bitcast <1 x i64> %18877 to <4 x i16>
  %18889 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1372, <4 x i16> %17169) #11
  %18890 = sext <4 x i16> %18876 to <4 x i32>
  %18891 = mul nsw <4 x i32> %18890, %17190
  %.cast1375 = bitcast <1 x i64> %18880 to <4 x i16>
  %18892 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1375, <4 x i16> %17174) #11
  %18893 = sext <4 x i16> %18879 to <4 x i32>
  %18894 = mul nsw <4 x i32> %18893, %17194
  %.cast1378 = bitcast <1 x i64> %18883 to <4 x i16>
  %18895 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1378, <4 x i16> %17179) #11
  %18896 = sext <4 x i16> %18882 to <4 x i32>
  %18897 = mul nsw <4 x i32> %18896, %17198
  %18898 = sext <4 x i16> %18885 to <4 x i32>
  %18899 = mul nsw <4 x i32> %18898, %17200
  %18900 = add <4 x i32> %18888, %16985
  %18901 = add <4 x i32> %18900, %18891
  %18902 = add <4 x i32> %18901, %18894
  %18903 = add <4 x i32> %18902, %18897
  %18904 = add <4 x i32> %18903, %18899
  %18905 = add <4 x i32> %18904, %18886
  %18906 = add <4 x i32> %18905, %18889
  %18907 = add <4 x i32> %18906, %18892
  %18908 = add <4 x i32> %18907, %18895
  %18909 = zext <8 x i8> %t4212 to <8 x i16>
  %18910 = bitcast <8 x i16> %18909 to <2 x i64>
  %18911 = shufflevector <2 x i64> %18910, <2 x i64> undef, <1 x i32> zeroinitializer
  %18912 = bitcast <1 x i64> %18911 to <4 x i16>
  %18913 = zext <8 x i8> %t4213 to <8 x i16>
  %18914 = bitcast <8 x i16> %18913 to <2 x i64>
  %18915 = shufflevector <2 x i64> %18914, <2 x i64> undef, <1 x i32> zeroinitializer
  %18916 = zext <8 x i8> %t4214 to <8 x i16>
  %18917 = bitcast <8 x i16> %18916 to <2 x i64>
  %18918 = shufflevector <2 x i64> %18917, <2 x i64> undef, <1 x i32> zeroinitializer
  %18919 = bitcast <1 x i64> %18918 to <4 x i16>
  %18920 = zext <8 x i8> %t4215 to <8 x i16>
  %18921 = bitcast <8 x i16> %18920 to <2 x i64>
  %18922 = shufflevector <2 x i64> %18921, <2 x i64> undef, <1 x i32> zeroinitializer
  %18923 = zext <8 x i8> %t4216 to <8 x i16>
  %18924 = bitcast <8 x i16> %18923 to <2 x i64>
  %18925 = shufflevector <2 x i64> %18924, <2 x i64> undef, <1 x i32> zeroinitializer
  %18926 = bitcast <1 x i64> %18925 to <4 x i16>
  %18927 = zext <8 x i8> %t4217 to <8 x i16>
  %18928 = bitcast <8 x i16> %18927 to <2 x i64>
  %18929 = shufflevector <2 x i64> %18928, <2 x i64> undef, <1 x i32> zeroinitializer
  %18930 = zext <8 x i8> %t4218 to <8 x i16>
  %18931 = bitcast <8 x i16> %18930 to <2 x i64>
  %18932 = shufflevector <2 x i64> %18931, <2 x i64> undef, <1 x i32> zeroinitializer
  %18933 = bitcast <1 x i64> %18932 to <4 x i16>
  %18934 = zext <8 x i8> %t4219 to <8 x i16>
  %18935 = bitcast <8 x i16> %18934 to <2 x i64>
  %18936 = shufflevector <2 x i64> %18935, <2 x i64> undef, <1 x i32> zeroinitializer
  %18937 = zext <8 x i8> %t4220 to <8 x i16>
  %18938 = bitcast <8 x i16> %18937 to <2 x i64>
  %18939 = shufflevector <2 x i64> %18938, <2 x i64> undef, <1 x i32> zeroinitializer
  %18940 = bitcast <1 x i64> %18939 to <4 x i16>
  %.cast1381 = bitcast <1 x i64> %18915 to <4 x i16>
  %18941 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1381, <4 x i16> %17217) #11
  %18942 = sext <4 x i16> %18912 to <4 x i32>
  %18943 = mul nsw <4 x i32> %18942, %17255
  %.cast1384 = bitcast <1 x i64> %18922 to <4 x i16>
  %18944 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1384, <4 x i16> %17226) #11
  %18945 = sext <4 x i16> %18919 to <4 x i32>
  %18946 = mul nsw <4 x i32> %18945, %17259
  %.cast1387 = bitcast <1 x i64> %18929 to <4 x i16>
  %18947 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1387, <4 x i16> %17235) #11
  %18948 = sext <4 x i16> %18926 to <4 x i32>
  %18949 = mul nsw <4 x i32> %18948, %17263
  %.cast1390 = bitcast <1 x i64> %18936 to <4 x i16>
  %18950 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1390, <4 x i16> %17244) #11
  %18951 = sext <4 x i16> %18933 to <4 x i32>
  %18952 = mul nsw <4 x i32> %18951, %17267
  %18953 = sext <4 x i16> %18940 to <4 x i32>
  %18954 = mul nsw <4 x i32> %18953, %17269
  %18955 = add <4 x i32> %18943, %16986
  %18956 = add <4 x i32> %18955, %18946
  %18957 = add <4 x i32> %18956, %18949
  %18958 = add <4 x i32> %18957, %18952
  %18959 = add <4 x i32> %18958, %18954
  %18960 = add <4 x i32> %18959, %18941
  %18961 = add <4 x i32> %18960, %18944
  %18962 = add <4 x i32> %18961, %18947
  %18963 = add <4 x i32> %18962, %18950
  %18964 = shufflevector <2 x i64> %18910, <2 x i64> undef, <1 x i32> <i32 1>
  %18965 = bitcast <1 x i64> %18964 to <4 x i16>
  %18966 = shufflevector <2 x i64> %18914, <2 x i64> undef, <1 x i32> <i32 1>
  %18967 = shufflevector <2 x i64> %18917, <2 x i64> undef, <1 x i32> <i32 1>
  %18968 = bitcast <1 x i64> %18967 to <4 x i16>
  %18969 = shufflevector <2 x i64> %18921, <2 x i64> undef, <1 x i32> <i32 1>
  %18970 = shufflevector <2 x i64> %18924, <2 x i64> undef, <1 x i32> <i32 1>
  %18971 = bitcast <1 x i64> %18970 to <4 x i16>
  %18972 = shufflevector <2 x i64> %18928, <2 x i64> undef, <1 x i32> <i32 1>
  %18973 = shufflevector <2 x i64> %18931, <2 x i64> undef, <1 x i32> <i32 1>
  %18974 = bitcast <1 x i64> %18973 to <4 x i16>
  %18975 = shufflevector <2 x i64> %18935, <2 x i64> undef, <1 x i32> <i32 1>
  %18976 = shufflevector <2 x i64> %18938, <2 x i64> undef, <1 x i32> <i32 1>
  %18977 = bitcast <1 x i64> %18976 to <4 x i16>
  %.cast1393 = bitcast <1 x i64> %18966 to <4 x i16>
  %18978 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1393, <4 x i16> %17284) #11
  %18979 = sext <4 x i16> %18965 to <4 x i32>
  %18980 = mul nsw <4 x i32> %18979, %17306
  %.cast1396 = bitcast <1 x i64> %18969 to <4 x i16>
  %18981 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1396, <4 x i16> %17289) #11
  %18982 = sext <4 x i16> %18968 to <4 x i32>
  %18983 = mul nsw <4 x i32> %18982, %17310
  %.cast1399 = bitcast <1 x i64> %18972 to <4 x i16>
  %18984 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1399, <4 x i16> %17294) #11
  %18985 = sext <4 x i16> %18971 to <4 x i32>
  %18986 = mul nsw <4 x i32> %18985, %17314
  %.cast1402 = bitcast <1 x i64> %18975 to <4 x i16>
  %18987 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1402, <4 x i16> %17299) #11
  %18988 = sext <4 x i16> %18974 to <4 x i32>
  %18989 = mul nsw <4 x i32> %18988, %17318
  %18990 = sext <4 x i16> %18977 to <4 x i32>
  %18991 = mul nsw <4 x i32> %18990, %17320
  %18992 = add <4 x i32> %18980, %16987
  %18993 = add <4 x i32> %18992, %18983
  %18994 = add <4 x i32> %18993, %18986
  %18995 = add <4 x i32> %18994, %18989
  %18996 = add <4 x i32> %18995, %18991
  %18997 = add <4 x i32> %18996, %18978
  %18998 = add <4 x i32> %18997, %18981
  %18999 = add <4 x i32> %18998, %18984
  %19000 = add <4 x i32> %18999, %18987
  %t4222 = add nsw i32 %t3997, %t2589824
  %19001 = sext i32 %t4222 to i64
  %19002 = shl nsw i64 %19001, 4
  %19003 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19002
  %19004 = bitcast i8* %19003 to <8 x i8>*
  %t4223 = load <8 x i8>, <8 x i8>* %19004, align 16, !tbaa !438
  %t4224 = add nsw i32 %t3997, %t2585823
  %19005 = sext i32 %t4224 to i64
  %19006 = shl nsw i64 %19005, 4
  %19007 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19006
  %19008 = bitcast i8* %19007 to <8 x i8>*
  %t4225 = load <8 x i8>, <8 x i8>* %19008, align 16, !tbaa !438
  %t4226 = add nsw i32 %t3997, %t2581802
  %19009 = sext i32 %t4226 to i64
  %19010 = shl nsw i64 %19009, 4
  %19011 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19010
  %19012 = bitcast i8* %19011 to <8 x i8>*
  %t4227 = load <8 x i8>, <8 x i8>* %19012, align 16, !tbaa !438
  %t4228 = add nsw i32 %t3997, %t2576822
  %19013 = sext i32 %t4228 to i64
  %19014 = shl nsw i64 %19013, 4
  %19015 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19014
  %19016 = bitcast i8* %19015 to <8 x i8>*
  %t4229 = load <8 x i8>, <8 x i8>* %19016, align 16, !tbaa !438
  %t4230 = add nsw i32 %t3997, %t2572821
  %19017 = sext i32 %t4230 to i64
  %19018 = shl nsw i64 %19017, 4
  %19019 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19018
  %19020 = bitcast i8* %19019 to <8 x i8>*
  %t4231 = load <8 x i8>, <8 x i8>* %19020, align 16, !tbaa !438
  %t4232 = add nsw i32 %t3997, %t2568801
  %19021 = sext i32 %t4232 to i64
  %19022 = shl nsw i64 %19021, 4
  %19023 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19022
  %19024 = bitcast i8* %19023 to <8 x i8>*
  %t4233 = load <8 x i8>, <8 x i8>* %19024, align 16, !tbaa !438
  %t4234 = add nsw i32 %t3997, %t2563810
  %19025 = sext i32 %t4234 to i64
  %19026 = shl nsw i64 %19025, 4
  %19027 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19026
  %19028 = bitcast i8* %19027 to <8 x i8>*
  %t4235 = load <8 x i8>, <8 x i8>* %19028, align 16, !tbaa !438
  %t4236 = add nsw i32 %t3997, %t2559809
  %19029 = sext i32 %t4236 to i64
  %19030 = shl nsw i64 %19029, 4
  %19031 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19030
  %19032 = bitcast i8* %19031 to <8 x i8>*
  %t4237 = load <8 x i8>, <8 x i8>* %19032, align 16, !tbaa !438
  %t4238 = add nsw i32 %t3997, %t2555795
  %19033 = sext i32 %t4238 to i64
  %19034 = shl nsw i64 %19033, 4
  %19035 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19034
  %19036 = bitcast i8* %19035 to <8 x i8>*
  %t4239 = load <8 x i8>, <8 x i8>* %19036, align 16, !tbaa !438
  %19037 = getelementptr inbounds i8, i8* %19003, i64 8
  %19038 = bitcast i8* %19037 to <8 x i8>*
  %t4240 = load <8 x i8>, <8 x i8>* %19038, align 8, !tbaa !438
  %19039 = getelementptr inbounds i8, i8* %19007, i64 8
  %19040 = bitcast i8* %19039 to <8 x i8>*
  %t4241 = load <8 x i8>, <8 x i8>* %19040, align 8, !tbaa !438
  %19041 = getelementptr inbounds i8, i8* %19011, i64 8
  %19042 = bitcast i8* %19041 to <8 x i8>*
  %t4242 = load <8 x i8>, <8 x i8>* %19042, align 8, !tbaa !438
  %19043 = getelementptr inbounds i8, i8* %19015, i64 8
  %19044 = bitcast i8* %19043 to <8 x i8>*
  %t4243 = load <8 x i8>, <8 x i8>* %19044, align 8, !tbaa !438
  %19045 = getelementptr inbounds i8, i8* %19019, i64 8
  %19046 = bitcast i8* %19045 to <8 x i8>*
  %t4244 = load <8 x i8>, <8 x i8>* %19046, align 8, !tbaa !438
  %19047 = getelementptr inbounds i8, i8* %19023, i64 8
  %19048 = bitcast i8* %19047 to <8 x i8>*
  %t4245 = load <8 x i8>, <8 x i8>* %19048, align 8, !tbaa !438
  %19049 = getelementptr inbounds i8, i8* %19027, i64 8
  %19050 = bitcast i8* %19049 to <8 x i8>*
  %t4246 = load <8 x i8>, <8 x i8>* %19050, align 8, !tbaa !438
  %19051 = getelementptr inbounds i8, i8* %19031, i64 8
  %19052 = bitcast i8* %19051 to <8 x i8>*
  %t4247 = load <8 x i8>, <8 x i8>* %19052, align 8, !tbaa !438
  %19053 = getelementptr inbounds i8, i8* %19035, i64 8
  %19054 = bitcast i8* %19053 to <8 x i8>*
  %t4248 = load <8 x i8>, <8 x i8>* %19054, align 8, !tbaa !438
  %19055 = zext <8 x i8> %t4223 to <8 x i16>
  %19056 = bitcast <8 x i16> %19055 to <2 x i64>
  %19057 = shufflevector <2 x i64> %19056, <2 x i64> undef, <1 x i32> zeroinitializer
  %19058 = bitcast <1 x i64> %19057 to <4 x i16>
  %19059 = zext <8 x i8> %t4225 to <8 x i16>
  %19060 = bitcast <8 x i16> %19059 to <2 x i64>
  %19061 = shufflevector <2 x i64> %19060, <2 x i64> undef, <1 x i32> zeroinitializer
  %19062 = zext <8 x i8> %t4227 to <8 x i16>
  %19063 = bitcast <8 x i16> %19062 to <2 x i64>
  %19064 = shufflevector <2 x i64> %19063, <2 x i64> undef, <1 x i32> zeroinitializer
  %19065 = bitcast <1 x i64> %19064 to <4 x i16>
  %19066 = zext <8 x i8> %t4229 to <8 x i16>
  %19067 = bitcast <8 x i16> %19066 to <2 x i64>
  %19068 = shufflevector <2 x i64> %19067, <2 x i64> undef, <1 x i32> zeroinitializer
  %19069 = zext <8 x i8> %t4231 to <8 x i16>
  %19070 = bitcast <8 x i16> %19069 to <2 x i64>
  %19071 = shufflevector <2 x i64> %19070, <2 x i64> undef, <1 x i32> zeroinitializer
  %19072 = bitcast <1 x i64> %19071 to <4 x i16>
  %19073 = zext <8 x i8> %t4233 to <8 x i16>
  %19074 = bitcast <8 x i16> %19073 to <2 x i64>
  %19075 = shufflevector <2 x i64> %19074, <2 x i64> undef, <1 x i32> zeroinitializer
  %19076 = zext <8 x i8> %t4235 to <8 x i16>
  %19077 = bitcast <8 x i16> %19076 to <2 x i64>
  %19078 = shufflevector <2 x i64> %19077, <2 x i64> undef, <1 x i32> zeroinitializer
  %19079 = bitcast <1 x i64> %19078 to <4 x i16>
  %19080 = zext <8 x i8> %t4237 to <8 x i16>
  %19081 = bitcast <8 x i16> %19080 to <2 x i64>
  %19082 = shufflevector <2 x i64> %19081, <2 x i64> undef, <1 x i32> zeroinitializer
  %19083 = zext <8 x i8> %t4239 to <8 x i16>
  %19084 = bitcast <8 x i16> %19083 to <2 x i64>
  %19085 = shufflevector <2 x i64> %19084, <2 x i64> undef, <1 x i32> zeroinitializer
  %19086 = bitcast <1 x i64> %19085 to <4 x i16>
  %.cast1405 = bitcast <1 x i64> %19061 to <4 x i16>
  %19087 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1405, <4 x i16> %17097) #11
  %19088 = sext <4 x i16> %19058 to <4 x i32>
  %19089 = mul nsw <4 x i32> %19088, %17135
  %.cast1408 = bitcast <1 x i64> %19068 to <4 x i16>
  %19090 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1408, <4 x i16> %17106) #11
  %19091 = sext <4 x i16> %19065 to <4 x i32>
  %19092 = mul nsw <4 x i32> %19091, %17139
  %.cast1411 = bitcast <1 x i64> %19075 to <4 x i16>
  %19093 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1411, <4 x i16> %17115) #11
  %19094 = sext <4 x i16> %19072 to <4 x i32>
  %19095 = mul nsw <4 x i32> %19094, %17143
  %.cast1414 = bitcast <1 x i64> %19082 to <4 x i16>
  %19096 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1414, <4 x i16> %17124) #11
  %19097 = sext <4 x i16> %19079 to <4 x i32>
  %19098 = mul nsw <4 x i32> %19097, %17147
  %19099 = sext <4 x i16> %19086 to <4 x i32>
  %19100 = mul nsw <4 x i32> %19099, %17149
  %19101 = add <4 x i32> %19089, %16984
  %19102 = add <4 x i32> %19101, %19092
  %19103 = add <4 x i32> %19102, %19095
  %19104 = add <4 x i32> %19103, %19098
  %19105 = add <4 x i32> %19104, %19100
  %19106 = add <4 x i32> %19105, %19087
  %19107 = add <4 x i32> %19106, %19090
  %19108 = add <4 x i32> %19107, %19093
  %19109 = add <4 x i32> %19108, %19096
  %19110 = shufflevector <2 x i64> %19056, <2 x i64> undef, <1 x i32> <i32 1>
  %19111 = bitcast <1 x i64> %19110 to <4 x i16>
  %19112 = shufflevector <2 x i64> %19060, <2 x i64> undef, <1 x i32> <i32 1>
  %19113 = shufflevector <2 x i64> %19063, <2 x i64> undef, <1 x i32> <i32 1>
  %19114 = bitcast <1 x i64> %19113 to <4 x i16>
  %19115 = shufflevector <2 x i64> %19067, <2 x i64> undef, <1 x i32> <i32 1>
  %19116 = shufflevector <2 x i64> %19070, <2 x i64> undef, <1 x i32> <i32 1>
  %19117 = bitcast <1 x i64> %19116 to <4 x i16>
  %19118 = shufflevector <2 x i64> %19074, <2 x i64> undef, <1 x i32> <i32 1>
  %19119 = shufflevector <2 x i64> %19077, <2 x i64> undef, <1 x i32> <i32 1>
  %19120 = bitcast <1 x i64> %19119 to <4 x i16>
  %19121 = shufflevector <2 x i64> %19081, <2 x i64> undef, <1 x i32> <i32 1>
  %19122 = shufflevector <2 x i64> %19084, <2 x i64> undef, <1 x i32> <i32 1>
  %19123 = bitcast <1 x i64> %19122 to <4 x i16>
  %.cast1417 = bitcast <1 x i64> %19112 to <4 x i16>
  %19124 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1417, <4 x i16> %17164) #11
  %19125 = sext <4 x i16> %19111 to <4 x i32>
  %19126 = mul nsw <4 x i32> %19125, %17186
  %.cast1420 = bitcast <1 x i64> %19115 to <4 x i16>
  %19127 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1420, <4 x i16> %17169) #11
  %19128 = sext <4 x i16> %19114 to <4 x i32>
  %19129 = mul nsw <4 x i32> %19128, %17190
  %.cast1423 = bitcast <1 x i64> %19118 to <4 x i16>
  %19130 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1423, <4 x i16> %17174) #11
  %19131 = sext <4 x i16> %19117 to <4 x i32>
  %19132 = mul nsw <4 x i32> %19131, %17194
  %.cast1426 = bitcast <1 x i64> %19121 to <4 x i16>
  %19133 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1426, <4 x i16> %17179) #11
  %19134 = sext <4 x i16> %19120 to <4 x i32>
  %19135 = mul nsw <4 x i32> %19134, %17198
  %19136 = sext <4 x i16> %19123 to <4 x i32>
  %19137 = mul nsw <4 x i32> %19136, %17200
  %19138 = add <4 x i32> %19126, %16985
  %19139 = add <4 x i32> %19138, %19129
  %19140 = add <4 x i32> %19139, %19132
  %19141 = add <4 x i32> %19140, %19135
  %19142 = add <4 x i32> %19141, %19137
  %19143 = add <4 x i32> %19142, %19124
  %19144 = add <4 x i32> %19143, %19127
  %19145 = add <4 x i32> %19144, %19130
  %19146 = add <4 x i32> %19145, %19133
  %19147 = zext <8 x i8> %t4240 to <8 x i16>
  %19148 = bitcast <8 x i16> %19147 to <2 x i64>
  %19149 = shufflevector <2 x i64> %19148, <2 x i64> undef, <1 x i32> zeroinitializer
  %19150 = bitcast <1 x i64> %19149 to <4 x i16>
  %19151 = zext <8 x i8> %t4241 to <8 x i16>
  %19152 = bitcast <8 x i16> %19151 to <2 x i64>
  %19153 = shufflevector <2 x i64> %19152, <2 x i64> undef, <1 x i32> zeroinitializer
  %19154 = zext <8 x i8> %t4242 to <8 x i16>
  %19155 = bitcast <8 x i16> %19154 to <2 x i64>
  %19156 = shufflevector <2 x i64> %19155, <2 x i64> undef, <1 x i32> zeroinitializer
  %19157 = bitcast <1 x i64> %19156 to <4 x i16>
  %19158 = zext <8 x i8> %t4243 to <8 x i16>
  %19159 = bitcast <8 x i16> %19158 to <2 x i64>
  %19160 = shufflevector <2 x i64> %19159, <2 x i64> undef, <1 x i32> zeroinitializer
  %19161 = zext <8 x i8> %t4244 to <8 x i16>
  %19162 = bitcast <8 x i16> %19161 to <2 x i64>
  %19163 = shufflevector <2 x i64> %19162, <2 x i64> undef, <1 x i32> zeroinitializer
  %19164 = bitcast <1 x i64> %19163 to <4 x i16>
  %19165 = zext <8 x i8> %t4245 to <8 x i16>
  %19166 = bitcast <8 x i16> %19165 to <2 x i64>
  %19167 = shufflevector <2 x i64> %19166, <2 x i64> undef, <1 x i32> zeroinitializer
  %19168 = zext <8 x i8> %t4246 to <8 x i16>
  %19169 = bitcast <8 x i16> %19168 to <2 x i64>
  %19170 = shufflevector <2 x i64> %19169, <2 x i64> undef, <1 x i32> zeroinitializer
  %19171 = bitcast <1 x i64> %19170 to <4 x i16>
  %19172 = zext <8 x i8> %t4247 to <8 x i16>
  %19173 = bitcast <8 x i16> %19172 to <2 x i64>
  %19174 = shufflevector <2 x i64> %19173, <2 x i64> undef, <1 x i32> zeroinitializer
  %19175 = zext <8 x i8> %t4248 to <8 x i16>
  %19176 = bitcast <8 x i16> %19175 to <2 x i64>
  %19177 = shufflevector <2 x i64> %19176, <2 x i64> undef, <1 x i32> zeroinitializer
  %19178 = bitcast <1 x i64> %19177 to <4 x i16>
  %.cast1429 = bitcast <1 x i64> %19153 to <4 x i16>
  %19179 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1429, <4 x i16> %17217) #11
  %19180 = sext <4 x i16> %19150 to <4 x i32>
  %19181 = mul nsw <4 x i32> %19180, %17255
  %.cast1432 = bitcast <1 x i64> %19160 to <4 x i16>
  %19182 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1432, <4 x i16> %17226) #11
  %19183 = sext <4 x i16> %19157 to <4 x i32>
  %19184 = mul nsw <4 x i32> %19183, %17259
  %.cast1435 = bitcast <1 x i64> %19167 to <4 x i16>
  %19185 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1435, <4 x i16> %17235) #11
  %19186 = sext <4 x i16> %19164 to <4 x i32>
  %19187 = mul nsw <4 x i32> %19186, %17263
  %.cast1438 = bitcast <1 x i64> %19174 to <4 x i16>
  %19188 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1438, <4 x i16> %17244) #11
  %19189 = sext <4 x i16> %19171 to <4 x i32>
  %19190 = mul nsw <4 x i32> %19189, %17267
  %19191 = sext <4 x i16> %19178 to <4 x i32>
  %19192 = mul nsw <4 x i32> %19191, %17269
  %19193 = add <4 x i32> %19181, %16986
  %19194 = add <4 x i32> %19193, %19184
  %19195 = add <4 x i32> %19194, %19187
  %19196 = add <4 x i32> %19195, %19190
  %19197 = add <4 x i32> %19196, %19192
  %19198 = add <4 x i32> %19197, %19179
  %19199 = add <4 x i32> %19198, %19182
  %19200 = add <4 x i32> %19199, %19185
  %19201 = add <4 x i32> %19200, %19188
  %19202 = shufflevector <2 x i64> %19148, <2 x i64> undef, <1 x i32> <i32 1>
  %19203 = bitcast <1 x i64> %19202 to <4 x i16>
  %19204 = shufflevector <2 x i64> %19152, <2 x i64> undef, <1 x i32> <i32 1>
  %19205 = shufflevector <2 x i64> %19155, <2 x i64> undef, <1 x i32> <i32 1>
  %19206 = bitcast <1 x i64> %19205 to <4 x i16>
  %19207 = shufflevector <2 x i64> %19159, <2 x i64> undef, <1 x i32> <i32 1>
  %19208 = shufflevector <2 x i64> %19162, <2 x i64> undef, <1 x i32> <i32 1>
  %19209 = bitcast <1 x i64> %19208 to <4 x i16>
  %19210 = shufflevector <2 x i64> %19166, <2 x i64> undef, <1 x i32> <i32 1>
  %19211 = shufflevector <2 x i64> %19169, <2 x i64> undef, <1 x i32> <i32 1>
  %19212 = bitcast <1 x i64> %19211 to <4 x i16>
  %19213 = shufflevector <2 x i64> %19173, <2 x i64> undef, <1 x i32> <i32 1>
  %19214 = shufflevector <2 x i64> %19176, <2 x i64> undef, <1 x i32> <i32 1>
  %19215 = bitcast <1 x i64> %19214 to <4 x i16>
  %.cast1441 = bitcast <1 x i64> %19204 to <4 x i16>
  %19216 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1441, <4 x i16> %17284) #11
  %19217 = sext <4 x i16> %19203 to <4 x i32>
  %19218 = mul nsw <4 x i32> %19217, %17306
  %.cast1444 = bitcast <1 x i64> %19207 to <4 x i16>
  %19219 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1444, <4 x i16> %17289) #11
  %19220 = sext <4 x i16> %19206 to <4 x i32>
  %19221 = mul nsw <4 x i32> %19220, %17310
  %.cast1447 = bitcast <1 x i64> %19210 to <4 x i16>
  %19222 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1447, <4 x i16> %17294) #11
  %19223 = sext <4 x i16> %19209 to <4 x i32>
  %19224 = mul nsw <4 x i32> %19223, %17314
  %.cast1450 = bitcast <1 x i64> %19213 to <4 x i16>
  %19225 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1450, <4 x i16> %17299) #11
  %19226 = sext <4 x i16> %19212 to <4 x i32>
  %19227 = mul nsw <4 x i32> %19226, %17318
  %19228 = sext <4 x i16> %19215 to <4 x i32>
  %19229 = mul nsw <4 x i32> %19228, %17320
  %19230 = add <4 x i32> %19218, %16987
  %19231 = add <4 x i32> %19230, %19221
  %19232 = add <4 x i32> %19231, %19224
  %19233 = add <4 x i32> %19232, %19227
  %19234 = add <4 x i32> %19233, %19229
  %19235 = add <4 x i32> %19234, %19216
  %19236 = add <4 x i32> %19235, %19219
  %19237 = add <4 x i32> %19236, %19222
  %19238 = add <4 x i32> %19237, %19225
  %t4250 = add nsw i32 %t4025, %t2589824
  %19239 = sext i32 %t4250 to i64
  %19240 = shl nsw i64 %19239, 4
  %19241 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19240
  %19242 = bitcast i8* %19241 to <8 x i8>*
  %t4251 = load <8 x i8>, <8 x i8>* %19242, align 16, !tbaa !438
  %t4252 = add nsw i32 %t4025, %t2585823
  %19243 = sext i32 %t4252 to i64
  %19244 = shl nsw i64 %19243, 4
  %19245 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19244
  %19246 = bitcast i8* %19245 to <8 x i8>*
  %t4253 = load <8 x i8>, <8 x i8>* %19246, align 16, !tbaa !438
  %t4254 = add nsw i32 %t4025, %t2581802
  %19247 = sext i32 %t4254 to i64
  %19248 = shl nsw i64 %19247, 4
  %19249 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19248
  %19250 = bitcast i8* %19249 to <8 x i8>*
  %t4255 = load <8 x i8>, <8 x i8>* %19250, align 16, !tbaa !438
  %t4256 = add nsw i32 %t4025, %t2576822
  %19251 = sext i32 %t4256 to i64
  %19252 = shl nsw i64 %19251, 4
  %19253 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19252
  %19254 = bitcast i8* %19253 to <8 x i8>*
  %t4257 = load <8 x i8>, <8 x i8>* %19254, align 16, !tbaa !438
  %t4258 = add nsw i32 %t4025, %t2572821
  %19255 = sext i32 %t4258 to i64
  %19256 = shl nsw i64 %19255, 4
  %19257 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19256
  %19258 = bitcast i8* %19257 to <8 x i8>*
  %t4259 = load <8 x i8>, <8 x i8>* %19258, align 16, !tbaa !438
  %t4260 = add nsw i32 %t4025, %t2568801
  %19259 = sext i32 %t4260 to i64
  %19260 = shl nsw i64 %19259, 4
  %19261 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19260
  %19262 = bitcast i8* %19261 to <8 x i8>*
  %t4261 = load <8 x i8>, <8 x i8>* %19262, align 16, !tbaa !438
  %t4262 = add nsw i32 %t4025, %t2563810
  %19263 = sext i32 %t4262 to i64
  %19264 = shl nsw i64 %19263, 4
  %19265 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19264
  %19266 = bitcast i8* %19265 to <8 x i8>*
  %t4263 = load <8 x i8>, <8 x i8>* %19266, align 16, !tbaa !438
  %t4264 = add nsw i32 %t4025, %t2559809
  %19267 = sext i32 %t4264 to i64
  %19268 = shl nsw i64 %19267, 4
  %19269 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19268
  %19270 = bitcast i8* %19269 to <8 x i8>*
  %t4265 = load <8 x i8>, <8 x i8>* %19270, align 16, !tbaa !438
  %t4266 = add nsw i32 %t4025, %t2555795
  %19271 = sext i32 %t4266 to i64
  %19272 = shl nsw i64 %19271, 4
  %19273 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19272
  %19274 = bitcast i8* %19273 to <8 x i8>*
  %t4267 = load <8 x i8>, <8 x i8>* %19274, align 16, !tbaa !438
  %19275 = getelementptr inbounds i8, i8* %19241, i64 8
  %19276 = bitcast i8* %19275 to <8 x i8>*
  %t4268 = load <8 x i8>, <8 x i8>* %19276, align 8, !tbaa !438
  %19277 = getelementptr inbounds i8, i8* %19245, i64 8
  %19278 = bitcast i8* %19277 to <8 x i8>*
  %t4269 = load <8 x i8>, <8 x i8>* %19278, align 8, !tbaa !438
  %19279 = getelementptr inbounds i8, i8* %19249, i64 8
  %19280 = bitcast i8* %19279 to <8 x i8>*
  %t4270 = load <8 x i8>, <8 x i8>* %19280, align 8, !tbaa !438
  %19281 = getelementptr inbounds i8, i8* %19253, i64 8
  %19282 = bitcast i8* %19281 to <8 x i8>*
  %t4271 = load <8 x i8>, <8 x i8>* %19282, align 8, !tbaa !438
  %19283 = getelementptr inbounds i8, i8* %19257, i64 8
  %19284 = bitcast i8* %19283 to <8 x i8>*
  %t4272 = load <8 x i8>, <8 x i8>* %19284, align 8, !tbaa !438
  %19285 = getelementptr inbounds i8, i8* %19261, i64 8
  %19286 = bitcast i8* %19285 to <8 x i8>*
  %t4273 = load <8 x i8>, <8 x i8>* %19286, align 8, !tbaa !438
  %19287 = getelementptr inbounds i8, i8* %19265, i64 8
  %19288 = bitcast i8* %19287 to <8 x i8>*
  %t4274 = load <8 x i8>, <8 x i8>* %19288, align 8, !tbaa !438
  %19289 = getelementptr inbounds i8, i8* %19269, i64 8
  %19290 = bitcast i8* %19289 to <8 x i8>*
  %t4275 = load <8 x i8>, <8 x i8>* %19290, align 8, !tbaa !438
  %19291 = getelementptr inbounds i8, i8* %19273, i64 8
  %19292 = bitcast i8* %19291 to <8 x i8>*
  %t4276 = load <8 x i8>, <8 x i8>* %19292, align 8, !tbaa !438
  %19293 = zext <8 x i8> %t4251 to <8 x i16>
  %19294 = bitcast <8 x i16> %19293 to <2 x i64>
  %19295 = shufflevector <2 x i64> %19294, <2 x i64> undef, <1 x i32> zeroinitializer
  %19296 = bitcast <1 x i64> %19295 to <4 x i16>
  %19297 = zext <8 x i8> %t4253 to <8 x i16>
  %19298 = bitcast <8 x i16> %19297 to <2 x i64>
  %19299 = shufflevector <2 x i64> %19298, <2 x i64> undef, <1 x i32> zeroinitializer
  %19300 = zext <8 x i8> %t4255 to <8 x i16>
  %19301 = bitcast <8 x i16> %19300 to <2 x i64>
  %19302 = shufflevector <2 x i64> %19301, <2 x i64> undef, <1 x i32> zeroinitializer
  %19303 = bitcast <1 x i64> %19302 to <4 x i16>
  %19304 = zext <8 x i8> %t4257 to <8 x i16>
  %19305 = bitcast <8 x i16> %19304 to <2 x i64>
  %19306 = shufflevector <2 x i64> %19305, <2 x i64> undef, <1 x i32> zeroinitializer
  %19307 = zext <8 x i8> %t4259 to <8 x i16>
  %19308 = bitcast <8 x i16> %19307 to <2 x i64>
  %19309 = shufflevector <2 x i64> %19308, <2 x i64> undef, <1 x i32> zeroinitializer
  %19310 = bitcast <1 x i64> %19309 to <4 x i16>
  %19311 = zext <8 x i8> %t4261 to <8 x i16>
  %19312 = bitcast <8 x i16> %19311 to <2 x i64>
  %19313 = shufflevector <2 x i64> %19312, <2 x i64> undef, <1 x i32> zeroinitializer
  %19314 = zext <8 x i8> %t4263 to <8 x i16>
  %19315 = bitcast <8 x i16> %19314 to <2 x i64>
  %19316 = shufflevector <2 x i64> %19315, <2 x i64> undef, <1 x i32> zeroinitializer
  %19317 = bitcast <1 x i64> %19316 to <4 x i16>
  %19318 = zext <8 x i8> %t4265 to <8 x i16>
  %19319 = bitcast <8 x i16> %19318 to <2 x i64>
  %19320 = shufflevector <2 x i64> %19319, <2 x i64> undef, <1 x i32> zeroinitializer
  %19321 = zext <8 x i8> %t4267 to <8 x i16>
  %19322 = bitcast <8 x i16> %19321 to <2 x i64>
  %19323 = shufflevector <2 x i64> %19322, <2 x i64> undef, <1 x i32> zeroinitializer
  %19324 = bitcast <1 x i64> %19323 to <4 x i16>
  %.cast1453 = bitcast <1 x i64> %19299 to <4 x i16>
  %19325 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1453, <4 x i16> %17097) #11
  %19326 = sext <4 x i16> %19296 to <4 x i32>
  %19327 = mul nsw <4 x i32> %19326, %17135
  %.cast1456 = bitcast <1 x i64> %19306 to <4 x i16>
  %19328 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1456, <4 x i16> %17106) #11
  %19329 = sext <4 x i16> %19303 to <4 x i32>
  %19330 = mul nsw <4 x i32> %19329, %17139
  %.cast1459 = bitcast <1 x i64> %19313 to <4 x i16>
  %19331 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1459, <4 x i16> %17115) #11
  %19332 = sext <4 x i16> %19310 to <4 x i32>
  %19333 = mul nsw <4 x i32> %19332, %17143
  %.cast1462 = bitcast <1 x i64> %19320 to <4 x i16>
  %19334 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1462, <4 x i16> %17124) #11
  %19335 = sext <4 x i16> %19317 to <4 x i32>
  %19336 = mul nsw <4 x i32> %19335, %17147
  %19337 = sext <4 x i16> %19324 to <4 x i32>
  %19338 = mul nsw <4 x i32> %19337, %17149
  %19339 = add <4 x i32> %19327, %16984
  %19340 = add <4 x i32> %19339, %19330
  %19341 = add <4 x i32> %19340, %19333
  %19342 = add <4 x i32> %19341, %19336
  %19343 = add <4 x i32> %19342, %19338
  %19344 = add <4 x i32> %19343, %19325
  %19345 = add <4 x i32> %19344, %19328
  %19346 = add <4 x i32> %19345, %19331
  %19347 = add <4 x i32> %19346, %19334
  %19348 = shufflevector <2 x i64> %19294, <2 x i64> undef, <1 x i32> <i32 1>
  %19349 = bitcast <1 x i64> %19348 to <4 x i16>
  %19350 = shufflevector <2 x i64> %19298, <2 x i64> undef, <1 x i32> <i32 1>
  %19351 = shufflevector <2 x i64> %19301, <2 x i64> undef, <1 x i32> <i32 1>
  %19352 = bitcast <1 x i64> %19351 to <4 x i16>
  %19353 = shufflevector <2 x i64> %19305, <2 x i64> undef, <1 x i32> <i32 1>
  %19354 = shufflevector <2 x i64> %19308, <2 x i64> undef, <1 x i32> <i32 1>
  %19355 = bitcast <1 x i64> %19354 to <4 x i16>
  %19356 = shufflevector <2 x i64> %19312, <2 x i64> undef, <1 x i32> <i32 1>
  %19357 = shufflevector <2 x i64> %19315, <2 x i64> undef, <1 x i32> <i32 1>
  %19358 = bitcast <1 x i64> %19357 to <4 x i16>
  %19359 = shufflevector <2 x i64> %19319, <2 x i64> undef, <1 x i32> <i32 1>
  %19360 = shufflevector <2 x i64> %19322, <2 x i64> undef, <1 x i32> <i32 1>
  %19361 = bitcast <1 x i64> %19360 to <4 x i16>
  %.cast1465 = bitcast <1 x i64> %19350 to <4 x i16>
  %19362 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1465, <4 x i16> %17164) #11
  %19363 = sext <4 x i16> %19349 to <4 x i32>
  %19364 = mul nsw <4 x i32> %19363, %17186
  %.cast1468 = bitcast <1 x i64> %19353 to <4 x i16>
  %19365 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1468, <4 x i16> %17169) #11
  %19366 = sext <4 x i16> %19352 to <4 x i32>
  %19367 = mul nsw <4 x i32> %19366, %17190
  %.cast1471 = bitcast <1 x i64> %19356 to <4 x i16>
  %19368 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1471, <4 x i16> %17174) #11
  %19369 = sext <4 x i16> %19355 to <4 x i32>
  %19370 = mul nsw <4 x i32> %19369, %17194
  %.cast1474 = bitcast <1 x i64> %19359 to <4 x i16>
  %19371 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1474, <4 x i16> %17179) #11
  %19372 = sext <4 x i16> %19358 to <4 x i32>
  %19373 = mul nsw <4 x i32> %19372, %17198
  %19374 = sext <4 x i16> %19361 to <4 x i32>
  %19375 = mul nsw <4 x i32> %19374, %17200
  %19376 = add <4 x i32> %19364, %16985
  %19377 = add <4 x i32> %19376, %19367
  %19378 = add <4 x i32> %19377, %19370
  %19379 = add <4 x i32> %19378, %19373
  %19380 = add <4 x i32> %19379, %19375
  %19381 = add <4 x i32> %19380, %19362
  %19382 = add <4 x i32> %19381, %19365
  %19383 = add <4 x i32> %19382, %19368
  %19384 = add <4 x i32> %19383, %19371
  %19385 = zext <8 x i8> %t4268 to <8 x i16>
  %19386 = bitcast <8 x i16> %19385 to <2 x i64>
  %19387 = shufflevector <2 x i64> %19386, <2 x i64> undef, <1 x i32> zeroinitializer
  %19388 = bitcast <1 x i64> %19387 to <4 x i16>
  %19389 = zext <8 x i8> %t4269 to <8 x i16>
  %19390 = bitcast <8 x i16> %19389 to <2 x i64>
  %19391 = shufflevector <2 x i64> %19390, <2 x i64> undef, <1 x i32> zeroinitializer
  %19392 = zext <8 x i8> %t4270 to <8 x i16>
  %19393 = bitcast <8 x i16> %19392 to <2 x i64>
  %19394 = shufflevector <2 x i64> %19393, <2 x i64> undef, <1 x i32> zeroinitializer
  %19395 = bitcast <1 x i64> %19394 to <4 x i16>
  %19396 = zext <8 x i8> %t4271 to <8 x i16>
  %19397 = bitcast <8 x i16> %19396 to <2 x i64>
  %19398 = shufflevector <2 x i64> %19397, <2 x i64> undef, <1 x i32> zeroinitializer
  %19399 = zext <8 x i8> %t4272 to <8 x i16>
  %19400 = bitcast <8 x i16> %19399 to <2 x i64>
  %19401 = shufflevector <2 x i64> %19400, <2 x i64> undef, <1 x i32> zeroinitializer
  %19402 = bitcast <1 x i64> %19401 to <4 x i16>
  %19403 = zext <8 x i8> %t4273 to <8 x i16>
  %19404 = bitcast <8 x i16> %19403 to <2 x i64>
  %19405 = shufflevector <2 x i64> %19404, <2 x i64> undef, <1 x i32> zeroinitializer
  %19406 = zext <8 x i8> %t4274 to <8 x i16>
  %19407 = bitcast <8 x i16> %19406 to <2 x i64>
  %19408 = shufflevector <2 x i64> %19407, <2 x i64> undef, <1 x i32> zeroinitializer
  %19409 = bitcast <1 x i64> %19408 to <4 x i16>
  %19410 = zext <8 x i8> %t4275 to <8 x i16>
  %19411 = bitcast <8 x i16> %19410 to <2 x i64>
  %19412 = shufflevector <2 x i64> %19411, <2 x i64> undef, <1 x i32> zeroinitializer
  %19413 = zext <8 x i8> %t4276 to <8 x i16>
  %19414 = bitcast <8 x i16> %19413 to <2 x i64>
  %19415 = shufflevector <2 x i64> %19414, <2 x i64> undef, <1 x i32> zeroinitializer
  %19416 = bitcast <1 x i64> %19415 to <4 x i16>
  %.cast1477 = bitcast <1 x i64> %19391 to <4 x i16>
  %19417 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1477, <4 x i16> %17217) #11
  %19418 = sext <4 x i16> %19388 to <4 x i32>
  %19419 = mul nsw <4 x i32> %19418, %17255
  %.cast1480 = bitcast <1 x i64> %19398 to <4 x i16>
  %19420 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1480, <4 x i16> %17226) #11
  %19421 = sext <4 x i16> %19395 to <4 x i32>
  %19422 = mul nsw <4 x i32> %19421, %17259
  %.cast1483 = bitcast <1 x i64> %19405 to <4 x i16>
  %19423 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1483, <4 x i16> %17235) #11
  %19424 = sext <4 x i16> %19402 to <4 x i32>
  %19425 = mul nsw <4 x i32> %19424, %17263
  %.cast1486 = bitcast <1 x i64> %19412 to <4 x i16>
  %19426 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1486, <4 x i16> %17244) #11
  %19427 = sext <4 x i16> %19409 to <4 x i32>
  %19428 = mul nsw <4 x i32> %19427, %17267
  %19429 = sext <4 x i16> %19416 to <4 x i32>
  %19430 = mul nsw <4 x i32> %19429, %17269
  %19431 = add <4 x i32> %19419, %16986
  %19432 = add <4 x i32> %19431, %19422
  %19433 = add <4 x i32> %19432, %19425
  %19434 = add <4 x i32> %19433, %19428
  %19435 = add <4 x i32> %19434, %19430
  %19436 = add <4 x i32> %19435, %19417
  %19437 = add <4 x i32> %19436, %19420
  %19438 = add <4 x i32> %19437, %19423
  %19439 = add <4 x i32> %19438, %19426
  %19440 = shufflevector <2 x i64> %19386, <2 x i64> undef, <1 x i32> <i32 1>
  %19441 = bitcast <1 x i64> %19440 to <4 x i16>
  %19442 = shufflevector <2 x i64> %19390, <2 x i64> undef, <1 x i32> <i32 1>
  %19443 = shufflevector <2 x i64> %19393, <2 x i64> undef, <1 x i32> <i32 1>
  %19444 = bitcast <1 x i64> %19443 to <4 x i16>
  %19445 = shufflevector <2 x i64> %19397, <2 x i64> undef, <1 x i32> <i32 1>
  %19446 = shufflevector <2 x i64> %19400, <2 x i64> undef, <1 x i32> <i32 1>
  %19447 = bitcast <1 x i64> %19446 to <4 x i16>
  %19448 = shufflevector <2 x i64> %19404, <2 x i64> undef, <1 x i32> <i32 1>
  %19449 = shufflevector <2 x i64> %19407, <2 x i64> undef, <1 x i32> <i32 1>
  %19450 = bitcast <1 x i64> %19449 to <4 x i16>
  %19451 = shufflevector <2 x i64> %19411, <2 x i64> undef, <1 x i32> <i32 1>
  %19452 = shufflevector <2 x i64> %19414, <2 x i64> undef, <1 x i32> <i32 1>
  %19453 = bitcast <1 x i64> %19452 to <4 x i16>
  %.cast1489 = bitcast <1 x i64> %19442 to <4 x i16>
  %19454 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1489, <4 x i16> %17284) #11
  %19455 = sext <4 x i16> %19441 to <4 x i32>
  %19456 = mul nsw <4 x i32> %19455, %17306
  %.cast1492 = bitcast <1 x i64> %19445 to <4 x i16>
  %19457 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1492, <4 x i16> %17289) #11
  %19458 = sext <4 x i16> %19444 to <4 x i32>
  %19459 = mul nsw <4 x i32> %19458, %17310
  %.cast1495 = bitcast <1 x i64> %19448 to <4 x i16>
  %19460 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1495, <4 x i16> %17294) #11
  %19461 = sext <4 x i16> %19447 to <4 x i32>
  %19462 = mul nsw <4 x i32> %19461, %17314
  %.cast1498 = bitcast <1 x i64> %19451 to <4 x i16>
  %19463 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1498, <4 x i16> %17299) #11
  %19464 = sext <4 x i16> %19450 to <4 x i32>
  %19465 = mul nsw <4 x i32> %19464, %17318
  %19466 = sext <4 x i16> %19453 to <4 x i32>
  %19467 = mul nsw <4 x i32> %19466, %17320
  %19468 = add <4 x i32> %19456, %16987
  %19469 = add <4 x i32> %19468, %19459
  %19470 = add <4 x i32> %19469, %19462
  %19471 = add <4 x i32> %19470, %19465
  %19472 = add <4 x i32> %19471, %19467
  %19473 = add <4 x i32> %19472, %19454
  %19474 = add <4 x i32> %19473, %19457
  %19475 = add <4 x i32> %19474, %19460
  %19476 = add <4 x i32> %19475, %19463
  %t4278 = add nsw i32 %t4053, %t2589824
  %19477 = sext i32 %t4278 to i64
  %19478 = shl nsw i64 %19477, 4
  %19479 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19478
  %19480 = bitcast i8* %19479 to <8 x i8>*
  %t4279 = load <8 x i8>, <8 x i8>* %19480, align 16, !tbaa !438
  %t4280 = add nsw i32 %t4053, %t2585823
  %19481 = sext i32 %t4280 to i64
  %19482 = shl nsw i64 %19481, 4
  %19483 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19482
  %19484 = bitcast i8* %19483 to <8 x i8>*
  %t4281 = load <8 x i8>, <8 x i8>* %19484, align 16, !tbaa !438
  %t4282 = add nsw i32 %t4053, %t2581802
  %19485 = sext i32 %t4282 to i64
  %19486 = shl nsw i64 %19485, 4
  %19487 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19486
  %19488 = bitcast i8* %19487 to <8 x i8>*
  %t4283 = load <8 x i8>, <8 x i8>* %19488, align 16, !tbaa !438
  %t4284 = add nsw i32 %t4053, %t2576822
  %19489 = sext i32 %t4284 to i64
  %19490 = shl nsw i64 %19489, 4
  %19491 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19490
  %19492 = bitcast i8* %19491 to <8 x i8>*
  %t4285 = load <8 x i8>, <8 x i8>* %19492, align 16, !tbaa !438
  %t4286 = add nsw i32 %t4053, %t2572821
  %19493 = sext i32 %t4286 to i64
  %19494 = shl nsw i64 %19493, 4
  %19495 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19494
  %19496 = bitcast i8* %19495 to <8 x i8>*
  %t4287 = load <8 x i8>, <8 x i8>* %19496, align 16, !tbaa !438
  %t4288 = add nsw i32 %t4053, %t2568801
  %19497 = sext i32 %t4288 to i64
  %19498 = shl nsw i64 %19497, 4
  %19499 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19498
  %19500 = bitcast i8* %19499 to <8 x i8>*
  %t4289 = load <8 x i8>, <8 x i8>* %19500, align 16, !tbaa !438
  %t4290 = add nsw i32 %t4053, %t2563810
  %19501 = sext i32 %t4290 to i64
  %19502 = shl nsw i64 %19501, 4
  %19503 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19502
  %19504 = bitcast i8* %19503 to <8 x i8>*
  %t4291 = load <8 x i8>, <8 x i8>* %19504, align 16, !tbaa !438
  %t4292 = add nsw i32 %t4053, %t2559809
  %19505 = sext i32 %t4292 to i64
  %19506 = shl nsw i64 %19505, 4
  %19507 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19506
  %19508 = bitcast i8* %19507 to <8 x i8>*
  %t4293 = load <8 x i8>, <8 x i8>* %19508, align 16, !tbaa !438
  %t4294 = add nsw i32 %t4053, %t2555795
  %19509 = sext i32 %t4294 to i64
  %19510 = shl nsw i64 %19509, 4
  %19511 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19510
  %19512 = bitcast i8* %19511 to <8 x i8>*
  %t4295 = load <8 x i8>, <8 x i8>* %19512, align 16, !tbaa !438
  %19513 = getelementptr inbounds i8, i8* %19479, i64 8
  %19514 = bitcast i8* %19513 to <8 x i8>*
  %t4296 = load <8 x i8>, <8 x i8>* %19514, align 8, !tbaa !438
  %19515 = getelementptr inbounds i8, i8* %19483, i64 8
  %19516 = bitcast i8* %19515 to <8 x i8>*
  %t4297 = load <8 x i8>, <8 x i8>* %19516, align 8, !tbaa !438
  %19517 = getelementptr inbounds i8, i8* %19487, i64 8
  %19518 = bitcast i8* %19517 to <8 x i8>*
  %t4298 = load <8 x i8>, <8 x i8>* %19518, align 8, !tbaa !438
  %19519 = getelementptr inbounds i8, i8* %19491, i64 8
  %19520 = bitcast i8* %19519 to <8 x i8>*
  %t4299 = load <8 x i8>, <8 x i8>* %19520, align 8, !tbaa !438
  %19521 = getelementptr inbounds i8, i8* %19495, i64 8
  %19522 = bitcast i8* %19521 to <8 x i8>*
  %t4300 = load <8 x i8>, <8 x i8>* %19522, align 8, !tbaa !438
  %19523 = getelementptr inbounds i8, i8* %19499, i64 8
  %19524 = bitcast i8* %19523 to <8 x i8>*
  %t4301 = load <8 x i8>, <8 x i8>* %19524, align 8, !tbaa !438
  %19525 = getelementptr inbounds i8, i8* %19503, i64 8
  %19526 = bitcast i8* %19525 to <8 x i8>*
  %t4302 = load <8 x i8>, <8 x i8>* %19526, align 8, !tbaa !438
  %19527 = getelementptr inbounds i8, i8* %19507, i64 8
  %19528 = bitcast i8* %19527 to <8 x i8>*
  %t4303 = load <8 x i8>, <8 x i8>* %19528, align 8, !tbaa !438
  %19529 = getelementptr inbounds i8, i8* %19511, i64 8
  %19530 = bitcast i8* %19529 to <8 x i8>*
  %t4304 = load <8 x i8>, <8 x i8>* %19530, align 8, !tbaa !438
  %19531 = zext <8 x i8> %t4279 to <8 x i16>
  %19532 = bitcast <8 x i16> %19531 to <2 x i64>
  %19533 = shufflevector <2 x i64> %19532, <2 x i64> undef, <1 x i32> zeroinitializer
  %19534 = bitcast <1 x i64> %19533 to <4 x i16>
  %19535 = zext <8 x i8> %t4281 to <8 x i16>
  %19536 = bitcast <8 x i16> %19535 to <2 x i64>
  %19537 = shufflevector <2 x i64> %19536, <2 x i64> undef, <1 x i32> zeroinitializer
  %19538 = zext <8 x i8> %t4283 to <8 x i16>
  %19539 = bitcast <8 x i16> %19538 to <2 x i64>
  %19540 = shufflevector <2 x i64> %19539, <2 x i64> undef, <1 x i32> zeroinitializer
  %19541 = bitcast <1 x i64> %19540 to <4 x i16>
  %19542 = zext <8 x i8> %t4285 to <8 x i16>
  %19543 = bitcast <8 x i16> %19542 to <2 x i64>
  %19544 = shufflevector <2 x i64> %19543, <2 x i64> undef, <1 x i32> zeroinitializer
  %19545 = zext <8 x i8> %t4287 to <8 x i16>
  %19546 = bitcast <8 x i16> %19545 to <2 x i64>
  %19547 = shufflevector <2 x i64> %19546, <2 x i64> undef, <1 x i32> zeroinitializer
  %19548 = bitcast <1 x i64> %19547 to <4 x i16>
  %19549 = zext <8 x i8> %t4289 to <8 x i16>
  %19550 = bitcast <8 x i16> %19549 to <2 x i64>
  %19551 = shufflevector <2 x i64> %19550, <2 x i64> undef, <1 x i32> zeroinitializer
  %19552 = zext <8 x i8> %t4291 to <8 x i16>
  %19553 = bitcast <8 x i16> %19552 to <2 x i64>
  %19554 = shufflevector <2 x i64> %19553, <2 x i64> undef, <1 x i32> zeroinitializer
  %19555 = bitcast <1 x i64> %19554 to <4 x i16>
  %19556 = zext <8 x i8> %t4293 to <8 x i16>
  %19557 = bitcast <8 x i16> %19556 to <2 x i64>
  %19558 = shufflevector <2 x i64> %19557, <2 x i64> undef, <1 x i32> zeroinitializer
  %19559 = zext <8 x i8> %t4295 to <8 x i16>
  %19560 = bitcast <8 x i16> %19559 to <2 x i64>
  %19561 = shufflevector <2 x i64> %19560, <2 x i64> undef, <1 x i32> zeroinitializer
  %19562 = bitcast <1 x i64> %19561 to <4 x i16>
  %.cast1501 = bitcast <1 x i64> %19537 to <4 x i16>
  %19563 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1501, <4 x i16> %17097) #11
  %19564 = sext <4 x i16> %19534 to <4 x i32>
  %19565 = mul nsw <4 x i32> %19564, %17135
  %.cast1504 = bitcast <1 x i64> %19544 to <4 x i16>
  %19566 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1504, <4 x i16> %17106) #11
  %19567 = sext <4 x i16> %19541 to <4 x i32>
  %19568 = mul nsw <4 x i32> %19567, %17139
  %.cast1507 = bitcast <1 x i64> %19551 to <4 x i16>
  %19569 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1507, <4 x i16> %17115) #11
  %19570 = sext <4 x i16> %19548 to <4 x i32>
  %19571 = mul nsw <4 x i32> %19570, %17143
  %.cast1510 = bitcast <1 x i64> %19558 to <4 x i16>
  %19572 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1510, <4 x i16> %17124) #11
  %19573 = sext <4 x i16> %19555 to <4 x i32>
  %19574 = mul nsw <4 x i32> %19573, %17147
  %19575 = sext <4 x i16> %19562 to <4 x i32>
  %19576 = mul nsw <4 x i32> %19575, %17149
  %19577 = add <4 x i32> %19565, %16984
  %19578 = add <4 x i32> %19577, %19568
  %19579 = add <4 x i32> %19578, %19571
  %19580 = add <4 x i32> %19579, %19574
  %19581 = add <4 x i32> %19580, %19576
  %19582 = add <4 x i32> %19581, %19563
  %19583 = add <4 x i32> %19582, %19566
  %19584 = add <4 x i32> %19583, %19569
  %19585 = add <4 x i32> %19584, %19572
  %19586 = shufflevector <2 x i64> %19532, <2 x i64> undef, <1 x i32> <i32 1>
  %19587 = bitcast <1 x i64> %19586 to <4 x i16>
  %19588 = shufflevector <2 x i64> %19536, <2 x i64> undef, <1 x i32> <i32 1>
  %19589 = shufflevector <2 x i64> %19539, <2 x i64> undef, <1 x i32> <i32 1>
  %19590 = bitcast <1 x i64> %19589 to <4 x i16>
  %19591 = shufflevector <2 x i64> %19543, <2 x i64> undef, <1 x i32> <i32 1>
  %19592 = shufflevector <2 x i64> %19546, <2 x i64> undef, <1 x i32> <i32 1>
  %19593 = bitcast <1 x i64> %19592 to <4 x i16>
  %19594 = shufflevector <2 x i64> %19550, <2 x i64> undef, <1 x i32> <i32 1>
  %19595 = shufflevector <2 x i64> %19553, <2 x i64> undef, <1 x i32> <i32 1>
  %19596 = bitcast <1 x i64> %19595 to <4 x i16>
  %19597 = shufflevector <2 x i64> %19557, <2 x i64> undef, <1 x i32> <i32 1>
  %19598 = shufflevector <2 x i64> %19560, <2 x i64> undef, <1 x i32> <i32 1>
  %19599 = bitcast <1 x i64> %19598 to <4 x i16>
  %.cast1513 = bitcast <1 x i64> %19588 to <4 x i16>
  %19600 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1513, <4 x i16> %17164) #11
  %19601 = sext <4 x i16> %19587 to <4 x i32>
  %19602 = mul nsw <4 x i32> %19601, %17186
  %.cast1516 = bitcast <1 x i64> %19591 to <4 x i16>
  %19603 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1516, <4 x i16> %17169) #11
  %19604 = sext <4 x i16> %19590 to <4 x i32>
  %19605 = mul nsw <4 x i32> %19604, %17190
  %.cast1519 = bitcast <1 x i64> %19594 to <4 x i16>
  %19606 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1519, <4 x i16> %17174) #11
  %19607 = sext <4 x i16> %19593 to <4 x i32>
  %19608 = mul nsw <4 x i32> %19607, %17194
  %.cast1522 = bitcast <1 x i64> %19597 to <4 x i16>
  %19609 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1522, <4 x i16> %17179) #11
  %19610 = sext <4 x i16> %19596 to <4 x i32>
  %19611 = mul nsw <4 x i32> %19610, %17198
  %19612 = sext <4 x i16> %19599 to <4 x i32>
  %19613 = mul nsw <4 x i32> %19612, %17200
  %19614 = add <4 x i32> %19602, %16985
  %19615 = add <4 x i32> %19614, %19605
  %19616 = add <4 x i32> %19615, %19608
  %19617 = add <4 x i32> %19616, %19611
  %19618 = add <4 x i32> %19617, %19613
  %19619 = add <4 x i32> %19618, %19600
  %19620 = add <4 x i32> %19619, %19603
  %19621 = add <4 x i32> %19620, %19606
  %19622 = add <4 x i32> %19621, %19609
  %19623 = zext <8 x i8> %t4296 to <8 x i16>
  %19624 = bitcast <8 x i16> %19623 to <2 x i64>
  %19625 = shufflevector <2 x i64> %19624, <2 x i64> undef, <1 x i32> zeroinitializer
  %19626 = bitcast <1 x i64> %19625 to <4 x i16>
  %19627 = zext <8 x i8> %t4297 to <8 x i16>
  %19628 = bitcast <8 x i16> %19627 to <2 x i64>
  %19629 = shufflevector <2 x i64> %19628, <2 x i64> undef, <1 x i32> zeroinitializer
  %19630 = zext <8 x i8> %t4298 to <8 x i16>
  %19631 = bitcast <8 x i16> %19630 to <2 x i64>
  %19632 = shufflevector <2 x i64> %19631, <2 x i64> undef, <1 x i32> zeroinitializer
  %19633 = bitcast <1 x i64> %19632 to <4 x i16>
  %19634 = zext <8 x i8> %t4299 to <8 x i16>
  %19635 = bitcast <8 x i16> %19634 to <2 x i64>
  %19636 = shufflevector <2 x i64> %19635, <2 x i64> undef, <1 x i32> zeroinitializer
  %19637 = zext <8 x i8> %t4300 to <8 x i16>
  %19638 = bitcast <8 x i16> %19637 to <2 x i64>
  %19639 = shufflevector <2 x i64> %19638, <2 x i64> undef, <1 x i32> zeroinitializer
  %19640 = bitcast <1 x i64> %19639 to <4 x i16>
  %19641 = zext <8 x i8> %t4301 to <8 x i16>
  %19642 = bitcast <8 x i16> %19641 to <2 x i64>
  %19643 = shufflevector <2 x i64> %19642, <2 x i64> undef, <1 x i32> zeroinitializer
  %19644 = zext <8 x i8> %t4302 to <8 x i16>
  %19645 = bitcast <8 x i16> %19644 to <2 x i64>
  %19646 = shufflevector <2 x i64> %19645, <2 x i64> undef, <1 x i32> zeroinitializer
  %19647 = bitcast <1 x i64> %19646 to <4 x i16>
  %19648 = zext <8 x i8> %t4303 to <8 x i16>
  %19649 = bitcast <8 x i16> %19648 to <2 x i64>
  %19650 = shufflevector <2 x i64> %19649, <2 x i64> undef, <1 x i32> zeroinitializer
  %19651 = zext <8 x i8> %t4304 to <8 x i16>
  %19652 = bitcast <8 x i16> %19651 to <2 x i64>
  %19653 = shufflevector <2 x i64> %19652, <2 x i64> undef, <1 x i32> zeroinitializer
  %19654 = bitcast <1 x i64> %19653 to <4 x i16>
  %.cast1525 = bitcast <1 x i64> %19629 to <4 x i16>
  %19655 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1525, <4 x i16> %17217) #11
  %19656 = sext <4 x i16> %19626 to <4 x i32>
  %19657 = mul nsw <4 x i32> %19656, %17255
  %.cast1528 = bitcast <1 x i64> %19636 to <4 x i16>
  %19658 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1528, <4 x i16> %17226) #11
  %19659 = sext <4 x i16> %19633 to <4 x i32>
  %19660 = mul nsw <4 x i32> %19659, %17259
  %.cast1531 = bitcast <1 x i64> %19643 to <4 x i16>
  %19661 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1531, <4 x i16> %17235) #11
  %19662 = sext <4 x i16> %19640 to <4 x i32>
  %19663 = mul nsw <4 x i32> %19662, %17263
  %.cast1534 = bitcast <1 x i64> %19650 to <4 x i16>
  %19664 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1534, <4 x i16> %17244) #11
  %19665 = sext <4 x i16> %19647 to <4 x i32>
  %19666 = mul nsw <4 x i32> %19665, %17267
  %19667 = sext <4 x i16> %19654 to <4 x i32>
  %19668 = mul nsw <4 x i32> %19667, %17269
  %19669 = add <4 x i32> %19657, %16986
  %19670 = add <4 x i32> %19669, %19660
  %19671 = add <4 x i32> %19670, %19663
  %19672 = add <4 x i32> %19671, %19666
  %19673 = add <4 x i32> %19672, %19668
  %19674 = add <4 x i32> %19673, %19655
  %19675 = add <4 x i32> %19674, %19658
  %19676 = add <4 x i32> %19675, %19661
  %19677 = add <4 x i32> %19676, %19664
  %19678 = shufflevector <2 x i64> %19624, <2 x i64> undef, <1 x i32> <i32 1>
  %19679 = bitcast <1 x i64> %19678 to <4 x i16>
  %19680 = shufflevector <2 x i64> %19628, <2 x i64> undef, <1 x i32> <i32 1>
  %19681 = shufflevector <2 x i64> %19631, <2 x i64> undef, <1 x i32> <i32 1>
  %19682 = bitcast <1 x i64> %19681 to <4 x i16>
  %19683 = shufflevector <2 x i64> %19635, <2 x i64> undef, <1 x i32> <i32 1>
  %19684 = shufflevector <2 x i64> %19638, <2 x i64> undef, <1 x i32> <i32 1>
  %19685 = bitcast <1 x i64> %19684 to <4 x i16>
  %19686 = shufflevector <2 x i64> %19642, <2 x i64> undef, <1 x i32> <i32 1>
  %19687 = shufflevector <2 x i64> %19645, <2 x i64> undef, <1 x i32> <i32 1>
  %19688 = bitcast <1 x i64> %19687 to <4 x i16>
  %19689 = shufflevector <2 x i64> %19649, <2 x i64> undef, <1 x i32> <i32 1>
  %19690 = shufflevector <2 x i64> %19652, <2 x i64> undef, <1 x i32> <i32 1>
  %19691 = bitcast <1 x i64> %19690 to <4 x i16>
  %.cast1537 = bitcast <1 x i64> %19680 to <4 x i16>
  %19692 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1537, <4 x i16> %17284) #11
  %19693 = sext <4 x i16> %19679 to <4 x i32>
  %19694 = mul nsw <4 x i32> %19693, %17306
  %.cast1540 = bitcast <1 x i64> %19683 to <4 x i16>
  %19695 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1540, <4 x i16> %17289) #11
  %19696 = sext <4 x i16> %19682 to <4 x i32>
  %19697 = mul nsw <4 x i32> %19696, %17310
  %.cast1543 = bitcast <1 x i64> %19686 to <4 x i16>
  %19698 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1543, <4 x i16> %17294) #11
  %19699 = sext <4 x i16> %19685 to <4 x i32>
  %19700 = mul nsw <4 x i32> %19699, %17314
  %.cast1546 = bitcast <1 x i64> %19689 to <4 x i16>
  %19701 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1546, <4 x i16> %17299) #11
  %19702 = sext <4 x i16> %19688 to <4 x i32>
  %19703 = mul nsw <4 x i32> %19702, %17318
  %19704 = sext <4 x i16> %19691 to <4 x i32>
  %19705 = mul nsw <4 x i32> %19704, %17320
  %19706 = add <4 x i32> %19694, %16987
  %19707 = add <4 x i32> %19706, %19697
  %19708 = add <4 x i32> %19707, %19700
  %19709 = add <4 x i32> %19708, %19703
  %19710 = add <4 x i32> %19709, %19705
  %19711 = add <4 x i32> %19710, %19692
  %19712 = add <4 x i32> %19711, %19695
  %19713 = add <4 x i32> %19712, %19698
  %19714 = add <4 x i32> %19713, %19701
  %t4306 = add nsw i32 %t4081, %t2589824
  %19715 = sext i32 %t4306 to i64
  %19716 = shl nsw i64 %19715, 4
  %19717 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19716
  %19718 = bitcast i8* %19717 to <8 x i8>*
  %t4307 = load <8 x i8>, <8 x i8>* %19718, align 16, !tbaa !438
  %t4308 = add nsw i32 %t4081, %t2585823
  %19719 = sext i32 %t4308 to i64
  %19720 = shl nsw i64 %19719, 4
  %19721 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19720
  %19722 = bitcast i8* %19721 to <8 x i8>*
  %t4309 = load <8 x i8>, <8 x i8>* %19722, align 16, !tbaa !438
  %t4310 = add nsw i32 %t4081, %t2581802
  %19723 = sext i32 %t4310 to i64
  %19724 = shl nsw i64 %19723, 4
  %19725 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19724
  %19726 = bitcast i8* %19725 to <8 x i8>*
  %t4311 = load <8 x i8>, <8 x i8>* %19726, align 16, !tbaa !438
  %t4312 = add nsw i32 %t4081, %t2576822
  %19727 = sext i32 %t4312 to i64
  %19728 = shl nsw i64 %19727, 4
  %19729 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19728
  %19730 = bitcast i8* %19729 to <8 x i8>*
  %t4313 = load <8 x i8>, <8 x i8>* %19730, align 16, !tbaa !438
  %t4314 = add nsw i32 %t4081, %t2572821
  %19731 = sext i32 %t4314 to i64
  %19732 = shl nsw i64 %19731, 4
  %19733 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19732
  %19734 = bitcast i8* %19733 to <8 x i8>*
  %t4315 = load <8 x i8>, <8 x i8>* %19734, align 16, !tbaa !438
  %t4316 = add nsw i32 %t4081, %t2568801
  %19735 = sext i32 %t4316 to i64
  %19736 = shl nsw i64 %19735, 4
  %19737 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19736
  %19738 = bitcast i8* %19737 to <8 x i8>*
  %t4317 = load <8 x i8>, <8 x i8>* %19738, align 16, !tbaa !438
  %t4318 = add nsw i32 %t4081, %t2563810
  %19739 = sext i32 %t4318 to i64
  %19740 = shl nsw i64 %19739, 4
  %19741 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19740
  %19742 = bitcast i8* %19741 to <8 x i8>*
  %t4319 = load <8 x i8>, <8 x i8>* %19742, align 16, !tbaa !438
  %t4320 = add nsw i32 %t4081, %t2559809
  %19743 = sext i32 %t4320 to i64
  %19744 = shl nsw i64 %19743, 4
  %19745 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19744
  %19746 = bitcast i8* %19745 to <8 x i8>*
  %t4321 = load <8 x i8>, <8 x i8>* %19746, align 16, !tbaa !438
  %t4322 = add nsw i32 %t4081, %t2555795
  %19747 = sext i32 %t4322 to i64
  %19748 = shl nsw i64 %19747, 4
  %19749 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19748
  %19750 = bitcast i8* %19749 to <8 x i8>*
  %t4323 = load <8 x i8>, <8 x i8>* %19750, align 16, !tbaa !438
  %19751 = getelementptr inbounds i8, i8* %19717, i64 8
  %19752 = bitcast i8* %19751 to <8 x i8>*
  %t4324 = load <8 x i8>, <8 x i8>* %19752, align 8, !tbaa !438
  %19753 = getelementptr inbounds i8, i8* %19721, i64 8
  %19754 = bitcast i8* %19753 to <8 x i8>*
  %t4325 = load <8 x i8>, <8 x i8>* %19754, align 8, !tbaa !438
  %19755 = getelementptr inbounds i8, i8* %19725, i64 8
  %19756 = bitcast i8* %19755 to <8 x i8>*
  %t4326 = load <8 x i8>, <8 x i8>* %19756, align 8, !tbaa !438
  %19757 = getelementptr inbounds i8, i8* %19729, i64 8
  %19758 = bitcast i8* %19757 to <8 x i8>*
  %t4327 = load <8 x i8>, <8 x i8>* %19758, align 8, !tbaa !438
  %19759 = getelementptr inbounds i8, i8* %19733, i64 8
  %19760 = bitcast i8* %19759 to <8 x i8>*
  %t4328 = load <8 x i8>, <8 x i8>* %19760, align 8, !tbaa !438
  %19761 = getelementptr inbounds i8, i8* %19737, i64 8
  %19762 = bitcast i8* %19761 to <8 x i8>*
  %t4329 = load <8 x i8>, <8 x i8>* %19762, align 8, !tbaa !438
  %19763 = getelementptr inbounds i8, i8* %19741, i64 8
  %19764 = bitcast i8* %19763 to <8 x i8>*
  %t4330 = load <8 x i8>, <8 x i8>* %19764, align 8, !tbaa !438
  %19765 = getelementptr inbounds i8, i8* %19745, i64 8
  %19766 = bitcast i8* %19765 to <8 x i8>*
  %t4331 = load <8 x i8>, <8 x i8>* %19766, align 8, !tbaa !438
  %19767 = getelementptr inbounds i8, i8* %19749, i64 8
  %19768 = bitcast i8* %19767 to <8 x i8>*
  %t4332 = load <8 x i8>, <8 x i8>* %19768, align 8, !tbaa !438
  %19769 = zext <8 x i8> %t4307 to <8 x i16>
  %19770 = bitcast <8 x i16> %19769 to <2 x i64>
  %19771 = shufflevector <2 x i64> %19770, <2 x i64> undef, <1 x i32> zeroinitializer
  %19772 = bitcast <1 x i64> %19771 to <4 x i16>
  %19773 = zext <8 x i8> %t4309 to <8 x i16>
  %19774 = bitcast <8 x i16> %19773 to <2 x i64>
  %19775 = shufflevector <2 x i64> %19774, <2 x i64> undef, <1 x i32> zeroinitializer
  %19776 = zext <8 x i8> %t4311 to <8 x i16>
  %19777 = bitcast <8 x i16> %19776 to <2 x i64>
  %19778 = shufflevector <2 x i64> %19777, <2 x i64> undef, <1 x i32> zeroinitializer
  %19779 = bitcast <1 x i64> %19778 to <4 x i16>
  %19780 = zext <8 x i8> %t4313 to <8 x i16>
  %19781 = bitcast <8 x i16> %19780 to <2 x i64>
  %19782 = shufflevector <2 x i64> %19781, <2 x i64> undef, <1 x i32> zeroinitializer
  %19783 = zext <8 x i8> %t4315 to <8 x i16>
  %19784 = bitcast <8 x i16> %19783 to <2 x i64>
  %19785 = shufflevector <2 x i64> %19784, <2 x i64> undef, <1 x i32> zeroinitializer
  %19786 = bitcast <1 x i64> %19785 to <4 x i16>
  %19787 = zext <8 x i8> %t4317 to <8 x i16>
  %19788 = bitcast <8 x i16> %19787 to <2 x i64>
  %19789 = shufflevector <2 x i64> %19788, <2 x i64> undef, <1 x i32> zeroinitializer
  %19790 = zext <8 x i8> %t4319 to <8 x i16>
  %19791 = bitcast <8 x i16> %19790 to <2 x i64>
  %19792 = shufflevector <2 x i64> %19791, <2 x i64> undef, <1 x i32> zeroinitializer
  %19793 = bitcast <1 x i64> %19792 to <4 x i16>
  %19794 = zext <8 x i8> %t4321 to <8 x i16>
  %19795 = bitcast <8 x i16> %19794 to <2 x i64>
  %19796 = shufflevector <2 x i64> %19795, <2 x i64> undef, <1 x i32> zeroinitializer
  %19797 = zext <8 x i8> %t4323 to <8 x i16>
  %19798 = bitcast <8 x i16> %19797 to <2 x i64>
  %19799 = shufflevector <2 x i64> %19798, <2 x i64> undef, <1 x i32> zeroinitializer
  %19800 = bitcast <1 x i64> %19799 to <4 x i16>
  %.cast1549 = bitcast <1 x i64> %19775 to <4 x i16>
  %19801 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1549, <4 x i16> %17097) #11
  %19802 = sext <4 x i16> %19772 to <4 x i32>
  %19803 = mul nsw <4 x i32> %19802, %17135
  %.cast1552 = bitcast <1 x i64> %19782 to <4 x i16>
  %19804 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1552, <4 x i16> %17106) #11
  %19805 = sext <4 x i16> %19779 to <4 x i32>
  %19806 = mul nsw <4 x i32> %19805, %17139
  %.cast1555 = bitcast <1 x i64> %19789 to <4 x i16>
  %19807 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1555, <4 x i16> %17115) #11
  %19808 = sext <4 x i16> %19786 to <4 x i32>
  %19809 = mul nsw <4 x i32> %19808, %17143
  %.cast1558 = bitcast <1 x i64> %19796 to <4 x i16>
  %19810 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1558, <4 x i16> %17124) #11
  %19811 = sext <4 x i16> %19793 to <4 x i32>
  %19812 = mul nsw <4 x i32> %19811, %17147
  %19813 = sext <4 x i16> %19800 to <4 x i32>
  %19814 = mul nsw <4 x i32> %19813, %17149
  %19815 = add <4 x i32> %19803, %16984
  %19816 = add <4 x i32> %19815, %19806
  %19817 = add <4 x i32> %19816, %19809
  %19818 = add <4 x i32> %19817, %19812
  %19819 = add <4 x i32> %19818, %19814
  %19820 = add <4 x i32> %19819, %19801
  %19821 = add <4 x i32> %19820, %19804
  %19822 = add <4 x i32> %19821, %19807
  %19823 = add <4 x i32> %19822, %19810
  %19824 = shufflevector <2 x i64> %19770, <2 x i64> undef, <1 x i32> <i32 1>
  %19825 = bitcast <1 x i64> %19824 to <4 x i16>
  %19826 = shufflevector <2 x i64> %19774, <2 x i64> undef, <1 x i32> <i32 1>
  %19827 = shufflevector <2 x i64> %19777, <2 x i64> undef, <1 x i32> <i32 1>
  %19828 = bitcast <1 x i64> %19827 to <4 x i16>
  %19829 = shufflevector <2 x i64> %19781, <2 x i64> undef, <1 x i32> <i32 1>
  %19830 = shufflevector <2 x i64> %19784, <2 x i64> undef, <1 x i32> <i32 1>
  %19831 = bitcast <1 x i64> %19830 to <4 x i16>
  %19832 = shufflevector <2 x i64> %19788, <2 x i64> undef, <1 x i32> <i32 1>
  %19833 = shufflevector <2 x i64> %19791, <2 x i64> undef, <1 x i32> <i32 1>
  %19834 = bitcast <1 x i64> %19833 to <4 x i16>
  %19835 = shufflevector <2 x i64> %19795, <2 x i64> undef, <1 x i32> <i32 1>
  %19836 = shufflevector <2 x i64> %19798, <2 x i64> undef, <1 x i32> <i32 1>
  %19837 = bitcast <1 x i64> %19836 to <4 x i16>
  %.cast1561 = bitcast <1 x i64> %19826 to <4 x i16>
  %19838 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1561, <4 x i16> %17164) #11
  %19839 = sext <4 x i16> %19825 to <4 x i32>
  %19840 = mul nsw <4 x i32> %19839, %17186
  %.cast1564 = bitcast <1 x i64> %19829 to <4 x i16>
  %19841 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1564, <4 x i16> %17169) #11
  %19842 = sext <4 x i16> %19828 to <4 x i32>
  %19843 = mul nsw <4 x i32> %19842, %17190
  %.cast1567 = bitcast <1 x i64> %19832 to <4 x i16>
  %19844 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1567, <4 x i16> %17174) #11
  %19845 = sext <4 x i16> %19831 to <4 x i32>
  %19846 = mul nsw <4 x i32> %19845, %17194
  %.cast1570 = bitcast <1 x i64> %19835 to <4 x i16>
  %19847 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1570, <4 x i16> %17179) #11
  %19848 = sext <4 x i16> %19834 to <4 x i32>
  %19849 = mul nsw <4 x i32> %19848, %17198
  %19850 = sext <4 x i16> %19837 to <4 x i32>
  %19851 = mul nsw <4 x i32> %19850, %17200
  %19852 = add <4 x i32> %19840, %16985
  %19853 = add <4 x i32> %19852, %19843
  %19854 = add <4 x i32> %19853, %19846
  %19855 = add <4 x i32> %19854, %19849
  %19856 = add <4 x i32> %19855, %19851
  %19857 = add <4 x i32> %19856, %19838
  %19858 = add <4 x i32> %19857, %19841
  %19859 = add <4 x i32> %19858, %19844
  %19860 = add <4 x i32> %19859, %19847
  %19861 = zext <8 x i8> %t4324 to <8 x i16>
  %19862 = bitcast <8 x i16> %19861 to <2 x i64>
  %19863 = shufflevector <2 x i64> %19862, <2 x i64> undef, <1 x i32> zeroinitializer
  %19864 = bitcast <1 x i64> %19863 to <4 x i16>
  %19865 = zext <8 x i8> %t4325 to <8 x i16>
  %19866 = bitcast <8 x i16> %19865 to <2 x i64>
  %19867 = shufflevector <2 x i64> %19866, <2 x i64> undef, <1 x i32> zeroinitializer
  %19868 = zext <8 x i8> %t4326 to <8 x i16>
  %19869 = bitcast <8 x i16> %19868 to <2 x i64>
  %19870 = shufflevector <2 x i64> %19869, <2 x i64> undef, <1 x i32> zeroinitializer
  %19871 = bitcast <1 x i64> %19870 to <4 x i16>
  %19872 = zext <8 x i8> %t4327 to <8 x i16>
  %19873 = bitcast <8 x i16> %19872 to <2 x i64>
  %19874 = shufflevector <2 x i64> %19873, <2 x i64> undef, <1 x i32> zeroinitializer
  %19875 = zext <8 x i8> %t4328 to <8 x i16>
  %19876 = bitcast <8 x i16> %19875 to <2 x i64>
  %19877 = shufflevector <2 x i64> %19876, <2 x i64> undef, <1 x i32> zeroinitializer
  %19878 = bitcast <1 x i64> %19877 to <4 x i16>
  %19879 = zext <8 x i8> %t4329 to <8 x i16>
  %19880 = bitcast <8 x i16> %19879 to <2 x i64>
  %19881 = shufflevector <2 x i64> %19880, <2 x i64> undef, <1 x i32> zeroinitializer
  %19882 = zext <8 x i8> %t4330 to <8 x i16>
  %19883 = bitcast <8 x i16> %19882 to <2 x i64>
  %19884 = shufflevector <2 x i64> %19883, <2 x i64> undef, <1 x i32> zeroinitializer
  %19885 = bitcast <1 x i64> %19884 to <4 x i16>
  %19886 = zext <8 x i8> %t4331 to <8 x i16>
  %19887 = bitcast <8 x i16> %19886 to <2 x i64>
  %19888 = shufflevector <2 x i64> %19887, <2 x i64> undef, <1 x i32> zeroinitializer
  %19889 = zext <8 x i8> %t4332 to <8 x i16>
  %19890 = bitcast <8 x i16> %19889 to <2 x i64>
  %19891 = shufflevector <2 x i64> %19890, <2 x i64> undef, <1 x i32> zeroinitializer
  %19892 = bitcast <1 x i64> %19891 to <4 x i16>
  %.cast1573 = bitcast <1 x i64> %19867 to <4 x i16>
  %19893 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1573, <4 x i16> %17217) #11
  %19894 = sext <4 x i16> %19864 to <4 x i32>
  %19895 = mul nsw <4 x i32> %19894, %17255
  %.cast1576 = bitcast <1 x i64> %19874 to <4 x i16>
  %19896 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1576, <4 x i16> %17226) #11
  %19897 = sext <4 x i16> %19871 to <4 x i32>
  %19898 = mul nsw <4 x i32> %19897, %17259
  %.cast1579 = bitcast <1 x i64> %19881 to <4 x i16>
  %19899 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1579, <4 x i16> %17235) #11
  %19900 = sext <4 x i16> %19878 to <4 x i32>
  %19901 = mul nsw <4 x i32> %19900, %17263
  %.cast1582 = bitcast <1 x i64> %19888 to <4 x i16>
  %19902 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1582, <4 x i16> %17244) #11
  %19903 = sext <4 x i16> %19885 to <4 x i32>
  %19904 = mul nsw <4 x i32> %19903, %17267
  %19905 = sext <4 x i16> %19892 to <4 x i32>
  %19906 = mul nsw <4 x i32> %19905, %17269
  %19907 = add <4 x i32> %19895, %16986
  %19908 = add <4 x i32> %19907, %19898
  %19909 = add <4 x i32> %19908, %19901
  %19910 = add <4 x i32> %19909, %19904
  %19911 = add <4 x i32> %19910, %19906
  %19912 = add <4 x i32> %19911, %19893
  %19913 = add <4 x i32> %19912, %19896
  %19914 = add <4 x i32> %19913, %19899
  %19915 = add <4 x i32> %19914, %19902
  %19916 = shufflevector <2 x i64> %19862, <2 x i64> undef, <1 x i32> <i32 1>
  %19917 = bitcast <1 x i64> %19916 to <4 x i16>
  %19918 = shufflevector <2 x i64> %19866, <2 x i64> undef, <1 x i32> <i32 1>
  %19919 = shufflevector <2 x i64> %19869, <2 x i64> undef, <1 x i32> <i32 1>
  %19920 = bitcast <1 x i64> %19919 to <4 x i16>
  %19921 = shufflevector <2 x i64> %19873, <2 x i64> undef, <1 x i32> <i32 1>
  %19922 = shufflevector <2 x i64> %19876, <2 x i64> undef, <1 x i32> <i32 1>
  %19923 = bitcast <1 x i64> %19922 to <4 x i16>
  %19924 = shufflevector <2 x i64> %19880, <2 x i64> undef, <1 x i32> <i32 1>
  %19925 = shufflevector <2 x i64> %19883, <2 x i64> undef, <1 x i32> <i32 1>
  %19926 = bitcast <1 x i64> %19925 to <4 x i16>
  %19927 = shufflevector <2 x i64> %19887, <2 x i64> undef, <1 x i32> <i32 1>
  %19928 = shufflevector <2 x i64> %19890, <2 x i64> undef, <1 x i32> <i32 1>
  %19929 = bitcast <1 x i64> %19928 to <4 x i16>
  %.cast1585 = bitcast <1 x i64> %19918 to <4 x i16>
  %19930 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1585, <4 x i16> %17284) #11
  %19931 = sext <4 x i16> %19917 to <4 x i32>
  %19932 = mul nsw <4 x i32> %19931, %17306
  %.cast1588 = bitcast <1 x i64> %19921 to <4 x i16>
  %19933 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1588, <4 x i16> %17289) #11
  %19934 = sext <4 x i16> %19920 to <4 x i32>
  %19935 = mul nsw <4 x i32> %19934, %17310
  %.cast1591 = bitcast <1 x i64> %19924 to <4 x i16>
  %19936 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1591, <4 x i16> %17294) #11
  %19937 = sext <4 x i16> %19923 to <4 x i32>
  %19938 = mul nsw <4 x i32> %19937, %17314
  %.cast1594 = bitcast <1 x i64> %19927 to <4 x i16>
  %19939 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1594, <4 x i16> %17299) #11
  %19940 = sext <4 x i16> %19926 to <4 x i32>
  %19941 = mul nsw <4 x i32> %19940, %17318
  %19942 = sext <4 x i16> %19929 to <4 x i32>
  %19943 = mul nsw <4 x i32> %19942, %17320
  %19944 = add <4 x i32> %19932, %16987
  %19945 = add <4 x i32> %19944, %19935
  %19946 = add <4 x i32> %19945, %19938
  %19947 = add <4 x i32> %19946, %19941
  %19948 = add <4 x i32> %19947, %19943
  %19949 = add <4 x i32> %19948, %19930
  %19950 = add <4 x i32> %19949, %19933
  %19951 = add <4 x i32> %19950, %19936
  %19952 = add <4 x i32> %19951, %19939
  %t4334 = add nsw i32 %t3997, %t2590820
  %19953 = sext i32 %t4334 to i64
  %19954 = shl nsw i64 %19953, 4
  %19955 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19954
  %19956 = bitcast i8* %19955 to <8 x i8>*
  %t4335 = load <8 x i8>, <8 x i8>* %19956, align 16, !tbaa !438
  %t4336 = add nsw i32 %t3997, %t2586819
  %19957 = sext i32 %t4336 to i64
  %19958 = shl nsw i64 %19957, 4
  %19959 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19958
  %19960 = bitcast i8* %19959 to <8 x i8>*
  %t4337 = load <8 x i8>, <8 x i8>* %19960, align 16, !tbaa !438
  %t4338 = add nsw i32 %t3997, %t2582800
  %19961 = sext i32 %t4338 to i64
  %19962 = shl nsw i64 %19961, 4
  %19963 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19962
  %19964 = bitcast i8* %19963 to <8 x i8>*
  %t4339 = load <8 x i8>, <8 x i8>* %19964, align 16, !tbaa !438
  %t4340 = add nsw i32 %t3997, %t2577818
  %19965 = sext i32 %t4340 to i64
  %19966 = shl nsw i64 %19965, 4
  %19967 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19966
  %19968 = bitcast i8* %19967 to <8 x i8>*
  %t4341 = load <8 x i8>, <8 x i8>* %19968, align 16, !tbaa !438
  %t4342 = add nsw i32 %t3997, %t2573817
  %19969 = sext i32 %t4342 to i64
  %19970 = shl nsw i64 %19969, 4
  %19971 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19970
  %19972 = bitcast i8* %19971 to <8 x i8>*
  %t4343 = load <8 x i8>, <8 x i8>* %19972, align 16, !tbaa !438
  %t4344 = add nsw i32 %t3997, %t2569799
  %19973 = sext i32 %t4344 to i64
  %19974 = shl nsw i64 %19973, 4
  %19975 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19974
  %19976 = bitcast i8* %19975 to <8 x i8>*
  %t4345 = load <8 x i8>, <8 x i8>* %19976, align 16, !tbaa !438
  %t4346 = add nsw i32 %t3997, %t2564808
  %19977 = sext i32 %t4346 to i64
  %19978 = shl nsw i64 %19977, 4
  %19979 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19978
  %19980 = bitcast i8* %19979 to <8 x i8>*
  %t4347 = load <8 x i8>, <8 x i8>* %19980, align 16, !tbaa !438
  %t4348 = add nsw i32 %t3997, %t2560807
  %19981 = sext i32 %t4348 to i64
  %19982 = shl nsw i64 %19981, 4
  %19983 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19982
  %19984 = bitcast i8* %19983 to <8 x i8>*
  %t4349 = load <8 x i8>, <8 x i8>* %19984, align 16, !tbaa !438
  %t4350 = add nsw i32 %t3997, %t2556794
  %19985 = sext i32 %t4350 to i64
  %19986 = shl nsw i64 %19985, 4
  %19987 = getelementptr inbounds i8, i8* %resampled_input681, i64 %19986
  %19988 = bitcast i8* %19987 to <8 x i8>*
  %t4351 = load <8 x i8>, <8 x i8>* %19988, align 16, !tbaa !438
  %19989 = getelementptr inbounds i8, i8* %19955, i64 8
  %19990 = bitcast i8* %19989 to <8 x i8>*
  %t4352 = load <8 x i8>, <8 x i8>* %19990, align 8, !tbaa !438
  %19991 = getelementptr inbounds i8, i8* %19959, i64 8
  %19992 = bitcast i8* %19991 to <8 x i8>*
  %t4353 = load <8 x i8>, <8 x i8>* %19992, align 8, !tbaa !438
  %19993 = getelementptr inbounds i8, i8* %19963, i64 8
  %19994 = bitcast i8* %19993 to <8 x i8>*
  %t4354 = load <8 x i8>, <8 x i8>* %19994, align 8, !tbaa !438
  %19995 = getelementptr inbounds i8, i8* %19967, i64 8
  %19996 = bitcast i8* %19995 to <8 x i8>*
  %t4355 = load <8 x i8>, <8 x i8>* %19996, align 8, !tbaa !438
  %19997 = getelementptr inbounds i8, i8* %19971, i64 8
  %19998 = bitcast i8* %19997 to <8 x i8>*
  %t4356 = load <8 x i8>, <8 x i8>* %19998, align 8, !tbaa !438
  %19999 = getelementptr inbounds i8, i8* %19975, i64 8
  %20000 = bitcast i8* %19999 to <8 x i8>*
  %t4357 = load <8 x i8>, <8 x i8>* %20000, align 8, !tbaa !438
  %20001 = getelementptr inbounds i8, i8* %19979, i64 8
  %20002 = bitcast i8* %20001 to <8 x i8>*
  %t4358 = load <8 x i8>, <8 x i8>* %20002, align 8, !tbaa !438
  %20003 = getelementptr inbounds i8, i8* %19983, i64 8
  %20004 = bitcast i8* %20003 to <8 x i8>*
  %t4359 = load <8 x i8>, <8 x i8>* %20004, align 8, !tbaa !438
  %20005 = getelementptr inbounds i8, i8* %19987, i64 8
  %20006 = bitcast i8* %20005 to <8 x i8>*
  %t4360 = load <8 x i8>, <8 x i8>* %20006, align 8, !tbaa !438
  %20007 = zext <8 x i8> %t4335 to <8 x i16>
  %20008 = bitcast <8 x i16> %20007 to <2 x i64>
  %20009 = shufflevector <2 x i64> %20008, <2 x i64> undef, <1 x i32> zeroinitializer
  %20010 = bitcast <1 x i64> %20009 to <4 x i16>
  %20011 = zext <8 x i8> %t4337 to <8 x i16>
  %20012 = bitcast <8 x i16> %20011 to <2 x i64>
  %20013 = shufflevector <2 x i64> %20012, <2 x i64> undef, <1 x i32> zeroinitializer
  %20014 = zext <8 x i8> %t4339 to <8 x i16>
  %20015 = bitcast <8 x i16> %20014 to <2 x i64>
  %20016 = shufflevector <2 x i64> %20015, <2 x i64> undef, <1 x i32> zeroinitializer
  %20017 = bitcast <1 x i64> %20016 to <4 x i16>
  %20018 = zext <8 x i8> %t4341 to <8 x i16>
  %20019 = bitcast <8 x i16> %20018 to <2 x i64>
  %20020 = shufflevector <2 x i64> %20019, <2 x i64> undef, <1 x i32> zeroinitializer
  %20021 = zext <8 x i8> %t4343 to <8 x i16>
  %20022 = bitcast <8 x i16> %20021 to <2 x i64>
  %20023 = shufflevector <2 x i64> %20022, <2 x i64> undef, <1 x i32> zeroinitializer
  %20024 = bitcast <1 x i64> %20023 to <4 x i16>
  %20025 = zext <8 x i8> %t4345 to <8 x i16>
  %20026 = bitcast <8 x i16> %20025 to <2 x i64>
  %20027 = shufflevector <2 x i64> %20026, <2 x i64> undef, <1 x i32> zeroinitializer
  %20028 = zext <8 x i8> %t4347 to <8 x i16>
  %20029 = bitcast <8 x i16> %20028 to <2 x i64>
  %20030 = shufflevector <2 x i64> %20029, <2 x i64> undef, <1 x i32> zeroinitializer
  %20031 = bitcast <1 x i64> %20030 to <4 x i16>
  %20032 = zext <8 x i8> %t4349 to <8 x i16>
  %20033 = bitcast <8 x i16> %20032 to <2 x i64>
  %20034 = shufflevector <2 x i64> %20033, <2 x i64> undef, <1 x i32> zeroinitializer
  %20035 = zext <8 x i8> %t4351 to <8 x i16>
  %20036 = bitcast <8 x i16> %20035 to <2 x i64>
  %20037 = shufflevector <2 x i64> %20036, <2 x i64> undef, <1 x i32> zeroinitializer
  %20038 = bitcast <1 x i64> %20037 to <4 x i16>
  %.cast1597 = bitcast <1 x i64> %20013 to <4 x i16>
  %20039 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1597, <4 x i16> %17097) #11
  %20040 = sext <4 x i16> %20010 to <4 x i32>
  %20041 = mul nsw <4 x i32> %20040, %17135
  %.cast1600 = bitcast <1 x i64> %20020 to <4 x i16>
  %20042 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1600, <4 x i16> %17106) #11
  %20043 = sext <4 x i16> %20017 to <4 x i32>
  %20044 = mul nsw <4 x i32> %20043, %17139
  %.cast1603 = bitcast <1 x i64> %20027 to <4 x i16>
  %20045 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1603, <4 x i16> %17115) #11
  %20046 = sext <4 x i16> %20024 to <4 x i32>
  %20047 = mul nsw <4 x i32> %20046, %17143
  %.cast1606 = bitcast <1 x i64> %20034 to <4 x i16>
  %20048 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1606, <4 x i16> %17124) #11
  %20049 = sext <4 x i16> %20031 to <4 x i32>
  %20050 = mul nsw <4 x i32> %20049, %17147
  %20051 = sext <4 x i16> %20038 to <4 x i32>
  %20052 = mul nsw <4 x i32> %20051, %17149
  %20053 = add <4 x i32> %20041, %16984
  %20054 = add <4 x i32> %20053, %20044
  %20055 = add <4 x i32> %20054, %20047
  %20056 = add <4 x i32> %20055, %20050
  %20057 = add <4 x i32> %20056, %20052
  %20058 = add <4 x i32> %20057, %20039
  %20059 = add <4 x i32> %20058, %20042
  %20060 = add <4 x i32> %20059, %20045
  %20061 = add <4 x i32> %20060, %20048
  %20062 = shufflevector <2 x i64> %20008, <2 x i64> undef, <1 x i32> <i32 1>
  %20063 = bitcast <1 x i64> %20062 to <4 x i16>
  %20064 = shufflevector <2 x i64> %20012, <2 x i64> undef, <1 x i32> <i32 1>
  %20065 = shufflevector <2 x i64> %20015, <2 x i64> undef, <1 x i32> <i32 1>
  %20066 = bitcast <1 x i64> %20065 to <4 x i16>
  %20067 = shufflevector <2 x i64> %20019, <2 x i64> undef, <1 x i32> <i32 1>
  %20068 = shufflevector <2 x i64> %20022, <2 x i64> undef, <1 x i32> <i32 1>
  %20069 = bitcast <1 x i64> %20068 to <4 x i16>
  %20070 = shufflevector <2 x i64> %20026, <2 x i64> undef, <1 x i32> <i32 1>
  %20071 = shufflevector <2 x i64> %20029, <2 x i64> undef, <1 x i32> <i32 1>
  %20072 = bitcast <1 x i64> %20071 to <4 x i16>
  %20073 = shufflevector <2 x i64> %20033, <2 x i64> undef, <1 x i32> <i32 1>
  %20074 = shufflevector <2 x i64> %20036, <2 x i64> undef, <1 x i32> <i32 1>
  %20075 = bitcast <1 x i64> %20074 to <4 x i16>
  %.cast1609 = bitcast <1 x i64> %20064 to <4 x i16>
  %20076 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1609, <4 x i16> %17164) #11
  %20077 = sext <4 x i16> %20063 to <4 x i32>
  %20078 = mul nsw <4 x i32> %20077, %17186
  %.cast1612 = bitcast <1 x i64> %20067 to <4 x i16>
  %20079 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1612, <4 x i16> %17169) #11
  %20080 = sext <4 x i16> %20066 to <4 x i32>
  %20081 = mul nsw <4 x i32> %20080, %17190
  %.cast1615 = bitcast <1 x i64> %20070 to <4 x i16>
  %20082 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1615, <4 x i16> %17174) #11
  %20083 = sext <4 x i16> %20069 to <4 x i32>
  %20084 = mul nsw <4 x i32> %20083, %17194
  %.cast1618 = bitcast <1 x i64> %20073 to <4 x i16>
  %20085 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1618, <4 x i16> %17179) #11
  %20086 = sext <4 x i16> %20072 to <4 x i32>
  %20087 = mul nsw <4 x i32> %20086, %17198
  %20088 = sext <4 x i16> %20075 to <4 x i32>
  %20089 = mul nsw <4 x i32> %20088, %17200
  %20090 = add <4 x i32> %20078, %16985
  %20091 = add <4 x i32> %20090, %20081
  %20092 = add <4 x i32> %20091, %20084
  %20093 = add <4 x i32> %20092, %20087
  %20094 = add <4 x i32> %20093, %20089
  %20095 = add <4 x i32> %20094, %20076
  %20096 = add <4 x i32> %20095, %20079
  %20097 = add <4 x i32> %20096, %20082
  %20098 = add <4 x i32> %20097, %20085
  %20099 = zext <8 x i8> %t4352 to <8 x i16>
  %20100 = bitcast <8 x i16> %20099 to <2 x i64>
  %20101 = shufflevector <2 x i64> %20100, <2 x i64> undef, <1 x i32> zeroinitializer
  %20102 = bitcast <1 x i64> %20101 to <4 x i16>
  %20103 = zext <8 x i8> %t4353 to <8 x i16>
  %20104 = bitcast <8 x i16> %20103 to <2 x i64>
  %20105 = shufflevector <2 x i64> %20104, <2 x i64> undef, <1 x i32> zeroinitializer
  %20106 = zext <8 x i8> %t4354 to <8 x i16>
  %20107 = bitcast <8 x i16> %20106 to <2 x i64>
  %20108 = shufflevector <2 x i64> %20107, <2 x i64> undef, <1 x i32> zeroinitializer
  %20109 = bitcast <1 x i64> %20108 to <4 x i16>
  %20110 = zext <8 x i8> %t4355 to <8 x i16>
  %20111 = bitcast <8 x i16> %20110 to <2 x i64>
  %20112 = shufflevector <2 x i64> %20111, <2 x i64> undef, <1 x i32> zeroinitializer
  %20113 = zext <8 x i8> %t4356 to <8 x i16>
  %20114 = bitcast <8 x i16> %20113 to <2 x i64>
  %20115 = shufflevector <2 x i64> %20114, <2 x i64> undef, <1 x i32> zeroinitializer
  %20116 = bitcast <1 x i64> %20115 to <4 x i16>
  %20117 = zext <8 x i8> %t4357 to <8 x i16>
  %20118 = bitcast <8 x i16> %20117 to <2 x i64>
  %20119 = shufflevector <2 x i64> %20118, <2 x i64> undef, <1 x i32> zeroinitializer
  %20120 = zext <8 x i8> %t4358 to <8 x i16>
  %20121 = bitcast <8 x i16> %20120 to <2 x i64>
  %20122 = shufflevector <2 x i64> %20121, <2 x i64> undef, <1 x i32> zeroinitializer
  %20123 = bitcast <1 x i64> %20122 to <4 x i16>
  %20124 = zext <8 x i8> %t4359 to <8 x i16>
  %20125 = bitcast <8 x i16> %20124 to <2 x i64>
  %20126 = shufflevector <2 x i64> %20125, <2 x i64> undef, <1 x i32> zeroinitializer
  %20127 = zext <8 x i8> %t4360 to <8 x i16>
  %20128 = bitcast <8 x i16> %20127 to <2 x i64>
  %20129 = shufflevector <2 x i64> %20128, <2 x i64> undef, <1 x i32> zeroinitializer
  %20130 = bitcast <1 x i64> %20129 to <4 x i16>
  %.cast1621 = bitcast <1 x i64> %20105 to <4 x i16>
  %20131 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1621, <4 x i16> %17217) #11
  %20132 = sext <4 x i16> %20102 to <4 x i32>
  %20133 = mul nsw <4 x i32> %20132, %17255
  %.cast1624 = bitcast <1 x i64> %20112 to <4 x i16>
  %20134 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1624, <4 x i16> %17226) #11
  %20135 = sext <4 x i16> %20109 to <4 x i32>
  %20136 = mul nsw <4 x i32> %20135, %17259
  %.cast1627 = bitcast <1 x i64> %20119 to <4 x i16>
  %20137 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1627, <4 x i16> %17235) #11
  %20138 = sext <4 x i16> %20116 to <4 x i32>
  %20139 = mul nsw <4 x i32> %20138, %17263
  %.cast1630 = bitcast <1 x i64> %20126 to <4 x i16>
  %20140 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1630, <4 x i16> %17244) #11
  %20141 = sext <4 x i16> %20123 to <4 x i32>
  %20142 = mul nsw <4 x i32> %20141, %17267
  %20143 = sext <4 x i16> %20130 to <4 x i32>
  %20144 = mul nsw <4 x i32> %20143, %17269
  %20145 = add <4 x i32> %20133, %16986
  %20146 = add <4 x i32> %20145, %20136
  %20147 = add <4 x i32> %20146, %20139
  %20148 = add <4 x i32> %20147, %20142
  %20149 = add <4 x i32> %20148, %20144
  %20150 = add <4 x i32> %20149, %20131
  %20151 = add <4 x i32> %20150, %20134
  %20152 = add <4 x i32> %20151, %20137
  %20153 = add <4 x i32> %20152, %20140
  %20154 = shufflevector <2 x i64> %20100, <2 x i64> undef, <1 x i32> <i32 1>
  %20155 = bitcast <1 x i64> %20154 to <4 x i16>
  %20156 = shufflevector <2 x i64> %20104, <2 x i64> undef, <1 x i32> <i32 1>
  %20157 = shufflevector <2 x i64> %20107, <2 x i64> undef, <1 x i32> <i32 1>
  %20158 = bitcast <1 x i64> %20157 to <4 x i16>
  %20159 = shufflevector <2 x i64> %20111, <2 x i64> undef, <1 x i32> <i32 1>
  %20160 = shufflevector <2 x i64> %20114, <2 x i64> undef, <1 x i32> <i32 1>
  %20161 = bitcast <1 x i64> %20160 to <4 x i16>
  %20162 = shufflevector <2 x i64> %20118, <2 x i64> undef, <1 x i32> <i32 1>
  %20163 = shufflevector <2 x i64> %20121, <2 x i64> undef, <1 x i32> <i32 1>
  %20164 = bitcast <1 x i64> %20163 to <4 x i16>
  %20165 = shufflevector <2 x i64> %20125, <2 x i64> undef, <1 x i32> <i32 1>
  %20166 = shufflevector <2 x i64> %20128, <2 x i64> undef, <1 x i32> <i32 1>
  %20167 = bitcast <1 x i64> %20166 to <4 x i16>
  %.cast1633 = bitcast <1 x i64> %20156 to <4 x i16>
  %20168 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1633, <4 x i16> %17284) #11
  %20169 = sext <4 x i16> %20155 to <4 x i32>
  %20170 = mul nsw <4 x i32> %20169, %17306
  %.cast1636 = bitcast <1 x i64> %20159 to <4 x i16>
  %20171 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1636, <4 x i16> %17289) #11
  %20172 = sext <4 x i16> %20158 to <4 x i32>
  %20173 = mul nsw <4 x i32> %20172, %17310
  %.cast1639 = bitcast <1 x i64> %20162 to <4 x i16>
  %20174 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1639, <4 x i16> %17294) #11
  %20175 = sext <4 x i16> %20161 to <4 x i32>
  %20176 = mul nsw <4 x i32> %20175, %17314
  %.cast1642 = bitcast <1 x i64> %20165 to <4 x i16>
  %20177 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1642, <4 x i16> %17299) #11
  %20178 = sext <4 x i16> %20164 to <4 x i32>
  %20179 = mul nsw <4 x i32> %20178, %17318
  %20180 = sext <4 x i16> %20167 to <4 x i32>
  %20181 = mul nsw <4 x i32> %20180, %17320
  %20182 = add <4 x i32> %20170, %16987
  %20183 = add <4 x i32> %20182, %20173
  %20184 = add <4 x i32> %20183, %20176
  %20185 = add <4 x i32> %20184, %20179
  %20186 = add <4 x i32> %20185, %20181
  %20187 = add <4 x i32> %20186, %20168
  %20188 = add <4 x i32> %20187, %20171
  %20189 = add <4 x i32> %20188, %20174
  %20190 = add <4 x i32> %20189, %20177
  %t4362 = add nsw i32 %t4025, %t2590820
  %20191 = sext i32 %t4362 to i64
  %20192 = shl nsw i64 %20191, 4
  %20193 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20192
  %20194 = bitcast i8* %20193 to <8 x i8>*
  %t4363 = load <8 x i8>, <8 x i8>* %20194, align 16, !tbaa !438
  %t4364 = add nsw i32 %t4025, %t2586819
  %20195 = sext i32 %t4364 to i64
  %20196 = shl nsw i64 %20195, 4
  %20197 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20196
  %20198 = bitcast i8* %20197 to <8 x i8>*
  %t4365 = load <8 x i8>, <8 x i8>* %20198, align 16, !tbaa !438
  %t4366 = add nsw i32 %t4025, %t2582800
  %20199 = sext i32 %t4366 to i64
  %20200 = shl nsw i64 %20199, 4
  %20201 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20200
  %20202 = bitcast i8* %20201 to <8 x i8>*
  %t4367 = load <8 x i8>, <8 x i8>* %20202, align 16, !tbaa !438
  %t4368 = add nsw i32 %t4025, %t2577818
  %20203 = sext i32 %t4368 to i64
  %20204 = shl nsw i64 %20203, 4
  %20205 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20204
  %20206 = bitcast i8* %20205 to <8 x i8>*
  %t4369 = load <8 x i8>, <8 x i8>* %20206, align 16, !tbaa !438
  %t4370 = add nsw i32 %t4025, %t2573817
  %20207 = sext i32 %t4370 to i64
  %20208 = shl nsw i64 %20207, 4
  %20209 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20208
  %20210 = bitcast i8* %20209 to <8 x i8>*
  %t4371 = load <8 x i8>, <8 x i8>* %20210, align 16, !tbaa !438
  %t4372 = add nsw i32 %t4025, %t2569799
  %20211 = sext i32 %t4372 to i64
  %20212 = shl nsw i64 %20211, 4
  %20213 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20212
  %20214 = bitcast i8* %20213 to <8 x i8>*
  %t4373 = load <8 x i8>, <8 x i8>* %20214, align 16, !tbaa !438
  %t4374 = add nsw i32 %t4025, %t2564808
  %20215 = sext i32 %t4374 to i64
  %20216 = shl nsw i64 %20215, 4
  %20217 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20216
  %20218 = bitcast i8* %20217 to <8 x i8>*
  %t4375 = load <8 x i8>, <8 x i8>* %20218, align 16, !tbaa !438
  %t4376 = add nsw i32 %t4025, %t2560807
  %20219 = sext i32 %t4376 to i64
  %20220 = shl nsw i64 %20219, 4
  %20221 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20220
  %20222 = bitcast i8* %20221 to <8 x i8>*
  %t4377 = load <8 x i8>, <8 x i8>* %20222, align 16, !tbaa !438
  %t4378 = add nsw i32 %t4025, %t2556794
  %20223 = sext i32 %t4378 to i64
  %20224 = shl nsw i64 %20223, 4
  %20225 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20224
  %20226 = bitcast i8* %20225 to <8 x i8>*
  %t4379 = load <8 x i8>, <8 x i8>* %20226, align 16, !tbaa !438
  %20227 = getelementptr inbounds i8, i8* %20193, i64 8
  %20228 = bitcast i8* %20227 to <8 x i8>*
  %t4380 = load <8 x i8>, <8 x i8>* %20228, align 8, !tbaa !438
  %20229 = getelementptr inbounds i8, i8* %20197, i64 8
  %20230 = bitcast i8* %20229 to <8 x i8>*
  %t4381 = load <8 x i8>, <8 x i8>* %20230, align 8, !tbaa !438
  %20231 = getelementptr inbounds i8, i8* %20201, i64 8
  %20232 = bitcast i8* %20231 to <8 x i8>*
  %t4382 = load <8 x i8>, <8 x i8>* %20232, align 8, !tbaa !438
  %20233 = getelementptr inbounds i8, i8* %20205, i64 8
  %20234 = bitcast i8* %20233 to <8 x i8>*
  %t4383 = load <8 x i8>, <8 x i8>* %20234, align 8, !tbaa !438
  %20235 = getelementptr inbounds i8, i8* %20209, i64 8
  %20236 = bitcast i8* %20235 to <8 x i8>*
  %t4384 = load <8 x i8>, <8 x i8>* %20236, align 8, !tbaa !438
  %20237 = getelementptr inbounds i8, i8* %20213, i64 8
  %20238 = bitcast i8* %20237 to <8 x i8>*
  %t4385 = load <8 x i8>, <8 x i8>* %20238, align 8, !tbaa !438
  %20239 = getelementptr inbounds i8, i8* %20217, i64 8
  %20240 = bitcast i8* %20239 to <8 x i8>*
  %t4386 = load <8 x i8>, <8 x i8>* %20240, align 8, !tbaa !438
  %20241 = getelementptr inbounds i8, i8* %20221, i64 8
  %20242 = bitcast i8* %20241 to <8 x i8>*
  %t4387 = load <8 x i8>, <8 x i8>* %20242, align 8, !tbaa !438
  %20243 = getelementptr inbounds i8, i8* %20225, i64 8
  %20244 = bitcast i8* %20243 to <8 x i8>*
  %t4388 = load <8 x i8>, <8 x i8>* %20244, align 8, !tbaa !438
  %20245 = zext <8 x i8> %t4363 to <8 x i16>
  %20246 = bitcast <8 x i16> %20245 to <2 x i64>
  %20247 = shufflevector <2 x i64> %20246, <2 x i64> undef, <1 x i32> zeroinitializer
  %20248 = bitcast <1 x i64> %20247 to <4 x i16>
  %20249 = zext <8 x i8> %t4365 to <8 x i16>
  %20250 = bitcast <8 x i16> %20249 to <2 x i64>
  %20251 = shufflevector <2 x i64> %20250, <2 x i64> undef, <1 x i32> zeroinitializer
  %20252 = zext <8 x i8> %t4367 to <8 x i16>
  %20253 = bitcast <8 x i16> %20252 to <2 x i64>
  %20254 = shufflevector <2 x i64> %20253, <2 x i64> undef, <1 x i32> zeroinitializer
  %20255 = bitcast <1 x i64> %20254 to <4 x i16>
  %20256 = zext <8 x i8> %t4369 to <8 x i16>
  %20257 = bitcast <8 x i16> %20256 to <2 x i64>
  %20258 = shufflevector <2 x i64> %20257, <2 x i64> undef, <1 x i32> zeroinitializer
  %20259 = zext <8 x i8> %t4371 to <8 x i16>
  %20260 = bitcast <8 x i16> %20259 to <2 x i64>
  %20261 = shufflevector <2 x i64> %20260, <2 x i64> undef, <1 x i32> zeroinitializer
  %20262 = bitcast <1 x i64> %20261 to <4 x i16>
  %20263 = zext <8 x i8> %t4373 to <8 x i16>
  %20264 = bitcast <8 x i16> %20263 to <2 x i64>
  %20265 = shufflevector <2 x i64> %20264, <2 x i64> undef, <1 x i32> zeroinitializer
  %20266 = zext <8 x i8> %t4375 to <8 x i16>
  %20267 = bitcast <8 x i16> %20266 to <2 x i64>
  %20268 = shufflevector <2 x i64> %20267, <2 x i64> undef, <1 x i32> zeroinitializer
  %20269 = bitcast <1 x i64> %20268 to <4 x i16>
  %20270 = zext <8 x i8> %t4377 to <8 x i16>
  %20271 = bitcast <8 x i16> %20270 to <2 x i64>
  %20272 = shufflevector <2 x i64> %20271, <2 x i64> undef, <1 x i32> zeroinitializer
  %20273 = zext <8 x i8> %t4379 to <8 x i16>
  %20274 = bitcast <8 x i16> %20273 to <2 x i64>
  %20275 = shufflevector <2 x i64> %20274, <2 x i64> undef, <1 x i32> zeroinitializer
  %20276 = bitcast <1 x i64> %20275 to <4 x i16>
  %.cast1645 = bitcast <1 x i64> %20251 to <4 x i16>
  %20277 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1645, <4 x i16> %17097) #11
  %20278 = sext <4 x i16> %20248 to <4 x i32>
  %20279 = mul nsw <4 x i32> %20278, %17135
  %.cast1648 = bitcast <1 x i64> %20258 to <4 x i16>
  %20280 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1648, <4 x i16> %17106) #11
  %20281 = sext <4 x i16> %20255 to <4 x i32>
  %20282 = mul nsw <4 x i32> %20281, %17139
  %.cast1651 = bitcast <1 x i64> %20265 to <4 x i16>
  %20283 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1651, <4 x i16> %17115) #11
  %20284 = sext <4 x i16> %20262 to <4 x i32>
  %20285 = mul nsw <4 x i32> %20284, %17143
  %.cast1654 = bitcast <1 x i64> %20272 to <4 x i16>
  %20286 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1654, <4 x i16> %17124) #11
  %20287 = sext <4 x i16> %20269 to <4 x i32>
  %20288 = mul nsw <4 x i32> %20287, %17147
  %20289 = sext <4 x i16> %20276 to <4 x i32>
  %20290 = mul nsw <4 x i32> %20289, %17149
  %20291 = add <4 x i32> %20279, %16984
  %20292 = add <4 x i32> %20291, %20282
  %20293 = add <4 x i32> %20292, %20285
  %20294 = add <4 x i32> %20293, %20288
  %20295 = add <4 x i32> %20294, %20290
  %20296 = add <4 x i32> %20295, %20277
  %20297 = add <4 x i32> %20296, %20280
  %20298 = add <4 x i32> %20297, %20283
  %20299 = add <4 x i32> %20298, %20286
  %20300 = shufflevector <2 x i64> %20246, <2 x i64> undef, <1 x i32> <i32 1>
  %20301 = bitcast <1 x i64> %20300 to <4 x i16>
  %20302 = shufflevector <2 x i64> %20250, <2 x i64> undef, <1 x i32> <i32 1>
  %20303 = shufflevector <2 x i64> %20253, <2 x i64> undef, <1 x i32> <i32 1>
  %20304 = bitcast <1 x i64> %20303 to <4 x i16>
  %20305 = shufflevector <2 x i64> %20257, <2 x i64> undef, <1 x i32> <i32 1>
  %20306 = shufflevector <2 x i64> %20260, <2 x i64> undef, <1 x i32> <i32 1>
  %20307 = bitcast <1 x i64> %20306 to <4 x i16>
  %20308 = shufflevector <2 x i64> %20264, <2 x i64> undef, <1 x i32> <i32 1>
  %20309 = shufflevector <2 x i64> %20267, <2 x i64> undef, <1 x i32> <i32 1>
  %20310 = bitcast <1 x i64> %20309 to <4 x i16>
  %20311 = shufflevector <2 x i64> %20271, <2 x i64> undef, <1 x i32> <i32 1>
  %20312 = shufflevector <2 x i64> %20274, <2 x i64> undef, <1 x i32> <i32 1>
  %20313 = bitcast <1 x i64> %20312 to <4 x i16>
  %.cast1657 = bitcast <1 x i64> %20302 to <4 x i16>
  %20314 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1657, <4 x i16> %17164) #11
  %20315 = sext <4 x i16> %20301 to <4 x i32>
  %20316 = mul nsw <4 x i32> %20315, %17186
  %.cast1660 = bitcast <1 x i64> %20305 to <4 x i16>
  %20317 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1660, <4 x i16> %17169) #11
  %20318 = sext <4 x i16> %20304 to <4 x i32>
  %20319 = mul nsw <4 x i32> %20318, %17190
  %.cast1663 = bitcast <1 x i64> %20308 to <4 x i16>
  %20320 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1663, <4 x i16> %17174) #11
  %20321 = sext <4 x i16> %20307 to <4 x i32>
  %20322 = mul nsw <4 x i32> %20321, %17194
  %.cast1666 = bitcast <1 x i64> %20311 to <4 x i16>
  %20323 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1666, <4 x i16> %17179) #11
  %20324 = sext <4 x i16> %20310 to <4 x i32>
  %20325 = mul nsw <4 x i32> %20324, %17198
  %20326 = sext <4 x i16> %20313 to <4 x i32>
  %20327 = mul nsw <4 x i32> %20326, %17200
  %20328 = add <4 x i32> %20316, %16985
  %20329 = add <4 x i32> %20328, %20319
  %20330 = add <4 x i32> %20329, %20322
  %20331 = add <4 x i32> %20330, %20325
  %20332 = add <4 x i32> %20331, %20327
  %20333 = add <4 x i32> %20332, %20314
  %20334 = add <4 x i32> %20333, %20317
  %20335 = add <4 x i32> %20334, %20320
  %20336 = add <4 x i32> %20335, %20323
  %20337 = zext <8 x i8> %t4380 to <8 x i16>
  %20338 = bitcast <8 x i16> %20337 to <2 x i64>
  %20339 = shufflevector <2 x i64> %20338, <2 x i64> undef, <1 x i32> zeroinitializer
  %20340 = bitcast <1 x i64> %20339 to <4 x i16>
  %20341 = zext <8 x i8> %t4381 to <8 x i16>
  %20342 = bitcast <8 x i16> %20341 to <2 x i64>
  %20343 = shufflevector <2 x i64> %20342, <2 x i64> undef, <1 x i32> zeroinitializer
  %20344 = zext <8 x i8> %t4382 to <8 x i16>
  %20345 = bitcast <8 x i16> %20344 to <2 x i64>
  %20346 = shufflevector <2 x i64> %20345, <2 x i64> undef, <1 x i32> zeroinitializer
  %20347 = bitcast <1 x i64> %20346 to <4 x i16>
  %20348 = zext <8 x i8> %t4383 to <8 x i16>
  %20349 = bitcast <8 x i16> %20348 to <2 x i64>
  %20350 = shufflevector <2 x i64> %20349, <2 x i64> undef, <1 x i32> zeroinitializer
  %20351 = zext <8 x i8> %t4384 to <8 x i16>
  %20352 = bitcast <8 x i16> %20351 to <2 x i64>
  %20353 = shufflevector <2 x i64> %20352, <2 x i64> undef, <1 x i32> zeroinitializer
  %20354 = bitcast <1 x i64> %20353 to <4 x i16>
  %20355 = zext <8 x i8> %t4385 to <8 x i16>
  %20356 = bitcast <8 x i16> %20355 to <2 x i64>
  %20357 = shufflevector <2 x i64> %20356, <2 x i64> undef, <1 x i32> zeroinitializer
  %20358 = zext <8 x i8> %t4386 to <8 x i16>
  %20359 = bitcast <8 x i16> %20358 to <2 x i64>
  %20360 = shufflevector <2 x i64> %20359, <2 x i64> undef, <1 x i32> zeroinitializer
  %20361 = bitcast <1 x i64> %20360 to <4 x i16>
  %20362 = zext <8 x i8> %t4387 to <8 x i16>
  %20363 = bitcast <8 x i16> %20362 to <2 x i64>
  %20364 = shufflevector <2 x i64> %20363, <2 x i64> undef, <1 x i32> zeroinitializer
  %20365 = zext <8 x i8> %t4388 to <8 x i16>
  %20366 = bitcast <8 x i16> %20365 to <2 x i64>
  %20367 = shufflevector <2 x i64> %20366, <2 x i64> undef, <1 x i32> zeroinitializer
  %20368 = bitcast <1 x i64> %20367 to <4 x i16>
  %.cast1669 = bitcast <1 x i64> %20343 to <4 x i16>
  %20369 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1669, <4 x i16> %17217) #11
  %20370 = sext <4 x i16> %20340 to <4 x i32>
  %20371 = mul nsw <4 x i32> %20370, %17255
  %.cast1672 = bitcast <1 x i64> %20350 to <4 x i16>
  %20372 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1672, <4 x i16> %17226) #11
  %20373 = sext <4 x i16> %20347 to <4 x i32>
  %20374 = mul nsw <4 x i32> %20373, %17259
  %.cast1675 = bitcast <1 x i64> %20357 to <4 x i16>
  %20375 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1675, <4 x i16> %17235) #11
  %20376 = sext <4 x i16> %20354 to <4 x i32>
  %20377 = mul nsw <4 x i32> %20376, %17263
  %.cast1678 = bitcast <1 x i64> %20364 to <4 x i16>
  %20378 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1678, <4 x i16> %17244) #11
  %20379 = sext <4 x i16> %20361 to <4 x i32>
  %20380 = mul nsw <4 x i32> %20379, %17267
  %20381 = sext <4 x i16> %20368 to <4 x i32>
  %20382 = mul nsw <4 x i32> %20381, %17269
  %20383 = add <4 x i32> %20371, %16986
  %20384 = add <4 x i32> %20383, %20374
  %20385 = add <4 x i32> %20384, %20377
  %20386 = add <4 x i32> %20385, %20380
  %20387 = add <4 x i32> %20386, %20382
  %20388 = add <4 x i32> %20387, %20369
  %20389 = add <4 x i32> %20388, %20372
  %20390 = add <4 x i32> %20389, %20375
  %20391 = add <4 x i32> %20390, %20378
  %20392 = shufflevector <2 x i64> %20338, <2 x i64> undef, <1 x i32> <i32 1>
  %20393 = bitcast <1 x i64> %20392 to <4 x i16>
  %20394 = shufflevector <2 x i64> %20342, <2 x i64> undef, <1 x i32> <i32 1>
  %20395 = shufflevector <2 x i64> %20345, <2 x i64> undef, <1 x i32> <i32 1>
  %20396 = bitcast <1 x i64> %20395 to <4 x i16>
  %20397 = shufflevector <2 x i64> %20349, <2 x i64> undef, <1 x i32> <i32 1>
  %20398 = shufflevector <2 x i64> %20352, <2 x i64> undef, <1 x i32> <i32 1>
  %20399 = bitcast <1 x i64> %20398 to <4 x i16>
  %20400 = shufflevector <2 x i64> %20356, <2 x i64> undef, <1 x i32> <i32 1>
  %20401 = shufflevector <2 x i64> %20359, <2 x i64> undef, <1 x i32> <i32 1>
  %20402 = bitcast <1 x i64> %20401 to <4 x i16>
  %20403 = shufflevector <2 x i64> %20363, <2 x i64> undef, <1 x i32> <i32 1>
  %20404 = shufflevector <2 x i64> %20366, <2 x i64> undef, <1 x i32> <i32 1>
  %20405 = bitcast <1 x i64> %20404 to <4 x i16>
  %.cast1681 = bitcast <1 x i64> %20394 to <4 x i16>
  %20406 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1681, <4 x i16> %17284) #11
  %20407 = sext <4 x i16> %20393 to <4 x i32>
  %20408 = mul nsw <4 x i32> %20407, %17306
  %.cast1684 = bitcast <1 x i64> %20397 to <4 x i16>
  %20409 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1684, <4 x i16> %17289) #11
  %20410 = sext <4 x i16> %20396 to <4 x i32>
  %20411 = mul nsw <4 x i32> %20410, %17310
  %.cast1687 = bitcast <1 x i64> %20400 to <4 x i16>
  %20412 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1687, <4 x i16> %17294) #11
  %20413 = sext <4 x i16> %20399 to <4 x i32>
  %20414 = mul nsw <4 x i32> %20413, %17314
  %.cast1690 = bitcast <1 x i64> %20403 to <4 x i16>
  %20415 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1690, <4 x i16> %17299) #11
  %20416 = sext <4 x i16> %20402 to <4 x i32>
  %20417 = mul nsw <4 x i32> %20416, %17318
  %20418 = sext <4 x i16> %20405 to <4 x i32>
  %20419 = mul nsw <4 x i32> %20418, %17320
  %20420 = add <4 x i32> %20408, %16987
  %20421 = add <4 x i32> %20420, %20411
  %20422 = add <4 x i32> %20421, %20414
  %20423 = add <4 x i32> %20422, %20417
  %20424 = add <4 x i32> %20423, %20419
  %20425 = add <4 x i32> %20424, %20406
  %20426 = add <4 x i32> %20425, %20409
  %20427 = add <4 x i32> %20426, %20412
  %20428 = add <4 x i32> %20427, %20415
  %t4390 = add nsw i32 %t4053, %t2590820
  %20429 = sext i32 %t4390 to i64
  %20430 = shl nsw i64 %20429, 4
  %20431 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20430
  %20432 = bitcast i8* %20431 to <8 x i8>*
  %t4391 = load <8 x i8>, <8 x i8>* %20432, align 16, !tbaa !438
  %t4392 = add nsw i32 %t4053, %t2586819
  %20433 = sext i32 %t4392 to i64
  %20434 = shl nsw i64 %20433, 4
  %20435 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20434
  %20436 = bitcast i8* %20435 to <8 x i8>*
  %t4393 = load <8 x i8>, <8 x i8>* %20436, align 16, !tbaa !438
  %t4394 = add nsw i32 %t4053, %t2582800
  %20437 = sext i32 %t4394 to i64
  %20438 = shl nsw i64 %20437, 4
  %20439 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20438
  %20440 = bitcast i8* %20439 to <8 x i8>*
  %t4395 = load <8 x i8>, <8 x i8>* %20440, align 16, !tbaa !438
  %t4396 = add nsw i32 %t4053, %t2577818
  %20441 = sext i32 %t4396 to i64
  %20442 = shl nsw i64 %20441, 4
  %20443 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20442
  %20444 = bitcast i8* %20443 to <8 x i8>*
  %t4397 = load <8 x i8>, <8 x i8>* %20444, align 16, !tbaa !438
  %t4398 = add nsw i32 %t4053, %t2573817
  %20445 = sext i32 %t4398 to i64
  %20446 = shl nsw i64 %20445, 4
  %20447 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20446
  %20448 = bitcast i8* %20447 to <8 x i8>*
  %t4399 = load <8 x i8>, <8 x i8>* %20448, align 16, !tbaa !438
  %t4400 = add nsw i32 %t4053, %t2569799
  %20449 = sext i32 %t4400 to i64
  %20450 = shl nsw i64 %20449, 4
  %20451 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20450
  %20452 = bitcast i8* %20451 to <8 x i8>*
  %t4401 = load <8 x i8>, <8 x i8>* %20452, align 16, !tbaa !438
  %t4402 = add nsw i32 %t4053, %t2564808
  %20453 = sext i32 %t4402 to i64
  %20454 = shl nsw i64 %20453, 4
  %20455 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20454
  %20456 = bitcast i8* %20455 to <8 x i8>*
  %t4403 = load <8 x i8>, <8 x i8>* %20456, align 16, !tbaa !438
  %t4404 = add nsw i32 %t4053, %t2560807
  %20457 = sext i32 %t4404 to i64
  %20458 = shl nsw i64 %20457, 4
  %20459 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20458
  %20460 = bitcast i8* %20459 to <8 x i8>*
  %t4405 = load <8 x i8>, <8 x i8>* %20460, align 16, !tbaa !438
  %t4406 = add nsw i32 %t4053, %t2556794
  %20461 = sext i32 %t4406 to i64
  %20462 = shl nsw i64 %20461, 4
  %20463 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20462
  %20464 = bitcast i8* %20463 to <8 x i8>*
  %t4407 = load <8 x i8>, <8 x i8>* %20464, align 16, !tbaa !438
  %20465 = getelementptr inbounds i8, i8* %20431, i64 8
  %20466 = bitcast i8* %20465 to <8 x i8>*
  %t4408 = load <8 x i8>, <8 x i8>* %20466, align 8, !tbaa !438
  %20467 = getelementptr inbounds i8, i8* %20435, i64 8
  %20468 = bitcast i8* %20467 to <8 x i8>*
  %t4409 = load <8 x i8>, <8 x i8>* %20468, align 8, !tbaa !438
  %20469 = getelementptr inbounds i8, i8* %20439, i64 8
  %20470 = bitcast i8* %20469 to <8 x i8>*
  %t4410 = load <8 x i8>, <8 x i8>* %20470, align 8, !tbaa !438
  %20471 = getelementptr inbounds i8, i8* %20443, i64 8
  %20472 = bitcast i8* %20471 to <8 x i8>*
  %t4411 = load <8 x i8>, <8 x i8>* %20472, align 8, !tbaa !438
  %20473 = getelementptr inbounds i8, i8* %20447, i64 8
  %20474 = bitcast i8* %20473 to <8 x i8>*
  %t4412 = load <8 x i8>, <8 x i8>* %20474, align 8, !tbaa !438
  %20475 = getelementptr inbounds i8, i8* %20451, i64 8
  %20476 = bitcast i8* %20475 to <8 x i8>*
  %t4413 = load <8 x i8>, <8 x i8>* %20476, align 8, !tbaa !438
  %20477 = getelementptr inbounds i8, i8* %20455, i64 8
  %20478 = bitcast i8* %20477 to <8 x i8>*
  %t4414 = load <8 x i8>, <8 x i8>* %20478, align 8, !tbaa !438
  %20479 = getelementptr inbounds i8, i8* %20459, i64 8
  %20480 = bitcast i8* %20479 to <8 x i8>*
  %t4415 = load <8 x i8>, <8 x i8>* %20480, align 8, !tbaa !438
  %20481 = getelementptr inbounds i8, i8* %20463, i64 8
  %20482 = bitcast i8* %20481 to <8 x i8>*
  %t4416 = load <8 x i8>, <8 x i8>* %20482, align 8, !tbaa !438
  %20483 = zext <8 x i8> %t4391 to <8 x i16>
  %20484 = bitcast <8 x i16> %20483 to <2 x i64>
  %20485 = shufflevector <2 x i64> %20484, <2 x i64> undef, <1 x i32> zeroinitializer
  %20486 = bitcast <1 x i64> %20485 to <4 x i16>
  %20487 = zext <8 x i8> %t4393 to <8 x i16>
  %20488 = bitcast <8 x i16> %20487 to <2 x i64>
  %20489 = shufflevector <2 x i64> %20488, <2 x i64> undef, <1 x i32> zeroinitializer
  %20490 = zext <8 x i8> %t4395 to <8 x i16>
  %20491 = bitcast <8 x i16> %20490 to <2 x i64>
  %20492 = shufflevector <2 x i64> %20491, <2 x i64> undef, <1 x i32> zeroinitializer
  %20493 = bitcast <1 x i64> %20492 to <4 x i16>
  %20494 = zext <8 x i8> %t4397 to <8 x i16>
  %20495 = bitcast <8 x i16> %20494 to <2 x i64>
  %20496 = shufflevector <2 x i64> %20495, <2 x i64> undef, <1 x i32> zeroinitializer
  %20497 = zext <8 x i8> %t4399 to <8 x i16>
  %20498 = bitcast <8 x i16> %20497 to <2 x i64>
  %20499 = shufflevector <2 x i64> %20498, <2 x i64> undef, <1 x i32> zeroinitializer
  %20500 = bitcast <1 x i64> %20499 to <4 x i16>
  %20501 = zext <8 x i8> %t4401 to <8 x i16>
  %20502 = bitcast <8 x i16> %20501 to <2 x i64>
  %20503 = shufflevector <2 x i64> %20502, <2 x i64> undef, <1 x i32> zeroinitializer
  %20504 = zext <8 x i8> %t4403 to <8 x i16>
  %20505 = bitcast <8 x i16> %20504 to <2 x i64>
  %20506 = shufflevector <2 x i64> %20505, <2 x i64> undef, <1 x i32> zeroinitializer
  %20507 = bitcast <1 x i64> %20506 to <4 x i16>
  %20508 = zext <8 x i8> %t4405 to <8 x i16>
  %20509 = bitcast <8 x i16> %20508 to <2 x i64>
  %20510 = shufflevector <2 x i64> %20509, <2 x i64> undef, <1 x i32> zeroinitializer
  %20511 = zext <8 x i8> %t4407 to <8 x i16>
  %20512 = bitcast <8 x i16> %20511 to <2 x i64>
  %20513 = shufflevector <2 x i64> %20512, <2 x i64> undef, <1 x i32> zeroinitializer
  %20514 = bitcast <1 x i64> %20513 to <4 x i16>
  %.cast1693 = bitcast <1 x i64> %20489 to <4 x i16>
  %20515 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1693, <4 x i16> %17097) #11
  %20516 = sext <4 x i16> %20486 to <4 x i32>
  %20517 = mul nsw <4 x i32> %20516, %17135
  %.cast1696 = bitcast <1 x i64> %20496 to <4 x i16>
  %20518 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1696, <4 x i16> %17106) #11
  %20519 = sext <4 x i16> %20493 to <4 x i32>
  %20520 = mul nsw <4 x i32> %20519, %17139
  %.cast1699 = bitcast <1 x i64> %20503 to <4 x i16>
  %20521 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1699, <4 x i16> %17115) #11
  %20522 = sext <4 x i16> %20500 to <4 x i32>
  %20523 = mul nsw <4 x i32> %20522, %17143
  %.cast1702 = bitcast <1 x i64> %20510 to <4 x i16>
  %20524 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1702, <4 x i16> %17124) #11
  %20525 = sext <4 x i16> %20507 to <4 x i32>
  %20526 = mul nsw <4 x i32> %20525, %17147
  %20527 = sext <4 x i16> %20514 to <4 x i32>
  %20528 = mul nsw <4 x i32> %20527, %17149
  %20529 = add <4 x i32> %20517, %16984
  %20530 = add <4 x i32> %20529, %20520
  %20531 = add <4 x i32> %20530, %20523
  %20532 = add <4 x i32> %20531, %20526
  %20533 = add <4 x i32> %20532, %20528
  %20534 = add <4 x i32> %20533, %20515
  %20535 = add <4 x i32> %20534, %20518
  %20536 = add <4 x i32> %20535, %20521
  %20537 = add <4 x i32> %20536, %20524
  %20538 = shufflevector <2 x i64> %20484, <2 x i64> undef, <1 x i32> <i32 1>
  %20539 = bitcast <1 x i64> %20538 to <4 x i16>
  %20540 = shufflevector <2 x i64> %20488, <2 x i64> undef, <1 x i32> <i32 1>
  %20541 = shufflevector <2 x i64> %20491, <2 x i64> undef, <1 x i32> <i32 1>
  %20542 = bitcast <1 x i64> %20541 to <4 x i16>
  %20543 = shufflevector <2 x i64> %20495, <2 x i64> undef, <1 x i32> <i32 1>
  %20544 = shufflevector <2 x i64> %20498, <2 x i64> undef, <1 x i32> <i32 1>
  %20545 = bitcast <1 x i64> %20544 to <4 x i16>
  %20546 = shufflevector <2 x i64> %20502, <2 x i64> undef, <1 x i32> <i32 1>
  %20547 = shufflevector <2 x i64> %20505, <2 x i64> undef, <1 x i32> <i32 1>
  %20548 = bitcast <1 x i64> %20547 to <4 x i16>
  %20549 = shufflevector <2 x i64> %20509, <2 x i64> undef, <1 x i32> <i32 1>
  %20550 = shufflevector <2 x i64> %20512, <2 x i64> undef, <1 x i32> <i32 1>
  %20551 = bitcast <1 x i64> %20550 to <4 x i16>
  %.cast1705 = bitcast <1 x i64> %20540 to <4 x i16>
  %20552 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1705, <4 x i16> %17164) #11
  %20553 = sext <4 x i16> %20539 to <4 x i32>
  %20554 = mul nsw <4 x i32> %20553, %17186
  %.cast1708 = bitcast <1 x i64> %20543 to <4 x i16>
  %20555 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1708, <4 x i16> %17169) #11
  %20556 = sext <4 x i16> %20542 to <4 x i32>
  %20557 = mul nsw <4 x i32> %20556, %17190
  %.cast1711 = bitcast <1 x i64> %20546 to <4 x i16>
  %20558 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1711, <4 x i16> %17174) #11
  %20559 = sext <4 x i16> %20545 to <4 x i32>
  %20560 = mul nsw <4 x i32> %20559, %17194
  %.cast1714 = bitcast <1 x i64> %20549 to <4 x i16>
  %20561 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1714, <4 x i16> %17179) #11
  %20562 = sext <4 x i16> %20548 to <4 x i32>
  %20563 = mul nsw <4 x i32> %20562, %17198
  %20564 = sext <4 x i16> %20551 to <4 x i32>
  %20565 = mul nsw <4 x i32> %20564, %17200
  %20566 = add <4 x i32> %20554, %16985
  %20567 = add <4 x i32> %20566, %20557
  %20568 = add <4 x i32> %20567, %20560
  %20569 = add <4 x i32> %20568, %20563
  %20570 = add <4 x i32> %20569, %20565
  %20571 = add <4 x i32> %20570, %20552
  %20572 = add <4 x i32> %20571, %20555
  %20573 = add <4 x i32> %20572, %20558
  %20574 = add <4 x i32> %20573, %20561
  %20575 = zext <8 x i8> %t4408 to <8 x i16>
  %20576 = bitcast <8 x i16> %20575 to <2 x i64>
  %20577 = shufflevector <2 x i64> %20576, <2 x i64> undef, <1 x i32> zeroinitializer
  %20578 = bitcast <1 x i64> %20577 to <4 x i16>
  %20579 = zext <8 x i8> %t4409 to <8 x i16>
  %20580 = bitcast <8 x i16> %20579 to <2 x i64>
  %20581 = shufflevector <2 x i64> %20580, <2 x i64> undef, <1 x i32> zeroinitializer
  %20582 = zext <8 x i8> %t4410 to <8 x i16>
  %20583 = bitcast <8 x i16> %20582 to <2 x i64>
  %20584 = shufflevector <2 x i64> %20583, <2 x i64> undef, <1 x i32> zeroinitializer
  %20585 = bitcast <1 x i64> %20584 to <4 x i16>
  %20586 = zext <8 x i8> %t4411 to <8 x i16>
  %20587 = bitcast <8 x i16> %20586 to <2 x i64>
  %20588 = shufflevector <2 x i64> %20587, <2 x i64> undef, <1 x i32> zeroinitializer
  %20589 = zext <8 x i8> %t4412 to <8 x i16>
  %20590 = bitcast <8 x i16> %20589 to <2 x i64>
  %20591 = shufflevector <2 x i64> %20590, <2 x i64> undef, <1 x i32> zeroinitializer
  %20592 = bitcast <1 x i64> %20591 to <4 x i16>
  %20593 = zext <8 x i8> %t4413 to <8 x i16>
  %20594 = bitcast <8 x i16> %20593 to <2 x i64>
  %20595 = shufflevector <2 x i64> %20594, <2 x i64> undef, <1 x i32> zeroinitializer
  %20596 = zext <8 x i8> %t4414 to <8 x i16>
  %20597 = bitcast <8 x i16> %20596 to <2 x i64>
  %20598 = shufflevector <2 x i64> %20597, <2 x i64> undef, <1 x i32> zeroinitializer
  %20599 = bitcast <1 x i64> %20598 to <4 x i16>
  %20600 = zext <8 x i8> %t4415 to <8 x i16>
  %20601 = bitcast <8 x i16> %20600 to <2 x i64>
  %20602 = shufflevector <2 x i64> %20601, <2 x i64> undef, <1 x i32> zeroinitializer
  %20603 = zext <8 x i8> %t4416 to <8 x i16>
  %20604 = bitcast <8 x i16> %20603 to <2 x i64>
  %20605 = shufflevector <2 x i64> %20604, <2 x i64> undef, <1 x i32> zeroinitializer
  %20606 = bitcast <1 x i64> %20605 to <4 x i16>
  %.cast1717 = bitcast <1 x i64> %20581 to <4 x i16>
  %20607 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1717, <4 x i16> %17217) #11
  %20608 = sext <4 x i16> %20578 to <4 x i32>
  %20609 = mul nsw <4 x i32> %20608, %17255
  %.cast1720 = bitcast <1 x i64> %20588 to <4 x i16>
  %20610 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1720, <4 x i16> %17226) #11
  %20611 = sext <4 x i16> %20585 to <4 x i32>
  %20612 = mul nsw <4 x i32> %20611, %17259
  %.cast1723 = bitcast <1 x i64> %20595 to <4 x i16>
  %20613 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1723, <4 x i16> %17235) #11
  %20614 = sext <4 x i16> %20592 to <4 x i32>
  %20615 = mul nsw <4 x i32> %20614, %17263
  %.cast1726 = bitcast <1 x i64> %20602 to <4 x i16>
  %20616 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1726, <4 x i16> %17244) #11
  %20617 = sext <4 x i16> %20599 to <4 x i32>
  %20618 = mul nsw <4 x i32> %20617, %17267
  %20619 = sext <4 x i16> %20606 to <4 x i32>
  %20620 = mul nsw <4 x i32> %20619, %17269
  %20621 = add <4 x i32> %20609, %16986
  %20622 = add <4 x i32> %20621, %20612
  %20623 = add <4 x i32> %20622, %20615
  %20624 = add <4 x i32> %20623, %20618
  %20625 = add <4 x i32> %20624, %20620
  %20626 = add <4 x i32> %20625, %20607
  %20627 = add <4 x i32> %20626, %20610
  %20628 = add <4 x i32> %20627, %20613
  %20629 = add <4 x i32> %20628, %20616
  %20630 = shufflevector <2 x i64> %20576, <2 x i64> undef, <1 x i32> <i32 1>
  %20631 = bitcast <1 x i64> %20630 to <4 x i16>
  %20632 = shufflevector <2 x i64> %20580, <2 x i64> undef, <1 x i32> <i32 1>
  %20633 = shufflevector <2 x i64> %20583, <2 x i64> undef, <1 x i32> <i32 1>
  %20634 = bitcast <1 x i64> %20633 to <4 x i16>
  %20635 = shufflevector <2 x i64> %20587, <2 x i64> undef, <1 x i32> <i32 1>
  %20636 = shufflevector <2 x i64> %20590, <2 x i64> undef, <1 x i32> <i32 1>
  %20637 = bitcast <1 x i64> %20636 to <4 x i16>
  %20638 = shufflevector <2 x i64> %20594, <2 x i64> undef, <1 x i32> <i32 1>
  %20639 = shufflevector <2 x i64> %20597, <2 x i64> undef, <1 x i32> <i32 1>
  %20640 = bitcast <1 x i64> %20639 to <4 x i16>
  %20641 = shufflevector <2 x i64> %20601, <2 x i64> undef, <1 x i32> <i32 1>
  %20642 = shufflevector <2 x i64> %20604, <2 x i64> undef, <1 x i32> <i32 1>
  %20643 = bitcast <1 x i64> %20642 to <4 x i16>
  %.cast1729 = bitcast <1 x i64> %20632 to <4 x i16>
  %20644 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1729, <4 x i16> %17284) #11
  %20645 = sext <4 x i16> %20631 to <4 x i32>
  %20646 = mul nsw <4 x i32> %20645, %17306
  %.cast1732 = bitcast <1 x i64> %20635 to <4 x i16>
  %20647 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1732, <4 x i16> %17289) #11
  %20648 = sext <4 x i16> %20634 to <4 x i32>
  %20649 = mul nsw <4 x i32> %20648, %17310
  %.cast1735 = bitcast <1 x i64> %20638 to <4 x i16>
  %20650 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1735, <4 x i16> %17294) #11
  %20651 = sext <4 x i16> %20637 to <4 x i32>
  %20652 = mul nsw <4 x i32> %20651, %17314
  %.cast1738 = bitcast <1 x i64> %20641 to <4 x i16>
  %20653 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1738, <4 x i16> %17299) #11
  %20654 = sext <4 x i16> %20640 to <4 x i32>
  %20655 = mul nsw <4 x i32> %20654, %17318
  %20656 = sext <4 x i16> %20643 to <4 x i32>
  %20657 = mul nsw <4 x i32> %20656, %17320
  %20658 = add <4 x i32> %20646, %16987
  %20659 = add <4 x i32> %20658, %20649
  %20660 = add <4 x i32> %20659, %20652
  %20661 = add <4 x i32> %20660, %20655
  %20662 = add <4 x i32> %20661, %20657
  %20663 = add <4 x i32> %20662, %20644
  %20664 = add <4 x i32> %20663, %20647
  %20665 = add <4 x i32> %20664, %20650
  %20666 = add <4 x i32> %20665, %20653
  %t4418 = add nsw i32 %t4081, %t2590820
  %20667 = sext i32 %t4418 to i64
  %20668 = shl nsw i64 %20667, 4
  %20669 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20668
  %20670 = bitcast i8* %20669 to <8 x i8>*
  %t4419 = load <8 x i8>, <8 x i8>* %20670, align 16, !tbaa !438
  %t4420 = add nsw i32 %t4081, %t2586819
  %20671 = sext i32 %t4420 to i64
  %20672 = shl nsw i64 %20671, 4
  %20673 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20672
  %20674 = bitcast i8* %20673 to <8 x i8>*
  %t4421 = load <8 x i8>, <8 x i8>* %20674, align 16, !tbaa !438
  %t4422 = add nsw i32 %t4081, %t2582800
  %20675 = sext i32 %t4422 to i64
  %20676 = shl nsw i64 %20675, 4
  %20677 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20676
  %20678 = bitcast i8* %20677 to <8 x i8>*
  %t4423 = load <8 x i8>, <8 x i8>* %20678, align 16, !tbaa !438
  %t4424 = add nsw i32 %t4081, %t2577818
  %20679 = sext i32 %t4424 to i64
  %20680 = shl nsw i64 %20679, 4
  %20681 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20680
  %20682 = bitcast i8* %20681 to <8 x i8>*
  %t4425 = load <8 x i8>, <8 x i8>* %20682, align 16, !tbaa !438
  %t4426 = add nsw i32 %t4081, %t2573817
  %20683 = sext i32 %t4426 to i64
  %20684 = shl nsw i64 %20683, 4
  %20685 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20684
  %20686 = bitcast i8* %20685 to <8 x i8>*
  %t4427 = load <8 x i8>, <8 x i8>* %20686, align 16, !tbaa !438
  %t4428 = add nsw i32 %t4081, %t2569799
  %20687 = sext i32 %t4428 to i64
  %20688 = shl nsw i64 %20687, 4
  %20689 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20688
  %20690 = bitcast i8* %20689 to <8 x i8>*
  %t4429 = load <8 x i8>, <8 x i8>* %20690, align 16, !tbaa !438
  %t4430 = add nsw i32 %t4081, %t2564808
  %20691 = sext i32 %t4430 to i64
  %20692 = shl nsw i64 %20691, 4
  %20693 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20692
  %20694 = bitcast i8* %20693 to <8 x i8>*
  %t4431 = load <8 x i8>, <8 x i8>* %20694, align 16, !tbaa !438
  %t4432 = add nsw i32 %t4081, %t2560807
  %20695 = sext i32 %t4432 to i64
  %20696 = shl nsw i64 %20695, 4
  %20697 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20696
  %20698 = bitcast i8* %20697 to <8 x i8>*
  %t4433 = load <8 x i8>, <8 x i8>* %20698, align 16, !tbaa !438
  %t4434 = add nsw i32 %t4081, %t2556794
  %20699 = sext i32 %t4434 to i64
  %20700 = shl nsw i64 %20699, 4
  %20701 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20700
  %20702 = bitcast i8* %20701 to <8 x i8>*
  %t4435 = load <8 x i8>, <8 x i8>* %20702, align 16, !tbaa !438
  %20703 = getelementptr inbounds i8, i8* %20669, i64 8
  %20704 = bitcast i8* %20703 to <8 x i8>*
  %t4436 = load <8 x i8>, <8 x i8>* %20704, align 8, !tbaa !438
  %20705 = getelementptr inbounds i8, i8* %20673, i64 8
  %20706 = bitcast i8* %20705 to <8 x i8>*
  %t4437 = load <8 x i8>, <8 x i8>* %20706, align 8, !tbaa !438
  %20707 = getelementptr inbounds i8, i8* %20677, i64 8
  %20708 = bitcast i8* %20707 to <8 x i8>*
  %t4438 = load <8 x i8>, <8 x i8>* %20708, align 8, !tbaa !438
  %20709 = getelementptr inbounds i8, i8* %20681, i64 8
  %20710 = bitcast i8* %20709 to <8 x i8>*
  %t4439 = load <8 x i8>, <8 x i8>* %20710, align 8, !tbaa !438
  %20711 = getelementptr inbounds i8, i8* %20685, i64 8
  %20712 = bitcast i8* %20711 to <8 x i8>*
  %t4440 = load <8 x i8>, <8 x i8>* %20712, align 8, !tbaa !438
  %20713 = getelementptr inbounds i8, i8* %20689, i64 8
  %20714 = bitcast i8* %20713 to <8 x i8>*
  %t4441 = load <8 x i8>, <8 x i8>* %20714, align 8, !tbaa !438
  %20715 = getelementptr inbounds i8, i8* %20693, i64 8
  %20716 = bitcast i8* %20715 to <8 x i8>*
  %t4442 = load <8 x i8>, <8 x i8>* %20716, align 8, !tbaa !438
  %20717 = getelementptr inbounds i8, i8* %20697, i64 8
  %20718 = bitcast i8* %20717 to <8 x i8>*
  %t4443 = load <8 x i8>, <8 x i8>* %20718, align 8, !tbaa !438
  %20719 = getelementptr inbounds i8, i8* %20701, i64 8
  %20720 = bitcast i8* %20719 to <8 x i8>*
  %t4444 = load <8 x i8>, <8 x i8>* %20720, align 8, !tbaa !438
  %20721 = zext <8 x i8> %t4419 to <8 x i16>
  %20722 = bitcast <8 x i16> %20721 to <2 x i64>
  %20723 = shufflevector <2 x i64> %20722, <2 x i64> undef, <1 x i32> zeroinitializer
  %20724 = bitcast <1 x i64> %20723 to <4 x i16>
  %20725 = zext <8 x i8> %t4421 to <8 x i16>
  %20726 = bitcast <8 x i16> %20725 to <2 x i64>
  %20727 = shufflevector <2 x i64> %20726, <2 x i64> undef, <1 x i32> zeroinitializer
  %20728 = zext <8 x i8> %t4423 to <8 x i16>
  %20729 = bitcast <8 x i16> %20728 to <2 x i64>
  %20730 = shufflevector <2 x i64> %20729, <2 x i64> undef, <1 x i32> zeroinitializer
  %20731 = bitcast <1 x i64> %20730 to <4 x i16>
  %20732 = zext <8 x i8> %t4425 to <8 x i16>
  %20733 = bitcast <8 x i16> %20732 to <2 x i64>
  %20734 = shufflevector <2 x i64> %20733, <2 x i64> undef, <1 x i32> zeroinitializer
  %20735 = zext <8 x i8> %t4427 to <8 x i16>
  %20736 = bitcast <8 x i16> %20735 to <2 x i64>
  %20737 = shufflevector <2 x i64> %20736, <2 x i64> undef, <1 x i32> zeroinitializer
  %20738 = bitcast <1 x i64> %20737 to <4 x i16>
  %20739 = zext <8 x i8> %t4429 to <8 x i16>
  %20740 = bitcast <8 x i16> %20739 to <2 x i64>
  %20741 = shufflevector <2 x i64> %20740, <2 x i64> undef, <1 x i32> zeroinitializer
  %20742 = zext <8 x i8> %t4431 to <8 x i16>
  %20743 = bitcast <8 x i16> %20742 to <2 x i64>
  %20744 = shufflevector <2 x i64> %20743, <2 x i64> undef, <1 x i32> zeroinitializer
  %20745 = bitcast <1 x i64> %20744 to <4 x i16>
  %20746 = zext <8 x i8> %t4433 to <8 x i16>
  %20747 = bitcast <8 x i16> %20746 to <2 x i64>
  %20748 = shufflevector <2 x i64> %20747, <2 x i64> undef, <1 x i32> zeroinitializer
  %20749 = zext <8 x i8> %t4435 to <8 x i16>
  %20750 = bitcast <8 x i16> %20749 to <2 x i64>
  %20751 = shufflevector <2 x i64> %20750, <2 x i64> undef, <1 x i32> zeroinitializer
  %20752 = bitcast <1 x i64> %20751 to <4 x i16>
  %.cast1741 = bitcast <1 x i64> %20727 to <4 x i16>
  %20753 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1741, <4 x i16> %17097) #11
  %20754 = sext <4 x i16> %20724 to <4 x i32>
  %20755 = mul nsw <4 x i32> %20754, %17135
  %.cast1744 = bitcast <1 x i64> %20734 to <4 x i16>
  %20756 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1744, <4 x i16> %17106) #11
  %20757 = sext <4 x i16> %20731 to <4 x i32>
  %20758 = mul nsw <4 x i32> %20757, %17139
  %.cast1747 = bitcast <1 x i64> %20741 to <4 x i16>
  %20759 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1747, <4 x i16> %17115) #11
  %20760 = sext <4 x i16> %20738 to <4 x i32>
  %20761 = mul nsw <4 x i32> %20760, %17143
  %.cast1750 = bitcast <1 x i64> %20748 to <4 x i16>
  %20762 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1750, <4 x i16> %17124) #11
  %20763 = sext <4 x i16> %20745 to <4 x i32>
  %20764 = mul nsw <4 x i32> %20763, %17147
  %20765 = sext <4 x i16> %20752 to <4 x i32>
  %20766 = mul nsw <4 x i32> %20765, %17149
  %20767 = add <4 x i32> %20755, %16984
  %20768 = add <4 x i32> %20767, %20758
  %20769 = add <4 x i32> %20768, %20761
  %20770 = add <4 x i32> %20769, %20764
  %20771 = add <4 x i32> %20770, %20766
  %20772 = add <4 x i32> %20771, %20753
  %20773 = add <4 x i32> %20772, %20756
  %20774 = add <4 x i32> %20773, %20759
  %20775 = add <4 x i32> %20774, %20762
  %20776 = shufflevector <2 x i64> %20722, <2 x i64> undef, <1 x i32> <i32 1>
  %20777 = bitcast <1 x i64> %20776 to <4 x i16>
  %20778 = shufflevector <2 x i64> %20726, <2 x i64> undef, <1 x i32> <i32 1>
  %20779 = shufflevector <2 x i64> %20729, <2 x i64> undef, <1 x i32> <i32 1>
  %20780 = bitcast <1 x i64> %20779 to <4 x i16>
  %20781 = shufflevector <2 x i64> %20733, <2 x i64> undef, <1 x i32> <i32 1>
  %20782 = shufflevector <2 x i64> %20736, <2 x i64> undef, <1 x i32> <i32 1>
  %20783 = bitcast <1 x i64> %20782 to <4 x i16>
  %20784 = shufflevector <2 x i64> %20740, <2 x i64> undef, <1 x i32> <i32 1>
  %20785 = shufflevector <2 x i64> %20743, <2 x i64> undef, <1 x i32> <i32 1>
  %20786 = bitcast <1 x i64> %20785 to <4 x i16>
  %20787 = shufflevector <2 x i64> %20747, <2 x i64> undef, <1 x i32> <i32 1>
  %20788 = shufflevector <2 x i64> %20750, <2 x i64> undef, <1 x i32> <i32 1>
  %20789 = bitcast <1 x i64> %20788 to <4 x i16>
  %.cast1753 = bitcast <1 x i64> %20778 to <4 x i16>
  %20790 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1753, <4 x i16> %17164) #11
  %20791 = sext <4 x i16> %20777 to <4 x i32>
  %20792 = mul nsw <4 x i32> %20791, %17186
  %.cast1756 = bitcast <1 x i64> %20781 to <4 x i16>
  %20793 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1756, <4 x i16> %17169) #11
  %20794 = sext <4 x i16> %20780 to <4 x i32>
  %20795 = mul nsw <4 x i32> %20794, %17190
  %.cast1759 = bitcast <1 x i64> %20784 to <4 x i16>
  %20796 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1759, <4 x i16> %17174) #11
  %20797 = sext <4 x i16> %20783 to <4 x i32>
  %20798 = mul nsw <4 x i32> %20797, %17194
  %.cast1762 = bitcast <1 x i64> %20787 to <4 x i16>
  %20799 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1762, <4 x i16> %17179) #11
  %20800 = sext <4 x i16> %20786 to <4 x i32>
  %20801 = mul nsw <4 x i32> %20800, %17198
  %20802 = sext <4 x i16> %20789 to <4 x i32>
  %20803 = mul nsw <4 x i32> %20802, %17200
  %20804 = add <4 x i32> %20792, %16985
  %20805 = add <4 x i32> %20804, %20795
  %20806 = add <4 x i32> %20805, %20798
  %20807 = add <4 x i32> %20806, %20801
  %20808 = add <4 x i32> %20807, %20803
  %20809 = add <4 x i32> %20808, %20790
  %20810 = add <4 x i32> %20809, %20793
  %20811 = add <4 x i32> %20810, %20796
  %20812 = add <4 x i32> %20811, %20799
  %20813 = zext <8 x i8> %t4436 to <8 x i16>
  %20814 = bitcast <8 x i16> %20813 to <2 x i64>
  %20815 = shufflevector <2 x i64> %20814, <2 x i64> undef, <1 x i32> zeroinitializer
  %20816 = bitcast <1 x i64> %20815 to <4 x i16>
  %20817 = zext <8 x i8> %t4437 to <8 x i16>
  %20818 = bitcast <8 x i16> %20817 to <2 x i64>
  %20819 = shufflevector <2 x i64> %20818, <2 x i64> undef, <1 x i32> zeroinitializer
  %20820 = zext <8 x i8> %t4438 to <8 x i16>
  %20821 = bitcast <8 x i16> %20820 to <2 x i64>
  %20822 = shufflevector <2 x i64> %20821, <2 x i64> undef, <1 x i32> zeroinitializer
  %20823 = bitcast <1 x i64> %20822 to <4 x i16>
  %20824 = zext <8 x i8> %t4439 to <8 x i16>
  %20825 = bitcast <8 x i16> %20824 to <2 x i64>
  %20826 = shufflevector <2 x i64> %20825, <2 x i64> undef, <1 x i32> zeroinitializer
  %20827 = zext <8 x i8> %t4440 to <8 x i16>
  %20828 = bitcast <8 x i16> %20827 to <2 x i64>
  %20829 = shufflevector <2 x i64> %20828, <2 x i64> undef, <1 x i32> zeroinitializer
  %20830 = bitcast <1 x i64> %20829 to <4 x i16>
  %20831 = zext <8 x i8> %t4441 to <8 x i16>
  %20832 = bitcast <8 x i16> %20831 to <2 x i64>
  %20833 = shufflevector <2 x i64> %20832, <2 x i64> undef, <1 x i32> zeroinitializer
  %20834 = zext <8 x i8> %t4442 to <8 x i16>
  %20835 = bitcast <8 x i16> %20834 to <2 x i64>
  %20836 = shufflevector <2 x i64> %20835, <2 x i64> undef, <1 x i32> zeroinitializer
  %20837 = bitcast <1 x i64> %20836 to <4 x i16>
  %20838 = zext <8 x i8> %t4443 to <8 x i16>
  %20839 = bitcast <8 x i16> %20838 to <2 x i64>
  %20840 = shufflevector <2 x i64> %20839, <2 x i64> undef, <1 x i32> zeroinitializer
  %20841 = zext <8 x i8> %t4444 to <8 x i16>
  %20842 = bitcast <8 x i16> %20841 to <2 x i64>
  %20843 = shufflevector <2 x i64> %20842, <2 x i64> undef, <1 x i32> zeroinitializer
  %20844 = bitcast <1 x i64> %20843 to <4 x i16>
  %.cast1765 = bitcast <1 x i64> %20819 to <4 x i16>
  %20845 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1765, <4 x i16> %17217) #11
  %20846 = sext <4 x i16> %20816 to <4 x i32>
  %20847 = mul nsw <4 x i32> %20846, %17255
  %.cast1768 = bitcast <1 x i64> %20826 to <4 x i16>
  %20848 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1768, <4 x i16> %17226) #11
  %20849 = sext <4 x i16> %20823 to <4 x i32>
  %20850 = mul nsw <4 x i32> %20849, %17259
  %.cast1771 = bitcast <1 x i64> %20833 to <4 x i16>
  %20851 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1771, <4 x i16> %17235) #11
  %20852 = sext <4 x i16> %20830 to <4 x i32>
  %20853 = mul nsw <4 x i32> %20852, %17263
  %.cast1774 = bitcast <1 x i64> %20840 to <4 x i16>
  %20854 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1774, <4 x i16> %17244) #11
  %20855 = sext <4 x i16> %20837 to <4 x i32>
  %20856 = mul nsw <4 x i32> %20855, %17267
  %20857 = sext <4 x i16> %20844 to <4 x i32>
  %20858 = mul nsw <4 x i32> %20857, %17269
  %20859 = add <4 x i32> %20847, %16986
  %20860 = add <4 x i32> %20859, %20850
  %20861 = add <4 x i32> %20860, %20853
  %20862 = add <4 x i32> %20861, %20856
  %20863 = add <4 x i32> %20862, %20858
  %20864 = add <4 x i32> %20863, %20845
  %20865 = add <4 x i32> %20864, %20848
  %20866 = add <4 x i32> %20865, %20851
  %20867 = add <4 x i32> %20866, %20854
  %20868 = shufflevector <2 x i64> %20814, <2 x i64> undef, <1 x i32> <i32 1>
  %20869 = bitcast <1 x i64> %20868 to <4 x i16>
  %20870 = shufflevector <2 x i64> %20818, <2 x i64> undef, <1 x i32> <i32 1>
  %20871 = shufflevector <2 x i64> %20821, <2 x i64> undef, <1 x i32> <i32 1>
  %20872 = bitcast <1 x i64> %20871 to <4 x i16>
  %20873 = shufflevector <2 x i64> %20825, <2 x i64> undef, <1 x i32> <i32 1>
  %20874 = shufflevector <2 x i64> %20828, <2 x i64> undef, <1 x i32> <i32 1>
  %20875 = bitcast <1 x i64> %20874 to <4 x i16>
  %20876 = shufflevector <2 x i64> %20832, <2 x i64> undef, <1 x i32> <i32 1>
  %20877 = shufflevector <2 x i64> %20835, <2 x i64> undef, <1 x i32> <i32 1>
  %20878 = bitcast <1 x i64> %20877 to <4 x i16>
  %20879 = shufflevector <2 x i64> %20839, <2 x i64> undef, <1 x i32> <i32 1>
  %20880 = shufflevector <2 x i64> %20842, <2 x i64> undef, <1 x i32> <i32 1>
  %20881 = bitcast <1 x i64> %20880 to <4 x i16>
  %.cast1777 = bitcast <1 x i64> %20870 to <4 x i16>
  %20882 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1777, <4 x i16> %17284) #11
  %20883 = sext <4 x i16> %20869 to <4 x i32>
  %20884 = mul nsw <4 x i32> %20883, %17306
  %.cast1780 = bitcast <1 x i64> %20873 to <4 x i16>
  %20885 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1780, <4 x i16> %17289) #11
  %20886 = sext <4 x i16> %20872 to <4 x i32>
  %20887 = mul nsw <4 x i32> %20886, %17310
  %.cast1783 = bitcast <1 x i64> %20876 to <4 x i16>
  %20888 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1783, <4 x i16> %17294) #11
  %20889 = sext <4 x i16> %20875 to <4 x i32>
  %20890 = mul nsw <4 x i32> %20889, %17314
  %.cast1786 = bitcast <1 x i64> %20879 to <4 x i16>
  %20891 = call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %.cast1786, <4 x i16> %17299) #11
  %20892 = sext <4 x i16> %20878 to <4 x i32>
  %20893 = mul nsw <4 x i32> %20892, %17318
  %20894 = sext <4 x i16> %20881 to <4 x i32>
  %20895 = mul nsw <4 x i32> %20894, %17320
  %20896 = add <4 x i32> %20884, %16987
  %20897 = add <4 x i32> %20896, %20887
  %20898 = add <4 x i32> %20897, %20890
  %20899 = add <4 x i32> %20898, %20893
  %20900 = add <4 x i32> %20899, %20895
  %20901 = add <4 x i32> %20900, %20882
  %20902 = add <4 x i32> %20901, %20885
  %20903 = add <4 x i32> %20902, %20888
  %20904 = add <4 x i32> %20903, %20891
  br label %"consume convolved873"

next_bb839:                                       ; preds = %"for output.s0.x.xo833"
  br i1 %16470, label %"for convolved.s1.r19$y845.preheader", label %"consume convolved873", !prof !391

"for convolved.s1.r19$y845.preheader":            ; preds = %next_bb839
  %20905 = add nsw i32 %17037, 1
  %20906 = mul nsw i32 %20905, %stride_x
  %20907 = add nsw i32 %17037, 2
  %20908 = mul nsw i32 %20907, %stride_x
  %20909 = add nsw i32 %17037, 3
  %20910 = mul nsw i32 %20909, %stride_x
  %20911 = mul nsw i32 %17037, %stride_x
  %20912 = sub nsw i32 %20910, %16712
  %20913 = sub nsw i32 %20908, %16712
  %20914 = sub nsw i32 %20906, %16712
  %20915 = sub nsw i32 %20911, %16712
  br i1 %16468, label %"for convolved.s1.r19$y845.us", label %"consume convolved873", !prof !391

"for convolved.s1.r19$y845.us":                   ; preds = %"for convolved.s1.r19$y845.preheader", %"end for convolved.s1.r19$x871.loopexit.us"
  %indvars.iv6423 = phi i64 [ %indvars.iv.next6424, %"end for convolved.s1.r19$x871.loopexit.us" ], [ 0, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3624.12.us = phi <4 x i32> [ %21429, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3568.12.us = phi <4 x i32> [ %21424, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3512.12.us = phi <4 x i32> [ %21417, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3456.12.us = phi <4 x i32> [ %21412, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3400.12.us = phi <4 x i32> [ %21399, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3344.12.us = phi <4 x i32> [ %21394, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3288.12.us = phi <4 x i32> [ %21387, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3232.12.us = phi <4 x i32> [ %21382, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3176.12.us = phi <4 x i32> [ %21369, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3120.12.us = phi <4 x i32> [ %21364, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3064.12.us = phi <4 x i32> [ %21357, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.3008.12.us = phi <4 x i32> [ %21352, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2952.12.us = phi <4 x i32> [ %21339, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2896.12.us = phi <4 x i32> [ %21334, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2840.12.us = phi <4 x i32> [ %21327, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2784.12.us = phi <4 x i32> [ %21322, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2728.12.us = phi <4 x i32> [ %21309, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2672.12.us = phi <4 x i32> [ %21304, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2616.12.us = phi <4 x i32> [ %21297, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2560.12.us = phi <4 x i32> [ %21292, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2504.12.us = phi <4 x i32> [ %21279, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2448.12.us = phi <4 x i32> [ %21274, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2392.12.us = phi <4 x i32> [ %21267, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2336.12.us = phi <4 x i32> [ %21262, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2280.12.us = phi <4 x i32> [ %21249, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2224.12.us = phi <4 x i32> [ %21244, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2168.12.us = phi <4 x i32> [ %21237, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2112.12.us = phi <4 x i32> [ %21232, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2056.12.us = phi <4 x i32> [ %21219, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.2000.12.us = phi <4 x i32> [ %21214, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1944.12.us = phi <4 x i32> [ %21207, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1888.12.us = phi <4 x i32> [ %21202, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1832.12.us = phi <4 x i32> [ %21189, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1776.12.us = phi <4 x i32> [ %21184, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1720.12.us = phi <4 x i32> [ %21177, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1664.12.us = phi <4 x i32> [ %21172, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1608.12.us = phi <4 x i32> [ %21159, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1552.12.us = phi <4 x i32> [ %21154, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1496.12.us = phi <4 x i32> [ %21147, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1440.12.us = phi <4 x i32> [ %21142, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1384.12.us = phi <4 x i32> [ %21129, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1328.12.us = phi <4 x i32> [ %21124, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1272.12.us = phi <4 x i32> [ %21117, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1216.12.us = phi <4 x i32> [ %21112, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1160.12.us = phi <4 x i32> [ %21099, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1104.12.us = phi <4 x i32> [ %21094, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.1048.12.us = phi <4 x i32> [ %21087, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.992.12.us = phi <4 x i32> [ %21082, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.930.16.us = phi <4 x i32> [ %21069, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.868.16.us = phi <4 x i32> [ %21064, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.806.16.us = phi <4 x i32> [ %21057, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.744.16.us = phi <4 x i32> [ %21052, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.682.16.us = phi <4 x i32> [ %21039, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.620.16.us = phi <4 x i32> [ %21034, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.558.16.us = phi <4 x i32> [ %21027, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.496.16.us = phi <4 x i32> [ %21022, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.434.16.us = phi <4 x i32> [ %21009, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.372.16.us = phi <4 x i32> [ %21004, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.310.16.us = phi <4 x i32> [ %20997, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.248.16.us = phi <4 x i32> [ %20992, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.186.16.us = phi <4 x i32> [ %20979, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16987, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.124.16.us = phi <4 x i32> [ %20970, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16986, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.62.16.us = phi <4 x i32> [ %20959, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16985, %"for convolved.s1.r19$y845.preheader" ]
  %convolved944.sroa.0.16.us = phi <4 x i32> [ %20950, %"end for convolved.s1.r19$x871.loopexit.us" ], [ %16984, %"for convolved.s1.r19$y845.preheader" ]
  %20916 = mul i64 %indvars.iv6423, %16815
  %20917 = mul nsw i64 %indvars.iv6423, %16816
  %20918 = trunc i64 %20916 to i32
  %20919 = add i32 %17029, %20918
  %20920 = mul i32 %20919, %16718
  %20921 = trunc i64 %20916 to i32
  %20922 = add i32 %17026, %20921
  %20923 = mul i32 %20922, %16718
  %20924 = trunc i64 %20916 to i32
  %20925 = add i32 %17027, %20924
  %20926 = mul i32 %20925, %16718
  %20927 = trunc i64 %20916 to i32
  %20928 = add i32 %17028, %20927
  %20929 = mul i32 %20928, %16718
  br label %"for convolved.s1.r19$x870.us"

"for convolved.s1.r19$x870.us":                   ; preds = %"for convolved.s1.r19$y845.us", %"for convolved.s1.r19$x870.us"
  %indvars.iv6420 = phi i64 [ 0, %"for convolved.s1.r19$y845.us" ], [ %indvars.iv.next6421, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3624.13.us = phi <4 x i32> [ %convolved944.sroa.3624.12.us, %"for convolved.s1.r19$y845.us" ], [ %21429, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3568.13.us = phi <4 x i32> [ %convolved944.sroa.3568.12.us, %"for convolved.s1.r19$y845.us" ], [ %21424, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3512.13.us = phi <4 x i32> [ %convolved944.sroa.3512.12.us, %"for convolved.s1.r19$y845.us" ], [ %21417, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3456.13.us = phi <4 x i32> [ %convolved944.sroa.3456.12.us, %"for convolved.s1.r19$y845.us" ], [ %21412, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3400.13.us = phi <4 x i32> [ %convolved944.sroa.3400.12.us, %"for convolved.s1.r19$y845.us" ], [ %21399, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3344.13.us = phi <4 x i32> [ %convolved944.sroa.3344.12.us, %"for convolved.s1.r19$y845.us" ], [ %21394, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3288.13.us = phi <4 x i32> [ %convolved944.sroa.3288.12.us, %"for convolved.s1.r19$y845.us" ], [ %21387, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3232.13.us = phi <4 x i32> [ %convolved944.sroa.3232.12.us, %"for convolved.s1.r19$y845.us" ], [ %21382, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3176.13.us = phi <4 x i32> [ %convolved944.sroa.3176.12.us, %"for convolved.s1.r19$y845.us" ], [ %21369, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3120.13.us = phi <4 x i32> [ %convolved944.sroa.3120.12.us, %"for convolved.s1.r19$y845.us" ], [ %21364, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3064.13.us = phi <4 x i32> [ %convolved944.sroa.3064.12.us, %"for convolved.s1.r19$y845.us" ], [ %21357, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.3008.13.us = phi <4 x i32> [ %convolved944.sroa.3008.12.us, %"for convolved.s1.r19$y845.us" ], [ %21352, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2952.13.us = phi <4 x i32> [ %convolved944.sroa.2952.12.us, %"for convolved.s1.r19$y845.us" ], [ %21339, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2896.13.us = phi <4 x i32> [ %convolved944.sroa.2896.12.us, %"for convolved.s1.r19$y845.us" ], [ %21334, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2840.13.us = phi <4 x i32> [ %convolved944.sroa.2840.12.us, %"for convolved.s1.r19$y845.us" ], [ %21327, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2784.13.us = phi <4 x i32> [ %convolved944.sroa.2784.12.us, %"for convolved.s1.r19$y845.us" ], [ %21322, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2728.13.us = phi <4 x i32> [ %convolved944.sroa.2728.12.us, %"for convolved.s1.r19$y845.us" ], [ %21309, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2672.13.us = phi <4 x i32> [ %convolved944.sroa.2672.12.us, %"for convolved.s1.r19$y845.us" ], [ %21304, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2616.13.us = phi <4 x i32> [ %convolved944.sroa.2616.12.us, %"for convolved.s1.r19$y845.us" ], [ %21297, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2560.13.us = phi <4 x i32> [ %convolved944.sroa.2560.12.us, %"for convolved.s1.r19$y845.us" ], [ %21292, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2504.13.us = phi <4 x i32> [ %convolved944.sroa.2504.12.us, %"for convolved.s1.r19$y845.us" ], [ %21279, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2448.13.us = phi <4 x i32> [ %convolved944.sroa.2448.12.us, %"for convolved.s1.r19$y845.us" ], [ %21274, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2392.13.us = phi <4 x i32> [ %convolved944.sroa.2392.12.us, %"for convolved.s1.r19$y845.us" ], [ %21267, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2336.13.us = phi <4 x i32> [ %convolved944.sroa.2336.12.us, %"for convolved.s1.r19$y845.us" ], [ %21262, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2280.13.us = phi <4 x i32> [ %convolved944.sroa.2280.12.us, %"for convolved.s1.r19$y845.us" ], [ %21249, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2224.13.us = phi <4 x i32> [ %convolved944.sroa.2224.12.us, %"for convolved.s1.r19$y845.us" ], [ %21244, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2168.13.us = phi <4 x i32> [ %convolved944.sroa.2168.12.us, %"for convolved.s1.r19$y845.us" ], [ %21237, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2112.13.us = phi <4 x i32> [ %convolved944.sroa.2112.12.us, %"for convolved.s1.r19$y845.us" ], [ %21232, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2056.13.us = phi <4 x i32> [ %convolved944.sroa.2056.12.us, %"for convolved.s1.r19$y845.us" ], [ %21219, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.2000.13.us = phi <4 x i32> [ %convolved944.sroa.2000.12.us, %"for convolved.s1.r19$y845.us" ], [ %21214, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1944.13.us = phi <4 x i32> [ %convolved944.sroa.1944.12.us, %"for convolved.s1.r19$y845.us" ], [ %21207, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1888.13.us = phi <4 x i32> [ %convolved944.sroa.1888.12.us, %"for convolved.s1.r19$y845.us" ], [ %21202, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1832.13.us = phi <4 x i32> [ %convolved944.sroa.1832.12.us, %"for convolved.s1.r19$y845.us" ], [ %21189, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1776.13.us = phi <4 x i32> [ %convolved944.sroa.1776.12.us, %"for convolved.s1.r19$y845.us" ], [ %21184, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1720.13.us = phi <4 x i32> [ %convolved944.sroa.1720.12.us, %"for convolved.s1.r19$y845.us" ], [ %21177, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1664.13.us = phi <4 x i32> [ %convolved944.sroa.1664.12.us, %"for convolved.s1.r19$y845.us" ], [ %21172, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1608.13.us = phi <4 x i32> [ %convolved944.sroa.1608.12.us, %"for convolved.s1.r19$y845.us" ], [ %21159, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1552.13.us = phi <4 x i32> [ %convolved944.sroa.1552.12.us, %"for convolved.s1.r19$y845.us" ], [ %21154, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1496.13.us = phi <4 x i32> [ %convolved944.sroa.1496.12.us, %"for convolved.s1.r19$y845.us" ], [ %21147, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1440.13.us = phi <4 x i32> [ %convolved944.sroa.1440.12.us, %"for convolved.s1.r19$y845.us" ], [ %21142, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1384.13.us = phi <4 x i32> [ %convolved944.sroa.1384.12.us, %"for convolved.s1.r19$y845.us" ], [ %21129, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1328.13.us = phi <4 x i32> [ %convolved944.sroa.1328.12.us, %"for convolved.s1.r19$y845.us" ], [ %21124, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1272.13.us = phi <4 x i32> [ %convolved944.sroa.1272.12.us, %"for convolved.s1.r19$y845.us" ], [ %21117, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1216.13.us = phi <4 x i32> [ %convolved944.sroa.1216.12.us, %"for convolved.s1.r19$y845.us" ], [ %21112, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1160.13.us = phi <4 x i32> [ %convolved944.sroa.1160.12.us, %"for convolved.s1.r19$y845.us" ], [ %21099, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1104.13.us = phi <4 x i32> [ %convolved944.sroa.1104.12.us, %"for convolved.s1.r19$y845.us" ], [ %21094, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.1048.13.us = phi <4 x i32> [ %convolved944.sroa.1048.12.us, %"for convolved.s1.r19$y845.us" ], [ %21087, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.992.13.us = phi <4 x i32> [ %convolved944.sroa.992.12.us, %"for convolved.s1.r19$y845.us" ], [ %21082, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.930.17.us = phi <4 x i32> [ %convolved944.sroa.930.16.us, %"for convolved.s1.r19$y845.us" ], [ %21069, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.868.17.us = phi <4 x i32> [ %convolved944.sroa.868.16.us, %"for convolved.s1.r19$y845.us" ], [ %21064, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.806.17.us = phi <4 x i32> [ %convolved944.sroa.806.16.us, %"for convolved.s1.r19$y845.us" ], [ %21057, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.744.17.us = phi <4 x i32> [ %convolved944.sroa.744.16.us, %"for convolved.s1.r19$y845.us" ], [ %21052, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.682.17.us = phi <4 x i32> [ %convolved944.sroa.682.16.us, %"for convolved.s1.r19$y845.us" ], [ %21039, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.620.17.us = phi <4 x i32> [ %convolved944.sroa.620.16.us, %"for convolved.s1.r19$y845.us" ], [ %21034, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.558.17.us = phi <4 x i32> [ %convolved944.sroa.558.16.us, %"for convolved.s1.r19$y845.us" ], [ %21027, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.496.17.us = phi <4 x i32> [ %convolved944.sroa.496.16.us, %"for convolved.s1.r19$y845.us" ], [ %21022, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.434.17.us = phi <4 x i32> [ %convolved944.sroa.434.16.us, %"for convolved.s1.r19$y845.us" ], [ %21009, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.372.17.us = phi <4 x i32> [ %convolved944.sroa.372.16.us, %"for convolved.s1.r19$y845.us" ], [ %21004, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.310.17.us = phi <4 x i32> [ %convolved944.sroa.310.16.us, %"for convolved.s1.r19$y845.us" ], [ %20997, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.248.17.us = phi <4 x i32> [ %convolved944.sroa.248.16.us, %"for convolved.s1.r19$y845.us" ], [ %20992, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.186.17.us = phi <4 x i32> [ %convolved944.sroa.186.16.us, %"for convolved.s1.r19$y845.us" ], [ %20979, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.124.17.us = phi <4 x i32> [ %convolved944.sroa.124.16.us, %"for convolved.s1.r19$y845.us" ], [ %20970, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.62.17.us = phi <4 x i32> [ %convolved944.sroa.62.16.us, %"for convolved.s1.r19$y845.us" ], [ %20959, %"for convolved.s1.r19$x870.us" ]
  %convolved944.sroa.0.17.us = phi <4 x i32> [ %convolved944.sroa.0.16.us, %"for convolved.s1.r19$y845.us" ], [ %20950, %"for convolved.s1.r19$x870.us" ]
  %20930 = add nsw i64 %indvars.iv6420, %20917
  %20931 = trunc i64 %indvars.iv6420 to i32
  %20932 = mul i32 %20931, %a614
  %t2631857.us = add i32 %20932, %20920
  %t4518.us = add i32 %t2631857.us, %20915
  %20933 = sext i32 %t4518.us to i64
  %20934 = shl nsw i64 %20933, 4
  %20935 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20934
  %20936 = bitcast i8* %20935 to <8 x i8>*
  %t4519.us = load <8 x i8>, <8 x i8>* %20936, align 16, !tbaa !438
  %20937 = getelementptr inbounds i8, i8* %20935, i64 8
  %20938 = bitcast i8* %20937 to <8 x i8>*
  %t4520.us = load <8 x i8>, <8 x i8>* %20938, align 8, !tbaa !438
  %20939 = shl nsw i64 %20930, 4
  %20940 = getelementptr inbounds i16, i16* %filter_zeroed618, i64 %20939
  %20941 = bitcast i16* %20940 to <4 x i16>*
  %20942 = load <4 x i16>, <4 x i16>* %20941, align 16, !tbaa !395
  %20943 = zext <8 x i8> %t4519.us to <8 x i16>
  %20944 = bitcast <8 x i16> %20943 to <2 x i64>
  %20945 = shufflevector <2 x i64> %20944, <2 x i64> undef, <1 x i32> zeroinitializer
  %20946 = bitcast <1 x i64> %20945 to <4 x i16>
  %20947 = sext <4 x i16> %20942 to <4 x i32>
  %20948 = sext <4 x i16> %20946 to <4 x i32>
  %20949 = mul nsw <4 x i32> %20948, %20947
  %20950 = add <4 x i32> %20949, %convolved944.sroa.0.17.us
  %20951 = getelementptr inbounds i16, i16* %20940, i64 4
  %20952 = bitcast i16* %20951 to <4 x i16>*
  %20953 = load <4 x i16>, <4 x i16>* %20952, align 8, !tbaa !395
  %20954 = shufflevector <2 x i64> %20944, <2 x i64> undef, <1 x i32> <i32 1>
  %20955 = bitcast <1 x i64> %20954 to <4 x i16>
  %20956 = sext <4 x i16> %20953 to <4 x i32>
  %20957 = sext <4 x i16> %20955 to <4 x i32>
  %20958 = mul nsw <4 x i32> %20957, %20956
  %20959 = add <4 x i32> %20958, %convolved944.sroa.62.17.us
  %20960 = getelementptr inbounds i16, i16* %20940, i64 8
  %20961 = bitcast i16* %20960 to <4 x i16>*
  %20962 = load <4 x i16>, <4 x i16>* %20961, align 16, !tbaa !395
  %20963 = zext <8 x i8> %t4520.us to <8 x i16>
  %20964 = bitcast <8 x i16> %20963 to <2 x i64>
  %20965 = shufflevector <2 x i64> %20964, <2 x i64> undef, <1 x i32> zeroinitializer
  %20966 = bitcast <1 x i64> %20965 to <4 x i16>
  %20967 = sext <4 x i16> %20962 to <4 x i32>
  %20968 = sext <4 x i16> %20966 to <4 x i32>
  %20969 = mul nsw <4 x i32> %20968, %20967
  %20970 = add <4 x i32> %20969, %convolved944.sroa.124.17.us
  %20971 = getelementptr inbounds i16, i16* %20940, i64 12
  %20972 = bitcast i16* %20971 to <4 x i16>*
  %20973 = load <4 x i16>, <4 x i16>* %20972, align 8, !tbaa !395
  %20974 = shufflevector <2 x i64> %20964, <2 x i64> undef, <1 x i32> <i32 1>
  %20975 = bitcast <1 x i64> %20974 to <4 x i16>
  %20976 = sext <4 x i16> %20973 to <4 x i32>
  %20977 = sext <4 x i16> %20975 to <4 x i32>
  %20978 = mul nsw <4 x i32> %20976, %20977
  %20979 = add <4 x i32> %20978, %convolved944.sroa.186.17.us
  %t4522.us = add i32 %t2631857.us, %20914
  %20980 = sext i32 %t4522.us to i64
  %20981 = shl nsw i64 %20980, 4
  %20982 = getelementptr inbounds i8, i8* %resampled_input681, i64 %20981
  %20983 = bitcast i8* %20982 to <8 x i8>*
  %t4523.us = load <8 x i8>, <8 x i8>* %20983, align 16, !tbaa !438
  %20984 = getelementptr inbounds i8, i8* %20982, i64 8
  %20985 = bitcast i8* %20984 to <8 x i8>*
  %t4524.us = load <8 x i8>, <8 x i8>* %20985, align 8, !tbaa !438
  %20986 = zext <8 x i8> %t4523.us to <8 x i16>
  %20987 = bitcast <8 x i16> %20986 to <2 x i64>
  %20988 = shufflevector <2 x i64> %20987, <2 x i64> undef, <1 x i32> zeroinitializer
  %20989 = bitcast <1 x i64> %20988 to <4 x i16>
  %20990 = sext <4 x i16> %20989 to <4 x i32>
  %20991 = mul nsw <4 x i32> %20990, %20947
  %20992 = add <4 x i32> %20991, %convolved944.sroa.248.17.us
  %20993 = shufflevector <2 x i64> %20987, <2 x i64> undef, <1 x i32> <i32 1>
  %20994 = bitcast <1 x i64> %20993 to <4 x i16>
  %20995 = sext <4 x i16> %20994 to <4 x i32>
  %20996 = mul nsw <4 x i32> %20995, %20956
  %20997 = add <4 x i32> %20996, %convolved944.sroa.310.17.us
  %20998 = zext <8 x i8> %t4524.us to <8 x i16>
  %20999 = bitcast <8 x i16> %20998 to <2 x i64>
  %21000 = shufflevector <2 x i64> %20999, <2 x i64> undef, <1 x i32> zeroinitializer
  %21001 = bitcast <1 x i64> %21000 to <4 x i16>
  %21002 = sext <4 x i16> %21001 to <4 x i32>
  %21003 = mul nsw <4 x i32> %21002, %20967
  %21004 = add <4 x i32> %21003, %convolved944.sroa.372.17.us
  %21005 = shufflevector <2 x i64> %20999, <2 x i64> undef, <1 x i32> <i32 1>
  %21006 = bitcast <1 x i64> %21005 to <4 x i16>
  %21007 = sext <4 x i16> %21006 to <4 x i32>
  %21008 = mul nsw <4 x i32> %21007, %20976
  %21009 = add <4 x i32> %21008, %convolved944.sroa.434.17.us
  %t4526.us = add i32 %t2631857.us, %20913
  %21010 = sext i32 %t4526.us to i64
  %21011 = shl nsw i64 %21010, 4
  %21012 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21011
  %21013 = bitcast i8* %21012 to <8 x i8>*
  %t4527.us = load <8 x i8>, <8 x i8>* %21013, align 16, !tbaa !438
  %21014 = getelementptr inbounds i8, i8* %21012, i64 8
  %21015 = bitcast i8* %21014 to <8 x i8>*
  %t4528.us = load <8 x i8>, <8 x i8>* %21015, align 8, !tbaa !438
  %21016 = zext <8 x i8> %t4527.us to <8 x i16>
  %21017 = bitcast <8 x i16> %21016 to <2 x i64>
  %21018 = shufflevector <2 x i64> %21017, <2 x i64> undef, <1 x i32> zeroinitializer
  %21019 = bitcast <1 x i64> %21018 to <4 x i16>
  %21020 = sext <4 x i16> %21019 to <4 x i32>
  %21021 = mul nsw <4 x i32> %21020, %20947
  %21022 = add <4 x i32> %21021, %convolved944.sroa.496.17.us
  %21023 = shufflevector <2 x i64> %21017, <2 x i64> undef, <1 x i32> <i32 1>
  %21024 = bitcast <1 x i64> %21023 to <4 x i16>
  %21025 = sext <4 x i16> %21024 to <4 x i32>
  %21026 = mul nsw <4 x i32> %21025, %20956
  %21027 = add <4 x i32> %21026, %convolved944.sroa.558.17.us
  %21028 = zext <8 x i8> %t4528.us to <8 x i16>
  %21029 = bitcast <8 x i16> %21028 to <2 x i64>
  %21030 = shufflevector <2 x i64> %21029, <2 x i64> undef, <1 x i32> zeroinitializer
  %21031 = bitcast <1 x i64> %21030 to <4 x i16>
  %21032 = sext <4 x i16> %21031 to <4 x i32>
  %21033 = mul nsw <4 x i32> %21032, %20967
  %21034 = add <4 x i32> %21033, %convolved944.sroa.620.17.us
  %21035 = shufflevector <2 x i64> %21029, <2 x i64> undef, <1 x i32> <i32 1>
  %21036 = bitcast <1 x i64> %21035 to <4 x i16>
  %21037 = sext <4 x i16> %21036 to <4 x i32>
  %21038 = mul nsw <4 x i32> %21037, %20976
  %21039 = add <4 x i32> %21038, %convolved944.sroa.682.17.us
  %t4530.us = add i32 %t2631857.us, %20912
  %21040 = sext i32 %t4530.us to i64
  %21041 = shl nsw i64 %21040, 4
  %21042 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21041
  %21043 = bitcast i8* %21042 to <8 x i8>*
  %t4531.us = load <8 x i8>, <8 x i8>* %21043, align 16, !tbaa !438
  %21044 = getelementptr inbounds i8, i8* %21042, i64 8
  %21045 = bitcast i8* %21044 to <8 x i8>*
  %t4532.us = load <8 x i8>, <8 x i8>* %21045, align 8, !tbaa !438
  %21046 = zext <8 x i8> %t4531.us to <8 x i16>
  %21047 = bitcast <8 x i16> %21046 to <2 x i64>
  %21048 = shufflevector <2 x i64> %21047, <2 x i64> undef, <1 x i32> zeroinitializer
  %21049 = bitcast <1 x i64> %21048 to <4 x i16>
  %21050 = sext <4 x i16> %21049 to <4 x i32>
  %21051 = mul nsw <4 x i32> %21050, %20947
  %21052 = add <4 x i32> %21051, %convolved944.sroa.744.17.us
  %21053 = shufflevector <2 x i64> %21047, <2 x i64> undef, <1 x i32> <i32 1>
  %21054 = bitcast <1 x i64> %21053 to <4 x i16>
  %21055 = sext <4 x i16> %21054 to <4 x i32>
  %21056 = mul nsw <4 x i32> %21055, %20956
  %21057 = add <4 x i32> %21056, %convolved944.sroa.806.17.us
  %21058 = zext <8 x i8> %t4532.us to <8 x i16>
  %21059 = bitcast <8 x i16> %21058 to <2 x i64>
  %21060 = shufflevector <2 x i64> %21059, <2 x i64> undef, <1 x i32> zeroinitializer
  %21061 = bitcast <1 x i64> %21060 to <4 x i16>
  %21062 = sext <4 x i16> %21061 to <4 x i32>
  %21063 = mul nsw <4 x i32> %21062, %20967
  %21064 = add <4 x i32> %21063, %convolved944.sroa.868.17.us
  %21065 = shufflevector <2 x i64> %21059, <2 x i64> undef, <1 x i32> <i32 1>
  %21066 = bitcast <1 x i64> %21065 to <4 x i16>
  %21067 = sext <4 x i16> %21066 to <4 x i32>
  %21068 = mul nsw <4 x i32> %21067, %20976
  %21069 = add <4 x i32> %21068, %convolved944.sroa.930.17.us
  %t2635869.us = add i32 %20932, %20923
  %t4534.us = add i32 %t2635869.us, %20915
  %21070 = sext i32 %t4534.us to i64
  %21071 = shl nsw i64 %21070, 4
  %21072 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21071
  %21073 = bitcast i8* %21072 to <8 x i8>*
  %t4535.us = load <8 x i8>, <8 x i8>* %21073, align 16, !tbaa !438
  %21074 = getelementptr inbounds i8, i8* %21072, i64 8
  %21075 = bitcast i8* %21074 to <8 x i8>*
  %t4536.us = load <8 x i8>, <8 x i8>* %21075, align 8, !tbaa !438
  %21076 = zext <8 x i8> %t4535.us to <8 x i16>
  %21077 = bitcast <8 x i16> %21076 to <2 x i64>
  %21078 = shufflevector <2 x i64> %21077, <2 x i64> undef, <1 x i32> zeroinitializer
  %21079 = bitcast <1 x i64> %21078 to <4 x i16>
  %21080 = sext <4 x i16> %21079 to <4 x i32>
  %21081 = mul nsw <4 x i32> %21080, %20947
  %21082 = add <4 x i32> %21081, %convolved944.sroa.992.13.us
  %21083 = shufflevector <2 x i64> %21077, <2 x i64> undef, <1 x i32> <i32 1>
  %21084 = bitcast <1 x i64> %21083 to <4 x i16>
  %21085 = sext <4 x i16> %21084 to <4 x i32>
  %21086 = mul nsw <4 x i32> %21085, %20956
  %21087 = add <4 x i32> %21086, %convolved944.sroa.1048.13.us
  %21088 = zext <8 x i8> %t4536.us to <8 x i16>
  %21089 = bitcast <8 x i16> %21088 to <2 x i64>
  %21090 = shufflevector <2 x i64> %21089, <2 x i64> undef, <1 x i32> zeroinitializer
  %21091 = bitcast <1 x i64> %21090 to <4 x i16>
  %21092 = sext <4 x i16> %21091 to <4 x i32>
  %21093 = mul nsw <4 x i32> %21092, %20967
  %21094 = add <4 x i32> %21093, %convolved944.sroa.1104.13.us
  %21095 = shufflevector <2 x i64> %21089, <2 x i64> undef, <1 x i32> <i32 1>
  %21096 = bitcast <1 x i64> %21095 to <4 x i16>
  %21097 = sext <4 x i16> %21096 to <4 x i32>
  %21098 = mul nsw <4 x i32> %21097, %20976
  %21099 = add <4 x i32> %21098, %convolved944.sroa.1160.13.us
  %t4538.us = add i32 %t2635869.us, %20914
  %21100 = sext i32 %t4538.us to i64
  %21101 = shl nsw i64 %21100, 4
  %21102 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21101
  %21103 = bitcast i8* %21102 to <8 x i8>*
  %t4539.us = load <8 x i8>, <8 x i8>* %21103, align 16, !tbaa !438
  %21104 = getelementptr inbounds i8, i8* %21102, i64 8
  %21105 = bitcast i8* %21104 to <8 x i8>*
  %t4540.us = load <8 x i8>, <8 x i8>* %21105, align 8, !tbaa !438
  %21106 = zext <8 x i8> %t4539.us to <8 x i16>
  %21107 = bitcast <8 x i16> %21106 to <2 x i64>
  %21108 = shufflevector <2 x i64> %21107, <2 x i64> undef, <1 x i32> zeroinitializer
  %21109 = bitcast <1 x i64> %21108 to <4 x i16>
  %21110 = sext <4 x i16> %21109 to <4 x i32>
  %21111 = mul nsw <4 x i32> %21110, %20947
  %21112 = add <4 x i32> %21111, %convolved944.sroa.1216.13.us
  %21113 = shufflevector <2 x i64> %21107, <2 x i64> undef, <1 x i32> <i32 1>
  %21114 = bitcast <1 x i64> %21113 to <4 x i16>
  %21115 = sext <4 x i16> %21114 to <4 x i32>
  %21116 = mul nsw <4 x i32> %21115, %20956
  %21117 = add <4 x i32> %21116, %convolved944.sroa.1272.13.us
  %21118 = zext <8 x i8> %t4540.us to <8 x i16>
  %21119 = bitcast <8 x i16> %21118 to <2 x i64>
  %21120 = shufflevector <2 x i64> %21119, <2 x i64> undef, <1 x i32> zeroinitializer
  %21121 = bitcast <1 x i64> %21120 to <4 x i16>
  %21122 = sext <4 x i16> %21121 to <4 x i32>
  %21123 = mul nsw <4 x i32> %21122, %20967
  %21124 = add <4 x i32> %21123, %convolved944.sroa.1328.13.us
  %21125 = shufflevector <2 x i64> %21119, <2 x i64> undef, <1 x i32> <i32 1>
  %21126 = bitcast <1 x i64> %21125 to <4 x i16>
  %21127 = sext <4 x i16> %21126 to <4 x i32>
  %21128 = mul nsw <4 x i32> %21127, %20976
  %21129 = add <4 x i32> %21128, %convolved944.sroa.1384.13.us
  %t4542.us = add i32 %t2635869.us, %20913
  %21130 = sext i32 %t4542.us to i64
  %21131 = shl nsw i64 %21130, 4
  %21132 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21131
  %21133 = bitcast i8* %21132 to <8 x i8>*
  %t4543.us = load <8 x i8>, <8 x i8>* %21133, align 16, !tbaa !438
  %21134 = getelementptr inbounds i8, i8* %21132, i64 8
  %21135 = bitcast i8* %21134 to <8 x i8>*
  %t4544.us = load <8 x i8>, <8 x i8>* %21135, align 8, !tbaa !438
  %21136 = zext <8 x i8> %t4543.us to <8 x i16>
  %21137 = bitcast <8 x i16> %21136 to <2 x i64>
  %21138 = shufflevector <2 x i64> %21137, <2 x i64> undef, <1 x i32> zeroinitializer
  %21139 = bitcast <1 x i64> %21138 to <4 x i16>
  %21140 = sext <4 x i16> %21139 to <4 x i32>
  %21141 = mul nsw <4 x i32> %21140, %20947
  %21142 = add <4 x i32> %21141, %convolved944.sroa.1440.13.us
  %21143 = shufflevector <2 x i64> %21137, <2 x i64> undef, <1 x i32> <i32 1>
  %21144 = bitcast <1 x i64> %21143 to <4 x i16>
  %21145 = sext <4 x i16> %21144 to <4 x i32>
  %21146 = mul nsw <4 x i32> %21145, %20956
  %21147 = add <4 x i32> %21146, %convolved944.sroa.1496.13.us
  %21148 = zext <8 x i8> %t4544.us to <8 x i16>
  %21149 = bitcast <8 x i16> %21148 to <2 x i64>
  %21150 = shufflevector <2 x i64> %21149, <2 x i64> undef, <1 x i32> zeroinitializer
  %21151 = bitcast <1 x i64> %21150 to <4 x i16>
  %21152 = sext <4 x i16> %21151 to <4 x i32>
  %21153 = mul nsw <4 x i32> %21152, %20967
  %21154 = add <4 x i32> %21153, %convolved944.sroa.1552.13.us
  %21155 = shufflevector <2 x i64> %21149, <2 x i64> undef, <1 x i32> <i32 1>
  %21156 = bitcast <1 x i64> %21155 to <4 x i16>
  %21157 = sext <4 x i16> %21156 to <4 x i32>
  %21158 = mul nsw <4 x i32> %21157, %20976
  %21159 = add <4 x i32> %21158, %convolved944.sroa.1608.13.us
  %t4546.us = add i32 %t2635869.us, %20912
  %21160 = sext i32 %t4546.us to i64
  %21161 = shl nsw i64 %21160, 4
  %21162 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21161
  %21163 = bitcast i8* %21162 to <8 x i8>*
  %t4547.us = load <8 x i8>, <8 x i8>* %21163, align 16, !tbaa !438
  %21164 = getelementptr inbounds i8, i8* %21162, i64 8
  %21165 = bitcast i8* %21164 to <8 x i8>*
  %t4548.us = load <8 x i8>, <8 x i8>* %21165, align 8, !tbaa !438
  %21166 = zext <8 x i8> %t4547.us to <8 x i16>
  %21167 = bitcast <8 x i16> %21166 to <2 x i64>
  %21168 = shufflevector <2 x i64> %21167, <2 x i64> undef, <1 x i32> zeroinitializer
  %21169 = bitcast <1 x i64> %21168 to <4 x i16>
  %21170 = sext <4 x i16> %21169 to <4 x i32>
  %21171 = mul nsw <4 x i32> %21170, %20947
  %21172 = add <4 x i32> %21171, %convolved944.sroa.1664.13.us
  %21173 = shufflevector <2 x i64> %21167, <2 x i64> undef, <1 x i32> <i32 1>
  %21174 = bitcast <1 x i64> %21173 to <4 x i16>
  %21175 = sext <4 x i16> %21174 to <4 x i32>
  %21176 = mul nsw <4 x i32> %21175, %20956
  %21177 = add <4 x i32> %21176, %convolved944.sroa.1720.13.us
  %21178 = zext <8 x i8> %t4548.us to <8 x i16>
  %21179 = bitcast <8 x i16> %21178 to <2 x i64>
  %21180 = shufflevector <2 x i64> %21179, <2 x i64> undef, <1 x i32> zeroinitializer
  %21181 = bitcast <1 x i64> %21180 to <4 x i16>
  %21182 = sext <4 x i16> %21181 to <4 x i32>
  %21183 = mul nsw <4 x i32> %21182, %20967
  %21184 = add <4 x i32> %21183, %convolved944.sroa.1776.13.us
  %21185 = shufflevector <2 x i64> %21179, <2 x i64> undef, <1 x i32> <i32 1>
  %21186 = bitcast <1 x i64> %21185 to <4 x i16>
  %21187 = sext <4 x i16> %21186 to <4 x i32>
  %21188 = mul nsw <4 x i32> %21187, %20976
  %21189 = add <4 x i32> %21188, %convolved944.sroa.1832.13.us
  %t2639865.us = add i32 %20932, %20926
  %t4550.us = add i32 %t2639865.us, %20915
  %21190 = sext i32 %t4550.us to i64
  %21191 = shl nsw i64 %21190, 4
  %21192 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21191
  %21193 = bitcast i8* %21192 to <8 x i8>*
  %t4551.us = load <8 x i8>, <8 x i8>* %21193, align 16, !tbaa !438
  %21194 = getelementptr inbounds i8, i8* %21192, i64 8
  %21195 = bitcast i8* %21194 to <8 x i8>*
  %t4552.us = load <8 x i8>, <8 x i8>* %21195, align 8, !tbaa !438
  %21196 = zext <8 x i8> %t4551.us to <8 x i16>
  %21197 = bitcast <8 x i16> %21196 to <2 x i64>
  %21198 = shufflevector <2 x i64> %21197, <2 x i64> undef, <1 x i32> zeroinitializer
  %21199 = bitcast <1 x i64> %21198 to <4 x i16>
  %21200 = sext <4 x i16> %21199 to <4 x i32>
  %21201 = mul nsw <4 x i32> %21200, %20947
  %21202 = add <4 x i32> %21201, %convolved944.sroa.1888.13.us
  %21203 = shufflevector <2 x i64> %21197, <2 x i64> undef, <1 x i32> <i32 1>
  %21204 = bitcast <1 x i64> %21203 to <4 x i16>
  %21205 = sext <4 x i16> %21204 to <4 x i32>
  %21206 = mul nsw <4 x i32> %21205, %20956
  %21207 = add <4 x i32> %21206, %convolved944.sroa.1944.13.us
  %21208 = zext <8 x i8> %t4552.us to <8 x i16>
  %21209 = bitcast <8 x i16> %21208 to <2 x i64>
  %21210 = shufflevector <2 x i64> %21209, <2 x i64> undef, <1 x i32> zeroinitializer
  %21211 = bitcast <1 x i64> %21210 to <4 x i16>
  %21212 = sext <4 x i16> %21211 to <4 x i32>
  %21213 = mul nsw <4 x i32> %21212, %20967
  %21214 = add <4 x i32> %21213, %convolved944.sroa.2000.13.us
  %21215 = shufflevector <2 x i64> %21209, <2 x i64> undef, <1 x i32> <i32 1>
  %21216 = bitcast <1 x i64> %21215 to <4 x i16>
  %21217 = sext <4 x i16> %21216 to <4 x i32>
  %21218 = mul nsw <4 x i32> %21217, %20976
  %21219 = add <4 x i32> %21218, %convolved944.sroa.2056.13.us
  %t4554.us = add i32 %t2639865.us, %20914
  %21220 = sext i32 %t4554.us to i64
  %21221 = shl nsw i64 %21220, 4
  %21222 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21221
  %21223 = bitcast i8* %21222 to <8 x i8>*
  %t4555.us = load <8 x i8>, <8 x i8>* %21223, align 16, !tbaa !438
  %21224 = getelementptr inbounds i8, i8* %21222, i64 8
  %21225 = bitcast i8* %21224 to <8 x i8>*
  %t4556.us = load <8 x i8>, <8 x i8>* %21225, align 8, !tbaa !438
  %21226 = zext <8 x i8> %t4555.us to <8 x i16>
  %21227 = bitcast <8 x i16> %21226 to <2 x i64>
  %21228 = shufflevector <2 x i64> %21227, <2 x i64> undef, <1 x i32> zeroinitializer
  %21229 = bitcast <1 x i64> %21228 to <4 x i16>
  %21230 = sext <4 x i16> %21229 to <4 x i32>
  %21231 = mul nsw <4 x i32> %21230, %20947
  %21232 = add <4 x i32> %21231, %convolved944.sroa.2112.13.us
  %21233 = shufflevector <2 x i64> %21227, <2 x i64> undef, <1 x i32> <i32 1>
  %21234 = bitcast <1 x i64> %21233 to <4 x i16>
  %21235 = sext <4 x i16> %21234 to <4 x i32>
  %21236 = mul nsw <4 x i32> %21235, %20956
  %21237 = add <4 x i32> %21236, %convolved944.sroa.2168.13.us
  %21238 = zext <8 x i8> %t4556.us to <8 x i16>
  %21239 = bitcast <8 x i16> %21238 to <2 x i64>
  %21240 = shufflevector <2 x i64> %21239, <2 x i64> undef, <1 x i32> zeroinitializer
  %21241 = bitcast <1 x i64> %21240 to <4 x i16>
  %21242 = sext <4 x i16> %21241 to <4 x i32>
  %21243 = mul nsw <4 x i32> %21242, %20967
  %21244 = add <4 x i32> %21243, %convolved944.sroa.2224.13.us
  %21245 = shufflevector <2 x i64> %21239, <2 x i64> undef, <1 x i32> <i32 1>
  %21246 = bitcast <1 x i64> %21245 to <4 x i16>
  %21247 = sext <4 x i16> %21246 to <4 x i32>
  %21248 = mul nsw <4 x i32> %21247, %20976
  %21249 = add <4 x i32> %21248, %convolved944.sroa.2280.13.us
  %t4558.us = add i32 %t2639865.us, %20913
  %21250 = sext i32 %t4558.us to i64
  %21251 = shl nsw i64 %21250, 4
  %21252 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21251
  %21253 = bitcast i8* %21252 to <8 x i8>*
  %t4559.us = load <8 x i8>, <8 x i8>* %21253, align 16, !tbaa !438
  %21254 = getelementptr inbounds i8, i8* %21252, i64 8
  %21255 = bitcast i8* %21254 to <8 x i8>*
  %t4560.us = load <8 x i8>, <8 x i8>* %21255, align 8, !tbaa !438
  %21256 = zext <8 x i8> %t4559.us to <8 x i16>
  %21257 = bitcast <8 x i16> %21256 to <2 x i64>
  %21258 = shufflevector <2 x i64> %21257, <2 x i64> undef, <1 x i32> zeroinitializer
  %21259 = bitcast <1 x i64> %21258 to <4 x i16>
  %21260 = sext <4 x i16> %21259 to <4 x i32>
  %21261 = mul nsw <4 x i32> %21260, %20947
  %21262 = add <4 x i32> %21261, %convolved944.sroa.2336.13.us
  %21263 = shufflevector <2 x i64> %21257, <2 x i64> undef, <1 x i32> <i32 1>
  %21264 = bitcast <1 x i64> %21263 to <4 x i16>
  %21265 = sext <4 x i16> %21264 to <4 x i32>
  %21266 = mul nsw <4 x i32> %21265, %20956
  %21267 = add <4 x i32> %21266, %convolved944.sroa.2392.13.us
  %21268 = zext <8 x i8> %t4560.us to <8 x i16>
  %21269 = bitcast <8 x i16> %21268 to <2 x i64>
  %21270 = shufflevector <2 x i64> %21269, <2 x i64> undef, <1 x i32> zeroinitializer
  %21271 = bitcast <1 x i64> %21270 to <4 x i16>
  %21272 = sext <4 x i16> %21271 to <4 x i32>
  %21273 = mul nsw <4 x i32> %21272, %20967
  %21274 = add <4 x i32> %21273, %convolved944.sroa.2448.13.us
  %21275 = shufflevector <2 x i64> %21269, <2 x i64> undef, <1 x i32> <i32 1>
  %21276 = bitcast <1 x i64> %21275 to <4 x i16>
  %21277 = sext <4 x i16> %21276 to <4 x i32>
  %21278 = mul nsw <4 x i32> %21277, %20976
  %21279 = add <4 x i32> %21278, %convolved944.sroa.2504.13.us
  %t4562.us = add i32 %t2639865.us, %20912
  %21280 = sext i32 %t4562.us to i64
  %21281 = shl nsw i64 %21280, 4
  %21282 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21281
  %21283 = bitcast i8* %21282 to <8 x i8>*
  %t4563.us = load <8 x i8>, <8 x i8>* %21283, align 16, !tbaa !438
  %21284 = getelementptr inbounds i8, i8* %21282, i64 8
  %21285 = bitcast i8* %21284 to <8 x i8>*
  %t4564.us = load <8 x i8>, <8 x i8>* %21285, align 8, !tbaa !438
  %21286 = zext <8 x i8> %t4563.us to <8 x i16>
  %21287 = bitcast <8 x i16> %21286 to <2 x i64>
  %21288 = shufflevector <2 x i64> %21287, <2 x i64> undef, <1 x i32> zeroinitializer
  %21289 = bitcast <1 x i64> %21288 to <4 x i16>
  %21290 = sext <4 x i16> %21289 to <4 x i32>
  %21291 = mul nsw <4 x i32> %21290, %20947
  %21292 = add <4 x i32> %21291, %convolved944.sroa.2560.13.us
  %21293 = shufflevector <2 x i64> %21287, <2 x i64> undef, <1 x i32> <i32 1>
  %21294 = bitcast <1 x i64> %21293 to <4 x i16>
  %21295 = sext <4 x i16> %21294 to <4 x i32>
  %21296 = mul nsw <4 x i32> %21295, %20956
  %21297 = add <4 x i32> %21296, %convolved944.sroa.2616.13.us
  %21298 = zext <8 x i8> %t4564.us to <8 x i16>
  %21299 = bitcast <8 x i16> %21298 to <2 x i64>
  %21300 = shufflevector <2 x i64> %21299, <2 x i64> undef, <1 x i32> zeroinitializer
  %21301 = bitcast <1 x i64> %21300 to <4 x i16>
  %21302 = sext <4 x i16> %21301 to <4 x i32>
  %21303 = mul nsw <4 x i32> %21302, %20967
  %21304 = add <4 x i32> %21303, %convolved944.sroa.2672.13.us
  %21305 = shufflevector <2 x i64> %21299, <2 x i64> undef, <1 x i32> <i32 1>
  %21306 = bitcast <1 x i64> %21305 to <4 x i16>
  %21307 = sext <4 x i16> %21306 to <4 x i32>
  %21308 = mul nsw <4 x i32> %21307, %20976
  %21309 = add <4 x i32> %21308, %convolved944.sroa.2728.13.us
  %t2643861.us = add i32 %20932, %20929
  %t4566.us = add i32 %t2643861.us, %20915
  %21310 = sext i32 %t4566.us to i64
  %21311 = shl nsw i64 %21310, 4
  %21312 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21311
  %21313 = bitcast i8* %21312 to <8 x i8>*
  %t4567.us = load <8 x i8>, <8 x i8>* %21313, align 16, !tbaa !438
  %21314 = getelementptr inbounds i8, i8* %21312, i64 8
  %21315 = bitcast i8* %21314 to <8 x i8>*
  %t4568.us = load <8 x i8>, <8 x i8>* %21315, align 8, !tbaa !438
  %21316 = zext <8 x i8> %t4567.us to <8 x i16>
  %21317 = bitcast <8 x i16> %21316 to <2 x i64>
  %21318 = shufflevector <2 x i64> %21317, <2 x i64> undef, <1 x i32> zeroinitializer
  %21319 = bitcast <1 x i64> %21318 to <4 x i16>
  %21320 = sext <4 x i16> %21319 to <4 x i32>
  %21321 = mul nsw <4 x i32> %21320, %20947
  %21322 = add <4 x i32> %21321, %convolved944.sroa.2784.13.us
  %21323 = shufflevector <2 x i64> %21317, <2 x i64> undef, <1 x i32> <i32 1>
  %21324 = bitcast <1 x i64> %21323 to <4 x i16>
  %21325 = sext <4 x i16> %21324 to <4 x i32>
  %21326 = mul nsw <4 x i32> %21325, %20956
  %21327 = add <4 x i32> %21326, %convolved944.sroa.2840.13.us
  %21328 = zext <8 x i8> %t4568.us to <8 x i16>
  %21329 = bitcast <8 x i16> %21328 to <2 x i64>
  %21330 = shufflevector <2 x i64> %21329, <2 x i64> undef, <1 x i32> zeroinitializer
  %21331 = bitcast <1 x i64> %21330 to <4 x i16>
  %21332 = sext <4 x i16> %21331 to <4 x i32>
  %21333 = mul nsw <4 x i32> %21332, %20967
  %21334 = add <4 x i32> %21333, %convolved944.sroa.2896.13.us
  %21335 = shufflevector <2 x i64> %21329, <2 x i64> undef, <1 x i32> <i32 1>
  %21336 = bitcast <1 x i64> %21335 to <4 x i16>
  %21337 = sext <4 x i16> %21336 to <4 x i32>
  %21338 = mul nsw <4 x i32> %21337, %20976
  %21339 = add <4 x i32> %21338, %convolved944.sroa.2952.13.us
  %t4570.us = add i32 %t2643861.us, %20914
  %21340 = sext i32 %t4570.us to i64
  %21341 = shl nsw i64 %21340, 4
  %21342 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21341
  %21343 = bitcast i8* %21342 to <8 x i8>*
  %t4571.us = load <8 x i8>, <8 x i8>* %21343, align 16, !tbaa !438
  %21344 = getelementptr inbounds i8, i8* %21342, i64 8
  %21345 = bitcast i8* %21344 to <8 x i8>*
  %t4572.us = load <8 x i8>, <8 x i8>* %21345, align 8, !tbaa !438
  %21346 = zext <8 x i8> %t4571.us to <8 x i16>
  %21347 = bitcast <8 x i16> %21346 to <2 x i64>
  %21348 = shufflevector <2 x i64> %21347, <2 x i64> undef, <1 x i32> zeroinitializer
  %21349 = bitcast <1 x i64> %21348 to <4 x i16>
  %21350 = sext <4 x i16> %21349 to <4 x i32>
  %21351 = mul nsw <4 x i32> %21350, %20947
  %21352 = add <4 x i32> %21351, %convolved944.sroa.3008.13.us
  %21353 = shufflevector <2 x i64> %21347, <2 x i64> undef, <1 x i32> <i32 1>
  %21354 = bitcast <1 x i64> %21353 to <4 x i16>
  %21355 = sext <4 x i16> %21354 to <4 x i32>
  %21356 = mul nsw <4 x i32> %21355, %20956
  %21357 = add <4 x i32> %21356, %convolved944.sroa.3064.13.us
  %21358 = zext <8 x i8> %t4572.us to <8 x i16>
  %21359 = bitcast <8 x i16> %21358 to <2 x i64>
  %21360 = shufflevector <2 x i64> %21359, <2 x i64> undef, <1 x i32> zeroinitializer
  %21361 = bitcast <1 x i64> %21360 to <4 x i16>
  %21362 = sext <4 x i16> %21361 to <4 x i32>
  %21363 = mul nsw <4 x i32> %21362, %20967
  %21364 = add <4 x i32> %21363, %convolved944.sroa.3120.13.us
  %21365 = shufflevector <2 x i64> %21359, <2 x i64> undef, <1 x i32> <i32 1>
  %21366 = bitcast <1 x i64> %21365 to <4 x i16>
  %21367 = sext <4 x i16> %21366 to <4 x i32>
  %21368 = mul nsw <4 x i32> %21367, %20976
  %21369 = add <4 x i32> %21368, %convolved944.sroa.3176.13.us
  %t4574.us = add i32 %t2643861.us, %20913
  %21370 = sext i32 %t4574.us to i64
  %21371 = shl nsw i64 %21370, 4
  %21372 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21371
  %21373 = bitcast i8* %21372 to <8 x i8>*
  %t4575.us = load <8 x i8>, <8 x i8>* %21373, align 16, !tbaa !438
  %21374 = getelementptr inbounds i8, i8* %21372, i64 8
  %21375 = bitcast i8* %21374 to <8 x i8>*
  %t4576.us = load <8 x i8>, <8 x i8>* %21375, align 8, !tbaa !438
  %21376 = zext <8 x i8> %t4575.us to <8 x i16>
  %21377 = bitcast <8 x i16> %21376 to <2 x i64>
  %21378 = shufflevector <2 x i64> %21377, <2 x i64> undef, <1 x i32> zeroinitializer
  %21379 = bitcast <1 x i64> %21378 to <4 x i16>
  %21380 = sext <4 x i16> %21379 to <4 x i32>
  %21381 = mul nsw <4 x i32> %21380, %20947
  %21382 = add <4 x i32> %21381, %convolved944.sroa.3232.13.us
  %21383 = shufflevector <2 x i64> %21377, <2 x i64> undef, <1 x i32> <i32 1>
  %21384 = bitcast <1 x i64> %21383 to <4 x i16>
  %21385 = sext <4 x i16> %21384 to <4 x i32>
  %21386 = mul nsw <4 x i32> %21385, %20956
  %21387 = add <4 x i32> %21386, %convolved944.sroa.3288.13.us
  %21388 = zext <8 x i8> %t4576.us to <8 x i16>
  %21389 = bitcast <8 x i16> %21388 to <2 x i64>
  %21390 = shufflevector <2 x i64> %21389, <2 x i64> undef, <1 x i32> zeroinitializer
  %21391 = bitcast <1 x i64> %21390 to <4 x i16>
  %21392 = sext <4 x i16> %21391 to <4 x i32>
  %21393 = mul nsw <4 x i32> %21392, %20967
  %21394 = add <4 x i32> %21393, %convolved944.sroa.3344.13.us
  %21395 = shufflevector <2 x i64> %21389, <2 x i64> undef, <1 x i32> <i32 1>
  %21396 = bitcast <1 x i64> %21395 to <4 x i16>
  %21397 = sext <4 x i16> %21396 to <4 x i32>
  %21398 = mul nsw <4 x i32> %21397, %20976
  %21399 = add <4 x i32> %21398, %convolved944.sroa.3400.13.us
  %t4578.us = add i32 %t2643861.us, %20912
  %21400 = sext i32 %t4578.us to i64
  %21401 = shl nsw i64 %21400, 4
  %21402 = getelementptr inbounds i8, i8* %resampled_input681, i64 %21401
  %21403 = bitcast i8* %21402 to <8 x i8>*
  %t4579.us = load <8 x i8>, <8 x i8>* %21403, align 16, !tbaa !438
  %21404 = getelementptr inbounds i8, i8* %21402, i64 8
  %21405 = bitcast i8* %21404 to <8 x i8>*
  %t4580.us = load <8 x i8>, <8 x i8>* %21405, align 8, !tbaa !438
  %21406 = zext <8 x i8> %t4579.us to <8 x i16>
  %21407 = bitcast <8 x i16> %21406 to <2 x i64>
  %21408 = shufflevector <2 x i64> %21407, <2 x i64> undef, <1 x i32> zeroinitializer
  %21409 = bitcast <1 x i64> %21408 to <4 x i16>
  %21410 = sext <4 x i16> %21409 to <4 x i32>
  %21411 = mul nsw <4 x i32> %21410, %20947
  %21412 = add <4 x i32> %21411, %convolved944.sroa.3456.13.us
  %21413 = shufflevector <2 x i64> %21407, <2 x i64> undef, <1 x i32> <i32 1>
  %21414 = bitcast <1 x i64> %21413 to <4 x i16>
  %21415 = sext <4 x i16> %21414 to <4 x i32>
  %21416 = mul nsw <4 x i32> %21415, %20956
  %21417 = add <4 x i32> %21416, %convolved944.sroa.3512.13.us
  %21418 = zext <8 x i8> %t4580.us to <8 x i16>
  %21419 = bitcast <8 x i16> %21418 to <2 x i64>
  %21420 = shufflevector <2 x i64> %21419, <2 x i64> undef, <1 x i32> zeroinitializer
  %21421 = bitcast <1 x i64> %21420 to <4 x i16>
  %21422 = sext <4 x i16> %21421 to <4 x i32>
  %21423 = mul nsw <4 x i32> %21422, %20967
  %21424 = add <4 x i32> %21423, %convolved944.sroa.3568.13.us
  %21425 = shufflevector <2 x i64> %21419, <2 x i64> undef, <1 x i32> <i32 1>
  %21426 = bitcast <1 x i64> %21425 to <4 x i16>
  %21427 = sext <4 x i16> %21426 to <4 x i32>
  %21428 = mul nsw <4 x i32> %21427, %20976
  %21429 = add <4 x i32> %21428, %convolved944.sroa.3624.13.us
  %indvars.iv.next6421 = add nuw nsw i64 %indvars.iv6420, 1
  %.not1021.us = icmp eq i64 %indvars.iv.next6421, %16814
  br i1 %.not1021.us, label %"end for convolved.s1.r19$x871.loopexit.us", label %"for convolved.s1.r19$x870.us"

"end for convolved.s1.r19$x871.loopexit.us":      ; preds = %"for convolved.s1.r19$x870.us"
  %indvars.iv.next6424 = add nuw nsw i64 %indvars.iv6423, 1
  %.not1020.us = icmp eq i64 %indvars.iv.next6424, %16817
  br i1 %.not1020.us, label %"consume convolved873", label %"for convolved.s1.r19$y845.us"

"consume convolved873":                           ; preds = %"end for convolved.s1.r19$x871.loopexit.us", %"for convolved.s1.r19$y845.preheader", %next_bb839, %then_bb838
  %convolved944.sroa.3624.15 = phi <4 x i32> [ %20904, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21429, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3568.15 = phi <4 x i32> [ %20867, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21424, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3512.15 = phi <4 x i32> [ %20812, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21417, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3456.15 = phi <4 x i32> [ %20775, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21412, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3400.15 = phi <4 x i32> [ %20666, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21399, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3344.15 = phi <4 x i32> [ %20629, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21394, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3288.15 = phi <4 x i32> [ %20574, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21387, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3232.15 = phi <4 x i32> [ %20537, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21382, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3176.15 = phi <4 x i32> [ %20428, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21369, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3120.15 = phi <4 x i32> [ %20391, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21364, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3064.15 = phi <4 x i32> [ %20336, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21357, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.3008.15 = phi <4 x i32> [ %20299, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21352, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2952.15 = phi <4 x i32> [ %20190, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21339, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2896.15 = phi <4 x i32> [ %20153, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21334, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2840.15 = phi <4 x i32> [ %20098, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21327, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2784.15 = phi <4 x i32> [ %20061, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21322, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2728.15 = phi <4 x i32> [ %19952, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21309, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2672.15 = phi <4 x i32> [ %19915, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21304, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2616.15 = phi <4 x i32> [ %19860, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21297, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2560.15 = phi <4 x i32> [ %19823, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21292, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2504.15 = phi <4 x i32> [ %19714, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21279, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2448.15 = phi <4 x i32> [ %19677, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21274, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2392.15 = phi <4 x i32> [ %19622, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21267, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2336.15 = phi <4 x i32> [ %19585, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21262, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2280.15 = phi <4 x i32> [ %19476, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21249, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2224.15 = phi <4 x i32> [ %19439, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21244, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2168.15 = phi <4 x i32> [ %19384, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21237, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2112.15 = phi <4 x i32> [ %19347, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21232, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2056.15 = phi <4 x i32> [ %19238, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21219, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.2000.15 = phi <4 x i32> [ %19201, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21214, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1944.15 = phi <4 x i32> [ %19146, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21207, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1888.15 = phi <4 x i32> [ %19109, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21202, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1832.15 = phi <4 x i32> [ %19000, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21189, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1776.15 = phi <4 x i32> [ %18963, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21184, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1720.15 = phi <4 x i32> [ %18908, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21177, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1664.15 = phi <4 x i32> [ %18871, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21172, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1608.15 = phi <4 x i32> [ %18762, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21159, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1552.15 = phi <4 x i32> [ %18725, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21154, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1496.15 = phi <4 x i32> [ %18670, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21147, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1440.15 = phi <4 x i32> [ %18633, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21142, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1384.15 = phi <4 x i32> [ %18524, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21129, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1328.15 = phi <4 x i32> [ %18487, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21124, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1272.15 = phi <4 x i32> [ %18432, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21117, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1216.15 = phi <4 x i32> [ %18395, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21112, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1160.15 = phi <4 x i32> [ %18286, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21099, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1104.15 = phi <4 x i32> [ %18249, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21094, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.1048.15 = phi <4 x i32> [ %18194, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21087, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.992.15 = phi <4 x i32> [ %18157, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21082, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.930.19 = phi <4 x i32> [ %18048, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21069, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.868.19 = phi <4 x i32> [ %18011, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21064, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.806.19 = phi <4 x i32> [ %17956, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21057, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.744.19 = phi <4 x i32> [ %17919, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21052, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.682.19 = phi <4 x i32> [ %17809, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21039, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.620.19 = phi <4 x i32> [ %17772, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21034, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.558.19 = phi <4 x i32> [ %17717, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %21027, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.496.19 = phi <4 x i32> [ %17680, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %21022, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.434.19 = phi <4 x i32> [ %17570, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %21009, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.372.19 = phi <4 x i32> [ %17533, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %21004, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.310.19 = phi <4 x i32> [ %17478, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %20997, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.248.19 = phi <4 x i32> [ %17441, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %20992, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.186.19 = phi <4 x i32> [ %17331, %then_bb838 ], [ %16987, %next_bb839 ], [ %16987, %"for convolved.s1.r19$y845.preheader" ], [ %20979, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.124.19 = phi <4 x i32> [ %17280, %then_bb838 ], [ %16986, %next_bb839 ], [ %16986, %"for convolved.s1.r19$y845.preheader" ], [ %20970, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.62.19 = phi <4 x i32> [ %17211, %then_bb838 ], [ %16985, %next_bb839 ], [ %16985, %"for convolved.s1.r19$y845.preheader" ], [ %20959, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %convolved944.sroa.0.19 = phi <4 x i32> [ %17160, %then_bb838 ], [ %16984, %next_bb839 ], [ %16984, %"for convolved.s1.r19$y845.preheader" ], [ %20950, %"end for convolved.s1.r19$x871.loopexit.us" ]
  %21430 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.0.19, <4 x i32> undef
  %21431 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21430, <4 x i32> %16791) #11
  %21432 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21431, <4 x i32> %16794) #11
  %21433 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21432, <4 x i32> %16797) #11
  %21434 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21433) #11
  %21435 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.62.19, <4 x i32> undef
  %21436 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21435, <4 x i32> %16791) #11
  %21437 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21436, <4 x i32> %16794) #11
  %21438 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21437, <4 x i32> %16797) #11
  %21439 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21438) #11
  %21440 = shufflevector <4 x i16> %21434, <4 x i16> %21439, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21441 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21440, <8 x i16> %16800) #11
  %21442 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21441) #11
  %21443 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.124.19, <4 x i32> undef
  %21444 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21443, <4 x i32> %16791) #11
  %21445 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21444, <4 x i32> %16794) #11
  %21446 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21445, <4 x i32> %16797) #11
  %21447 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21446) #11
  %21448 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.186.19, <4 x i32> undef
  %21449 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21448, <4 x i32> %16791) #11
  %21450 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21449, <4 x i32> %16794) #11
  %21451 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21450, <4 x i32> %16797) #11
  %21452 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21451) #11
  %21453 = shufflevector <4 x i16> %21447, <4 x i16> %21452, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21454 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21453, <8 x i16> %16800) #11
  %21455 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21454) #11
  %21456 = shufflevector <8 x i8> %21442, <8 x i8> %21455, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21457 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21456) #11
  %21458 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21457, <16 x i8> %16804) #11
  %21459 = shl nuw nsw i64 %indvars.iv6426, 2
  %21460 = add nsw i64 %21459, %16805
  %21461 = mul nsw i64 %21460, %16806
  %21462 = add nsw i64 %21461, %17030
  %21463 = getelementptr inbounds i8, i8* %59, i64 %21462
  %21464 = bitcast i8* %21463 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21458, <16 x i8>* %21464, i32 1, <16 x i1> %16541), !tbaa !515
  %21465 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.248.19, <4 x i32> undef
  %21466 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21465, <4 x i32> %16791) #11
  %21467 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21466, <4 x i32> %16794) #11
  %21468 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21467, <4 x i32> %16797) #11
  %21469 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21468) #11
  %21470 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.310.19, <4 x i32> undef
  %21471 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21470, <4 x i32> %16791) #11
  %21472 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21471, <4 x i32> %16794) #11
  %21473 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21472, <4 x i32> %16797) #11
  %21474 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21473) #11
  %21475 = shufflevector <4 x i16> %21469, <4 x i16> %21474, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21476 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21475, <8 x i16> %16800) #11
  %21477 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21476) #11
  %21478 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.372.19, <4 x i32> undef
  %21479 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21478, <4 x i32> %16791) #11
  %21480 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21479, <4 x i32> %16794) #11
  %21481 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21480, <4 x i32> %16797) #11
  %21482 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21481) #11
  %21483 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.434.19, <4 x i32> undef
  %21484 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21483, <4 x i32> %16791) #11
  %21485 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21484, <4 x i32> %16794) #11
  %21486 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21485, <4 x i32> %16797) #11
  %21487 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21486) #11
  %21488 = shufflevector <4 x i16> %21482, <4 x i16> %21487, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21489 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21488, <8 x i16> %16800) #11
  %21490 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21489) #11
  %21491 = shufflevector <8 x i8> %21477, <8 x i8> %21490, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21492 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21491) #11
  %21493 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21492, <16 x i8> %16804) #11
  %21494 = add nsw i64 %21460, 1
  %21495 = mul nsw i64 %21494, %16806
  %21496 = add nsw i64 %21495, %17030
  %21497 = getelementptr inbounds i8, i8* %59, i64 %21496
  %21498 = bitcast i8* %21497 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21493, <16 x i8>* %21498, i32 1, <16 x i1> %16541), !tbaa !515
  %21499 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.496.19, <4 x i32> undef
  %21500 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21499, <4 x i32> %16791) #11
  %21501 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21500, <4 x i32> %16794) #11
  %21502 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21501, <4 x i32> %16797) #11
  %21503 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21502) #11
  %21504 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.558.19, <4 x i32> undef
  %21505 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21504, <4 x i32> %16791) #11
  %21506 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21505, <4 x i32> %16794) #11
  %21507 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21506, <4 x i32> %16797) #11
  %21508 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21507) #11
  %21509 = shufflevector <4 x i16> %21503, <4 x i16> %21508, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21510 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21509, <8 x i16> %16800) #11
  %21511 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21510) #11
  %21512 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.620.19, <4 x i32> undef
  %21513 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21512, <4 x i32> %16791) #11
  %21514 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21513, <4 x i32> %16794) #11
  %21515 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21514, <4 x i32> %16797) #11
  %21516 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21515) #11
  %21517 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.682.19, <4 x i32> undef
  %21518 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21517, <4 x i32> %16791) #11
  %21519 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21518, <4 x i32> %16794) #11
  %21520 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21519, <4 x i32> %16797) #11
  %21521 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21520) #11
  %21522 = shufflevector <4 x i16> %21516, <4 x i16> %21521, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21523 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21522, <8 x i16> %16800) #11
  %21524 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21523) #11
  %21525 = shufflevector <8 x i8> %21511, <8 x i8> %21524, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21526 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21525) #11
  %21527 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21526, <16 x i8> %16804) #11
  %21528 = add nsw i64 %21460, 2
  %21529 = mul nsw i64 %21528, %16806
  %21530 = add nsw i64 %21529, %17030
  %21531 = getelementptr inbounds i8, i8* %59, i64 %21530
  %21532 = bitcast i8* %21531 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21527, <16 x i8>* %21532, i32 1, <16 x i1> %16541), !tbaa !515
  %21533 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.744.19, <4 x i32> undef
  %21534 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21533, <4 x i32> %16791) #11
  %21535 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21534, <4 x i32> %16794) #11
  %21536 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21535, <4 x i32> %16797) #11
  %21537 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21536) #11
  %21538 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.806.19, <4 x i32> undef
  %21539 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21538, <4 x i32> %16791) #11
  %21540 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21539, <4 x i32> %16794) #11
  %21541 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21540, <4 x i32> %16797) #11
  %21542 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21541) #11
  %21543 = shufflevector <4 x i16> %21537, <4 x i16> %21542, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21544 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21543, <8 x i16> %16800) #11
  %21545 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21544) #11
  %21546 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.868.19, <4 x i32> undef
  %21547 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21546, <4 x i32> %16791) #11
  %21548 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21547, <4 x i32> %16794) #11
  %21549 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21548, <4 x i32> %16797) #11
  %21550 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21549) #11
  %21551 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.930.19, <4 x i32> undef
  %21552 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21551, <4 x i32> %16791) #11
  %21553 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21552, <4 x i32> %16794) #11
  %21554 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21553, <4 x i32> %16797) #11
  %21555 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21554) #11
  %21556 = shufflevector <4 x i16> %21550, <4 x i16> %21555, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21557 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21556, <8 x i16> %16800) #11
  %21558 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21557) #11
  %21559 = shufflevector <8 x i8> %21545, <8 x i8> %21558, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21560 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21559) #11
  %21561 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21560, <16 x i8> %16804) #11
  %21562 = add nsw i64 %21460, 3
  %21563 = mul nsw i64 %21562, %16806
  %21564 = add nsw i64 %21563, %17030
  %21565 = getelementptr inbounds i8, i8* %59, i64 %21564
  %21566 = bitcast i8* %21565 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21561, <16 x i8>* %21566, i32 1, <16 x i1> %16541), !tbaa !515
  %21567 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.992.15, <4 x i32> undef
  %21568 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21567, <4 x i32> %16791) #11
  %21569 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21568, <4 x i32> %16794) #11
  %21570 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21569, <4 x i32> %16797) #11
  %21571 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21570) #11
  %21572 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.1048.15, <4 x i32> undef
  %21573 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21572, <4 x i32> %16791) #11
  %21574 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21573, <4 x i32> %16794) #11
  %21575 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21574, <4 x i32> %16797) #11
  %21576 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21575) #11
  %21577 = shufflevector <4 x i16> %21571, <4 x i16> %21576, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21578 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21577, <8 x i16> %16800) #11
  %21579 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21578) #11
  %21580 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.1104.15, <4 x i32> undef
  %21581 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21580, <4 x i32> %16791) #11
  %21582 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21581, <4 x i32> %16794) #11
  %21583 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21582, <4 x i32> %16797) #11
  %21584 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21583) #11
  %21585 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.1160.15, <4 x i32> undef
  %21586 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21585, <4 x i32> %16791) #11
  %21587 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21586, <4 x i32> %16794) #11
  %21588 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21587, <4 x i32> %16797) #11
  %21589 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21588) #11
  %21590 = shufflevector <4 x i16> %21584, <4 x i16> %21589, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21591 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21590, <8 x i16> %16800) #11
  %21592 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21591) #11
  %21593 = shufflevector <8 x i8> %21579, <8 x i8> %21592, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21594 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21593) #11
  %21595 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21594, <16 x i8> %16804) #11
  %21596 = add nsw i64 %21461, %17031
  %21597 = getelementptr inbounds i8, i8* %59, i64 %21596
  %21598 = bitcast i8* %21597 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21595, <16 x i8>* %21598, i32 1, <16 x i1> %16541), !tbaa !515
  %21599 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.1216.15, <4 x i32> undef
  %21600 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21599, <4 x i32> %16791) #11
  %21601 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21600, <4 x i32> %16794) #11
  %21602 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21601, <4 x i32> %16797) #11
  %21603 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21602) #11
  %21604 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.1272.15, <4 x i32> undef
  %21605 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21604, <4 x i32> %16791) #11
  %21606 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21605, <4 x i32> %16794) #11
  %21607 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21606, <4 x i32> %16797) #11
  %21608 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21607) #11
  %21609 = shufflevector <4 x i16> %21603, <4 x i16> %21608, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21610 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21609, <8 x i16> %16800) #11
  %21611 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21610) #11
  %21612 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.1328.15, <4 x i32> undef
  %21613 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21612, <4 x i32> %16791) #11
  %21614 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21613, <4 x i32> %16794) #11
  %21615 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21614, <4 x i32> %16797) #11
  %21616 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21615) #11
  %21617 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.1384.15, <4 x i32> undef
  %21618 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21617, <4 x i32> %16791) #11
  %21619 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21618, <4 x i32> %16794) #11
  %21620 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21619, <4 x i32> %16797) #11
  %21621 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21620) #11
  %21622 = shufflevector <4 x i16> %21616, <4 x i16> %21621, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21623 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21622, <8 x i16> %16800) #11
  %21624 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21623) #11
  %21625 = shufflevector <8 x i8> %21611, <8 x i8> %21624, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21626 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21625) #11
  %21627 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21626, <16 x i8> %16804) #11
  %21628 = add nsw i64 %21495, %17031
  %21629 = getelementptr inbounds i8, i8* %59, i64 %21628
  %21630 = bitcast i8* %21629 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21627, <16 x i8>* %21630, i32 1, <16 x i1> %16541), !tbaa !515
  %21631 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.1440.15, <4 x i32> undef
  %21632 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21631, <4 x i32> %16791) #11
  %21633 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21632, <4 x i32> %16794) #11
  %21634 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21633, <4 x i32> %16797) #11
  %21635 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21634) #11
  %21636 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.1496.15, <4 x i32> undef
  %21637 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21636, <4 x i32> %16791) #11
  %21638 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21637, <4 x i32> %16794) #11
  %21639 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21638, <4 x i32> %16797) #11
  %21640 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21639) #11
  %21641 = shufflevector <4 x i16> %21635, <4 x i16> %21640, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21642 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21641, <8 x i16> %16800) #11
  %21643 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21642) #11
  %21644 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.1552.15, <4 x i32> undef
  %21645 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21644, <4 x i32> %16791) #11
  %21646 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21645, <4 x i32> %16794) #11
  %21647 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21646, <4 x i32> %16797) #11
  %21648 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21647) #11
  %21649 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.1608.15, <4 x i32> undef
  %21650 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21649, <4 x i32> %16791) #11
  %21651 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21650, <4 x i32> %16794) #11
  %21652 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21651, <4 x i32> %16797) #11
  %21653 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21652) #11
  %21654 = shufflevector <4 x i16> %21648, <4 x i16> %21653, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21655 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21654, <8 x i16> %16800) #11
  %21656 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21655) #11
  %21657 = shufflevector <8 x i8> %21643, <8 x i8> %21656, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21658 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21657) #11
  %21659 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21658, <16 x i8> %16804) #11
  %21660 = add nsw i64 %21529, %17031
  %21661 = getelementptr inbounds i8, i8* %59, i64 %21660
  %21662 = bitcast i8* %21661 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21659, <16 x i8>* %21662, i32 1, <16 x i1> %16541), !tbaa !515
  %21663 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.1664.15, <4 x i32> undef
  %21664 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21663, <4 x i32> %16791) #11
  %21665 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21664, <4 x i32> %16794) #11
  %21666 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21665, <4 x i32> %16797) #11
  %21667 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21666) #11
  %21668 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.1720.15, <4 x i32> undef
  %21669 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21668, <4 x i32> %16791) #11
  %21670 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21669, <4 x i32> %16794) #11
  %21671 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21670, <4 x i32> %16797) #11
  %21672 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21671) #11
  %21673 = shufflevector <4 x i16> %21667, <4 x i16> %21672, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21674 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21673, <8 x i16> %16800) #11
  %21675 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21674) #11
  %21676 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.1776.15, <4 x i32> undef
  %21677 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21676, <4 x i32> %16791) #11
  %21678 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21677, <4 x i32> %16794) #11
  %21679 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21678, <4 x i32> %16797) #11
  %21680 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21679) #11
  %21681 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.1832.15, <4 x i32> undef
  %21682 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21681, <4 x i32> %16791) #11
  %21683 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21682, <4 x i32> %16794) #11
  %21684 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21683, <4 x i32> %16797) #11
  %21685 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21684) #11
  %21686 = shufflevector <4 x i16> %21680, <4 x i16> %21685, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21687 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21686, <8 x i16> %16800) #11
  %21688 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21687) #11
  %21689 = shufflevector <8 x i8> %21675, <8 x i8> %21688, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21690 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21689) #11
  %21691 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21690, <16 x i8> %16804) #11
  %21692 = add nsw i64 %21563, %17031
  %21693 = getelementptr inbounds i8, i8* %59, i64 %21692
  %21694 = bitcast i8* %21693 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21691, <16 x i8>* %21694, i32 1, <16 x i1> %16541), !tbaa !515
  %21695 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.1888.15, <4 x i32> undef
  %21696 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21695, <4 x i32> %16791) #11
  %21697 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21696, <4 x i32> %16794) #11
  %21698 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21697, <4 x i32> %16797) #11
  %21699 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21698) #11
  %21700 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.1944.15, <4 x i32> undef
  %21701 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21700, <4 x i32> %16791) #11
  %21702 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21701, <4 x i32> %16794) #11
  %21703 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21702, <4 x i32> %16797) #11
  %21704 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21703) #11
  %21705 = shufflevector <4 x i16> %21699, <4 x i16> %21704, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21706 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21705, <8 x i16> %16800) #11
  %21707 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21706) #11
  %21708 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.2000.15, <4 x i32> undef
  %21709 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21708, <4 x i32> %16791) #11
  %21710 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21709, <4 x i32> %16794) #11
  %21711 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21710, <4 x i32> %16797) #11
  %21712 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21711) #11
  %21713 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.2056.15, <4 x i32> undef
  %21714 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21713, <4 x i32> %16791) #11
  %21715 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21714, <4 x i32> %16794) #11
  %21716 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21715, <4 x i32> %16797) #11
  %21717 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21716) #11
  %21718 = shufflevector <4 x i16> %21712, <4 x i16> %21717, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21719 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21718, <8 x i16> %16800) #11
  %21720 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21719) #11
  %21721 = shufflevector <8 x i8> %21707, <8 x i8> %21720, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21722 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21721) #11
  %21723 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21722, <16 x i8> %16804) #11
  %21724 = add nsw i64 %21461, %17032
  %21725 = getelementptr inbounds i8, i8* %59, i64 %21724
  %21726 = bitcast i8* %21725 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21723, <16 x i8>* %21726, i32 1, <16 x i1> %16541), !tbaa !515
  %21727 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.2112.15, <4 x i32> undef
  %21728 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21727, <4 x i32> %16791) #11
  %21729 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21728, <4 x i32> %16794) #11
  %21730 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21729, <4 x i32> %16797) #11
  %21731 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21730) #11
  %21732 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.2168.15, <4 x i32> undef
  %21733 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21732, <4 x i32> %16791) #11
  %21734 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21733, <4 x i32> %16794) #11
  %21735 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21734, <4 x i32> %16797) #11
  %21736 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21735) #11
  %21737 = shufflevector <4 x i16> %21731, <4 x i16> %21736, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21738 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21737, <8 x i16> %16800) #11
  %21739 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21738) #11
  %21740 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.2224.15, <4 x i32> undef
  %21741 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21740, <4 x i32> %16791) #11
  %21742 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21741, <4 x i32> %16794) #11
  %21743 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21742, <4 x i32> %16797) #11
  %21744 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21743) #11
  %21745 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.2280.15, <4 x i32> undef
  %21746 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21745, <4 x i32> %16791) #11
  %21747 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21746, <4 x i32> %16794) #11
  %21748 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21747, <4 x i32> %16797) #11
  %21749 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21748) #11
  %21750 = shufflevector <4 x i16> %21744, <4 x i16> %21749, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21751 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21750, <8 x i16> %16800) #11
  %21752 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21751) #11
  %21753 = shufflevector <8 x i8> %21739, <8 x i8> %21752, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21754 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21753) #11
  %21755 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21754, <16 x i8> %16804) #11
  %21756 = add nsw i64 %21495, %17032
  %21757 = getelementptr inbounds i8, i8* %59, i64 %21756
  %21758 = bitcast i8* %21757 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21755, <16 x i8>* %21758, i32 1, <16 x i1> %16541), !tbaa !515
  %21759 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.2336.15, <4 x i32> undef
  %21760 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21759, <4 x i32> %16791) #11
  %21761 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21760, <4 x i32> %16794) #11
  %21762 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21761, <4 x i32> %16797) #11
  %21763 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21762) #11
  %21764 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.2392.15, <4 x i32> undef
  %21765 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21764, <4 x i32> %16791) #11
  %21766 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21765, <4 x i32> %16794) #11
  %21767 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21766, <4 x i32> %16797) #11
  %21768 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21767) #11
  %21769 = shufflevector <4 x i16> %21763, <4 x i16> %21768, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21770 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21769, <8 x i16> %16800) #11
  %21771 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21770) #11
  %21772 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.2448.15, <4 x i32> undef
  %21773 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21772, <4 x i32> %16791) #11
  %21774 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21773, <4 x i32> %16794) #11
  %21775 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21774, <4 x i32> %16797) #11
  %21776 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21775) #11
  %21777 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.2504.15, <4 x i32> undef
  %21778 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21777, <4 x i32> %16791) #11
  %21779 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21778, <4 x i32> %16794) #11
  %21780 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21779, <4 x i32> %16797) #11
  %21781 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21780) #11
  %21782 = shufflevector <4 x i16> %21776, <4 x i16> %21781, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21783 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21782, <8 x i16> %16800) #11
  %21784 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21783) #11
  %21785 = shufflevector <8 x i8> %21771, <8 x i8> %21784, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21786 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21785) #11
  %21787 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21786, <16 x i8> %16804) #11
  %21788 = add nsw i64 %21529, %17032
  %21789 = getelementptr inbounds i8, i8* %59, i64 %21788
  %21790 = bitcast i8* %21789 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21787, <16 x i8>* %21790, i32 1, <16 x i1> %16541), !tbaa !515
  %21791 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.2560.15, <4 x i32> undef
  %21792 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21791, <4 x i32> %16791) #11
  %21793 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21792, <4 x i32> %16794) #11
  %21794 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21793, <4 x i32> %16797) #11
  %21795 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21794) #11
  %21796 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.2616.15, <4 x i32> undef
  %21797 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21796, <4 x i32> %16791) #11
  %21798 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21797, <4 x i32> %16794) #11
  %21799 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21798, <4 x i32> %16797) #11
  %21800 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21799) #11
  %21801 = shufflevector <4 x i16> %21795, <4 x i16> %21800, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21802 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21801, <8 x i16> %16800) #11
  %21803 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21802) #11
  %21804 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.2672.15, <4 x i32> undef
  %21805 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21804, <4 x i32> %16791) #11
  %21806 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21805, <4 x i32> %16794) #11
  %21807 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21806, <4 x i32> %16797) #11
  %21808 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21807) #11
  %21809 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.2728.15, <4 x i32> undef
  %21810 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21809, <4 x i32> %16791) #11
  %21811 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21810, <4 x i32> %16794) #11
  %21812 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21811, <4 x i32> %16797) #11
  %21813 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21812) #11
  %21814 = shufflevector <4 x i16> %21808, <4 x i16> %21813, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21815 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21814, <8 x i16> %16800) #11
  %21816 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21815) #11
  %21817 = shufflevector <8 x i8> %21803, <8 x i8> %21816, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21818 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21817) #11
  %21819 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21818, <16 x i8> %16804) #11
  %21820 = add nsw i64 %21563, %17032
  %21821 = getelementptr inbounds i8, i8* %59, i64 %21820
  %21822 = bitcast i8* %21821 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21819, <16 x i8>* %21822, i32 1, <16 x i1> %16541), !tbaa !515
  %21823 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.2784.15, <4 x i32> undef
  %21824 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21823, <4 x i32> %16791) #11
  %21825 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21824, <4 x i32> %16794) #11
  %21826 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21825, <4 x i32> %16797) #11
  %21827 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21826) #11
  %21828 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.2840.15, <4 x i32> undef
  %21829 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21828, <4 x i32> %16791) #11
  %21830 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21829, <4 x i32> %16794) #11
  %21831 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21830, <4 x i32> %16797) #11
  %21832 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21831) #11
  %21833 = shufflevector <4 x i16> %21827, <4 x i16> %21832, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21834 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21833, <8 x i16> %16800) #11
  %21835 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21834) #11
  %21836 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.2896.15, <4 x i32> undef
  %21837 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21836, <4 x i32> %16791) #11
  %21838 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21837, <4 x i32> %16794) #11
  %21839 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21838, <4 x i32> %16797) #11
  %21840 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21839) #11
  %21841 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.2952.15, <4 x i32> undef
  %21842 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21841, <4 x i32> %16791) #11
  %21843 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21842, <4 x i32> %16794) #11
  %21844 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21843, <4 x i32> %16797) #11
  %21845 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21844) #11
  %21846 = shufflevector <4 x i16> %21840, <4 x i16> %21845, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21847 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21846, <8 x i16> %16800) #11
  %21848 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21847) #11
  %21849 = shufflevector <8 x i8> %21835, <8 x i8> %21848, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21850 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21849) #11
  %21851 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21850, <16 x i8> %16804) #11
  %21852 = add nsw i64 %21461, %17033
  %21853 = getelementptr inbounds i8, i8* %59, i64 %21852
  %21854 = bitcast i8* %21853 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21851, <16 x i8>* %21854, i32 1, <16 x i1> %16541), !tbaa !515
  %21855 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.3008.15, <4 x i32> undef
  %21856 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21855, <4 x i32> %16791) #11
  %21857 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21856, <4 x i32> %16794) #11
  %21858 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21857, <4 x i32> %16797) #11
  %21859 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21858) #11
  %21860 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.3064.15, <4 x i32> undef
  %21861 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21860, <4 x i32> %16791) #11
  %21862 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21861, <4 x i32> %16794) #11
  %21863 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21862, <4 x i32> %16797) #11
  %21864 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21863) #11
  %21865 = shufflevector <4 x i16> %21859, <4 x i16> %21864, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21866 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21865, <8 x i16> %16800) #11
  %21867 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21866) #11
  %21868 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.3120.15, <4 x i32> undef
  %21869 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21868, <4 x i32> %16791) #11
  %21870 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21869, <4 x i32> %16794) #11
  %21871 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21870, <4 x i32> %16797) #11
  %21872 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21871) #11
  %21873 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.3176.15, <4 x i32> undef
  %21874 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21873, <4 x i32> %16791) #11
  %21875 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21874, <4 x i32> %16794) #11
  %21876 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21875, <4 x i32> %16797) #11
  %21877 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21876) #11
  %21878 = shufflevector <4 x i16> %21872, <4 x i16> %21877, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21879 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21878, <8 x i16> %16800) #11
  %21880 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21879) #11
  %21881 = shufflevector <8 x i8> %21867, <8 x i8> %21880, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21882 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21881) #11
  %21883 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21882, <16 x i8> %16804) #11
  %21884 = add nsw i64 %21495, %17033
  %21885 = getelementptr inbounds i8, i8* %59, i64 %21884
  %21886 = bitcast i8* %21885 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21883, <16 x i8>* %21886, i32 1, <16 x i1> %16541), !tbaa !515
  %21887 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.3232.15, <4 x i32> undef
  %21888 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21887, <4 x i32> %16791) #11
  %21889 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21888, <4 x i32> %16794) #11
  %21890 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21889, <4 x i32> %16797) #11
  %21891 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21890) #11
  %21892 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.3288.15, <4 x i32> undef
  %21893 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21892, <4 x i32> %16791) #11
  %21894 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21893, <4 x i32> %16794) #11
  %21895 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21894, <4 x i32> %16797) #11
  %21896 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21895) #11
  %21897 = shufflevector <4 x i16> %21891, <4 x i16> %21896, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21898 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21897, <8 x i16> %16800) #11
  %21899 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21898) #11
  %21900 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.3344.15, <4 x i32> undef
  %21901 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21900, <4 x i32> %16791) #11
  %21902 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21901, <4 x i32> %16794) #11
  %21903 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21902, <4 x i32> %16797) #11
  %21904 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21903) #11
  %21905 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.3400.15, <4 x i32> undef
  %21906 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21905, <4 x i32> %16791) #11
  %21907 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21906, <4 x i32> %16794) #11
  %21908 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21907, <4 x i32> %16797) #11
  %21909 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21908) #11
  %21910 = shufflevector <4 x i16> %21904, <4 x i16> %21909, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21911 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21910, <8 x i16> %16800) #11
  %21912 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21911) #11
  %21913 = shufflevector <8 x i8> %21899, <8 x i8> %21912, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21914 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21913) #11
  %21915 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21914, <16 x i8> %16804) #11
  %21916 = add nsw i64 %21529, %17033
  %21917 = getelementptr inbounds i8, i8* %59, i64 %21916
  %21918 = bitcast i8* %21917 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21915, <16 x i8>* %21918, i32 1, <16 x i1> %16541), !tbaa !515
  %21919 = select <4 x i1> %t3807, <4 x i32> %convolved944.sroa.3456.15, <4 x i32> undef
  %21920 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21919, <4 x i32> %16791) #11
  %21921 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21920, <4 x i32> %16794) #11
  %21922 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21921, <4 x i32> %16797) #11
  %21923 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21922) #11
  %21924 = select <4 x i1> %t3808, <4 x i32> %convolved944.sroa.3512.15, <4 x i32> undef
  %21925 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21924, <4 x i32> %16791) #11
  %21926 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21925, <4 x i32> %16794) #11
  %21927 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21926, <4 x i32> %16797) #11
  %21928 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21927) #11
  %21929 = shufflevector <4 x i16> %21923, <4 x i16> %21928, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21930 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21929, <8 x i16> %16800) #11
  %21931 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21930) #11
  %21932 = select <4 x i1> %t3809, <4 x i32> %convolved944.sroa.3568.15, <4 x i32> undef
  %21933 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21932, <4 x i32> %16791) #11
  %21934 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21933, <4 x i32> %16794) #11
  %21935 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21934, <4 x i32> %16797) #11
  %21936 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21935) #11
  %21937 = select <4 x i1> %t3810, <4 x i32> %convolved944.sroa.3624.15, <4 x i32> undef
  %21938 = call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %21937, <4 x i32> %16791) #11
  %21939 = call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %21938, <4 x i32> %16794) #11
  %21940 = call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %21939, <4 x i32> %16797) #11
  %21941 = call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %21940) #11
  %21942 = shufflevector <4 x i16> %21936, <4 x i16> %21941, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %21943 = call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %21942, <8 x i16> %16800) #11
  %21944 = call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %21943) #11
  %21945 = shufflevector <8 x i8> %21931, <8 x i8> %21944, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %21946 = call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %16802, <16 x i8> %21945) #11
  %21947 = call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %21946, <16 x i8> %16804) #11
  %21948 = add nsw i64 %21563, %17033
  %21949 = getelementptr inbounds i8, i8* %59, i64 %21948
  %21950 = bitcast i8* %21949 to <16 x i8>*
  call void @llvm.masked.store.v16i8.p0v16i8(<16 x i8> %21947, <16 x i8>* %21950, i32 1, <16 x i1> %16541), !tbaa !515
  %indvars.iv.next6427 = add nuw nsw i64 %indvars.iv6426, 1
  %.not1019 = icmp eq i64 %indvars.iv.next6427, %16818
  br i1 %.not1019, label %"end for output.s0.x.xo834", label %"for output.s0.x.xo833"
}

; Function Attrs: nounwind
define i32 @depthwise_conv_argv(i8** nocapture readonly %0) local_unnamed_addr #11 {
entry:
  %1 = bitcast i8** %0 to %struct.halide_buffer_t**
  %2 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %1, align 8
  %3 = getelementptr i8*, i8** %0, i64 1
  %4 = load i8*, i8** %3, align 8
  %5 = load i8, i8* %4, align 1
  %6 = getelementptr i8*, i8** %0, i64 2
  %7 = bitcast i8** %6 to %struct.halide_buffer_t**
  %8 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %7, align 8
  %9 = getelementptr i8*, i8** %0, i64 3
  %10 = load i8*, i8** %9, align 8
  %11 = load i8, i8* %10, align 1
  %12 = getelementptr i8*, i8** %0, i64 4
  %13 = bitcast i8** %12 to %struct.halide_buffer_t**
  %14 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %13, align 8
  %15 = getelementptr i8*, i8** %0, i64 5
  %16 = bitcast i8** %15 to i32**
  %17 = load i32*, i32** %16, align 8
  %18 = load i32, i32* %17, align 4
  %19 = getelementptr i8*, i8** %0, i64 6
  %20 = bitcast i8** %19 to i32**
  %21 = load i32*, i32** %20, align 8
  %22 = load i32, i32* %21, align 4
  %23 = getelementptr i8*, i8** %0, i64 7
  %24 = bitcast i8** %23 to i32**
  %25 = load i32*, i32** %24, align 8
  %26 = load i32, i32* %25, align 4
  %27 = getelementptr i8*, i8** %0, i64 8
  %28 = bitcast i8** %27 to i32**
  %29 = load i32*, i32** %28, align 8
  %30 = load i32, i32* %29, align 4
  %31 = getelementptr i8*, i8** %0, i64 9
  %32 = bitcast i8** %31 to i32**
  %33 = load i32*, i32** %32, align 8
  %34 = load i32, i32* %33, align 4
  %35 = getelementptr i8*, i8** %0, i64 10
  %36 = bitcast i8** %35 to i32**
  %37 = load i32*, i32** %36, align 8
  %38 = load i32, i32* %37, align 4
  %39 = getelementptr i8*, i8** %0, i64 11
  %40 = bitcast i8** %39 to i32**
  %41 = load i32*, i32** %40, align 8
  %42 = load i32, i32* %41, align 4
  %43 = getelementptr i8*, i8** %0, i64 12
  %44 = load i8*, i8** %43, align 8
  %45 = load i8, i8* %44, align 1
  %46 = getelementptr i8*, i8** %0, i64 13
  %47 = load i8*, i8** %46, align 8
  %48 = load i8, i8* %47, align 1
  %49 = getelementptr i8*, i8** %0, i64 14
  %50 = load i8*, i8** %49, align 8
  %51 = load i8, i8* %50, align 1
  %52 = getelementptr i8*, i8** %0, i64 15
  %53 = bitcast i8** %52 to %struct.halide_buffer_t**
  %54 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %53, align 8
  %55 = tail call i32 @depthwise_conv(%struct.halide_buffer_t* %2, i8 %5, %struct.halide_buffer_t* %8, i8 %11, %struct.halide_buffer_t* %14, i32 %18, i32 %22, i32 %26, i32 %30, i32 %34, i32 %38, i32 %42, i8 %45, i8 %48, i8 %51, %struct.halide_buffer_t* %54) #17
  ret i32 0
}

; Function Attrs: norecurse nounwind readnone willreturn
define nonnull %struct.halide_filter_metadata_t* @depthwise_conv_metadata() local_unnamed_addr #12 {
entry:
  ret %struct.halide_filter_metadata_t* @depthwise_conv_metadata_storage
}

; Function Attrs: argmemonly nofree nosync nounwind readonly willreturn
declare <8 x i8> @llvm.masked.load.v8i8.p0v8i8(<8 x i8>*, i32 immarg, <8 x i1>, <8 x i8>) #13

; Function Attrs: argmemonly nofree nosync nounwind willreturn writeonly
declare void @llvm.masked.store.v8i16.p0v8i16(<8 x i16>, <8 x i16>*, i32 immarg, <8 x i1>) #5

; Function Attrs: argmemonly nofree nosync nounwind willreturn writeonly
declare void @llvm.masked.store.v4i32.p0v4i32(<4 x i32>, <4 x i32>*, i32 immarg, <4 x i1>) #5

; Function Attrs: argmemonly nofree nosync nounwind readonly willreturn
declare <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>*, i32 immarg, <4 x i1>, <4 x i32>) #13

; Function Attrs: argmemonly nofree nosync nounwind readonly willreturn
declare <8 x i16> @llvm.masked.load.v8i16.p0v8i16(<8 x i16>*, i32 immarg, <8 x i1>, <8 x i16>) #13

; Function Attrs: argmemonly nofree nosync nounwind readonly willreturn
declare <16 x i8> @llvm.masked.load.v16i8.p0v16i8(<16 x i8>*, i32 immarg, <16 x i1>, <16 x i8>) #13

; Function Attrs: argmemonly nofree nosync nounwind willreturn writeonly
declare void @llvm.masked.store.v16i8.p0v16i8(<16 x i8>, <16 x i8>*, i32 immarg, <16 x i1>) #5

; Function Attrs: argmemonly nofree nosync nounwind readonly willreturn
declare <4 x i16> @llvm.masked.load.v4i16.p0v4i16(<4 x i16>*, i32 immarg, <4 x i1>, <4 x i16>) #13

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.0(<8 x i8> %arg, <8 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  %1 = sub <8 x i16> %0, %arg.1
  ret <8 x i16> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1(<8 x i8> %arg, <8 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  %1 = sub <8 x i16> %0, %arg.1
  ret <8 x i16> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.2(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.3(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.4(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.5(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.6(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.7(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.8(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.9(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.10(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.11(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.12(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.13(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.14(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.15(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.16(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.17(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.18(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.19(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.20(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.21(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.22(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.23(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.24(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.25(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.26(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.27(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.28(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.29(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.30(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.31(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.32(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.33(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.34(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.35(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.36(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.37(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.38(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.39(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.40(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.41(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.42(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.43(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.44(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.45(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.46(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.47(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.48(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.49(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.50(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.51(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.52(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.53(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.54(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.55(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.56(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.57(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.58(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.59(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.60(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.61(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.62(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.63(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.64(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.65(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.66(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.67(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.68(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.69(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.70(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.71(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.72(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.73(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.74(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.75(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.76(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.77(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.78(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.79(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.80(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.81(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.82(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.83(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.84(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.85(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.86(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.87(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.88(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.89(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.90(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.91(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.92(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.93(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.94(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.95(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.96(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.97(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.98(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.99(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.100(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.101(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.102(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.103(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.104(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.105(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.106(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.107(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.108(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.109(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.110(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.111(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.112(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.113(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.114(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.115(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.116(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.117(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.118(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.119(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.120(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.121(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.122(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.123(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.124(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.125(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.126(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.127(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.128(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.129(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.130(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.131(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.132(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.133(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.134(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.135(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.136(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.137(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.138(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.139(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.140(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.141(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.142(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.143(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.144(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.145(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.146(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.147(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.148(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.149(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.150(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.151(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.152(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.153(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.154(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.155(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.156(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.157(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.158(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.159(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.160(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.161(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.162(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.163(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.164(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.165(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.166(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.167(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.168(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.169(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.170(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.171(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.172(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.173(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.174(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.175(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.176(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.177(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.178(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.179(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.180(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.181(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.182(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.183(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.184(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.185(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.186(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.187(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.188(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.189(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.190(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.191(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.192(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.193(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.194(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.195(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.196(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.197(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.198(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.199(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.200(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.201(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.202(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.203(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.204(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.205(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.206(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.207(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.208(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.209(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.210(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.211(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.212(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.213(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.214(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.215(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.216(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.217(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.218(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.219(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.220(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.221(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.222(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.223(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.224(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.225(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.226(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.227(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.228(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.229(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.230(<8 x i8> %arg, <8 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  %1 = sub <8 x i16> %0, %arg.1
  ret <8 x i16> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.231(<8 x i8> %arg, <8 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  %1 = sub <8 x i16> %0, %arg.1
  ret <8 x i16> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.232(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.233(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.234(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.235(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.236(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.237(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.238(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.239(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.240(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.241(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.242(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.243(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.244(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.245(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.246(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.247(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.248(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.249(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.250(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.251(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.252(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.253(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.254(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.255(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.256(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.257(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.258(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.259(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.260(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.261(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.262(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.263(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.264(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.265(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.266(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.267(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.268(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.269(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.270(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.271(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.272(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.273(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.274(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.275(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.276(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.277(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.278(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.279(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.280(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.281(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.282(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.283(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.284(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.285(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.286(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.287(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.288(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.289(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.290(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.291(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.292(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.293(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.294(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.295(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.296(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.297(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.298(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.299(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.300(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.301(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.302(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.303(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.304(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.305(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.306(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.307(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.308(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.309(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.310(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.311(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.312(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.313(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.314(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.315(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.316(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.317(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.318(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.319(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.320(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.321(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.322(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.323(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.324(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.325(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.326(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.327(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.328(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.329(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.330(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.331(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.332(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.333(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.334(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.335(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.336(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.337(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.338(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.339(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.340(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.341(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.342(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.343(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.344(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.345(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.346(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.347(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.348(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.349(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.350(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.351(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.352(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.353(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.354(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.355(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.356(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.357(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.358(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.359(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.360(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.361(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.362(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.363(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.364(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.365(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.366(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.367(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.368(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.369(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.370(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.371(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.372(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.373(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.374(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.375(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.376(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.377(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.378(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.379(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.380(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.381(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.382(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.383(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.384(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.385(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.386(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.387(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.388(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.389(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.390(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.391(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.392(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.393(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.394(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.395(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.396(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.397(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.398(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.399(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.400(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.401(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.402(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.403(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.404(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.405(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.406(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.407(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.408(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.409(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.410(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.411(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.412(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.413(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.414(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.415(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.416(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.417(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.418(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.419(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.420(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.421(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.422(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.423(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.424(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.425(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.426(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.427(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.428(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.429(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.430(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.431(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.432(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.433(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.434(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.435(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.436(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.437(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.438(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.439(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.440(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.441(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.442(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.443(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.444(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.445(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.446(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.447(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.448(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.449(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.450(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.451(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.452(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.453(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.454(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.455(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.456(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.457(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.458(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.459(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.460(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.461(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.462(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.463(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.464(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.465(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.466(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.467(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.468(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.469(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.470(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.471(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.472(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.473(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.474(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.475(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.476(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.477(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.478(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.479(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.480(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.481(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.482(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.483(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.484(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.485(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.486(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.487(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.488(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.489(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.490(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.491(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.492(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.493(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.494(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.495(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.496(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.497(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.498(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.499(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.500(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.501(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.502(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.503(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.504(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.505(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.506(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.507(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.508(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.509(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.510(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.511(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.512(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.513(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.514(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.515(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.516(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.517(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.518(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.519(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.520(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.521(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.522(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.523(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.524(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.525(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.526(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.527(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.528(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.529(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.530(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.531(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.532(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.533(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.534(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.535(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.536(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.537(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.538(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.539(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.540(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.541(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.542(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.543(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.544(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.545(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.546(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.547(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.548(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.549(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.550(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.551(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.552(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.553(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.554(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.555(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.556(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.557(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.558(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.559(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.560(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.561(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.562(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.563(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.564(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.565(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.566(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.567(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.568(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.569(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.570(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.571(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.572(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.573(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.574(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.575(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.576(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.577(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.578(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.579(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.580(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.581(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.582(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.583(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.584(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.585(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.586(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.587(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.588(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.589(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.590(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.591(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.592(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.593(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.594(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.595(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.596(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.597(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.598(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.599(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.600(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.601(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.602(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.603(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.604(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.605(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.606(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.607(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.608(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.609(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.610(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.611(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.612(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.613(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.614(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.615(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.616(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.617(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.618(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.619(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.620(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.621(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.622(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.623(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.624(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.625(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.626(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.627(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.628(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.629(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.630(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.631(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.632(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.633(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.634(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.635(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.636(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.637(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.638(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.639(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.640(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.641(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.642(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.643(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.644(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.645(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.646(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.647(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.648(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.649(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.650(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.651(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.652(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.653(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.654(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.655(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.656(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.657(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.658(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.659(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.660(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.661(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.662(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.663(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.664(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.665(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.666(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.667(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.668(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.669(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.670(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.671(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.672(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.673(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.674(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.675(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.676(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.677(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.678(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.679(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.680(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.681(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.682(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.683(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.684(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.685(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.686(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.687(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.688(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.689(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.690(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.691(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.692(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.693(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.694(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.695(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.696(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.697(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.698(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.699(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.700(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.701(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.702(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.703(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.704(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.705(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.706(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.707(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.708(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.709(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.710(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.711(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.712(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.713(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.714(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.715(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.716(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.717(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.718(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.719(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.720(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.721(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.722(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.723(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.724(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.725(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.726(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.727(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.728(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.729(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.730(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.731(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.732(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.733(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.734(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.735(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.736(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.737(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.738(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.739(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.740(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.741(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.742(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.743(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.744(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.745(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.746(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.747(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.748(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.749(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.750(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.751(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.752(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.753(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.754(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.755(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.756(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.757(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.758(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.759(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.760(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.761(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.762(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.763(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.764(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.765(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.766(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.767(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.768(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.769(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.770(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.771(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.772(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.773(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.774(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.775(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.776(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.777(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.778(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.779(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.780(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.781(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.782(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.783(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.784(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.785(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.786(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.787(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.788(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.789(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.790(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.791(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.792(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.793(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.794(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.795(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.796(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.797(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.798(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.799(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.800(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.801(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.802(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.803(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.804(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.805(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.806(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.807(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.808(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.809(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.810(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.811(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.812(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.813(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.814(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.815(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.816(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.817(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.818(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.819(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.820(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.821(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.822(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.823(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.824(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.825(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.826(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.827(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.828(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.829(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.830(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.831(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.832(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.833(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.834(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.835(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.836(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.837(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.838(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.839(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.840(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.841(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.842(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.843(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.844(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.845(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.846(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.847(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.848(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.849(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.850(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.851(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.852(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.853(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.854(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.855(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.856(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.857(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.858(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.859(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.860(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.861(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.862(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.863(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.864(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.865(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.866(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.867(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.868(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.869(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.870(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.871(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.872(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.873(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.874(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.875(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.876(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.877(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.878(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.879(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.880(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.881(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.882(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.883(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.884(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.885(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.886(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.887(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.888(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.889(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.890(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.891(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.892(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.893(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.894(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.895(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.896(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.897(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.898(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.899(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.900(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.901(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.902(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.903(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.904(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.905(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.906(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.907(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.908(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.909(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.910(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.911(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.912(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.913(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.914(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.915(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.916(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.917(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.918(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.919(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.920(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.921(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.922(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.923(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.924(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.925(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.926(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.927(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.928(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.929(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.930(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.931(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.932(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.933(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.934(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.935(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.936(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.937(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.938(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.939(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.940(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.941(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.942(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.943(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.944(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.945(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.946(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.947(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.948(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.949(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.950(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.951(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.952(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.953(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.954(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.955(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.956(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.957(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.958(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.959(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.960(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.961(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.962(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.963(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.964(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.965(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.966(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.967(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.968(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.969(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.970(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.971(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.972(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.973(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.974(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.975(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.976(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.977(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.978(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.979(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.980(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.981(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.982(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.983(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.984(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.985(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.986(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.987(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.988(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.989(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.990(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.991(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.992(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.993(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.994(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.995(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.996(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.997(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.998(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.999(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1000(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1001(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1002(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1003(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1004(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1005(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1006(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1007(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1008(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1009(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1010(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1011(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1012(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1013(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1014(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1015(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1016(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1017(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1018(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1019(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1020(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1021(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1022(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1023(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1024(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1025(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1026(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1027(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1028(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1029(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1030(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1031(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1032(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1033(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1034(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1035(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1036(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1037(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1038(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1039(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1040(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1041(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1042(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1043(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1044(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1045(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1046(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1047(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1048(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1049(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1050(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1051(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1052(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1053(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1054(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1055(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1056(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1057(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1058(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1059(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1060(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1061(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1062(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1063(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1064(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1065(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1066(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1067(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1068(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1069(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1070(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1071(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1072(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1073(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1074(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1075(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1076(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1077(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1078(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1079(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1080(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1081(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1082(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1083(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1084(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1085(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1086(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1087(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1088(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1089(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1090(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1091(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1092(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1093(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1094(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1095(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1096(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1097(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1098(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1099(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1100(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1101(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1102(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1103(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1104(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1105(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1106(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1107(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1108(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1109(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1110(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1111(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1112(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1113(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1114(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1115(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1116(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1117(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1118(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1119(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1120(<8 x i8> %arg, <8 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  %1 = sub <8 x i16> %0, %arg.1
  ret <8 x i16> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1121(<8 x i8> %arg, <8 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  %1 = sub <8 x i16> %0, %arg.1
  ret <8 x i16> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1122(<8 x i8> %arg, <8 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  %1 = sub <8 x i16> %0, %arg.1
  ret <8 x i16> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1123(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1124(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1125(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1126(<4 x i32> %arg, <4 x i16> %arg.1) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = add <4 x i32> %0, %arg
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1127(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1128(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1129(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1130(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = mul <4 x i32> %arg.2, %arg.1
  %1 = sub <4 x i32> %arg, %0
  ret <4 x i32> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1131(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1132(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1133(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1134(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1135(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1136(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1137(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1138(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1139(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1140(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1141(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1142(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1143(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1144(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1145(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1146(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1147(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1148(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1149(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1150(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1151(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1152(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1153(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1154(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1155(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1156(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1157(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1158(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1159(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1160(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1161(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1162(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1163(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1164(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1165(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1166(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1167(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1168(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1169(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1170(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1171(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1172(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1173(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1174(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1175(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1176(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1177(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1178(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1179(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1180(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1181(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1182(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1183(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1184(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1185(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1186(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1187(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1188(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1189(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1190(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1191(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1192(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1193(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1194(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1195(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1196(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1197(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1198(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1199(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1200(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1201(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1202(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1203(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1204(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1205(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1206(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1207(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1208(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1209(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1210(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1211(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1212(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1213(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1214(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1215(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1216(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1217(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1218(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1219(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1220(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1221(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1222(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1223(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1224(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1225(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1226(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1227(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1228(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1229(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1230(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1231(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1232(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1233(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1234(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1235(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1236(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1237(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1238(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1239(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1240(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1241(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1242(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1243(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1244(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1245(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1246(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1247(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1248(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1249(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1250(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1251(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1252(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1253(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1254(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1255(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1256(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1257(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1258(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1259(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1260(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1261(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1262(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1263(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1264(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1265(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1266(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1267(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1268(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1269(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1270(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1271(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1272(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1273(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1274(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1275(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1276(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1277(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1278(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1279(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1280(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1281(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1282(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1283(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1284(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1285(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1286(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1287(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1288(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1289(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1290(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1291(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1292(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1293(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1294(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1295(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1296(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1297(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1298(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1299(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1300(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1301(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1302(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1303(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1304(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1305(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1306(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1307(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1308(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1309(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1310(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1311(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1312(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1313(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1314(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1315(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1316(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1317(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1318(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1319(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1320(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1321(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1322(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1323(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1324(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1325(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1326(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1327(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1328(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1329(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1330(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1331(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1332(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1333(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1334(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1335(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1336(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1337(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1338(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1339(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1340(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1341(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1342(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1343(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1344(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1345(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1346(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1347(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1348(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1349(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1350(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1351(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1352(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1353(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1354(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1355(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1356(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1357(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1358(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1359(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1360(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1361(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1362(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1363(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1364(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1365(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1366(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1367(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1368(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1369(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1370(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1371(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1372(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1373(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1374(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1375(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1376(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1377(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1378(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1379(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1380(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1381(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1382(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1383(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1384(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1385(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1386(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1387(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1388(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1389(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1390(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1391(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1392(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1393(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1394(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1395(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1396(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1397(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1398(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1399(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1400(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1401(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1402(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1403(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1404(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1405(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1406(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1407(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1408(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1409(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1410(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1411(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1412(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1413(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1414(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1415(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1416(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1417(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1418(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1419(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1420(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1421(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1422(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1423(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1424(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1425(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1426(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1427(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1428(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1429(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1430(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1431(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1432(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1433(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1434(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1435(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1436(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1437(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1438(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1439(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1440(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1441(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1442(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1443(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1444(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1445(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1446(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1447(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1448(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1449(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1450(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1451(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1452(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1453(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1454(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1455(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1456(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1457(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1458(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1459(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1460(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1461(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1462(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1463(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1464(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1465(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1466(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1467(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1468(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1469(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1470(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1471(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1472(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1473(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1474(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1475(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1476(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1477(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1478(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1479(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1480(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1481(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1482(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1483(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1484(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1485(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1486(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1487(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1488(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1489(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1490(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1491(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1492(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1493(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1494(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1495(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1496(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1497(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1498(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1499(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1500(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1501(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1502(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1503(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1504(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1505(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1506(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1507(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1508(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1509(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1510(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1511(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1512(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1513(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1514(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1515(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1516(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1517(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1518(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1519(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1520(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1521(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1522(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1523(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1524(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1525(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1526(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1527(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1528(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1529(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1530(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1531(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1532(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1533(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1534(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1535(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1536(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1537(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1538(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1539(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1540(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1541(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1542(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1543(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1544(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1545(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1546(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1547(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1548(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1549(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1550(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1551(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1552(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1553(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1554(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1555(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1556(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1557(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1558(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1559(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1560(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1561(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1562(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1563(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1564(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1565(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1566(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1567(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1568(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1569(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1570(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1571(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1572(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1573(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1574(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1575(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1576(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1577(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1578(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1579(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1580(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1581(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1582(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1583(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1584(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1585(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1586(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1587(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1588(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1589(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1590(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1591(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1592(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1593(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1594(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1595(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1596(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1597(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1598(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1599(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1600(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1601(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1602(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1603(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1604(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1605(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1606(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1607(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1608(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1609(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1610(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1611(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1612(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1613(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1614(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1615(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1616(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1617(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1618(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1619(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1620(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1621(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1622(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1623(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1624(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1625(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1626(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1627(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1628(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1629(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1630(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1631(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1632(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1633(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1634(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1635(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1636(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1637(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1638(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1639(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1640(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1641(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1642(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1643(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1644(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1645(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1646(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1647(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1648(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1649(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1650(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1651(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1652(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1653(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1654(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1655(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1656(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1657(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1658(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1659(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1660(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1661(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1662(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1663(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1664(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1665(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1666(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1667(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1668(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1669(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1670(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1671(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1672(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1673(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1674(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1675(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1676(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1677(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1678(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1679(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1680(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1681(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1682(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1683(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1684(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1685(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1686(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1687(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1688(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1689(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1690(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1691(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1692(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1693(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1694(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1695(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1696(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1697(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1698(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1699(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1700(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1701(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1702(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1703(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1704(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1705(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1706(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1707(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1708(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1709(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1710(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1711(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1712(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1713(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1714(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1715(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1716(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1717(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1718(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1719(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1720(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1721(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1722(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1723(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1724(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1725(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1726(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1727(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1728(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1729(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1730(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1731(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1732(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1733(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1734(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1735(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1736(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1737(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1738(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1739(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1740(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1741(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1742(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1743(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1744(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1745(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1746(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1747(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1748(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1749(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1750(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1751(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1752(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1753(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1754(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1755(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1756(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1757(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1758(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1759(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1760(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1761(<4 x i16> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2, <4 x i16> %arg.3, <4 x i16> %arg.4, <4 x i16> %arg.5, <4 x i16> %arg.6, <4 x i16> %arg.7, <4 x i16> %arg.8, <4 x i16> %arg.9, <4 x i16> %arg.10, <4 x i16> %arg.11, <4 x i16> %arg.12, <4 x i16> %arg.13, <4 x i16> %arg.14, <4 x i16> %arg.15, <4 x i32> %arg.16, <4 x i16> %arg.17, <4 x i16> %arg.18) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.3, <4 x i16> %arg.2) #11
  %1 = sext <4 x i16> %arg.1 to <4 x i32>
  %2 = sext <4 x i16> %arg to <4 x i32>
  %3 = mul nsw <4 x i32> %1, %2
  %4 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.7, <4 x i16> %arg.6) #11
  %5 = sext <4 x i16> %arg.5 to <4 x i32>
  %6 = sext <4 x i16> %arg.4 to <4 x i32>
  %7 = mul nsw <4 x i32> %5, %6
  %8 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.11, <4 x i16> %arg.10) #11
  %9 = sext <4 x i16> %arg.9 to <4 x i32>
  %10 = sext <4 x i16> %arg.8 to <4 x i32>
  %11 = mul nsw <4 x i32> %9, %10
  %12 = tail call <4 x i32> @llvm.aarch64.neon.smull.v4i32(<4 x i16> %arg.15, <4 x i16> %arg.14) #11
  %13 = sext <4 x i16> %arg.13 to <4 x i32>
  %14 = sext <4 x i16> %arg.12 to <4 x i32>
  %15 = mul nsw <4 x i32> %13, %14
  %16 = sext <4 x i16> %arg.17 to <4 x i32>
  %17 = sext <4 x i16> %arg.18 to <4 x i32>
  %18 = mul nsw <4 x i32> %17, %16
  %19 = add <4 x i32> %7, %3
  %20 = add <4 x i32> %19, %11
  %21 = add <4 x i32> %20, %15
  %22 = add <4 x i32> %21, %arg.16
  %23 = add <4 x i32> %22, %18
  %24 = add <4 x i32> %23, %0
  %25 = add <4 x i32> %24, %4
  %26 = add <4 x i32> %25, %8
  %27 = add <4 x i32> %26, %12
  ret <4 x i32> %27
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1762(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1763(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1764(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1765(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1766(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1767(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1768(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1769(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1770(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1771(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1772(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1773(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1774(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1775(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1776(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1777(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1778(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1779(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1780(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1781(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1782(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1783(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1784(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1785(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1786(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1787(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1788(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1789(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1790(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1791(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1792(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1793(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1794(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1795(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1796(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1797(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1798(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1799(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1800(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1801(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1802(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1803(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1804(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1805(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1806(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1807(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1808(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1809(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1810(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1811(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1812(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1813(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1814(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1815(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1816(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1817(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1818(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1819(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1820(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1821(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1822(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1823(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1824(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1825(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1826(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1827(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1828(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1829(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1830(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1831(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1832(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1833(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1834(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1835(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1836(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1837(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1838(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1839(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1840(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1841(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1842(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1843(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1844(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1845(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1846(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1847(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1848(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1849(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1850(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1851(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1852(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1853(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1854(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1855(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1856(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1857(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1858(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1859(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1860(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1861(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1862(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1863(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1864(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1865(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1866(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1867(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1868(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1869(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1870(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1871(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1872(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1873(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1874(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1875(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1876(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1877(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1878(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1879(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1880(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1881(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1882(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1883(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1884(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1885(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1886(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1887(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1888(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1889(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1890(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1891(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1892(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1893(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1894(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1895(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1896(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: norecurse nounwind readnone willreturn
define <4 x i32> @hydride.node.depthwise_conv_arm_depth2.1897(<4 x i32> %arg, <4 x i16> %arg.1, <4 x i16> %arg.2) local_unnamed_addr #12 {
entry:
  %0 = sext <4 x i16> %arg.1 to <4 x i32>
  %1 = sext <4 x i16> %arg.2 to <4 x i32>
  %2 = mul nsw <4 x i32> %1, %0
  %3 = add <4 x i32> %2, %arg
  ret <4 x i32> %3
}

; Function Attrs: norecurse nounwind readnone willreturn
define <8 x i16> @hydride.node.depthwise_conv_arm_depth2.1898(<8 x i8> %arg) local_unnamed_addr #12 {
entry:
  %0 = zext <8 x i8> %arg to <8 x i16>
  ret <8 x i16> %0
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1899(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1900(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1901(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1902(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1903(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1904(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1905(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1906(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1907(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1908(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1909(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1910(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1911(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1912(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1913(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1914(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1915(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1916(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1917(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1918(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1919(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1920(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1921(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1922(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1923(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1924(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1925(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1926(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1927(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1928(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1929(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1930(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1931(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1932(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1933(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1934(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1935(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1936(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1937(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1938(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1939(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1940(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1941(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1942(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1943(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1944(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1945(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1946(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1947(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1948(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1949(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1950(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1951(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1952(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1953(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1954(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1955(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1956(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1957(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1958(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1959(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1960(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1961(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1962(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1963(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1964(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1965(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1966(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1967(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1968(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1969(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1970(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1971(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1972(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1973(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1974(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1975(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1976(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1977(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1978(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1979(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1980(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1981(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1982(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1983(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1984(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1985(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1986(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1987(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1988(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1989(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1990(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1991(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1992(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1993(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1994(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1995(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1996(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.1997(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.1998(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.1999(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.2000(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.2001(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.2002(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.2003(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <16 x i8> @hydride.node.depthwise_conv_arm_depth2.2004(<16 x i8> %arg, <16 x i8> %arg.1, <16 x i8> %arg.2) local_unnamed_addr #14 {
entry:
  %0 = tail call <16 x i8> @llvm.aarch64.neon.umin.v16i8(<16 x i8> %arg.1, <16 x i8> %arg) #11
  %1 = tail call <16 x i8> @llvm.aarch64.neon.umax.v16i8(<16 x i8> %0, <16 x i8> %arg.2) #11
  ret <16 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.2005(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.2006(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.2007(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <8 x i8> @hydride.node.depthwise_conv_arm_depth2.2008(<8 x i16> %arg, <8 x i16> %arg.1) local_unnamed_addr #14 {
entry:
  %0 = tail call <8 x i16> @llvm.aarch64.neon.sqadd.v8i16(<8 x i16> %arg, <8 x i16> %arg.1) #11
  %1 = tail call <8 x i8> @llvm.aarch64.neon.sqxtun.v8i8(<8 x i16> %0) #11
  ret <8 x i8> %1
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.2009(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nounwind readnone willreturn
define <4 x i16> @hydride.node.depthwise_conv_arm_depth2.2010(<4 x i32> %arg, <4 x i32> %arg.1, <4 x i32> %arg.2, <4 x i32> %arg.3) local_unnamed_addr #14 {
entry:
  %0 = tail call <4 x i32> @llvm.aarch64.neon.sqrdmulh.v4i32(<4 x i32> %arg, <4 x i32> %arg.1) #11
  %1 = tail call <4 x i32> @llvm.aarch64.neon.sqadd.v4i32(<4 x i32> %0, <4 x i32> %arg.2) #11
  %2 = tail call <4 x i32> @llvm.aarch64.neon.sshl.v4i32(<4 x i32> %1, <4 x i32> %arg.3) #11
  %3 = tail call <4 x i16> @llvm.aarch64.neon.sqxtn.v4i16(<4 x i32> %2) #11
  ret <4 x i16> %3
}

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i64 @llvm.vector.reduce.add.v2i64(<2 x i64>) #9

attributes #0 = { nounwind mustprogress "disable-tail-calls"="false" "frame-pointer"="all" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { "disable-tail-calls"="false" "frame-pointer"="all" "less-precise-fpmad"="false" "no-builtins" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nounwind willreturn mustprogress "disable-tail-calls"="false" "frame-pointer"="all" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { argmemonly nofree nosync nounwind willreturn }
attributes #4 = { nounwind "disable-tail-calls"="false" "frame-pointer"="all" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { argmemonly nofree nosync nounwind willreturn writeonly }
attributes #6 = { alwaysinline nounwind willreturn mustprogress "disable-tail-calls"="false" "frame-pointer"="all" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #7 = { nofree nosync nounwind readnone speculatable willreturn }
attributes #8 = { nounwind willreturn "disable-tail-calls"="false" "frame-pointer"="all" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-builtins" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #9 = { nofree nosync nounwind readnone willreturn }
attributes #10 = { nounwind "reciprocal-estimates"="none" }
attributes #11 = { nounwind }
attributes #12 = { norecurse nounwind readnone willreturn }
attributes #13 = { argmemonly nofree nosync nounwind readonly willreturn }
attributes #14 = { nounwind readnone willreturn }
attributes #15 = { nobuiltin nounwind "no-builtins" }
attributes #16 = { nobuiltin "no-builtins" }
attributes #17 = { noinline }

!llvm.module.flags = !{!0, !1, !2, !3, !4, !5, !6, !7, !8, !9, !10, !11, !12}
!llvm.ident = !{!13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13, !13}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 2, !"halide_use_soft_float_abi", i32 0}
!3 = !{i32 2, !"halide_mcpu", !"apple-a12"}
!4 = !{i32 2, !"halide_mattrs", !"+reserve-x18"}
!5 = !{i32 2, !"halide_mabi", !""}
!6 = !{i32 2, !"halide_use_pic", i32 1}
!7 = !{i32 2, !"halide_use_large_code_model", i32 0}
!8 = !{i32 2, !"halide_per_instruction_fast_math_flags", i32 0}
!9 = !{i32 1, !"branch-target-enforcement", i32 0}
!10 = !{i32 1, !"sign-return-address", i32 0}
!11 = !{i32 1, !"sign-return-address-all", i32 0}
!12 = !{i32 1, !"sign-return-address-with-bkey", i32 0}
!13 = !{!"clang version 12.0.1 (https://github.com/llvm/llvm-project.git fed41342a82f5a3a9201819a82bf7a48313e296b)"}
!14 = !{!15, !15, i64 0}
!15 = !{!"any pointer", !16, i64 0}
!16 = !{!"omnipotent char", !17, i64 0}
!17 = !{!"Simple C++ TBAA"}
!18 = !{!16, !16, i64 0}
!19 = !{!20, !20, i64 0}
!20 = !{!"bool", !16, i64 0}
!21 = !{i8 0, i8 2}
!22 = !{!23, !23, i64 0}
!23 = !{!"long long", !16, i64 0}
!24 = !{!25, !26, i64 0}
!25 = !{!"_ZTS18mach_timebase_info", !26, i64 0, !26, i64 4}
!26 = !{!"int", !16, i64 0}
!27 = !{!25, !26, i64 4}
!28 = !{!29, !15, i64 0}
!29 = !{!"_ZTSN6Halide7Runtime8Internal4workE", !30, i64 0, !15, i64 56, !15, i64 64, !15, i64 72, !26, i64 80, !15, i64 88, !26, i64 96, !15, i64 104, !26, i64 112, !26, i64 116, !26, i64 120, !20, i64 124}
!30 = !{!"_ZTS22halide_parallel_task_t", !15, i64 0, !15, i64 8, !15, i64 16, !15, i64 24, !26, i64 32, !26, i64 36, !26, i64 40, !26, i64 44, !20, i64 48}
!31 = !{!29, !26, i64 36}
!32 = !{!29, !26, i64 40}
!33 = !{!29, !20, i64 48}
!34 = !{!29, !15, i64 24}
!35 = !{!29, !26, i64 32}
!36 = !{!29, !15, i64 8}
!37 = !{!29, !26, i64 44}
!38 = !{!29, !15, i64 16}
!39 = !{!29, !15, i64 56}
!40 = !{!29, !15, i64 104}
!41 = !{!26, !26, i64 0}
!42 = !{!29, !26, i64 120}
!43 = !{!29, !20, i64 124}
!44 = !{!29, !15, i64 72}
!45 = !{!29, !26, i64 80}
!46 = !{!29, !15, i64 88}
!47 = !{!29, !26, i64 116}
!48 = !{!49, !49, i64 0}
!49 = !{!"vtable pointer", !17, i64 0}
!50 = !{!51, !15, i64 8}
!51 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization21mutex_parking_controlE", !15, i64 8}
!52 = !{!53, !20, i64 2121}
!53 = !{!"_ZTSN6Halide7Runtime8Internal12work_queue_tE", !54, i64 0, !26, i64 8, !26, i64 12, !15, i64 16, !26, i64 24, !26, i64 28, !26, i64 32, !55, i64 40, !55, i64 48, !55, i64 56, !26, i64 64, !26, i64 68, !16, i64 72, !20, i64 2120, !20, i64 2121, !26, i64 2124}
!54 = !{!"_ZTS12halide_mutex", !16, i64 0}
!55 = !{!"_ZTS11halide_cond", !16, i64 0}
!56 = distinct !{!56, !57}
!57 = !{!"llvm.loop.mustprogress"}
!58 = !{!53, !26, i64 8}
!59 = distinct !{!59, !57}
!60 = !{!53, !26, i64 24}
!61 = !{!53, !26, i64 2124}
!62 = !{!53, !26, i64 28}
!63 = distinct !{!63, !57}
!64 = !{!29, !26, i64 112}
!65 = !{!29, !26, i64 96}
!66 = !{!53, !15, i64 16}
!67 = !{!29, !15, i64 64}
!68 = distinct !{!68, !57, !69}
!69 = !{!"llvm.loop.isvectorized", i32 1}
!70 = !{!53, !26, i64 68}
!71 = !{!53, !26, i64 32}
!72 = distinct !{!72, !57, !69}
!73 = !{!53, !20, i64 2120}
!74 = distinct !{!74, !57}
!75 = !{!76, !15, i64 0}
!76 = !{!"_ZTS26halide_semaphore_acquire_t", !15, i64 0, !26, i64 8}
!77 = !{!76, !26, i64 8}
!78 = distinct !{!78, !57}
!79 = !{!53, !26, i64 64}
!80 = distinct !{!80, !57}
!81 = distinct !{!81, !57}
!82 = !{i64 0, i64 8, !14, i64 8, i64 8, !14, i64 16, i64 8, !14, i64 24, i64 8, !14, i64 32, i64 4, !41, i64 36, i64 4, !41, i64 40, i64 4, !41, i64 44, i64 4, !41, i64 48, i64 1, !19, i64 56, i64 8, !14, i64 64, i64 8, !14, i64 72, i64 8, !14, i64 80, i64 4, !41, i64 88, i64 8, !14, i64 96, i64 4, !41, i64 104, i64 8, !14, i64 112, i64 4, !41, i64 116, i64 4, !41, i64 120, i64 4, !41, i64 124, i64 1, !19}
!83 = !{i64 0, i64 8, !14, i64 8, i64 8, !14, i64 16, i64 8, !14, i64 24, i64 4, !41, i64 28, i64 4, !41, i64 32, i64 4, !41, i64 36, i64 4, !41, i64 40, i64 1, !19, i64 48, i64 8, !14, i64 56, i64 8, !14, i64 64, i64 8, !14, i64 72, i64 4, !41, i64 80, i64 8, !14, i64 88, i64 4, !41, i64 96, i64 8, !14, i64 104, i64 4, !41, i64 108, i64 4, !41, i64 112, i64 4, !41, i64 116, i64 1, !19}
!84 = !{i64 0, i64 8, !14, i64 8, i64 8, !14, i64 16, i64 8, !14, i64 24, i64 4, !41, i64 32, i64 8, !14, i64 40, i64 4, !41, i64 48, i64 8, !14, i64 56, i64 4, !41, i64 60, i64 4, !41, i64 64, i64 4, !41, i64 68, i64 1, !19}
!85 = !{i64 0, i64 8, !14, i64 8, i64 4, !41, i64 12, i64 4, !41, i64 16, i64 4, !41, i64 20, i64 1, !19}
!86 = distinct !{!86, !57}
!87 = distinct !{!87, !57}
!88 = !{!89, !15, i64 144}
!89 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization10queue_dataE", !90, i64 0, !23, i64 136, !15, i64 144, !23, i64 152}
!90 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization13thread_parkerE", !91, i64 0, !92, i64 64, !20, i64 128}
!91 = !{!"_ZTS15pthread_mutex_t", !16, i64 0}
!92 = !{!"_ZTS14pthread_cond_t", !16, i64 0}
!93 = !{!94, !15, i64 16}
!94 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization11hash_bucketE", !95, i64 0, !15, i64 8, !15, i64 16}
!95 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization9word_lockE", !23, i64 0}
!96 = distinct !{!96, !57}
!97 = !{!89, !23, i64 152}
!98 = !{!90, !20, i64 128}
!99 = distinct !{!99, !57}
!100 = !{!101, !15, i64 152}
!101 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization20word_lock_queue_dataE", !90, i64 0, !15, i64 136, !15, i64 144, !15, i64 152}
!102 = !{!101, !15, i64 136}
!103 = !{!101, !15, i64 144}
!104 = distinct !{!104, !57}
!105 = distinct !{!105, !57}
!106 = distinct !{!106, !57}
!107 = !{!108, !15, i64 8}
!108 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization25broadcast_parking_controlE", !15, i64 8, !15, i64 16}
!109 = !{!108, !15, i64 16}
!110 = !{!111, !15, i64 8}
!111 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization20wait_parking_controlE", !15, i64 8, !15, i64 16}
!112 = !{!111, !15, i64 16}
!113 = !{!89, !23, i64 136}
!114 = !{!115, !20, i64 0}
!115 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization15validate_actionE", !20, i64 0, !23, i64 8}
!116 = !{!115, !23, i64 8}
!117 = !{!94, !15, i64 8}
!118 = !{!119, !15, i64 0}
!119 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization11bucket_pairE", !15, i64 0, !15, i64 8}
!120 = distinct !{!120, !57}
!121 = !{!119, !15, i64 8}
!122 = !{!123, !15, i64 0}
!123 = !{!"_ZTSN6Halide7Runtime8Internal14spawned_threadE", !15, i64 0, !15, i64 8, !124, i64 16}
!124 = !{!"long", !16, i64 0}
!125 = !{!123, !15, i64 8}
!126 = !{!123, !124, i64 16}
!127 = !{!30, !26, i64 40}
!128 = !{i64 0, i64 8, !14, i64 8, i64 8, !14, i64 16, i64 8, !14, i64 24, i64 8, !14, i64 32, i64 4, !41, i64 36, i64 4, !41, i64 40, i64 4, !41, i64 44, i64 4, !41, i64 48, i64 1, !19}
!129 = distinct !{!129, !57}
!130 = distinct !{!130, !57}
!131 = distinct !{!131, !57}
!132 = !{!133, !15, i64 8}
!133 = !{!"_ZTSN6Halide7Runtime8Internal15Synchronization22signal_parking_controlE", !15, i64 8, !15, i64 16}
!134 = !{!133, !15, i64 16}
!135 = !{!136, !15, i64 0}
!136 = !{!"_ZTS18halide_mutex_array", !15, i64 0}
!137 = distinct !{!137, !57}
!138 = !{!139, !142, i64 34}
!139 = !{!"_ZTS20halide_trace_event_t", !15, i64 0, !15, i64 8, !15, i64 16, !15, i64 24, !140, i64 32, !143, i64 36, !26, i64 40, !26, i64 44, !26, i64 48}
!140 = !{!"_ZTS13halide_type_t", !141, i64 0, !16, i64 1, !142, i64 2}
!141 = !{!"_ZTS18halide_type_code_t", !16, i64 0}
!142 = !{!"short", !16, i64 0}
!143 = !{!"_ZTS25halide_trace_event_code_t", !16, i64 0}
!144 = !{!140, !16, i64 1}
!145 = !{!139, !26, i64 48}
!146 = !{!139, !15, i64 0}
!147 = !{!139, !15, i64 24}
!148 = !{!149, !26, i64 0}
!149 = !{!"_ZTSN6Halide7Runtime8Internal23SharedExclusiveSpinLockE", !26, i64 0}
!150 = !{!151, !26, i64 4}
!151 = !{!"_ZTSN6Halide7Runtime8Internal11TraceBufferE", !149, i64 0, !26, i64 4, !26, i64 8, !16, i64 12}
!152 = !{!151, !26, i64 8}
!153 = distinct !{!153, !57}
!154 = !{!155, !26, i64 0}
!155 = !{!"_ZTS21halide_trace_packet_t", !26, i64 0, !26, i64 4, !140, i64 8, !143, i64 12, !26, i64 16, !26, i64 20, !26, i64 24}
!156 = !{!155, !26, i64 4}
!157 = !{!139, !15, i64 16}
!158 = !{!139, !15, i64 8}
!159 = !{!155, !26, i64 24}
!160 = !{!155, !142, i64 10}
!161 = distinct !{!161, !57}
!162 = !{!139, !143, i64 36}
!163 = !{!139, !16, i64 33}
!164 = distinct !{!164, !57}
!165 = !{!139, !26, i64 44}
!166 = distinct !{!166, !57, !167}
!167 = !{!"llvm.loop.peeled.count", i32 1}
!168 = !{!139, !141, i64 32}
!169 = !{!142, !142, i64 0}
!170 = !{!171, !171, i64 0}
!171 = !{!"float", !16, i64 0}
!172 = !{!173, !173, i64 0}
!173 = !{!"double", !16, i64 0}
!174 = distinct !{!174, !57, !167}
!175 = distinct !{!175, !57}
!176 = !{!139, !26, i64 40}
!177 = distinct !{!177, !57}
!178 = distinct !{!178, !57}
!179 = distinct !{!179, !57}
!180 = !{!181, !15, i64 16}
!181 = !{!"_ZTS15halide_buffer_t", !23, i64 0, !15, i64 8, !15, i64 16, !23, i64 24, !140, i64 32, !26, i64 36, !15, i64 40, !15, i64 48}
!182 = !{!181, !23, i64 0}
!183 = !{!181, !26, i64 36}
!184 = !{!181, !15, i64 40}
!185 = !{!186, !26, i64 8}
!186 = !{!"_ZTS18halide_dimension_t", !26, i64 0, !26, i64 4, !26, i64 8, !26, i64 12}
!187 = distinct !{!187, !57, !69}
!188 = !{i64 0, i64 4, !41, i64 4, i64 4, !41, i64 8, i64 4, !41, i64 12, i64 4, !41}
!189 = !{!186, !26, i64 4}
!190 = distinct !{!190, !57}
!191 = distinct !{!191, !57, !69}
!192 = !{!193, !142, i64 0}
!193 = !{!"_ZTSN6Halide7Runtime8Internal18halide_tiff_headerE", !142, i64 0, !142, i64 2, !26, i64 4, !142, i64 8, !16, i64 10, !26, i64 190, !16, i64 194, !16, i64 202}
!194 = !{!193, !142, i64 2}
!195 = !{!193, !26, i64 4}
!196 = !{!193, !142, i64 8}
!197 = !{!198, !142, i64 0}
!198 = !{!"_ZTSN6Halide7Runtime8Internal8tiff_tagE", !142, i64 0, !142, i64 2, !26, i64 4, !16, i64 8}
!199 = !{!198, !142, i64 2}
!200 = !{!198, !26, i64 4}
!201 = distinct !{!201, !57}
!202 = distinct !{!202, !57}
!203 = distinct !{!203, !57}
!204 = distinct !{!204, !57}
!205 = distinct !{!205, !57}
!206 = !{!207}
!207 = distinct !{!207, !208}
!208 = distinct !{!208, !"LVerDomain"}
!209 = !{!210}
!210 = distinct !{!210, !208}
!211 = distinct !{!211, !57, !69}
!212 = distinct !{!212, !57, !69}
!213 = distinct !{!213, !57, !69}
!214 = distinct !{!214, !57, !69}
!215 = distinct !{!215, !57, !216, !69}
!216 = !{!"llvm.loop.unroll.runtime.disable"}
!217 = distinct !{!217, !57, !69}
!218 = distinct !{!218, !57, !69}
!219 = distinct !{!219, !57, !69}
!220 = distinct !{!220, !57, !69}
!221 = !{!186, !26, i64 0}
!222 = distinct !{!222, !57}
!223 = distinct !{!223, !57, !69}
!224 = distinct !{!224, !57, !216, !69}
!225 = distinct !{!225, !57}
!226 = distinct !{!226, !57}
!227 = distinct !{!227, !57}
!228 = distinct !{!228, !57}
!229 = !{!230, !15, i64 0}
!230 = !{!"_ZTSN6Halide7Runtime8Internal10CacheEntryE", !15, i64 0, !15, i64 8, !15, i64 16, !15, i64 24, !124, i64 32, !15, i64 40, !26, i64 48, !26, i64 52, !26, i64 56, !26, i64 60, !15, i64 64, !15, i64 72, !23, i64 80, !20, i64 88}
!231 = distinct !{!231, !57}
!232 = !{!230, !26, i64 56}
!233 = !{!230, !15, i64 24}
!234 = !{!230, !15, i64 72}
!235 = distinct !{!235, !57}
!236 = distinct !{!236, !57}
!237 = !{!238, !23, i64 0}
!238 = !{!"_ZTSN6Halide7Runtime8Internal11device_copyE", !23, i64 0, !23, i64 8, !23, i64 16, !16, i64 24, !16, i64 152, !16, i64 280, !23, i64 408}
!239 = !{!238, !23, i64 8}
!240 = !{!238, !23, i64 408}
!241 = distinct !{!241, !57}
!242 = !{!238, !23, i64 16}
!243 = distinct !{!243, !57, !69}
!244 = distinct !{!244, !57, !216, !69}
!245 = distinct !{!245, !57}
!246 = distinct !{!246, !57}
!247 = distinct !{!247, !57}
!248 = distinct !{!248, !57}
!249 = !{i64 0, i64 8, !22, i64 8, i64 8, !22, i64 16, i64 8, !22, i64 24, i64 128, !18, i64 152, i64 128, !18, i64 280, i64 128, !18, i64 408, i64 8, !22}
!250 = distinct !{!250, !57}
!251 = !{!186, !26, i64 12}
!252 = !{!230, !15, i64 16}
!253 = !{!230, !124, i64 32}
!254 = !{!230, !26, i64 48}
!255 = !{!230, !26, i64 52}
!256 = !{!230, !26, i64 60}
!257 = !{!230, !15, i64 64}
!258 = !{!230, !15, i64 40}
!259 = distinct !{!259, !57}
!260 = distinct !{!260, !57}
!261 = distinct !{!261, !57}
!262 = !{!230, !20, i64 88}
!263 = !{!230, !23, i64 80}
!264 = !{i64 0, i64 8, !22, i64 8, i64 8, !14, i64 16, i64 8, !14, i64 24, i64 8, !22, i64 32, i64 1, !265, i64 33, i64 1, !18, i64 34, i64 2, !169, i64 36, i64 4, !41, i64 40, i64 8, !14, i64 48, i64 8, !14}
!265 = !{!141, !141, i64 0}
!266 = distinct !{!266, !57}
!267 = distinct !{!267, !57}
!268 = !{!230, !15, i64 8}
!269 = distinct !{!269, !57}
!270 = distinct !{!270, !57, !69}
!271 = distinct !{!271, !57, !69}
!272 = distinct !{!272, !57, !69}
!273 = distinct !{!273, !57, !69}
!274 = distinct !{!274, !57}
!275 = distinct !{!275, !57}
!276 = distinct !{!276, !57}
!277 = distinct !{!277, !57}
!278 = distinct !{!278, !57}
!279 = distinct !{!279, !57, !69}
!280 = distinct !{!280, !57, !69}
!281 = distinct !{!281, !57, !69}
!282 = distinct !{!282, !57, !69}
!283 = distinct !{!283, !57}
!284 = !{!285, !26, i64 8}
!285 = !{!"_ZTSN6Halide7Runtime8Internal16CacheBlockHeaderE", !15, i64 0, !26, i64 8}
!286 = !{!285, !15, i64 0}
!287 = distinct !{!287, !57}
!288 = distinct !{!288, !57}
!289 = distinct !{!289, !57}
!290 = distinct !{!290, !57}
!291 = distinct !{!291, !57, !69}
!292 = distinct !{!292, !57, !69}
!293 = distinct !{!293, !57, !69}
!294 = distinct !{!294, !57, !69}
!295 = distinct !{!295, !57}
!296 = distinct !{!296, !57}
!297 = distinct !{!297, !57}
!298 = distinct !{!298, !57}
!299 = distinct !{!299, !57}
!300 = distinct !{!300, !57}
!301 = distinct !{!301, !57}
!302 = distinct !{!302, !57}
!303 = distinct !{!303, !57}
!304 = !{!140, !141, i64 0}
!305 = !{!140, !142, i64 2}
!306 = !{!181, !15, i64 8}
!307 = !{!181, !23, i64 24}
!308 = distinct !{!308, !57}
!309 = !{!310, !15, i64 0}
!310 = !{!"_ZTS29halide_device_allocation_pool", !15, i64 0, !15, i64 8}
!311 = distinct !{!311, !57}
!312 = !{!310, !15, i64 8}
!313 = !{!314, !15, i64 120}
!314 = !{!"_ZTS25halide_device_interface_t", !15, i64 0, !15, i64 8, !15, i64 16, !15, i64 24, !15, i64 32, !15, i64 40, !15, i64 48, !15, i64 56, !15, i64 64, !15, i64 72, !15, i64 80, !15, i64 88, !15, i64 96, !15, i64 104, !15, i64 112, !15, i64 120}
!315 = !{!316, !15, i64 48}
!316 = !{!"_ZTS30halide_device_interface_impl_t", !15, i64 0, !15, i64 8, !15, i64 16, !15, i64 24, !15, i64 32, !15, i64 40, !15, i64 48, !15, i64 56, !15, i64 64, !15, i64 72, !15, i64 80, !15, i64 88, !15, i64 96, !15, i64 104, !15, i64 112, !15, i64 120}
!317 = !{!316, !15, i64 40}
!318 = !{!316, !15, i64 56}
!319 = !{!316, !15, i64 0}
!320 = !{!316, !15, i64 16}
!321 = !{!316, !15, i64 8}
!322 = !{!316, !15, i64 32}
!323 = !{!316, !15, i64 24}
!324 = !{!316, !15, i64 64}
!325 = !{!316, !15, i64 72}
!326 = distinct !{!326, !57, !69}
!327 = distinct !{!327, !57, !69}
!328 = distinct !{!328, !57, !69}
!329 = distinct !{!329, !57, !69}
!330 = !{!316, !15, i64 112}
!331 = !{!316, !15, i64 120}
!332 = !{!316, !15, i64 80}
!333 = !{!316, !15, i64 88}
!334 = !{!316, !15, i64 96}
!335 = !{!316, !15, i64 104}
!336 = !{i32 22, i32 33}
!337 = !{!338, !15, i64 40}
!338 = !{!"_ZTS21halide_profiler_state", !54, i64 0, !26, i64 8, !26, i64 12, !26, i64 16, !26, i64 20, !15, i64 24, !15, i64 32, !15, i64 40}
!339 = !{!338, !26, i64 16}
!340 = !{!341, !23, i64 0}
!341 = !{!"_ZTS30halide_profiler_pipeline_stats", !23, i64 0, !23, i64 8, !23, i64 16, !23, i64 24, !23, i64 32, !23, i64 40, !15, i64 48, !15, i64 56, !15, i64 64, !26, i64 72, !26, i64 76, !26, i64 80, !26, i64 84, !26, i64 88}
!342 = !{!341, !26, i64 80}
!343 = !{!341, !23, i64 32}
!344 = !{!341, !23, i64 40}
!345 = !{!341, !15, i64 48}
!346 = !{!341, !26, i64 84}
!347 = !{!341, !26, i64 88}
!348 = !{!341, !23, i64 16}
!349 = !{!341, !23, i64 24}
!350 = !{!341, !26, i64 72}
!351 = !{!341, !15, i64 56}
!352 = distinct !{!352, !57}
!353 = !{!354, !23, i64 32}
!354 = !{!"_ZTS26halide_profiler_func_stats", !23, i64 0, !23, i64 8, !23, i64 16, !23, i64 24, !23, i64 32, !23, i64 40, !23, i64 48, !15, i64 56, !26, i64 64}
!355 = !{!354, !23, i64 0}
!356 = !{!354, !15, i64 56}
!357 = distinct !{!357, !57}
!358 = distinct !{!358, !57}
!359 = distinct !{!359, !57}
!360 = !{!354, !23, i64 40}
!361 = !{!354, !23, i64 48}
!362 = distinct !{!362, !57}
!363 = !{!354, !23, i64 16}
!364 = distinct !{!364, !57}
!365 = !{!354, !26, i64 64}
!366 = distinct !{!366, !57}
!367 = !{!354, !23, i64 24}
!368 = distinct !{!368, !57}
!369 = distinct !{!369, !57}
!370 = !{!338, !15, i64 24}
!371 = !{!341, !15, i64 64}
!372 = distinct !{!372, !57}
!373 = !{!338, !26, i64 12}
!374 = distinct !{!374, !57}
!375 = !{!341, !26, i64 76}
!376 = distinct !{!376, !57}
!377 = distinct !{!377, !57}
!378 = !{!338, !15, i64 32}
!379 = !{!338, !26, i64 20}
!380 = !{!338, !26, i64 8}
!381 = distinct !{!381, !57}
!382 = distinct !{!382, !57}
!383 = distinct !{!383, !57}
!384 = distinct !{!384, !57}
!385 = !{!386, !124, i64 8}
!386 = !{!"_ZTS25halide_pseudostack_slot_t", !15, i64 0, !124, i64 8, !124, i64 16}
!387 = !{!386, !15, i64 0}
!388 = !{!"branch_weights", i32 1, i32 2000}
!389 = !{!386, !124, i64 16}
!390 = !{!"branch_weights", i32 0, i32 1073741824}
!391 = !{!"branch_weights", i32 1073741824, i32 0}
!392 = !{!393, !393, i64 0}
!393 = !{!"filter", !394, i64 0}
!394 = !{!"Halide buffer"}
!395 = !{!396, !396, i64 0}
!396 = !{!"filter_zeroed", !394, i64 0}
!397 = !{!398, !398, i64 0}
!398 = !{!"sum_filter.width4.base0", !399, i64 0}
!399 = !{!"sum_filter.width8.base0", !400, i64 0}
!400 = !{!"sum_filter.width16.base0", !401, i64 0}
!401 = !{!"sum_filter.width32.base0", !402, i64 0}
!402 = !{!"sum_filter.width64.base0", !403, i64 0}
!403 = !{!"sum_filter.width128.base0", !404, i64 0}
!404 = !{!"sum_filter.width256.base0", !405, i64 0}
!405 = !{!"sum_filter.width512.base0", !406, i64 0}
!406 = !{!"sum_filter.width1024.base0", !407, i64 0}
!407 = !{!"sum_filter", !394, i64 0}
!408 = !{!409, !409, i64 0}
!409 = !{!"sum_filter.width4.base4", !399, i64 0}
!410 = !{!411, !411, i64 0}
!411 = !{!"sum_filter.width4.base8", !412, i64 0}
!412 = !{!"sum_filter.width8.base8", !400, i64 0}
!413 = !{!414, !414, i64 0}
!414 = !{!"sum_filter.width4.base12", !412, i64 0}
!415 = !{!416, !416, i64 0}
!416 = !{!"bias", !394, i64 0}
!417 = !{!418, !418, i64 0}
!418 = !{!"offset_c.width4.base0", !419, i64 0}
!419 = !{!"offset_c.width8.base0", !420, i64 0}
!420 = !{!"offset_c.width16.base0", !421, i64 0}
!421 = !{!"offset_c.width32.base0", !422, i64 0}
!422 = !{!"offset_c.width64.base0", !423, i64 0}
!423 = !{!"offset_c.width128.base0", !424, i64 0}
!424 = !{!"offset_c.width256.base0", !425, i64 0}
!425 = !{!"offset_c.width512.base0", !426, i64 0}
!426 = !{!"offset_c.width1024.base0", !427, i64 0}
!427 = !{!"offset_c", !394, i64 0}
!428 = !{!429, !429, i64 0}
!429 = !{!"offset_c.width4.base4", !419, i64 0}
!430 = !{!431, !431, i64 0}
!431 = !{!"offset_c.width4.base8", !432, i64 0}
!432 = !{!"offset_c.width8.base8", !420, i64 0}
!433 = !{!434, !434, i64 0}
!434 = !{!"offset_c.width4.base12", !432, i64 0}
!435 = !{!"branch_weights", i32 -2147483648, i32 0}
!436 = !{!437, !437, i64 0}
!437 = !{!"input", !394, i64 0}
!438 = !{!439, !439, i64 0}
!439 = !{!"resampled_input", !394, i64 0}
!440 = !{!"branch_weights", i32 0, i32 -2147483648}
!441 = !{!442, !442, i64 0}
!442 = !{!"convolved.width4.base0", !443, i64 0}
!443 = !{!"convolved.width8.base0", !444, i64 0}
!444 = !{!"convolved.width16.base0", !445, i64 0}
!445 = !{!"convolved.width32.base0", !446, i64 0}
!446 = !{!"convolved.width64.base0", !447, i64 0}
!447 = !{!"convolved.width128.base0", !448, i64 0}
!448 = !{!"convolved.width256.base0", !449, i64 0}
!449 = !{!"convolved.width512.base0", !450, i64 0}
!450 = !{!"convolved.width1024.base0", !451, i64 0}
!451 = !{!"convolved", !394, i64 0}
!452 = !{!453, !453, i64 0}
!453 = !{!"convolved.width4.base4", !443, i64 0}
!454 = !{!455, !455, i64 0}
!455 = !{!"convolved.width4.base8", !456, i64 0}
!456 = !{!"convolved.width8.base8", !444, i64 0}
!457 = !{!458, !458, i64 0}
!458 = !{!"convolved.width4.base12", !456, i64 0}
!459 = !{!460, !460, i64 0}
!460 = !{!"convolved.width4.base16", !461, i64 0}
!461 = !{!"convolved.width8.base16", !462, i64 0}
!462 = !{!"convolved.width16.base16", !445, i64 0}
!463 = !{!464, !464, i64 0}
!464 = !{!"convolved.width4.base20", !461, i64 0}
!465 = !{!466, !466, i64 0}
!466 = !{!"convolved.width4.base24", !467, i64 0}
!467 = !{!"convolved.width8.base24", !462, i64 0}
!468 = !{!469, !469, i64 0}
!469 = !{!"convolved.width4.base28", !467, i64 0}
!470 = !{!471, !471, i64 0}
!471 = !{!"convolved.width4.base32", !472, i64 0}
!472 = !{!"convolved.width8.base32", !473, i64 0}
!473 = !{!"convolved.width16.base32", !474, i64 0}
!474 = !{!"convolved.width32.base32", !446, i64 0}
!475 = !{!476, !476, i64 0}
!476 = !{!"convolved.width4.base36", !472, i64 0}
!477 = !{!478, !478, i64 0}
!478 = !{!"convolved.width4.base40", !479, i64 0}
!479 = !{!"convolved.width8.base40", !473, i64 0}
!480 = !{!481, !481, i64 0}
!481 = !{!"convolved.width4.base44", !479, i64 0}
!482 = !{!483, !483, i64 0}
!483 = !{!"convolved.width4.base48", !484, i64 0}
!484 = !{!"convolved.width8.base48", !485, i64 0}
!485 = !{!"convolved.width16.base48", !474, i64 0}
!486 = !{!487, !487, i64 0}
!487 = !{!"convolved.width4.base52", !484, i64 0}
!488 = !{!489, !489, i64 0}
!489 = !{!"convolved.width4.base56", !490, i64 0}
!490 = !{!"convolved.width8.base56", !485, i64 0}
!491 = !{!492, !492, i64 0}
!492 = !{!"convolved.width4.base60", !490, i64 0}
!493 = !{!494, !494, i64 0}
!494 = !{!"filter_zeroed.width8.base0", !495, i64 0}
!495 = !{!"filter_zeroed.width16.base0", !496, i64 0}
!496 = !{!"filter_zeroed.width32.base0", !497, i64 0}
!497 = !{!"filter_zeroed.width64.base0", !498, i64 0}
!498 = !{!"filter_zeroed.width128.base0", !499, i64 0}
!499 = !{!"filter_zeroed.width256.base0", !500, i64 0}
!500 = !{!"filter_zeroed.width512.base0", !501, i64 0}
!501 = !{!"filter_zeroed.width1024.base0", !396, i64 0}
!502 = !{!503, !503, i64 0}
!503 = !{!"filter_zeroed.width8.base8", !495, i64 0}
!504 = !{!505, !505, i64 0}
!505 = !{!"filter_zeroed.width8.base16", !506, i64 0}
!506 = !{!"filter_zeroed.width16.base16", !496, i64 0}
!507 = !{!508, !508, i64 0}
!508 = !{!"filter_zeroed.width8.base24", !506, i64 0}
!509 = !{!510, !510, i64 0}
!510 = !{!"filter_zeroed.width8.base32", !511, i64 0}
!511 = !{!"filter_zeroed.width16.base32", !512, i64 0}
!512 = !{!"filter_zeroed.width32.base32", !497, i64 0}
!513 = !{!514, !514, i64 0}
!514 = !{!"filter_zeroed.width8.base40", !511, i64 0}
!515 = !{!516, !516, i64 0}
!516 = !{!"output", !394, i64 0}
!517 = !{!518, !518, i64 0}
!518 = !{!"bias.width4.base0", !519, i64 0}
!519 = !{!"bias.width8.base0", !520, i64 0}
!520 = !{!"bias.width16.base0", !521, i64 0}
!521 = !{!"bias.width32.base0", !522, i64 0}
!522 = !{!"bias.width64.base0", !523, i64 0}
!523 = !{!"bias.width128.base0", !524, i64 0}
!524 = !{!"bias.width256.base0", !525, i64 0}
!525 = !{!"bias.width512.base0", !526, i64 0}
!526 = !{!"bias.width1024.base0", !416, i64 0}
!527 = !{!528, !528, i64 0}
!528 = !{!"bias.width4.base4", !519, i64 0}
!529 = !{!530, !530, i64 0}
!530 = !{!"bias.width4.base8", !531, i64 0}
!531 = !{!"bias.width8.base8", !520, i64 0}
!532 = !{!533, !533, i64 0}
!533 = !{!"bias.width4.base12", !531, i64 0}
!534 = !{!535, !535, i64 0}
!535 = !{!"filter_zeroed.width4.base32", !510, i64 0}
!536 = !{!537, !537, i64 0}
!537 = !{!"filter_zeroed.width4.base16", !505, i64 0}
!538 = !{!539, !539, i64 0}
!539 = !{!"filter_zeroed.width4.base0", !494, i64 0}
!540 = !{!541, !541, i64 0}
!541 = !{!"filter_zeroed.width4.base36", !510, i64 0}
!542 = !{!543, !543, i64 0}
!543 = !{!"filter_zeroed.width4.base20", !505, i64 0}
!544 = !{!545, !545, i64 0}
!545 = !{!"filter_zeroed.width4.base4", !494, i64 0}
!546 = !{!547, !547, i64 0}
!547 = !{!"filter_zeroed.width4.base40", !514, i64 0}
!548 = !{!549, !549, i64 0}
!549 = !{!"filter_zeroed.width4.base24", !508, i64 0}
!550 = !{!551, !551, i64 0}
!551 = !{!"filter_zeroed.width4.base8", !503, i64 0}
!552 = !{!553, !553, i64 0}
!553 = !{!"filter_zeroed.width4.base44", !514, i64 0}
!554 = !{!555, !555, i64 0}
!555 = !{!"filter_zeroed.width4.base28", !508, i64 0}
!556 = !{!557, !557, i64 0}
!557 = !{!"filter_zeroed.width4.base12", !503, i64 0}

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.16"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>AI Engine-ML Intrinsics User Guide: Multiply Accumulate</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" async="async" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra_css.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="amd-logo.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">AI Engine-ML Intrinsics User Guide
   &#160;<span id="projectnumber">(v2023.2)</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.16 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('group__intr__gpvectorop__mul.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#groups">Modules</a>  </div>
  <div class="headertitle">
<div class="title">Multiply Accumulate<div class="ingroups"><a class="el" href="group__intr__gpvectorop.html">Vector Operations</a></div></div>  </div>
</div><!--header-->
<div class="contents">

<p>Intrinsics allowing you to perform MUL/MAC operations and a few of their variants.  
<a href="#details">More...</a></p>
<a name="details" id="details"></a><h2 class="groupheader">Overview</h2>
<p>Intrinsics allowing you to perform MUL/MAC operations and a few of their variants. </p>
<p>For integer datatypes, a matrix A of size MxN is multiplied with a matrix B of size NxP. The naming convention for these operations is: <b>[operation][_MxN_NxP]{_Cch}{_conf}</b> or <b>[operation]_conv_MxN{_Cch}{_conf}</b>. Properties in [] are mandatory, properties in {} are optional. In this naming, <em>conv</em> indicates a convolutional operation, <em>conf</em> indicates the use of sub, zero or shift masks and C gives the number of channels. <br  />
 For an MxN vector multiply convolution operation, the calculation performed is:</p>
<p class="formulaDsp">
\[ \text{mul_conv_MxN}(F,G) = \sum_{u=0}^{\text{N}-1}{G(u) F(x+u)} \]
</p>
<p>where the vector \(F\) has length \(\text{M}+\text{N}-1\), and the vector \(G\) has length \(\text{N}\). <br  />
 <br  />
 For element-wise operations, the naming is <b>[operation_elem_C]{_N}</b>. Here, C is the number of channels and N is the number of columns of matrix A/rows of matrix B. N is either two or it is omitted. The element-wise operations are executed channel by channel. The output will also be a matrix of with C channels.</p>
<p>For complex datatypes, a multiplication of two matrices with complex elements is performed. The naming convention for these operations is <b>[operation_elem_8]{_conf}</b> for <a class="el" href="group__intr__gpvectorop__mul__32bx16b__complex.html">Multiply-accumulate of 32b x 16b complex integer datatypes</a> and <b>[operation_elem_8_2]{_conf}</b> for <a class="el" href="group__intr__gpvectorop__mul__16bx16b__complex.html">Multiply-accumulate of 16b x 16b complex integer datatypes</a>. Here, eight is the number of channels and the two is the number columns of matrix A/rows of matrix B. The matrix multiplication is performed indvidually for each channel of the input matrices. The output will also be a matrix with eight channels.</p>
<p>The following table shows the matrix multiplications that can be completed within a single cycle.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Precision Mode  </th><th class="markdownTableHeadNone">Channels  </th><th class="markdownTableHeadNone">Matrix A  </th><th class="markdownTableHeadNone">Matrix B  </th><th class="markdownTableHeadNone">Matrix C   </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">8-bit x 4-bit = 32-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x16  </td><td class="markdownTableBodyNone">16x8  </td><td class="markdownTableBodyNone">4x8   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">8-bit x 4-bit = 32-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x32  </td><td class="markdownTableBodyNone">32x8 (sparse)  </td><td class="markdownTableBodyNone">4x8   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">8-bit x 8-bit = 32-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x8  </td><td class="markdownTableBodyNone">8x8  </td><td class="markdownTableBodyNone">4x8   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">8-bit x 8-bit = 32-bit  </td><td class="markdownTableBodyNone">32  </td><td class="markdownTableBodyNone">1x2  </td><td class="markdownTableBodyNone">2x1  </td><td class="markdownTableBodyNone">1x1   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">8-bit x 8-bit = 32-bit  </td><td class="markdownTableBodyNone">8  </td><td class="markdownTableBodyNone">4x4 (convolution)  </td><td class="markdownTableBodyNone">4x1  </td><td class="markdownTableBodyNone">4x1   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">8-bit x 8-bit = 32-bit  </td><td class="markdownTableBodyNone">4  </td><td class="markdownTableBodyNone">8x8 (convolution)  </td><td class="markdownTableBodyNone">8x1  </td><td class="markdownTableBodyNone">8x1   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">8-bit x 8-bit = 32-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">32x8 (convolution)  </td><td class="markdownTableBodyNone">8x1  </td><td class="markdownTableBodyNone">32x1   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">8-bit x 8-bit = 32-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x16  </td><td class="markdownTableBodyNone">16x8 (sparse)  </td><td class="markdownTableBodyNone">4x8   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">16-bit x 8-bit = 32-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x4  </td><td class="markdownTableBodyNone">4x8  </td><td class="markdownTableBodyNone">4x8   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">16-bit x 8-bit = 32-bit  </td><td class="markdownTableBodyNone">2  </td><td class="markdownTableBodyNone">4x4  </td><td class="markdownTableBodyNone">4x4  </td><td class="markdownTableBodyNone">4x4   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">16-bit x 16-bit = 32-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x2  </td><td class="markdownTableBodyNone">2x8  </td><td class="markdownTableBodyNone">4x8   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">16-bit x 16-bit = 32-bit  </td><td class="markdownTableBodyNone">32  </td><td class="markdownTableBodyNone">1x1  </td><td class="markdownTableBodyNone">1x1  </td><td class="markdownTableBodyNone">1x1   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">16-bit x 8-bit = 64-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">2x8  </td><td class="markdownTableBodyNone">8x8  </td><td class="markdownTableBodyNone">2x8   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">16-bit x 8-bit = 64-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x8  </td><td class="markdownTableBodyNone">8x4  </td><td class="markdownTableBodyNone">4x4   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">16-bit x 8-bit = 64-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">2x16  </td><td class="markdownTableBodyNone">16x8 (sparse)  </td><td class="markdownTableBodyNone">2x8   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">16-bit x 16-bit = 64-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">2x4  </td><td class="markdownTableBodyNone">4x8  </td><td class="markdownTableBodyNone">2x8   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">16-bit x 16-bit = 64-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x4  </td><td class="markdownTableBodyNone">4x4  </td><td class="markdownTableBodyNone">4x4   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">16-bit x 16-bit = 64-bit  </td><td class="markdownTableBodyNone">16  </td><td class="markdownTableBodyNone">1x2  </td><td class="markdownTableBodyNone">2x1  </td><td class="markdownTableBodyNone">1x1   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">16-bit x 16-bit = 64-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">16x4 (convolution)  </td><td class="markdownTableBodyNone">4x1  </td><td class="markdownTableBodyNone">16x1   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Complex 16-bit x Complex 16-bit = 64-bit  </td><td class="markdownTableBodyNone">8  </td><td class="markdownTableBodyNone">1x2  </td><td class="markdownTableBodyNone">2x1  </td><td class="markdownTableBodyNone">1x1   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">16-bit x 16-bit = 64-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">2x8  </td><td class="markdownTableBodyNone">8x8 (sparse)  </td><td class="markdownTableBodyNone">2x8   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">32-bit x 16-bit = 64-bit  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x2  </td><td class="markdownTableBodyNone">2x4  </td><td class="markdownTableBodyNone">4x4   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Complex 32-bit x Complex 16-bit = 64-bit  </td><td class="markdownTableBodyNone">8  </td><td class="markdownTableBodyNone">1x1  </td><td class="markdownTableBodyNone">1x1  </td><td class="markdownTableBodyNone">1x1   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><a class="el" href="classbfloat16.html">bfloat16</a> x <a class="el" href="classbfloat16.html">bfloat16</a> = fp32  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x8  </td><td class="markdownTableBodyNone">8x4  </td><td class="markdownTableBodyNone">4x4   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><a class="el" href="classbfloat16.html">bfloat16</a> x <a class="el" href="classbfloat16.html">bfloat16</a> = fp32  </td><td class="markdownTableBodyNone">16  </td><td class="markdownTableBodyNone">1x2  </td><td class="markdownTableBodyNone">2x1  </td><td class="markdownTableBodyNone">1x1   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><a class="el" href="classbfloat16.html">bfloat16</a> x <a class="el" href="classbfloat16.html">bfloat16</a> = fp32  </td><td class="markdownTableBodyNone">1  </td><td class="markdownTableBodyNone">4x16  </td><td class="markdownTableBodyNone">16x4 (sparse)  </td><td class="markdownTableBodyNone">4x4   </td></tr>
</table>
<h3>Matrix mult intrinsics</h3>
<p>We can summarize the MUL and the MAC operation like this: </p><div class="fragment"><div class="line">MAC: res = acc_in1 + (X_vec x Y_vec)</div>
<div class="line">MUL: res = (X_vec x Y_vec)</div>
</div><!-- fragment --><p> The 'x' operator being the matrix multiplication operator. The same way we can summarize the MSC, NEGMUL, MACMUL and MAC/MSC variants with additional acc_in2 input operations as this: </p><div class="fragment"><div class="line">MSC:    res = acc_in1 - (X_vec x Y_vec)</div>
<div class="line">NEGMUL: res = - (X_vec x Y_vec)</div>
<div class="line">MACMUL: res = (zero_acc1 ? 0 : acc_in1) + (X_vec x Y_vec)</div>
<div class="line">ADDMAC: res = acc_in1 + acc_in2 + (X_vec x Y_vec)</div>
<div class="line">ADDMSC: res = acc_in1 + acc_in2 - (X_vec x Y_vec)</div>
<div class="line">SUBMAC: res = acc_in1 - acc_in2 + (X_vec x Y_vec)</div>
<div class="line">SUBMSC: res = acc_in1 - acc_in2 - (X_vec x Y_vec)</div>
</div><!-- fragment --><h3>The convolve variants</h3>
<p>The convolve variants of these intrinsics differs as they apply a convolution product on the vectors instead of a matrix multiplication. The '*' operator being the vector convolution operator. Therefore, the X_vec is the matrix, and Y_vec the kernel. </p><div class="fragment"><div class="line">MAC: res = acc_in1 + (X_vec * Y_vec)</div>
<div class="line">MUL: res = (X_vec * Y_vec)</div>
<div class="line"> </div>
<div class="line">MSC:    res = acc_in1 - (X_vec * Y_vec)</div>
<div class="line">NEGMUL: res = - (X_vec * Y_vec)</div>
<div class="line">MACMUL: res = (zero_acc1 ? 0 : acc_in1) + (X_vec * Y_vec)</div>
<div class="line">ADDMAC: res = acc_in1 + acc_in2 + (X_vec * Y_vec)</div>
<div class="line">ADDMSC: res = acc_in1 + acc_in2 - (X_vec * Y_vec)</div>
<div class="line">SUBMAC: res = acc_in1 - acc_in2 + (X_vec * Y_vec)</div>
<div class="line">SUBMSC: res = acc_in1 - acc_in2 - (X_vec * Y_vec)</div>
</div><!-- fragment --><h3>Zeroing, sign and negation masks</h3>
<p>Some variant allow the passing of masks that are used to determine sign, zeroing and negation of vector or accumulator lanes. These masks are the following:</p>
<div class="fragment"><div class="line">int sgn_x: Sign mask of matrix X. If it is one matrix X is interpreted as signed, else it treated as unsigned.</div>
<div class="line">int sgn_y: Sign mask of matrix Y. If it is one matrix Y is interpreted as signed, else it treated as unsigned.</div>
<div class="line"> </div>
<div class="line">int zero_acc1: Zeroing of acc1. If it is one then acc1 is zeroed.</div>
<div class="line">int zero_acc2: Zeroing of acc2. If it is one then acc2 is zeroed.</div>
<div class="line"> </div>
<div class="line">int sub_mul: Negation mask of the matrix multiplication result. If it is one the result of the operation will be negated.</div>
<div class="line"> </div>
<div class="line">int sub_acc1: Negation mask of acc1. If it is one acc1 will be negated.</div>
<div class="line">int sub_acc2: Negation mask of acc2. If it is one acc2 will be negated.</div>
<div class="line"> </div>
<div class="line">int shift16: Shift mask of acc1. If a bit is set the &lt;&lt;16 operation will be executed on acc1.</div>
<div class="line"> </div>
<div class="line">int sub_mask: Negation mask of complex multiplications. Negates a term of a complex multiplication.</div>
</div><!-- fragment --><p>Complex multiplications require some terms to be negated in order to implement conjugation and minus j multiplication. This is done through the <em>sub_mask</em>. The following examples show how this mask is used when two complex numbers, X and Y, are multiplied to get an output O. For <a class="el" href="group__intr__gpvectorop__mul__16bx16b__complex.html">Multiply-accumulate of 16b x 16b complex integer datatypes</a> there are two complex numbers post-added. They are indicated by the postfix 0/1:</p>
<div class="fragment"><div class="line">O[re] = -1^sub_mask[0] * X[re0] * Y[re0] + -1^sub_mask[1] * X[im0] * Y[im0] </div>
<div class="line">      + -1^sub_mask[2] * X[re1] * Y[re1] + -1^sub_mask[3] * X[im1] * Y[im1]</div>
<div class="line">O[im] = -1^sub_mask[4] * X[re0] * Y[im0] + -1^sub_mask[5] * X[im0] * Y[re0] </div>
<div class="line">      + -1^sub_mask[6] * X[re1] * Y[im1] + -1^sub_mask[7] * X[im1] * Y[re1] </div>
</div><!-- fragment --><p>For <a class="el" href="group__intr__gpvectorop__mul__32bx16b__complex.html">Multiply-accumulate of 32b x 16b complex integer datatypes</a> there is no postadding and only four unique terms are needed. However, all 8 bit must be specified apropriately. In the following equation the index bits used for one term must be the same value.</p>
<div class="fragment"><div class="line">O[re] = -1^sub_mask[0|2] * X[re] * Y[re] + -1^sub_mask[1|3] * X[im] * Y[im] </div>
<div class="line">O[im] = -1^sub_mask[4|6] * X[re] * Y[im] + -1^sub_mask[5|6] * X[im] * Y[re] </div>
</div><!-- fragment --><h3>Multiplication of matrices with multiple channels</h3>
<p>Some intrinsics are used for multiplications of matrices with a given number of channels. Each MxN matrix is stored in row-major and channel-minor fashion. The following example shows the resulting layout of elements in the vector for a 4x4 matrix with two channels. The indexes for each element are given as (m,n,c)</p>
<div class="fragment"><div class="line">[a(0,0,0)  a(0,0,1)  a(0,1,0)  a(0,1,1)  a(0,2,1)  a(0,3,0)  a(0,3,1)</div>
<div class="line"> a(1,0,0)  a(1,0,1)  a(1,1,0)  a(1,1,1)  a(1,2,1)  a(1,3,0)  a(1,3,1)</div>
<div class="line"> a(2,0,0)  a(2,0,1)  a(2,1,0)  a(2,1,1)  a(2,2,1)  a(2,3,0)  a(2,3,1)</div>
<div class="line"> a(3,0,0)  a(3,0,1)  a(3,1,0)  a(3,1,1)  a(3,2,1)  a(3,3,0)  a(3,3,1)]</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>Matrices with multiple channels are used for convolutional and element-wise operations. Element-wise operations are performed along the channels. E.g. an element-wise mutltiplication of two matrices with 32 channels would perform a matrix multiplication for each individual channel. The output would again have 32 channels.</dd></dl>
<h4>Element-wise multiplication</h4>
<p>The <b>elem</b> variants allow you to perform element-wise operations. The operations are performed along the channels. For example, if you perform a (1x1x32) x (1x1x32) operation a multiplication will be done between the elements of the same channel. So, the elements of channel zero will be multiplied, the elements of channel one will be multiplied etc... The end result would again have 32 channels.</p>
<p>Some of the <b>elem</b> variants perform matrix multiplications along the channels. For those cases the multiplication (1x2xC) x (2x1xC) is performed. The end result is a (1x1x32) matrix. Despite the name, this is not a true element-wise multiplication.</p>
<h4>Convolution operation</h4>
<p>Convolutional operations work similar to element-wise multiplication. In every step the kernel will be multiplied with the matrix before it is shifted to the next position. The same is done for each channel. The difference to a regular element-wise multiplication is that after the multiplications for each channel have been completed the resulting matrices are added together so that the final result will have only one channel.</p>
<h3>Considerations when using <a class="el" href="classbfloat16.html">bfloat16</a> data type</h3>
<p>When multiplying with a scalar <a class="el" href="classbfloat16.html">bfloat16</a> it will be internally cast to float which influences the rounding behaviour with negation. The following example shows how this behaviour affects the multiplication. As the cast involves a rounding operation it matters if the negation is performed before or after the cast. In the first case, the rounding happens to the positive result before the negation. For the second and third case the rounding happens before that which will lead to a different result.</p>
<div class="fragment"><div class="line"><a class="code" href="classbfloat16.html">bfloat16</a> a, b;</div>
<div class="line"> </div>
<div class="line"><span class="keyword">auto</span> v1 = -(a * v[0]); <span class="comment">//This will not match the other operations because the rounding is done to the positive result before negation</span></div>
<div class="line"><span class="keyword">auto</span> v2 = (-a * v[0]);</div>
<div class="line"><span class="keyword">auto</span> v3 = (a * -v[0]);</div>
</div><!-- fragment --><h3>Considerations when using emulated FP32 Intrinsics</h3>
<p>elementwise multiplication and matrix multiplication intrinsics for FP32 input type are emulated using <a class="el" href="classbfloat16.html">bfloat16</a> data-path. There are 3 options to chose from. Default option (Most accurate but slow): </p><div class="fragment"><div class="line"><span class="preprocessor">### _accuracy_safe intrinsics</span></div>
<div class="line">Most accurate option since input fp32 number is split in to 3 <a class="code" href="classbfloat16.html">bfloat16</a> numbers to extract all the bits of the mantissa.</div>
<div class="line">float a, b;</div>
<div class="line">a*b would require 9 mac operations due to 3 <a class="code" href="classbfloat16.html">bfloat16</a> splits each.</div>
</div><!-- fragment --><p>Fast and accurate option: </p><div class="fragment"><div class="line"><span class="preprocessor">### _accuracy_fast intrinsics</span></div>
<div class="line">Application compile time flag <span class="stringliteral">&quot;AIE2_FP32_EMULATION_ACCURACY_FAST&quot;</span>: Fast and Accurate option. </div>
<div class="line">Input fp32 number is split in to 3 <a class="code" href="classbfloat16.html">bfloat16</a> numbers to extract all the bits of the mantissa.</div>
<div class="line">float a,b;</div>
<div class="line">both a and b are split in to 3 <a class="code" href="classbfloat16.html">bfloat16</a> numbers each. Hence there would be 9 mac operations in multiplication of a and b.</div>
<div class="line">In the 9 mac operations to emulate fp32 mul, mac operations with LSBs are ignored. (3 last terms). </div>
<div class="line">This helps improve cycle count of mul and has least impact on accuracy of result.</div>
<div class="line">float a, b;</div>
<div class="line">a*b would require 6 mac operations.</div>
</div><!-- fragment --><p>Fastest option with loss of accuracy: </p><div class="fragment"><div class="line"><span class="preprocessor">### _accuracy_low intrinsics</span></div>
<div class="line">Application compile time flag <span class="stringliteral">&quot;AIE2_FP32_EMULATION_ACCURACY_LOW&quot;</span>: Fast and least accurate option. </div>
<div class="line">Input fp32 number is split in to 2 <a class="code" href="classbfloat16.html">bfloat16</a> numbers. Hence not all the bits from mantissa can be used.</div>
<div class="line">float a,b;</div>
<div class="line">Both a and b are split in to 2 <a class="code" href="classbfloat16.html">bfloat16</a> numbers each. Hence there would be 4 mac operations in multiplication of a and b.</div>
<div class="line">In the 4 mac operations to emulate fp32 mul, mac operations with LSBs are ignored. (1 last term). </div>
<div class="line">This helps improve cycle count of mul</div>
<div class="line"><span class="keywordtype">float</span> a, b;</div>
<div class="line">a*b would require 3 mac operations.</div>
</div><!-- fragment --> <table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="groups"></a>
Modules</h2></td></tr>
<tr class="memitem:group__intr__gpvectorop__emul__16bx32b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__emul__16bx32b.html">Emulated Multiply-accumulate of 16b x 32b datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__emul__16bx32b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A has data elements of 16 bit and matrix B has data elements of 32 bit. These operations are emulated on top of <a class="el" href="group__intr__gpvectorop__mul__16bx16b.html">Multiply-accumulate of 16b x 16b integer datatypes</a> and might not have optimal performance. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__emul__32bx16b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__emul__32bx16b.html">Emulated Multiply-accumulate of 32b x 16b datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__emul__32bx16b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A has data elements of 32 bit and matrix B has data elements of 16 bit. These operations are emulated on top of <a class="el" href="group__intr__gpvectorop__mul__16bx16b.html">Multiply-accumulate of 16b x 16b integer datatypes</a> and might not have optimal performance. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__emul__32bx32b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__emul__32bx32b.html">Emulated Multiply-accumulate of 32b x 32b datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__emul__32bx32b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A has data elements of 32 bit and matrix B has data elements of 32 bit. These operations are emulated on top of <a class="el" href="group__intr__gpvectorop__mul__32bx16b.html">Multiply-accumulate of 32b x 16b integer datatypes</a> and <a class="el" href="group__intr__gpvectorop__mul__16bx16b.html">Multiply-accumulate of 16b x 16b integer datatypes</a> and might not have optimal performance. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__emul__c32bxc32b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__emul__c32bxc32b.html">Emulated Multiply-accumulate of Complex 32b x Complex 32b datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__emul__c32bxc32b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A has data elements of complex 32 bit and matrix B has data elements of complex 32 bit. These operations are emulated on top of <a class="el" href="group__intr__gpvectorop__mul__32bx16b__complex.html">Multiply-accumulate of 32b x 16b complex integer datatypes</a> and might not have optimal performance. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__16bx16b__complex"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__16bx16b__complex.html">Multiply-accumulate of 16b x 16b complex integer datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__16bx16b__complex"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A and matrix B have complex data elements of 16 bit. For an explanation how these operations works see <a class="el" href="group__intr__gpvectorop__mul.html">Multiply Accumulate</a>. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__16bx16b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__16bx16b.html">Multiply-accumulate of 16b x 16b integer datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__16bx16b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A and matrix B have data elements of 16 bit. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__16bx8b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__16bx8b.html">Multiply-accumulate of 16b x 8b integer datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__16bx8b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A has data elements of 16 bit and matrix B has data elements of 8 bit. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__32bx16b__complex"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__32bx16b__complex.html">Multiply-accumulate of 32b x 16b complex integer datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__32bx16b__complex"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A has complex data elements of 32 bit and matrix B has complex data elements of 16 bit. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__32bx16b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__32bx16b.html">Multiply-accumulate of 32b x 16b integer datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__32bx16b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A has data elements of 32 bit and matrix B has data elements of 16 bit. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__8bx4b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__8bx4b.html">Multiply-accumulate of 8b x 4b datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__8bx4b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A has data elements of 8 bit and matrix B has data elements of 4 bit. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__8bx8b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__8bx8b.html">Multiply-accumulate of 8b x 8b integer datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__8bx8b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A and matrix B have data elements of 8 bit. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__bf16xbf16"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__bf16xbf16.html">Multiply-accumulate of bfloat16 datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__bf16xbf16"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix A and B have <a class="el" href="classbfloat16.html">bfloat16</a> data elements. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__fp32xfp32"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__fp32xfp32.html">Multiply-accumulate of fp32 x fp32 datatypes</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__fp32xfp32"><td class="mdescLeft">&#160;</td><td class="mdescRight">Elementwise-multiplication and matrix multiplication using <a class="el" href="classbfloat16.html">bfloat16</a> datapath. 2 options available. With or without set_rnd(0) for truncation before using these intrinsics. Use flag AIE2_FP32_EMULATION_SET_RND_MODE flag to set rnd mode to truncation. For an explanation how these operations works see <a class="el" href="group__intr__gpvectorop__mul.html">Multiply Accumulate</a>. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:group__intr__gpvectorop__mul__sparse"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__intr__gpvectorop__mul__sparse.html">Multiply-accumulate with a sparse matrix</a></td></tr>
<tr class="memdesc:group__intr__gpvectorop__mul__sparse"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix multiplications in which matrix B is a sparse matrix. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
</div><!-- contents -->
</div><!-- doc-content -->
<div class="ttc" id="aclassbfloat16_html"><div class="ttname"><a href="classbfloat16.html">bfloat16</a></div><div class="ttdef"><b>Definition:</b> me_bfloat16.h:72</div></div>
<!-- HTML footer for doxygen 1.9.0-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">UG1583 &copy; 2023 Advanced Micro Devices, Inc. All rights reserved.</li>
  </ul>
</div>
</body>
</html>
